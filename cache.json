{"2024-04-11T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2404.07989v1","updated":"2024-04-11T17:59:45Z","published":"2024-04-11T17:59:45Z","title":"Any2Point: Empowering Any-modality Large Models for Efficient 3D\n  Understanding","summary":"  Large foundation models have recently emerged as a prominent focus of\ninterest, attaining superior performance in widespread scenarios. Due to the\nscarcity of 3D data, many efforts have been made to adapt pre-trained\ntransformers from vision to 3D domains. However, such 2D-to-3D approaches are\nstill limited, due to the potential loss of spatial geometries and high\ncomputation cost. More importantly, their frameworks are mainly designed for 2D\nmodels, lacking a general any-to-3D paradigm. In this paper, we introduce\nAny2Point, a parameter-efficient method to empower any-modality large models\n(vision, language, audio) for 3D understanding. Given a frozen transformer from\nany source modality, we propose a 3D-to-any (1D or 2D) virtual projection\nstrategy that correlates the input 3D points to the original 1D or 2D positions\nwithin the source modality. This mechanism enables us to assign each 3D token\nwith a positional encoding paired with the pre-trained model, which avoids 3D\ngeometry loss caused by the true projection and better motivates the\ntransformer for 3D learning with 1D/2D positional priors. Then, within each\ntransformer block, we insert an any-to-3D guided adapter module for\nparameter-efficient fine-tuning. The adapter incorporates prior spatial\nknowledge from the source modality to guide the local feature aggregation of 3D\ntokens, compelling the semantic adaption of any-modality transformers. We\nconduct extensive experiments to showcase the effectiveness and efficiency of\nour method. Code and models are released at\nhttps://github.com/Ivan-Tang-3D/Any2Point.\n","authors":["Yiwen Tang","Jiaming Liu","Dong Wang","Zhigang Wang","Shanghang Zhang","Bin Zhao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2404.07989v1.pdf","comment":"Code and models are released at\n  https://github.com/Ivan-Tang-3D/Any2Point"},{"id":"http://arxiv.org/abs/2404.07982v1","updated":"2024-04-11T17:58:05Z","published":"2024-04-11T17:58:05Z","title":"Language Imbalance Can Boost Cross-lingual Generalisation","summary":"  Multilinguality is crucial for extending recent advancements in language\nmodelling to diverse linguistic communities. To maintain high performance while\nrepresenting multiple languages, multilingual models ideally align\nrepresentations, allowing what is learned in one language to generalise to\nothers. Prior research has emphasised the importance of parallel data and\nshared vocabulary elements as key factors for such alignment. In this study, we\ninvestigate an unintuitive novel driver of cross-lingual generalisation:\nlanguage imbalance. In controlled experiments on perfectly equivalent cloned\nlanguages, we observe that the existence of a predominant language during\ntraining boosts the performance of less frequent languages and leads to\nstronger alignment of model representations across languages. Furthermore, we\nfind that this trend is amplified with scale: with large enough models or long\nenough training, we observe that bilingual training data with a 90/10 language\nsplit yields better performance on both languages than a balanced 50/50 split.\nBuilding on these insights, we design training schemes that can improve\nperformance in all cloned languages, even without altering the training data.\nAs we extend our analysis to real languages, we find that infrequent languages\nstill benefit from frequent ones, yet whether language imbalance causes\ncross-lingual generalisation there is not conclusive.\n","authors":["Anton Sch√§fer","Shauli Ravfogel","Thomas Hofmann","Tiago Pimentel","Imanol Schlag"],"pdf_url":"https://arxiv.org/pdf/2404.07982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07981v1","updated":"2024-04-11T17:57:32Z","published":"2024-04-11T17:57:32Z","title":"Manipulating Large Language Models to Increase Product Visibility","summary":"  Large language models (LLMs) are increasingly being integrated into search\nengines to provide natural language responses tailored to user queries.\nCustomers and end-users are also becoming more dependent on these models for\nquick and easy purchase decisions. In this work, we investigate whether\nrecommendations from LLMs can be manipulated to enhance a product's visibility.\nWe demonstrate that adding a strategic text sequence (STS) -- a carefully\ncrafted message -- to a product's information page can significantly increase\nits likelihood of being listed as the LLM's top recommendation. To understand\nthe impact of STS, we use a catalog of fictitious coffee machines and analyze\nits effect on two target products: one that seldom appears in the LLM's\nrecommendations and another that usually ranks second. We observe that the\nstrategic text sequence significantly enhances the visibility of both products\nby increasing their chances of appearing as the top recommendation. This\nability to manipulate LLM-generated search responses provides vendors with a\nconsiderable competitive advantage and has the potential to disrupt fair market\ncompetition. Just as search engine optimization (SEO) revolutionized how\nwebpages are customized to rank higher in search engine results, influencing\nLLM recommendations could profoundly impact content optimization for AI-driven\nsearch services. Code for our experiments is available at\nhttps://github.com/aounon/llm-rank-optimizer.\n","authors":["Aounon Kumar","Himabindu Lakkaraju"],"pdf_url":"https://arxiv.org/pdf/2404.07981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07979v1","updated":"2024-04-11T17:57:22Z","published":"2024-04-11T17:57:22Z","title":"LLoCO: Learning Long Contexts Offline","summary":"  Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose a\nnovel approach to address this problem by learning contexts offline through\ncontext compression and in-domain parameter-efficient finetuning. Our method\nenables an LLM to create a concise representation of the original context and\nefficiently retrieve relevant information to answer questions accurately. We\nintroduce LLoCO, a technique that combines context compression, retrieval, and\nparameter-efficient finetuning using LoRA. Our approach extends the effective\ncontext window of a 4k token LLaMA2-7B model to handle up to 128k tokens. We\nevaluate our approach on several long-context question-answering datasets,\ndemonstrating that LLoCO significantly outperforms in-context learning while\nusing $30\\times$ fewer tokens during inference. LLoCO achieves up to\n$7.62\\times$ speed-up and substantially reduces the cost of long document\nquestion answering, making it a promising solution for efficient long context\nprocessing. Our code is publicly available at\nhttps://github.com/jeffreysijuntan/lloco.\n","authors":["Sijun Tan","Xiuyu Li","Shishir Patil","Ziyang Wu","Tianjun Zhang","Kurt Keutzer","Joseph E. Gonzalez","Raluca Ada Popa"],"pdf_url":"https://arxiv.org/pdf/2404.07979v1.pdf","comment":"The first two authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2404.07972v1","updated":"2024-04-11T17:56:05Z","published":"2024-04-11T17:56:05Z","title":"OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real\n  Computer Environments","summary":"  Autonomous agents that accomplish complex computer tasks with minimal human\ninterventions have the potential to transform human-computer interaction,\nsignificantly enhancing accessibility and productivity. However, existing\nbenchmarks either lack an interactive environment or are limited to\nenvironments specific to certain applications or domains, failing to reflect\nthe diverse and complex nature of real-world computer use, thereby limiting the\nscope of tasks and agent scalability. To address this issue, we introduce\nOSWorld, the first-of-its-kind scalable, real computer environment for\nmultimodal agents, supporting task setup, execution-based evaluation, and\ninteractive learning across various operating systems such as Ubuntu, Windows,\nand macOS. OSWorld can serve as a unified, integrated computer environment for\nassessing open-ended computer tasks that involve arbitrary applications.\nBuilding upon OSWorld, we create a benchmark of 369 computer tasks involving\nreal web and desktop apps in open domains, OS file I/O, and workflows spanning\nmultiple applications. Each task example is derived from real-world computer\nuse cases and includes a detailed initial state setup configuration and a\ncustom execution-based evaluation script for reliable, reproducible evaluation.\nExtensive evaluation of state-of-the-art LLM/VLM-based agents on OSWorld\nreveals significant deficiencies in their ability to serve as computer\nassistants. While humans can accomplish over 72.36% of the tasks, the best\nmodel achieves only 12.24% success, primarily struggling with GUI grounding and\noperational knowledge. Comprehensive analysis using OSWorld provides valuable\ninsights for developing multimodal generalist agents that were not possible\nwith previous benchmarks. Our code, environment, baseline models, and data are\npublicly available at https://os-world.github.io.\n","authors":["Tianbao Xie","Danyang Zhang","Jixuan Chen","Xiaochuan Li","Siheng Zhao","Ruisheng Cao","Toh Jing Hua","Zhoujun Cheng","Dongchan Shin","Fangyu Lei","Yitao Liu","Yiheng Xu","Shuyan Zhou","Silvio Savarese","Caiming Xiong","Victor Zhong","Tao Yu"],"pdf_url":"https://arxiv.org/pdf/2404.07972v1.pdf","comment":"51 pages, 21 figures"},{"id":"http://arxiv.org/abs/2404.07965v1","updated":"2024-04-11T17:52:01Z","published":"2024-04-11T17:52:01Z","title":"Rho-1: Not All Tokens Are What You Need","summary":"  Previous language model pre-training methods have uniformly applied a\nnext-token prediction loss to all training tokens. Challenging this norm, we\nposit that \"Not all tokens in a corpus are equally important for language model\ntraining\". Our initial analysis delves into token-level training dynamics of\nlanguage model, revealing distinct loss patterns for different tokens.\nLeveraging these insights, we introduce a new language model called Rho-1.\nUnlike traditional LMs that learn to predict every next token in a corpus,\nRho-1 employs Selective Language Modeling (SLM), which selectively trains on\nuseful tokens that aligned with the desired distribution. This approach\ninvolves scoring pretraining tokens using a reference model, and then training\nthe language model with a focused loss on tokens with higher excess loss. When\ncontinual pretraining on 15B OpenWebMath corpus, Rho-1 yields an absolute\nimprovement in few-shot accuracy of up to 30% in 9 math tasks. After\nfine-tuning, Rho-1-1B and 7B achieved state-of-the-art results of 40.6% and\n51.8% on MATH dataset, respectively - matching DeepSeekMath with only 3% of the\npretraining tokens. Furthermore, when pretraining on 80B general tokens, Rho-1\nachieves 6.8% average enhancement across 15 diverse tasks, increasing both\nefficiency and performance of the language model pre-training.\n","authors":["Zhenghao Lin","Zhibin Gou","Yeyun Gong","Xiao Liu","Yelong Shen","Ruochen Xu","Chen Lin","Yujiu Yang","Jian Jiao","Nan Duan","Weizhu Chen"],"pdf_url":"https://arxiv.org/pdf/2404.07965v1.pdf","comment":"First two authors equal contribution"},{"id":"http://arxiv.org/abs/2311.09828v2","updated":"2024-04-11T17:38:09Z","published":"2023-11-16T11:52:52Z","title":"AfriMTE and AfriCOMET: Enhancing COMET to Embrace Under-resourced\n  African Languages","summary":"  Despite the recent progress on scaling multilingual machine translation (MT)\nto several under-resourced African languages, accurately measuring this\nprogress remains challenging, since evaluation is often performed on n-gram\nmatching metrics such as BLEU, which typically show a weaker correlation with\nhuman judgments. Learned metrics such as COMET have higher correlation;\nhowever, the lack of evaluation data with human ratings for under-resourced\nlanguages, complexity of annotation guidelines like Multidimensional Quality\nMetrics (MQM), and limited language coverage of multilingual encoders have\nhampered their applicability to African languages. In this paper, we address\nthese challenges by creating high-quality human evaluation data with simplified\nMQM guidelines for error detection and direct assessment (DA) scoring for 13\ntypologically diverse African languages. Furthermore, we develop AfriCOMET:\nCOMET evaluation metrics for African languages by leveraging DA data from\nwell-resourced languages and an African-centric multilingual encoder\n(AfroXLM-R) to create the state-of-the-art MT evaluation metrics for African\nlanguages with respect to Spearman-rank correlation with human judgments\n(0.441).\n","authors":["Jiayi Wang","David Ifeoluwa Adelani","Sweta Agrawal","Marek Masiak","Ricardo Rei","Eleftheria Briakou","Marine Carpuat","Xuanli He","Sofia Bourhim","Andiswa Bukula","Muhidin Mohamed","Temitayo Olatoye","Tosin Adewumi","Hamam Mokayede","Christine Mwase","Wangui Kimotho","Foutse Yuehgoh","Anuoluwapo Aremu","Jessica Ojo","Shamsuddeen Hassan Muhammad","Salomey Osei","Abdul-Hakeem Omotayo","Chiamaka Chukwuneke","Perez Ogayo","Oumaima Hourrane","Salma El Anigri","Lolwethu Ndolela","Thabiso Mangwana","Shafie Abdi Mohamed","Ayinde Hassan","Oluwabusayo Olufunke Awoyomi","Lama Alkhaled","Sana Al-Azzawi","Naome A. Etori","Millicent Ochieng","Clemencia Siro","Samuel Njoroge","Eric Muchiri","Wangari Kimotho","Lyse Naomi Wamba Momo","Daud Abolade","Simbiat Ajao","Iyanuoluwa Shode","Ricky Macharm","Ruqayya Nasir Iro","Saheed S. Abdullahi","Stephen E. Moore","Bernard Opoku","Zainab Akinjobi","Abeeb Afolabi","Nnaemeka Obiefuna","Onyekachi Raphael Ogbu","Sam Brian","Verrah Akinyi Otiende","Chinedu Emmanuel Mbonu","Sakayo Toadoum Sari","Yao Lu","Pontus Stenetorp"],"pdf_url":"https://arxiv.org/pdf/2311.09828v2.pdf","comment":"Accepted by NAACL 2024"},{"id":"http://arxiv.org/abs/2404.07922v1","updated":"2024-04-11T17:09:28Z","published":"2024-04-11T17:09:28Z","title":"LaVy: Vietnamese Multimodal Large Language Model","summary":"  Large Language Models (LLMs) and Multimodal Large language models (MLLMs)\nhave taken the world by storm with impressive abilities in complex reasoning\nand linguistic comprehension. Meanwhile there are plethora of works related to\nVietnamese Large Language Models, the lack of high-quality resources in\nmultimodality limits the progress of Vietnamese MLLMs. In this paper, we\npioneer in address this by introducing LaVy, a state-of-the-art Vietnamese\nMLLM, and we also introduce LaVy-Bench benchmark designated for evaluating\nMLLMs's understanding on Vietnamese visual language tasks. All code and model\nweights are public at https://github.com/baochi0212/LaVy\n","authors":["Chi Tran","Huong Le Thanh"],"pdf_url":"https://arxiv.org/pdf/2404.07922v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2404.07921v1","updated":"2024-04-11T17:05:50Z","published":"2024-04-11T17:05:50Z","title":"AmpleGCG: Learning a Universal and Transferable Generative Model of\n  Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs","summary":"  As large language models (LLMs) become increasingly prevalent and integrated\ninto autonomous systems, ensuring their safety is imperative. Despite\nsignificant strides toward safety alignment, recent work\nGCG~\\citep{zou2023universal} proposes a discrete token optimization algorithm\nand selects the single suffix with the lowest loss to successfully jailbreak\naligned LLMs. In this work, we first discuss the drawbacks of solely picking\nthe suffix with the lowest loss during GCG optimization for jailbreaking and\nuncover the missed successful suffixes during the intermediate steps. Moreover,\nwe utilize those successful suffixes as training data to learn a generative\nmodel, named AmpleGCG, which captures the distribution of adversarial suffixes\ngiven a harmful query and enables the rapid generation of hundreds of suffixes\nfor any harmful queries in seconds. AmpleGCG achieves near 100\\% attack success\nrate (ASR) on two aligned LLMs (Llama-2-7B-chat and Vicuna-7B), surpassing two\nstrongest attack baselines. More interestingly, AmpleGCG also transfers\nseamlessly to attack different models, including closed-source LLMs, achieving\na 99\\% ASR on the latest GPT-3.5. To summarize, our work amplifies the impact\nof GCG by training a generative model of adversarial suffixes that is universal\nto any harmful queries and transferable from attacking open-source LLMs to\nclosed-source LLMs. In addition, it can generate 200 adversarial suffixes for\none harmful query in only 4 seconds, rendering it more challenging to defend.\n","authors":["Zeyi Liao","Huan Sun"],"pdf_url":"https://arxiv.org/pdf/2404.07921v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07917v1","updated":"2024-04-11T16:59:54Z","published":"2024-04-11T16:59:54Z","title":"DesignQA: A Multimodal Benchmark for Evaluating Large Language Models'\n  Understanding of Engineering Documentation","summary":"  This research introduces DesignQA, a novel benchmark aimed at evaluating the\nproficiency of multimodal large language models (MLLMs) in comprehending and\napplying engineering requirements in technical documentation. Developed with a\nfocus on real-world engineering challenges, DesignQA uniquely combines\nmultimodal data-including textual design requirements, CAD images, and\nengineering drawings-derived from the Formula SAE student competition.\nDifferent from many existing MLLM benchmarks, DesignQA contains\ndocument-grounded visual questions where the input image and input document\ncome from different sources. The benchmark features automatic evaluation\nmetrics and is divided into segments-Rule Comprehension, Rule Compliance, and\nRule Extraction-based on tasks that engineers perform when designing according\nto requirements. We evaluate state-of-the-art models like GPT4 and LLaVA\nagainst the benchmark, and our study uncovers the existing gaps in MLLMs'\nabilities to interpret complex engineering documentation. Key findings suggest\nthat while MLLMs demonstrate potential in navigating technical documents,\nsubstantial limitations exist, particularly in accurately extracting and\napplying detailed requirements to engineering designs. This benchmark sets a\nfoundation for future advancements in AI-supported engineering design\nprocesses. DesignQA is publicly available at:\nhttps://github.com/anniedoris/design_qa/.\n","authors":["Anna C. Doris","Daniele Grandi","Ryan Tomich","Md Ferdous Alam","Hyunmin Cheong","Faez Ahmed"],"pdf_url":"https://arxiv.org/pdf/2404.07917v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.00445v2","updated":"2024-04-11T16:49:57Z","published":"2023-11-01T11:13:06Z","title":"A Systematic Comparison of Syllogistic Reasoning in Humans and Language\n  Models","summary":"  A central component of rational behavior is logical inference: the process of\ndetermining which conclusions follow from a set of premises. Psychologists have\ndocumented several ways in which humans' inferences deviate from the rules of\nlogic. Do language models, which are trained on text generated by humans,\nreplicate such human biases, or are they able to overcome them? Focusing on the\ncase of syllogisms -- inferences from two simple premises -- we show that,\nwithin the PaLM2 family of transformer language models, larger models are more\nlogical than smaller ones, and also more logical than humans. At the same time,\neven the largest models make systematic errors, some of which mirror human\nreasoning biases: they show sensitivity to the (irrelevant) ordering of the\nvariables in the syllogism, and draw confident but incorrect inferences from\nparticular syllogisms (syllogistic fallacies). Overall, we find that language\nmodels often mimic the human biases included in their training data, but are\nable to overcome them in some cases.\n","authors":["Tiwalayo Eisape","MH Tessler","Ishita Dasgupta","Fei Sha","Sjoerd van Steenkiste","Tal Linzen"],"pdf_url":"https://arxiv.org/pdf/2311.00445v2.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2404.07904v1","updated":"2024-04-11T16:43:03Z","published":"2024-04-11T16:43:03Z","title":"HGRN2: Gated Linear RNNs with State Expansion","summary":"  Hierarchically gated linear RNN (HGRN,Qin et al. 2023) has demonstrated\ncompetitive training speed and performance in language modeling, while offering\nefficient inference. However, the recurrent state size of HGRN remains\nrelatively small, which limits its expressiveness.To address this issue,\ninspired by linear attention, we introduce a simple outer-product-based state\nexpansion mechanism so that the recurrent state size can be significantly\nenlarged without introducing any additional parameters. The linear attention\nform also allows for hardware-efficient training.Our extensive experiments\nverify the advantage of HGRN2 over HGRN1 in language modeling, image\nclassification, and Long Range Arena.Our largest 3B HGRN2 model slightly\noutperforms Mamba and LLaMa Architecture Transformer for language modeling in a\ncontrolled experiment setting; and performs competitively with many open-source\n3B models in downstream evaluation while using much fewer total training\ntokens.\n","authors":["Zhen Qin","Songlin Yang","Weixuan Sun","Xuyang Shen","Dong Li","Weigao Sun","Yiran Zhong"],"pdf_url":"https://arxiv.org/pdf/2404.07904v1.pdf","comment":"Techinical Report. Yiran Zhong is the corresponding author. The\n  source code is available at https://github.com/OpenNLPLab/HGRN2"},{"id":"http://arxiv.org/abs/2402.12749v4","updated":"2024-04-11T16:42:55Z","published":"2024-02-20T06:37:31Z","title":"Me LLaMA: Foundation Large Language Models for Medical Applications","summary":"  Recent advancements in large language models (LLMs) such as ChatGPT and LLaMA\nhave hinted at their potential to revolutionize medical applications, yet their\napplication in clinical settings often reveals limitations due to a lack of\nspecialized training on medical-specific data. In response to this challenge,\nthis study introduces Me-LLaMA, a novel medical LLM family that includes\nfoundation models - Me-LLaMA 13/70B, along with their chat-enhanced versions -\nMe-LLaMA 13/70B-chat, developed through continual pre-training and instruction\ntuning of LLaMA2 using large medical datasets. Our methodology leverages a\ncomprehensive domain-specific data suite, including a large-scale, continual\npre-training dataset with 129B tokens, an instruction tuning dataset with 214k\nsamples, and a new medical evaluation benchmark (MIBE) across six critical\nmedical tasks with 12 datasets. Our extensive evaluation using the MIBE shows\nthat Me-LLaMA models achieve overall better performance than existing\nopen-source medical LLMs in zero-shot, few-shot and supervised learning\nabilities. With task-specific instruction tuning, Me-LLaMA models outperform\nChatGPT on 7 out of 8 datasets and GPT-4 on 5 out of 8 datasets. In addition,\nwe investigated the catastrophic forgetting problem, and our results show that\nMe-LLaMA models outperform other open-source medical LLMs in mitigating this\nissue. Me-LLaMA is one of the largest open-source medical foundation LLMs that\nuse both biomedical and clinical data. It exhibits superior performance across\nboth general and medical tasks compared to other open-source medical LLMs,\nrendering it an attractive choice for medical AI applications. We release our\nmodels, datasets, and evaluation scripts at:\nhttps://github.com/BIDS-Xu-Lab/Me-LLaMA.\n","authors":["Qianqian Xie","Qingyu Chen","Aokun Chen","Cheng Peng","Yan Hu","Fongci Lin","Xueqing Peng","Jimin Huang","Jeffrey Zhang","Vipina Keloth","Xinyu Zhou","Huan He","Lucila Ohno-Machado","Yonghui Wu","Hua Xu","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2402.12749v4.pdf","comment":"21 pages, 3 figures, 8 tables"},{"id":"http://arxiv.org/abs/2404.07900v1","updated":"2024-04-11T16:39:00Z","published":"2024-04-11T16:39:00Z","title":"High-Dimension Human Value Representation in Large Language Models","summary":"  The widespread application of Large Language Models (LLMs) across various\ntasks and fields has necessitated the alignment of these models with human\nvalues and preferences. Given various approaches of human value alignment,\nranging from Reinforcement Learning with Human Feedback (RLHF), to\nconstitutional learning, etc. there is an urgent need to understand the scope\nand nature of human values injected into these models before their release.\nThere is also a need for model alignment without a costly large scale human\nannotation effort. We propose UniVaR, a high-dimensional representation of\nhuman value distributions in LLMs, orthogonal to model architecture and\ntraining data. Trained from the value-relevant output of eight multilingual\nLLMs and tested on the output from four multilingual LLMs, namely LlaMA2,\nChatGPT, JAIS and Yi, we show that UniVaR is a powerful tool to compare the\ndistribution of human values embedded in different LLMs with different langauge\nsources. Through UniVaR, we explore how different LLMs prioritize various\nvalues in different languages and cultures, shedding light on the complex\ninterplay between human values and language modeling.\n","authors":["Samuel Cahyawijaya","Delong Chen","Yejin Bang","Leila Khalatbari","Bryan Wilie","Ziwei Ji","Etsuko Ishii","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2404.07900v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.01019v2","updated":"2024-04-11T16:32:26Z","published":"2024-04-01T09:39:38Z","title":"Source-Aware Training Enables Knowledge Attribution in Language Models","summary":"  Large language models (LLMs) learn a vast amount of knowledge during\npretraining, but they are often oblivious to the source(s) of such knowledge.\nWe investigate the problem of intrinsic source citation, where LLMs are\nrequired to cite the pretraining source supporting a generated response.\nIntrinsic source citation can enhance LLM transparency, interpretability, and\nverifiability. To give LLMs such ability, we explore source-aware training -- a\npost pretraining recipe that involves (i) training the LLM to associate unique\nsource document identifiers with the knowledge in each document, followed by\n(ii) an instruction-tuning to teach the LLM to cite a supporting pretraining\nsource when prompted. Source-aware training can easily be applied to pretrained\nLLMs off the shelf, and diverges minimally from existing\npretraining/fine-tuning frameworks. Through experiments on carefully curated\ndata, we demonstrate that our training recipe can enable faithful attribution\nto the pretraining data without a substantial impact on the model's quality\ncompared to standard pretraining. Our results also highlight the importance of\ndata augmentation in achieving attribution. Code and data available here:\n\\url{https://github.com/mukhal/intrinsic-source-citation}\n","authors":["Muhammad Khalifa","David Wadden","Emma Strubell","Honglak Lee","Lu Wang","Iz Beltagy","Hao Peng"],"pdf_url":"https://arxiv.org/pdf/2404.01019v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07879v1","updated":"2024-04-11T16:10:44Z","published":"2024-04-11T16:10:44Z","title":"Analyzing Toxicity in Deep Conversations: A Reddit Case Study","summary":"  Online social media has become increasingly popular in recent years due to\nits ease of access and ability to connect with others. One of social media's\nmain draws is its anonymity, allowing users to share their thoughts and\nopinions without fear of judgment or retribution. This anonymity has also made\nsocial media prone to harmful content, which requires moderation to ensure\nresponsible and productive use. Several methods using artificial intelligence\nhave been employed to detect harmful content. However, conversation and\ncontextual analysis of hate speech are still understudied. Most promising works\nonly analyze a single text at a time rather than the conversation supporting\nit. In this work, we employ a tree-based approach to understand how users\nbehave concerning toxicity in public conversation settings. To this end, we\ncollect both the posts and the comment sections of the top 100 posts from 8\nReddit communities that allow profanity, totaling over 1 million responses. We\nfind that toxic comments increase the likelihood of subsequent toxic comments\nbeing produced in online conversations. Our analysis also shows that immediate\ncontext plays a vital role in shaping a response rather than the original post.\nWe also study the effect of consensual profanity and observe overlapping\nsimilarities with non-consensual profanity in terms of user behavior and\npatterns.\n","authors":["Vigneshwaran Shankaran","Rajesh Sharma"],"pdf_url":"https://arxiv.org/pdf/2404.07879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07851v1","updated":"2024-04-11T15:47:10Z","published":"2024-04-11T15:47:10Z","title":"Guiding Large Language Models to Post-Edit Machine Translation with\n  Error Annotations","summary":"  Machine Translation (MT) remains one of the last NLP tasks where large\nlanguage models (LLMs) have not yet replaced dedicated supervised systems. This\nwork exploits the complementary strengths of LLMs and supervised MT by guiding\nLLMs to automatically post-edit MT with external feedback on its quality,\nderived from Multidimensional Quality Metric (MQM) annotations. Working with\nLLaMA-2 models, we consider prompting strategies varying the nature of feedback\nprovided and then fine-tune the LLM to improve its ability to exploit the\nprovided guidance. Through experiments on Chinese-English, English-German, and\nEnglish-Russian MQM data, we demonstrate that prompting LLMs to post-edit MT\nimproves TER, BLEU and COMET scores, although the benefits of fine-grained\nfeedback are not clear. Fine-tuning helps integrate fine-grained feedback more\neffectively and further improves translation quality based on both automatic\nand human evaluation.\n","authors":["Dayeon Ki","Marine Carpuat"],"pdf_url":"https://arxiv.org/pdf/2404.07851v1.pdf","comment":"21 pages, 8 figures"},{"id":"http://arxiv.org/abs/2404.06948v2","updated":"2024-04-11T15:39:44Z","published":"2024-04-10T11:56:01Z","title":"MetaCheckGPT -- A Multi-task Hallucination Detector Using LLM\n  Uncertainty and Meta-models","summary":"  Hallucinations in large language models (LLMs) have recently become a\nsignificant problem. A recent effort in this direction is a shared task at\nSemeval 2024 Task 6, SHROOM, a Shared-task on Hallucinations and Related\nObservable Overgeneration Mistakes. This paper describes our winning solution\nranked 1st and 2nd in the 2 sub-tasks of model agnostic and model aware tracks\nrespectively. We propose a meta-regressor framework of LLMs for model\nevaluation and integration that achieves the highest scores on the leaderboard.\nWe also experiment with various transformer-based models and black box methods\nlike ChatGPT, Vectara, and others. In addition, we perform an error analysis\ncomparing GPT4 against our best model which shows the limitations of the\nformer.\n","authors":["Rahul Mehta","Andrew Hoblitzell","Jack O'Keefe","Hyeju Jang","Vasudeva Varma"],"pdf_url":"https://arxiv.org/pdf/2404.06948v2.pdf","comment":"Entry for SemEval-2024 Shared Task 6: SHROOM, a Shared-task on\n  Hallucinations and Related Observable Overgeneration Mistakes"},{"id":"http://arxiv.org/abs/2404.07840v1","updated":"2024-04-11T15:27:56Z","published":"2024-04-11T15:27:56Z","title":"On Training Data Influence of GPT Models","summary":"  Amidst the rapid advancements in generative language models, the\ninvestigation of how training data shapes the performance of GPT models is\nstill emerging. This paper presents GPTfluence, a novel approach that leverages\na featurized simulation to assess the impact of training examples on the\ntraining dynamics of GPT models. Our approach not only traces the influence of\nindividual training instances on performance trajectories, such as loss and\nother key metrics, on targeted test points but also enables a comprehensive\ncomparison with existing methods across various training scenarios in GPT\nmodels, ranging from 14 million to 2.8 billion parameters, across a range of\ndownstream tasks. Contrary to earlier methods that struggle with generalization\nto new data, GPTfluence introduces a parameterized simulation of training\ndynamics, demonstrating robust generalization capabilities to unseen training\ndata. This adaptability is evident across both fine-tuning and\ninstruction-tuning scenarios, spanning tasks in natural language understanding\nand generation. We will make our code and data publicly available.\n","authors":["Qingyi Liu","Yekun Chai","Shuohuan Wang","Yu Sun","Keze Wang","Hua Wu"],"pdf_url":"https://arxiv.org/pdf/2404.07840v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07839v1","updated":"2024-04-11T15:27:22Z","published":"2024-04-11T15:27:22Z","title":"RecurrentGemma: Moving Past Transformers for Efficient Open Language\n  Models","summary":"  We introduce RecurrentGemma, an open language model which uses Google's novel\nGriffin architecture. Griffin combines linear recurrences with local attention\nto achieve excellent performance on language. It has a fixed-sized state, which\nreduces memory use and enables efficient inference on long sequences. We\nprovide a pre-trained model with 2B non-embedding parameters, and an\ninstruction tuned variant. Both models achieve comparable performance to\nGemma-2B despite being trained on fewer tokens.\n","authors":["Aleksandar Botev","Soham De","Samuel L Smith","Anushan Fernando","George-Cristian Muraru","Ruba Haroun","Leonard Berrada","Razvan Pascanu","Pier Giuseppe Sessa","Robert Dadashi","L√©onard Hussenot","Johan Ferret","Sertan Girgin","Olivier Bachem","Alek Andreev","Kathleen Kenealy","Thomas Mesnard","Cassidy Hardin","Surya Bhupatiraju","Shreya Pathak","Laurent Sifre","Morgane Rivi√®re","Mihir Sanjay Kale","Juliette Love","Pouya Tafti","Armand Joulin","Noah Fiedel","Evan Senter","Yutian Chen","Srivatsan Srinivasan","Guillaume Desjardins","David Budden","Arnaud Doucet","Sharad Vikram","Adam Paszke","Trevor Gale","Sebastian Borgeaud","Charlie Chen","Andy Brock","Antonia Paterson","Jenny Brennan","Meg Risdal","Raj Gundluru","Nesh Devanathan","Paul Mooney","Nilay Chauhan","Phil Culliton","Luiz GUStavo Martins","Elisa Bandy","David Huntsperger","Glenn Cameron","Arthur Zucker","Tris Warkentin","Ludovic Peran","Minh Giang","Zoubin Ghahramani","Cl√©ment Farabet","Koray Kavukcuoglu","Demis Hassabis","Raia Hadsell","Yee Whye Teh","Nando de Frietas"],"pdf_url":"https://arxiv.org/pdf/2404.07839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07836v1","updated":"2024-04-11T15:24:50Z","published":"2024-04-11T15:24:50Z","title":"Question Generation in Knowledge-Driven Dialog: Explainability and\n  Evaluation","summary":"  We explore question generation in the context of knowledge-grounded dialogs\nfocusing on explainability and evaluation. Inspired by previous work on\nplanning-based summarisation, we present a model which instead of directly\ngenerating a question, sequentially predicts first a fact then a question. We\nevaluate our approach on 37k test dialogs adapted from the KGConv dataset and\nwe show that, although more demanding in terms of inference, our approach\nperforms on par with a standard model which solely generates a question while\nallowing for a detailed referenceless evaluation of the model behaviour in\nterms of relevance, factuality and pronominalisation.\n","authors":["Juliette Faille","Quentin Brabant","Gwenole Lecorve","Lina M. Rojas-Barahona","Claire Gardent"],"pdf_url":"https://arxiv.org/pdf/2404.07836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07824v1","updated":"2024-04-11T15:09:22Z","published":"2024-04-11T15:09:22Z","title":"Heron-Bench: A Benchmark for Evaluating Vision Language Models in\n  Japanese","summary":"  Vision Language Models (VLMs) have undergone a rapid evolution, giving rise\nto significant advancements in the realm of multimodal understanding tasks.\nHowever, the majority of these models are trained and evaluated on\nEnglish-centric datasets, leaving a gap in the development and evaluation of\nVLMs for other languages, such as Japanese. This gap can be attributed to the\nlack of methodologies for constructing VLMs and the absence of benchmarks to\naccurately measure their performance. To address this issue, we introduce a\nnovel benchmark, Japanese Heron-Bench, for evaluating Japanese capabilities of\nVLMs. The Japanese Heron-Bench consists of a variety of imagequestion answer\npairs tailored to the Japanese context. Additionally, we present a baseline\nJapanese VLM that has been trained with Japanese visual instruction tuning\ndatasets. Our Heron-Bench reveals the strengths and limitations of the proposed\nVLM across various ability dimensions. Furthermore, we clarify the capability\ngap between strong closed models like GPT-4V and the baseline model, providing\nvaluable insights for future research in this domain. We release the benchmark\ndataset and training code to facilitate further developments in Japanese VLM\nresearch.\n","authors":["Yuichi Inoue","Kento Sasaki","Yuma Ochi","Kazuki Fujii","Kotaro Tanahashi","Yu Yamaguchi"],"pdf_url":"https://arxiv.org/pdf/2404.07824v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07814v1","updated":"2024-04-11T14:57:19Z","published":"2024-04-11T14:57:19Z","title":"MultiLS-SP/CA: Lexical Complexity Prediction and Lexical Simplification\n  Resources for Catalan and Spanish","summary":"  Automatic lexical simplification is a task to substitute lexical items that\nmay be unfamiliar and difficult to understand with easier and more common\nwords. This paper presents MultiLS-SP/CA, a novel dataset for lexical\nsimplification in Spanish and Catalan. This dataset represents the first of its\nkind in Catalan and a substantial addition to the sparse data on automatic\nlexical simplification which is available for Spanish. Specifically, MultiLS-SP\nis the first dataset for Spanish which includes scalar ratings of the\nunderstanding difficulty of lexical items. In addition, we describe experiments\nwith this dataset, which can serve as a baseline for future work on the same\ndata.\n","authors":["Stefan Bott","Horacio Saggion","Nelson Per√©z Rojas","Martin Solis Salazar","Saul Calderon Ramirez"],"pdf_url":"https://arxiv.org/pdf/2404.07814v1.pdf","comment":"Submitted to the 40th edition of the SEPLN Conference. Under Revision"},{"id":"http://arxiv.org/abs/2404.01140v2","updated":"2024-04-11T14:57:10Z","published":"2024-04-01T14:36:35Z","title":"KoCoNovel: Annotated Dataset of Character Coreference in Korean Novels","summary":"  In this paper, we present KoCoNovel, a novel character coreference dataset\nderived from Korean literary texts, complete with detailed annotation\nguidelines. Comprising 178K tokens from 50 modern and contemporary novels,\nKoCoNovel stands as one of the largest public coreference resolution corpora in\nKorean, and the first to be based on literary texts. KoCoNovel offers four\ndistinct versions to accommodate a wide range of literary coreference analysis\nneeds. These versions are designed to support perspectives of the omniscient\nauthor or readers, and to manage multiple entities as either separate or\noverlapping, thereby broadening its applicability. One of KoCoNovel's\ndistinctive features is that 24% of all character mentions are single common\nnouns, lacking possessive markers or articles. This feature is particularly\ninfluenced by the nuances of Korean address term culture, which favors the use\nof terms denoting social relationships and kinship over personal names. In\nexperiments with a BERT-based coreference model, we observe notable performance\nenhancements with KoCoNovel in character coreference tasks within literary\ntexts, compared to a larger non-literary coreference dataset. Such findings\nunderscore KoCoNovel's potential to significantly enhance coreference\nresolution models through the integration of Korean cultural and linguistic\ndynamics.\n","authors":["Kyuhee Kim","Surin Lee","Sangah Lee"],"pdf_url":"https://arxiv.org/pdf/2404.01140v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2404.06405v2","updated":"2024-04-11T14:37:29Z","published":"2024-04-09T15:54:00Z","title":"Wu's Method can Boost Symbolic AI to Rival Silver Medalists and\n  AlphaGeometry to Outperform Gold Medalists at IMO Geometry","summary":"  Proving geometric theorems constitutes a hallmark of visual reasoning\ncombining both intuitive and logical skills. Therefore, automated theorem\nproving of Olympiad-level geometry problems is considered a notable milestone\nin human-level automated reasoning. The introduction of AlphaGeometry, a\nneuro-symbolic model trained with 100 million synthetic samples, marked a major\nbreakthrough. It solved 25 of 30 International Mathematical Olympiad (IMO)\nproblems whereas the reported baseline based on Wu's method solved only ten. In\nthis note, we revisit the IMO-AG-30 Challenge introduced with AlphaGeometry,\nand find that Wu's method is surprisingly strong. Wu's method alone can solve\n15 problems, and some of them are not solved by any of the other methods. This\nleads to two key findings: (i) Combining Wu's method with the classic synthetic\nmethods of deductive databases and angle, ratio, and distance chasing solves 21\nout of 30 methods by just using a CPU-only laptop with a time limit of 5\nminutes per problem. Essentially, this classic method solves just 4 problems\nless than AlphaGeometry and establishes the first fully symbolic baseline\nstrong enough to rival the performance of an IMO silver medalist. (ii) Wu's\nmethod even solves 2 of the 5 problems that AlphaGeometry failed to solve.\nThus, by combining AlphaGeometry with Wu's method we set a new state-of-the-art\nfor automated theorem proving on IMO-AG-30, solving 27 out of 30 problems, the\nfirst AI method which outperforms an IMO gold medalist.\n","authors":["Shiven Sinha","Ameya Prabhu","Ponnurangam Kumaraguru","Siddharth Bhat","Matthias Bethge"],"pdf_url":"https://arxiv.org/pdf/2404.06405v2.pdf","comment":"Work in Progress. Released for wider feedback"},{"id":"http://arxiv.org/abs/2404.07792v1","updated":"2024-04-11T14:35:23Z","published":"2024-04-11T14:35:23Z","title":"Nostra Domina at EvaLatin 2024: Improving Latin Polarity Detection\n  through Data Augmentation","summary":"  This paper describes submissions from the team Nostra Domina to the EvaLatin\n2024 shared task of emotion polarity detection. Given the low-resource\nenvironment of Latin and the complexity of sentiment in rhetorical genres like\npoetry, we augmented the available data through automatic polarity annotation.\nWe present two methods for doing so on the basis of the $k$-means algorithm,\nand we employ a variety of Latin large language models (LLMs) in a neural\narchitecture to better capture the underlying contextual sentiment\nrepresentations. Our best approach achieved the second highest macro-averaged\nMacro-$F_1$ score on the shared task's test set.\n","authors":["Stephen Bothwell","Abigail Swenor","David Chiang"],"pdf_url":"https://arxiv.org/pdf/2404.07792v1.pdf","comment":"Proceedings of the Third Workshop on Language Technologies for\n  Historical and Ancient Languages"},{"id":"http://arxiv.org/abs/2402.01115v2","updated":"2024-04-11T14:14:58Z","published":"2024-02-02T03:15:13Z","title":"Interpretation of Intracardiac Electrograms Through Textual\n  Representations","summary":"  Understanding the irregular electrical activity of atrial fibrillation (AFib)\nhas been a key challenge in electrocardiography. For serious cases of AFib,\ncatheter ablations are performed to collect intracardiac electrograms (EGMs).\nEGMs offer intricately detailed and localized electrical activity of the heart\nand are an ideal modality for interpretable cardiac studies. Recent\nadvancements in artificial intelligence (AI) has allowed some works to utilize\ndeep learning frameworks to interpret EGMs during AFib. Additionally, language\nmodels (LMs) have shown exceptional performance in being able to generalize to\nunseen domains, especially in healthcare. In this study, we are the first to\nleverage pretrained LMs for finetuning of EGM interpolation and AFib\nclassification via masked language modeling. We formulate the EGM as a textual\nsequence and present competitive performances on AFib classification compared\nagainst other representations. Lastly, we provide a comprehensive\ninterpretability study to provide a multi-perspective intuition of the model's\nbehavior, which could greatly benefit the clinical use.\n","authors":["William Jongwon Han","Diana Gomez","Avi Alok","Chaojing Duan","Michael A. Rosenberg","Douglas Weber","Emerson Liu","Ding Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.01115v2.pdf","comment":"16 pages, 7 figures; Accepted to CHIL 2024"},{"id":"http://arxiv.org/abs/2404.07775v1","updated":"2024-04-11T14:13:44Z","published":"2024-04-11T14:13:44Z","title":"Discourse-Aware In-Context Learning for Temporal Expression\n  Normalization","summary":"  Temporal expression (TE) normalization is a well-studied problem. However,\nthe predominately used rule-based systems are highly restricted to specific\nsettings, and upcoming machine learning approaches suffer from a lack of\nlabeled data. In this work, we explore the feasibility of proprietary and\nopen-source large language models (LLMs) for TE normalization using in-context\nlearning to inject task, document, and example information into the model. We\nexplore various sample selection strategies to retrieve the most relevant set\nof examples. By using a window-based prompt design approach, we can perform TE\nnormalization across sentences, while leveraging the LLM knowledge without\ntraining the model. Our experiments show competitive results to models designed\nfor this task. In particular, our method achieves large performance\nimprovements for non-standard settings by dynamically including relevant\nexamples during inference.\n","authors":["Akash Kumar Gautam","Lukas Lange","Jannik Str√∂tgen"],"pdf_url":"https://arxiv.org/pdf/2404.07775v1.pdf","comment":"Accepted at NAACL 2024"},{"id":"http://arxiv.org/abs/2404.07768v1","updated":"2024-04-11T14:06:39Z","published":"2024-04-11T14:06:39Z","title":"Using Letter Positional Probabilities to Assess Word Complexity","summary":"  Word complexity is defined in a number of different ways. Psycholinguistic,\nmorphological and lexical proxies are often used. Human ratings are also used.\nThe problem here is that these proxies do not measure complexity directly, and\nhuman ratings are subject to subjective bias. In this study we contend that\nsome form of 'latent complexity' can be approximated by using samples of simple\nand complex words. We use a sample of 'simple' words from primary school\npicture books and a sample of 'complex' words from high school and academic\nsettings. In order to analyse the differences between these classes, we look at\nthe letter positional probabilities (LPPs). We find a strong statistical\nassociation between simple and complex words on the basis of LPPs. For example,\nsimple words are significantly (p<.001) more likely to start with w, b, s, h,\ng,k, j,t y or f, while complex words are significantly (p<.001) more likely to\nstart with i, a, e, r, v, u or d. We find similar strong associations for\nsubsequent letter positions, with 84 letter-position variables in the first 6\npositions being significant at the p<.001 level. We then use LPPs as variables\nin creating a classifier which can classify the two classes with an 83%\naccuracy. We test these findings using a second data set, with 66 LPPs\nsignificant (p<.001) in the first 6 positions common to both datasets. We use\nthese 66 variables to create a classifier that is able to classify a third\ndataset with an accuracy of 70%. Finally, we create a fourth sample by\ncombining the extreme high and low scoring words generated by three classifiers\nbuilt on the first three separate datasets and use this sample to build a\nclassifier which has an accuracy of 97%. We use this to score the four levels\nof English word groups from an ESL program.\n","authors":["Michael Dalvean"],"pdf_url":"https://arxiv.org/pdf/2404.07768v1.pdf","comment":"25 Pages, 15 Tables"},{"id":"http://arxiv.org/abs/2404.07765v1","updated":"2024-04-11T14:04:36Z","published":"2024-04-11T14:04:36Z","title":"AnnoCTR: A Dataset for Detecting and Linking Entities, Tactics, and\n  Techniques in Cyber Threat Reports","summary":"  Monitoring the threat landscape to be aware of actual or potential attacks is\nof utmost importance to cybersecurity professionals. Information about cyber\nthreats is typically distributed using natural language reports. Natural\nlanguage processing can help with managing this large amount of unstructured\ninformation, yet to date, the topic has received little attention. With this\npaper, we present AnnoCTR, a new CC-BY-SA-licensed dataset of cyber threat\nreports. The reports have been annotated by a domain expert with named\nentities, temporal expressions, and cybersecurity-specific concepts including\nimplicitly mentioned techniques and tactics. Entities and concepts are linked\nto Wikipedia and the MITRE ATT&CK knowledge base, the most widely-used taxonomy\nfor classifying types of attacks. Prior datasets linking to MITRE ATT&CK either\nprovide a single label per document or annotate sentences out-of-context; our\ndataset annotates entire documents in a much finer-grained way. In an\nexperimental study, we model the annotations of our dataset using\nstate-of-the-art neural models. In our few-shot scenario, we find that for\nidentifying the MITRE ATT&CK concepts that are mentioned explicitly or\nimplicitly in a text, concept descriptions from MITRE ATT&CK are an effective\nsource for training data augmentation.\n","authors":["Lukas Lange","Marc M√ºller","Ghazaleh Haratinezhad Torbati","Dragan Milchevski","Patrick Grau","Subhash Pujari","Annemarie Friedrich"],"pdf_url":"https://arxiv.org/pdf/2404.07765v1.pdf","comment":"Accepted at LREC-COLING 2024. Corpus available at\n  https://github.com/boschresearch/anno-ctr-lrec-coling-2024"},{"id":"http://arxiv.org/abs/2403.08295v3","updated":"2024-04-11T13:53:29Z","published":"2024-03-13T06:59:16Z","title":"Gemma: Open Models Based on Gemini Research and Technology","summary":"  This work introduces Gemma, a family of lightweight, state-of-the art open\nmodels built from the research and technology used to create Gemini models.\nGemma models demonstrate strong performance across academic benchmarks for\nlanguage understanding, reasoning, and safety. We release two sizes of models\n(2 billion and 7 billion parameters), and provide both pretrained and\nfine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out\nof 18 text-based tasks, and we present comprehensive evaluations of safety and\nresponsibility aspects of the models, alongside a detailed description of model\ndevelopment. We believe the responsible release of LLMs is critical for\nimproving the safety of frontier models, and for enabling the next wave of LLM\ninnovations.\n","authors":[" Gemma Team","Thomas Mesnard","Cassidy Hardin","Robert Dadashi","Surya Bhupatiraju","Shreya Pathak","Laurent Sifre","Morgane Rivi√®re","Mihir Sanjay Kale","Juliette Love","Pouya Tafti","L√©onard Hussenot","Pier Giuseppe Sessa","Aakanksha Chowdhery","Adam Roberts","Aditya Barua","Alex Botev","Alex Castro-Ros","Ambrose Slone","Am√©lie H√©liou","Andrea Tacchetti","Anna Bulanova","Antonia Paterson","Beth Tsai","Bobak Shahriari","Charline Le Lan","Christopher A. Choquette-Choo","Cl√©ment Crepy","Daniel Cer","Daphne Ippolito","David Reid","Elena Buchatskaya","Eric Ni","Eric Noland","Geng Yan","George Tucker","George-Christian Muraru","Grigory Rozhdestvenskiy","Henryk Michalewski","Ian Tenney","Ivan Grishchenko","Jacob Austin","James Keeling","Jane Labanowski","Jean-Baptiste Lespiau","Jeff Stanway","Jenny Brennan","Jeremy Chen","Johan Ferret","Justin Chiu","Justin Mao-Jones","Katherine Lee","Kathy Yu","Katie Millican","Lars Lowe Sjoesund","Lisa Lee","Lucas Dixon","Machel Reid","Maciej Miku≈Ça","Mateo Wirth","Michael Sharman","Nikolai Chinaev","Nithum Thain","Olivier Bachem","Oscar Chang","Oscar Wahltinez","Paige Bailey","Paul Michel","Petko Yotov","Rahma Chaabouni","Ramona Comanescu","Reena Jana","Rohan Anil","Ross McIlroy","Ruibo Liu","Ryan Mullins","Samuel L Smith","Sebastian Borgeaud","Sertan Girgin","Sholto Douglas","Shree Pandya","Siamak Shakeri","Soham De","Ted Klimenko","Tom Hennigan","Vlad Feinberg","Wojciech Stokowiec","Yu-hui Chen","Zafarali Ahmed","Zhitao Gong","Tris Warkentin","Ludovic Peran","Minh Giang","Cl√©ment Farabet","Oriol Vinyals","Jeff Dean","Koray Kavukcuoglu","Demis Hassabis","Zoubin Ghahramani","Douglas Eck","Joelle Barral","Fernando Pereira","Eli Collins","Armand Joulin","Noah Fiedel","Evan Senter","Alek Andreev","Kathleen Kenealy"],"pdf_url":"https://arxiv.org/pdf/2403.08295v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07738v1","updated":"2024-04-11T13:36:29Z","published":"2024-04-11T13:36:29Z","title":"ResearchAgent: Iterative Research Idea Generation over Scientific\n  Literature with Large Language Models","summary":"  Scientific Research, vital for improving human life, is hindered by its\ninherent complexity, slow pace, and the need for specialized experts. To\nenhance its productivity, we propose a ResearchAgent, a large language\nmodel-powered research idea writing agent, which automatically generates\nproblems, methods, and experiment designs while iteratively refining them based\non scientific literature. Specifically, starting with a core paper as the\nprimary focus to generate ideas, our ResearchAgent is augmented not only with\nrelevant publications through connecting information over an academic graph but\nalso entities retrieved from an entity-centric knowledge store based on their\nunderlying concepts, mined and shared across numerous papers. In addition,\nmirroring the human approach to iteratively improving ideas with peer\ndiscussions, we leverage multiple ReviewingAgents that provide reviews and\nfeedback iteratively. Further, they are instantiated with human\npreference-aligned large language models whose criteria for evaluation are\nderived from actual human judgments. We experimentally validate our\nResearchAgent on scientific publications across multiple disciplines,\nshowcasing its effectiveness in generating novel, clear, and valid research\nideas based on human and model-based evaluation results.\n","authors":["Jinheon Baek","Sujay Kumar Jauhar","Silviu Cucerzan","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2404.07738v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07720v1","updated":"2024-04-11T13:11:21Z","published":"2024-04-11T13:11:21Z","title":"Automatic Generation and Evaluation of Reading Comprehension Test Items\n  with Large Language Models","summary":"  Reading comprehension tests are used in a variety of applications, reaching\nfrom education to assessing the comprehensibility of simplified texts. However,\ncreating such tests manually and ensuring their quality is difficult and\ntime-consuming. In this paper, we explore how large language models (LLMs) can\nbe used to generate and evaluate multiple-choice reading comprehension items.\nTo this end, we compiled a dataset of German reading comprehension items and\ndeveloped a new protocol for human and automatic evaluation, including a metric\nwe call text informativity, which is based on guessability and answerability.\nWe then used this protocol and the dataset to evaluate the quality of items\ngenerated by Llama 2 and GPT-4. Our results suggest that both models are\ncapable of generating items of acceptable quality in a zero-shot setting, but\nGPT-4 clearly outperforms Llama 2. We also show that LLMs can be used for\nautomatic evaluation by eliciting item reponses from them. In this scenario,\nevaluation results with GPT-4 were the most similar to human annotators.\nOverall, zero-shot generation with LLMs is a promising approach for generating\nand evaluating reading comprehension test items, in particular for languages\nwithout large amounts of available data.\n","authors":["Andreas S√§uberli","Simon Clematide"],"pdf_url":"https://arxiv.org/pdf/2404.07720v1.pdf","comment":"Accepted for publication at the 3rd Workshop on Tools and Resources\n  for People with REAding DIfficulties (READI) at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2404.04067v2","updated":"2024-04-11T13:10:30Z","published":"2024-04-05T12:51:37Z","title":"CLUE: A Clinical Language Understanding Evaluation for LLMs","summary":"  Large Language Models (LLMs) have shown the potential to significantly\ncontribute to patient care, diagnostics, and administrative processes. Emerging\nbiomedical LLMs address healthcare-specific challenges, including privacy\ndemands and computational constraints. However, evaluation of these models has\nprimarily been limited to non-clinical tasks, which do not reflect the\ncomplexity of practical clinical applications. Additionally, there has been no\nthorough comparison between biomedical and general-domain LLMs for clinical\ntasks. To fill this gap, we present the Clinical Language Understanding\nEvaluation (CLUE), a benchmark tailored to evaluate LLMs on real-world clinical\ntasks. CLUE includes two novel datasets derived from MIMIC IV discharge letters\nand four existing tasks designed to test the practical applicability of LLMs in\nhealthcare settings. Our evaluation covers several biomedical and general\ndomain LLMs, providing insights into their clinical performance and\napplicability. CLUE represents a step towards a standardized approach to\nevaluating and developing LLMs in healthcare to align future model development\nwith the real-world needs of clinical application. We publish our evaluation\nand data generation scripts: https://github.com/TIO-IKIM/CLUE.\n","authors":["Amin Dada","Marie Bauer","Amanda Butler Contreras","Osman Alperen Kora≈ü","Constantin Marc Seibold","Kaleb E Smith","Jens Kleesiek"],"pdf_url":"https://arxiv.org/pdf/2404.04067v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07677v1","updated":"2024-04-11T12:16:16Z","published":"2024-04-11T12:16:16Z","title":"ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs","summary":"  The integration of Large Language Models (LLMs) and knowledge graphs (KGs)\nhas achieved remarkable success in various natural language processing tasks.\nHowever, existing methodologies that integrate LLMs and KGs often navigate the\ntask-solving process solely based on the LLM's analysis of the question,\noverlooking the rich cognitive potential inherent in the vast knowledge\nencapsulated in KGs. To address this, we introduce Observation-Driven Agent\n(ODA), a novel AI agent framework tailored for tasks involving KGs. ODA\nincorporates KG reasoning abilities via global observation that enhances\nreasoning capabilities through a cyclical paradigm of observation, action, and\nreflection. Confronting the exponential explosion of knowledge during\nobservation, we innovatively design a recursive observation mechanism.\nSubsequently, we integrate the observed knowledge into the action and\nreflection modules. Through extensive experiments, ODA demonstrates\nstate-of-the-art performance on several datasets, notably achieving accuracy\nimprovements of 12.87% and 8.9%.\n","authors":["Lei Sun","Zhengwei Tao","Youdi Li","Hiroshi Arakawa"],"pdf_url":"https://arxiv.org/pdf/2404.07677v1.pdf","comment":"LLM+KG"},{"id":"http://arxiv.org/abs/2404.07673v1","updated":"2024-04-11T12:09:47Z","published":"2024-04-11T12:09:47Z","title":"Curated Datasets and Neural Models for Machine Translation of Informal\n  Registers between Mayan and Spanish Vernaculars","summary":"  The Mayan languages comprise a language family with an ancient history,\nmillions of speakers, and immense cultural value, that, nevertheless, remains\nseverely underrepresented in terms of resources and global exposure. In this\npaper we develop, curate, and publicly release a set of corpora in several\nMayan languages spoken in Guatemala and Southern Mexico, which we call MayanV.\nThe datasets are parallel with Spanish, the dominant language of the region,\nand are taken from official native sources focused on representing informal,\nday-to-day, and non-domain-specific language. As such, and according to our\ndialectometric analysis, they differ in register from most other available\nresources. Additionally, we present neural machine translation models, trained\non as many resources and Mayan languages as possible, and evaluated exclusively\non our datasets. We observe lexical divergences between the dialects of Spanish\nin our resources and the more widespread written standard of Spanish, and that\nresources other than the ones we present do not seem to improve translation\nperformance, indicating that many such resources may not accurately capture\ncommon, real-life language usage. The MayanV dataset is available at\nhttps://github.com/transducens/mayanv.\n","authors":["Andr√©s Lou","Juan Antonio P√©rez-Ortiz","Felipe S√°nchez-Mart√≠nez","V√≠ctor M. S√°nchez-Cartagena"],"pdf_url":"https://arxiv.org/pdf/2404.07673v1.pdf","comment":"13 pages, 3 figures, 8 tables, Submitted to NAACL 2024"},{"id":"http://arxiv.org/abs/2403.17645v3","updated":"2024-04-11T12:07:33Z","published":"2024-03-26T12:27:32Z","title":"DANCER: Entity Description Augmented Named Entity Corrector for\n  Automatic Speech Recognition","summary":"  End-to-end automatic speech recognition (E2E ASR) systems often suffer from\nmistranscription of domain-specific phrases, such as named entities, sometimes\nleading to catastrophic failures in downstream tasks. A family of fast and\nlightweight named entity correction (NEC) models for ASR have recently been\nproposed, which normally build on phonetic-level edit distance algorithms and\nhave shown impressive NEC performance. However, as the named entity (NE) list\ngrows, the problems of phonetic confusion in the NE list are exacerbated; for\nexample, homophone ambiguities increase substantially. In view of this, we\nproposed a novel Description Augmented Named entity CorrEctoR (dubbed DANCER),\nwhich leverages entity descriptions to provide additional information to\nfacilitate mitigation of phonetic confusion for NEC on ASR transcription. To\nthis end, an efficient entity description augmented masked language model\n(EDA-MLM) comprised of a dense retrieval model is introduced, enabling MLM to\nadapt swiftly to domain-specific entities for the NEC task. A series of\nexperiments conducted on the AISHELL-1 and Homophone datasets confirm the\neffectiveness of our modeling approach. DANCER outperforms a strong baseline,\nthe phonetic edit-distance-based NEC model (PED-NEC), by a character error rate\n(CER) reduction of about 7% relatively on AISHELL-1 for named entities. More\nnotably, when tested on Homophone that contain named entities of high phonetic\nconfusion, DANCER offers a more pronounced CER reduction of 46% relatively over\nPED-NEC for named entities.\n","authors":["Yi-Cheng Wang","Hsin-Wei Wang","Bi-Cheng Yan","Chi-Han Lin","Berlin Chen"],"pdf_url":"https://arxiv.org/pdf/2403.17645v3.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2304.02017v9","updated":"2024-04-11T11:44:38Z","published":"2023-03-27T21:27:58Z","title":"Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its\n  Applications, Advantages, Limitations, and Future Directions in Natural\n  Language Processing","summary":"  Large language models have revolutionized the field of artificial\nintelligence and have been used in various applications. Among these models,\nChatGPT (Chat Generative Pre-trained Transformer) has been developed by OpenAI,\nit stands out as a powerful tool that has been widely adopted. ChatGPT has been\nsuccessfully applied in numerous areas, including chatbots, content generation,\nlanguage translation, personalized recommendations, and even medical diagnosis\nand treatment. Its success in these applications can be attributed to its\nability to generate human-like responses, understand natural language, and\nadapt to different contexts. Its versatility and accuracy make it a powerful\ntool for natural language processing (NLP). However, there are also limitations\nto ChatGPT, such as its tendency to produce biased responses and its potential\nto perpetuate harmful language patterns. This article provides a comprehensive\noverview of ChatGPT, its applications, advantages, and limitations.\nAdditionally, the paper emphasizes the importance of ethical considerations\nwhen using this robust tool in real-world scenarios. Finally, This paper\ncontributes to ongoing discussions surrounding artificial intelligence and its\nimpact on vision and NLP domains by providing insights into prompt engineering\ntechniques.\n","authors":["Walid Hariri"],"pdf_url":"https://arxiv.org/pdf/2304.02017v9.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07654v1","updated":"2024-04-11T11:37:18Z","published":"2024-04-11T11:37:18Z","title":"rollama: An R package for using generative large language models through\n  Ollama","summary":"  rollama is an R package that wraps the Ollama API, which allows you to run\ndifferent Generative Large Language Models (GLLM) locally. The package and\nlearning material focus on making it easy to use Ollama for annotating textual\nor imagine data with open-source models as well as use these models for\ndocument embedding. But users can use or extend rollama to do essentially\nanything else that is possible through OpenAI's API, yet more private,\nreproducible and for free.\n","authors":["Johannes B. Gruber","Maximilian Weber"],"pdf_url":"https://arxiv.org/pdf/2404.07654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14785v2","updated":"2024-04-11T11:16:45Z","published":"2023-05-24T06:41:09Z","title":"Simple Linguistic Inferences of Large Language Models (LLMs): Blind\n  Spots and Blinds","summary":"  We evaluate LLMs' language understanding capacities on simple inference tasks\nthat most humans find trivial. Specifically, we target (i)\ngrammatically-specified entailments, (ii) premises with evidential adverbs of\nuncertainty, and (iii) monotonicity entailments. We design evaluation sets for\nthese tasks and conduct experiments in both zero-shot and chain-of-thought\nsetups, and with multiple prompts and LLMs. The models exhibit moderate to low\nperformance on these evaluation sets. Subsequent experiments show that\nembedding the premise in syntactic constructions that should preserve the\nentailment relations (presupposition triggers) or change them (non-factives),\nfurther confuses the models, causing them to either under-predict or\nover-predict certain entailment labels regardless of the true relation, and\noften disregarding the nature of the embedding context. Overall these results\nsuggest that, despite LLMs' celebrated language understanding capacity, even\nthe strongest models have blindspots with respect to certain types of\nentailments, and certain information-packaging structures act as ``blinds''\novershadowing the semantics of the embedded premise.\n","authors":["Victoria Basmov","Yoav Goldberg","Reut Tsarfaty"],"pdf_url":"https://arxiv.org/pdf/2305.14785v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07647v1","updated":"2024-04-11T11:10:36Z","published":"2024-04-11T11:10:36Z","title":"Why do small language models underperform? Studying Language Model\n  Saturation via the Softmax Bottleneck","summary":"  Recent advances in language modeling consist in pretraining highly\nparameterized neural networks on extremely large web-mined text corpora.\nTraining and inference with such models can be costly in practice, which\nincentivizes the use of smaller counterparts. However, it has been observed\nthat smaller models can suffer from saturation, characterized as a drop in\nperformance at some advanced point in training followed by a plateau. In this\npaper, we find that such saturation can be explained by a mismatch between the\nhidden dimension of smaller models and the high rank of the target contextual\nprobability distribution. This mismatch affects the performance of the linear\nprediction head used in such models through the well-known softmax bottleneck\nphenomenon. We measure the effect of the softmax bottleneck in various settings\nand find that models based on less than 1000 hidden dimensions tend to adopt\ndegenerate latent representations in late pretraining, which leads to reduced\nevaluation performance.\n","authors":["Nathan Godey","√âric de la Clergerie","Beno√Æt Sagot"],"pdf_url":"https://arxiv.org/pdf/2404.07647v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01957v3","updated":"2024-04-11T10:54:19Z","published":"2023-12-04T15:16:12Z","title":"Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian\n  Perspective","summary":"  This paper proposes an interpretation of RLAIF as Bayesian inference by\nintroducing distilled Self-Critique (dSC), which refines the outputs of a LLM\nthrough a Gibbs sampler that is later distilled into a fine-tuned model. Only\nrequiring synthetic data, dSC is exercised in experiments regarding safety,\nsentiment, and privacy control, showing it can be a viable and cheap\nalternative to align LLMs. Code released at\n\\url{https://github.com/vicgalle/distilled-self-critique}.\n","authors":["Victor Gallego"],"pdf_url":"https://arxiv.org/pdf/2312.01957v3.pdf","comment":"Accepted to ICLR 2024 (TinyPapers track)"},{"id":"http://arxiv.org/abs/2404.07622v1","updated":"2024-04-11T10:16:44Z","published":"2024-04-11T10:16:44Z","title":"Multi-Image Visual Question Answering for Unsupervised Anomaly Detection","summary":"  Unsupervised anomaly detection enables the identification of potential\npathological areas by juxtaposing original images with their pseudo-healthy\nreconstructions generated by models trained exclusively on normal images.\nHowever, the clinical interpretation of resultant anomaly maps presents a\nchallenge due to a lack of detailed, understandable explanations. Recent\nadvancements in language models have shown the capability of mimicking\nhuman-like understanding and providing detailed descriptions. This raises an\ninteresting question: \\textit{How can language models be employed to make the\nanomaly maps more explainable?} To the best of our knowledge, we are the first\nto leverage a language model for unsupervised anomaly detection, for which we\nconstruct a dataset with different questions and answers. Additionally, we\npresent a novel multi-image visual question answering framework tailored for\nanomaly detection, incorporating diverse feature fusion strategies to enhance\nvisual knowledge extraction. Our experiments reveal that the framework,\naugmented by our new Knowledge Q-Former module, adeptly answers questions on\nthe anomaly detection dataset. Besides, integrating anomaly maps as inputs\ndistinctly aids in improving the detection of unseen pathologies.\n","authors":["Jun Li","Cosmin I. Bercea","Philip M√ºller","Lina Felsner","Suhwan Kim","Daniel Rueckert","Benedikt Wiestler","Julia A. Schnabel"],"pdf_url":"https://arxiv.org/pdf/2404.07622v1.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2404.07616v1","updated":"2024-04-11T10:08:34Z","published":"2024-04-11T10:08:34Z","title":"Audio Dialogues: Dialogues dataset for audio and music understanding","summary":"  Existing datasets for audio understanding primarily focus on single-turn\ninteractions (i.e. audio captioning, audio question answering) for describing\naudio in natural language, thus limiting understanding audio via interactive\ndialogue. To address this gap, we introduce Audio Dialogues: a multi-turn\ndialogue dataset containing 163.8k samples for general audio sounds and music.\nIn addition to dialogues, Audio Dialogues also has question-answer pairs to\nunderstand and compare multiple input audios together. Audio Dialogues\nleverages a prompting-based approach and caption annotations from existing\ndatasets to generate multi-turn dialogues using a Large Language Model (LLM).\nWe evaluate existing audio-augmented large language models on our proposed\ndataset to demonstrate the complexity and applicability of Audio Dialogues. Our\ncode for generating the dataset will be made publicly available. Detailed\nprompts and generated dialogues can be found on the demo website\nhttps://audiodialogues.github.io/.\n","authors":["Arushi Goel","Zhifeng Kong","Rafael Valle","Bryan Catanzaro"],"pdf_url":"https://arxiv.org/pdf/2404.07616v1.pdf","comment":"Demo website: https://audiodialogues.github.io/"},{"id":"http://arxiv.org/abs/2404.07613v1","updated":"2024-04-11T10:01:32Z","published":"2024-04-11T10:01:32Z","title":"Medical mT5: An Open-Source Multilingual Text-to-Text LLM for The\n  Medical Domain","summary":"  Research on language technology for the development of medical applications\nis currently a hot topic in Natural Language Understanding and Generation.\nThus, a number of large language models (LLMs) have recently been adapted to\nthe medical domain, so that they can be used as a tool for mediating in\nhuman-AI interaction. While these LLMs display competitive performance on\nautomated medical texts benchmarks, they have been pre-trained and evaluated\nwith a focus on a single language (English mostly). This is particularly true\nof text-to-text models, which typically require large amounts of\ndomain-specific pre-training data, often not easily accessible for many\nlanguages. In this paper, we address these shortcomings by compiling, to the\nbest of our knowledge, the largest multilingual corpus for the medical domain\nin four languages, namely English, French, Italian and Spanish. This new corpus\nhas been used to train Medical mT5, the first open-source text-to-text\nmultilingual model for the medical domain. Additionally, we present two new\nevaluation benchmarks for all four languages with the aim of facilitating\nmultilingual research in this domain. A comprehensive evaluation shows that\nMedical mT5 outperforms both encoders and similarly sized text-to-text models\nfor the Spanish, French, and Italian benchmarks, while being competitive with\ncurrent state-of-the-art LLMs in English.\n","authors":["Iker Garc√≠a-Ferrero","Rodrigo Agerri","Aitziber Atutxa Salazar","Elena Cabrio","Iker de la Iglesia","Alberto Lavelli","Bernardo Magnini","Benjamin Molinet","Johana Ramirez-Romero","German Rigau","Jose Maria Villa-Gonzalez","Serena Villata","Andrea Zaninello"],"pdf_url":"https://arxiv.org/pdf/2404.07613v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2404.07611v1","updated":"2024-04-11T09:59:01Z","published":"2024-04-11T09:59:01Z","title":"NoticIA: A Clickbait Article Summarization Dataset in Spanish","summary":"  We present NoticIA, a dataset consisting of 850 Spanish news articles\nfeaturing prominent clickbait headlines, each paired with high-quality,\nsingle-sentence generative summarizations written by humans. This task demands\nadvanced text understanding and summarization abilities, challenging the\nmodels' capacity to infer and connect diverse pieces of information to meet the\nuser's informational needs generated by the clickbait headline. We evaluate the\nSpanish text comprehension capabilities of a wide range of state-of-the-art\nlarge language models. Additionally, we use the dataset to train\nClickbaitFighter, a task-specific model that achieves near-human performance in\nthis task.\n","authors":["Iker Garc√≠a-Ferrero","Bego√±a Altuna"],"pdf_url":"https://arxiv.org/pdf/2404.07611v1.pdf","comment":"Under review in the journal Procesamiento del Lenguaje Natural"},{"id":"http://arxiv.org/abs/2403.16099v2","updated":"2024-04-11T09:58:17Z","published":"2024-03-24T11:29:55Z","title":"A Multi-Label Dataset of French Fake News: Human and Machine Insights","summary":"  We present a corpus of 100 documents, OBSINFOX, selected from 17 sources of\nFrench press considered unreliable by expert agencies, annotated using 11\nlabels by 8 annotators. By collecting more labels than usual, by more\nannotators than is typically done, we can identify features that humans\nconsider as characteristic of fake news, and compare them to the predictions of\nautomated classifiers. We present a topic and genre analysis using Gate Cloud,\nindicative of the prevalence of satire-like text in the corpus. We then use the\nsubjectivity analyzer VAGO, and a neural version of it, to clarify the link\nbetween ascriptions of the label Subjective and ascriptions of the label Fake\nNews. The annotated dataset is available online at the following url:\nhttps://github.com/obs-info/obsinfox\n  Keywords: Fake News, Multi-Labels, Subjectivity, Vagueness, Detail, Opinion,\nExaggeration, French Press\n","authors":["Benjamin Icard","Fran√ßois Maine","Morgane Casanova","G√©raud Faye","Julien Chanson","Guillaume Gadek","Ghislain Atemezing","Fran√ßois Bancilhon","Paul √âgr√©"],"pdf_url":"https://arxiv.org/pdf/2403.16099v2.pdf","comment":"Paper to appear in the Proceedings of the 2024 Joint International\n  Conference on Computational Linguistics, Language Resources and Evaluation\n  (LREC-COLING 2024)"},{"id":"http://arxiv.org/abs/2404.07584v1","updated":"2024-04-11T09:17:12Z","published":"2024-04-11T09:17:12Z","title":"UltraEval: A Lightweight Platform for Flexible and Comprehensive\n  Evaluation for LLMs","summary":"  Evaluation is pivotal for honing Large Language Models (LLMs), pinpointing\ntheir capabilities and guiding enhancements. The rapid development of LLMs\ncalls for a lightweight and easy-to-use framework for swift evaluation\ndeployment. However, due to the various implementation details to consider,\ndeveloping a comprehensive evaluation platform is never easy. Existing\nplatforms are often complex and poorly modularized, hindering seamless\nincorporation into researcher's workflows. This paper introduces UltraEval, a\nuser-friendly evaluation framework characterized by lightweight,\ncomprehensiveness, modularity, and efficiency. We identify and reimplement\nthree core components of model evaluation (models, data, and metrics). The\nresulting composability allows for the free combination of different models,\ntasks, prompts, and metrics within a unified evaluation workflow. Additionally,\nUltraEval supports diverse models owing to a unified HTTP service and provides\nsufficient inference acceleration. UltraEval is now available for researchers\npublicly~\\footnote{Website is at \\url{https://github.com/OpenBMB/UltraEval}}.\n","authors":["Chaoqun He","Renjie Luo","Shengding Hu","Yuanqian Zhao","Jie Zhou","Hanghao Wu","Jiajie Zhang","Xu Han","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2404.07584v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07549v1","updated":"2024-04-11T08:30:46Z","published":"2024-04-11T08:30:46Z","title":"Comments as Natural Logic Pivots: Improve Code Generation via Comment\n  Perspective","summary":"  Code generation aims to understand the problem description and generate\ncorresponding code snippets, where existing works generally decompose such\ncomplex tasks into intermediate steps by prompting strategies, such as\nChain-of-Thought and its variants. While these studies have achieved some\nsuccess, their effectiveness is highly dependent on the capabilities of\nadvanced Large Language Models (LLMs) such as GPT-4, particularly in terms of\nAPI calls, which significantly limits their practical applicability.\nConsequently, how to enhance the code generation capabilities of small and\nmedium-scale code LLMs without significantly increasing training costs is an\nappealing challenge. In this paper, we suggest that code comments are the\nnatural logic pivot between natural language and code language and propose\nusing comments to boost the code generation ability of code LLMs. Concretely,\nwe propose MANGO (comMents As Natural loGic pivOts), including a comment\ncontrastive training strategy and a corresponding logical comment decoding\nstrategy. Experiments are performed on HumanEval and MBPP, utilizing StarCoder\nand WizardCoder as backbone models, and encompassing model parameter sizes\nbetween 3B and 7B. The results indicate that MANGO significantly improves the\ncode pass rate based on the strong baselines. Meanwhile, the robustness of the\nlogical comment decoding strategy is notably higher than the Chain-of-thoughts\nprompting. The code is publicly available at\n\\url{https://github.com/pppa2019/Mango}.\n","authors":["Yijie Chen","Yijin Liu","Fandong Meng","Yufeng Chen","Jinan Xu","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2404.07549v1.pdf","comment":"The code is publicly available at https://github.com/pppa2019/Mango"},{"id":"http://arxiv.org/abs/2404.07546v1","updated":"2024-04-11T08:20:10Z","published":"2024-04-11T08:20:10Z","title":"Decomposing Label Space, Format and Discrimination: Rethinking How LLMs\n  Respond and Solve Tasks via In-Context Learning","summary":"  In-context Learning (ICL) has emerged as a powerful capability alongside the\ndevelopment of scaled-up large language models (LLMs). By instructing LLMs\nusing few-shot demonstrative examples, ICL enables them to perform a wide range\nof tasks without updating millions of parameters. However, the precise\ncontributions of demonstrations towards improving end-task performance have not\nbeen thoroughly investigated in recent analytical studies. In this paper, we\nempirically decompose the overall performance of ICL into three dimensions,\nlabel space, format, and discrimination, and we evaluate four general-purpose\nLLMs across a diverse range of tasks. Counter-intuitively, we find that the\ndemonstrations have a marginal impact on provoking discriminative knowledge of\nlanguage models. However, ICL exhibits significant efficacy in regulating the\nlabel space and format which helps LLMs to respond in desired label words. We\nthen demonstrate this ability functions similar to detailed instructions for\nLLMs to follow. We additionally provide an in-depth analysis of the mechanism\nof retrieval helping with ICL and find that retrieving the most semantically\nsimilar examples notably boosts model's discriminative capability.\n","authors":["Quanyu Long","Yin Wu","Wenya Wang","Sinno Jialin Pan"],"pdf_url":"https://arxiv.org/pdf/2404.07546v1.pdf","comment":"36 pages, 8 figures"},{"id":"http://arxiv.org/abs/2404.07544v1","updated":"2024-04-11T08:12:43Z","published":"2024-04-11T08:12:43Z","title":"From Words to Numbers: Your Large Language Model Is Secretly A Capable\n  Regressor When Given In-Context Examples","summary":"  We analyze how well pre-trained large language models (e.g., Llama2, GPT-4,\nClaude 3, etc) can do linear and non-linear regression when given in-context\nexamples, without any additional training or gradient updates. Our findings\nreveal that several large language models (e.g., GPT-4, Claude 3) are able to\nperform regression tasks with a performance rivaling (or even outperforming)\nthat of traditional supervised methods such as Random Forest, Bagging, or\nGradient Boosting. For example, on the challenging Friedman #2 regression\ndataset, Claude 3 outperforms many supervised methods such as AdaBoost, SVM,\nRandom Forest, KNN, or Gradient Boosting. We then investigate how well the\nperformance of large language models scales with the number of in-context\nexemplars. We borrow from the notion of regret from online learning and\nempirically show that LLMs are capable of obtaining a sub-linear regret.\n","authors":["Robert Vacareanu","Vlad-Andrei Negru","Vasile Suciu","Mihai Surdeanu"],"pdf_url":"https://arxiv.org/pdf/2404.07544v1.pdf","comment":"50 pages, 48 figures, preprint"},{"id":"http://arxiv.org/abs/2304.13567v4","updated":"2024-04-11T08:10:11Z","published":"2023-04-26T13:57:25Z","title":"Technical Report: Impact of Position Bias on Language Models in Token\n  Classification","summary":"  Language Models (LMs) have shown state-of-the-art performance in Natural\nLanguage Processing (NLP) tasks. Downstream tasks such as Named Entity\nRecognition (NER) or Part-of-Speech (POS) tagging are known to suffer from data\nimbalance issues, particularly regarding the ratio of positive to negative\nexamples and class disparities. This paper investigates an often-overlooked\nissue of encoder models, specifically the position bias of positive examples in\ntoken classification tasks. For completeness, we also include decoders in the\nevaluation. We evaluate the impact of position bias using different position\nembedding techniques, focusing on BERT with Absolute Position Embedding (APE),\nRelative Position Embedding (RPE), and Rotary Position Embedding (RoPE).\nTherefore, we conduct an in-depth evaluation of the impact of position bias on\nthe performance of LMs when fine-tuned on token classification benchmarks. Our\nstudy includes CoNLL03 and OntoNote5.0 for NER, English Tree Bank UD\\_en, and\nTweeBank for POS tagging. We propose an evaluation approach to investigate\nposition bias in transformer models. We show that LMs can suffer from this bias\nwith an average drop ranging from 3\\% to 9\\% in their performance. To mitigate\nthis effect, we propose two methods: Random Position Shifting and Context\nPerturbation, that we apply on batches during the training process. The results\nshow an improvement of $\\approx$ 2\\% in the performance of the model on\nCoNLL03, UD\\_en, and TweeBank.\n","authors":["Mehdi Ben Amor","Michael Granitzer","Jelena Mitroviƒá"],"pdf_url":"https://arxiv.org/pdf/2304.13567v4.pdf","comment":"Updated content of the preprint"},{"id":"http://arxiv.org/abs/2404.07520v1","updated":"2024-04-11T07:26:00Z","published":"2024-04-11T07:26:00Z","title":"PromptSync: Bridging Domain Gaps in Vision-Language Models through\n  Class-Aware Prototype Alignment and Discrimination","summary":"  The potential for zero-shot generalization in vision-language (V-L) models\nsuch as CLIP has spurred their widespread adoption in addressing numerous\ndownstream tasks. Previous methods have employed test-time prompt tuning to\nadapt the model to unseen domains, but they overlooked the issue of imbalanced\nclass distributions. In this study, we explicitly address this problem by\nemploying class-aware prototype alignment weighted by mean class probabilities\nobtained for the test sample and filtered augmented views. Additionally, we\nensure that the class probabilities are as accurate as possible by performing\nprototype discrimination using contrastive learning. The combination of\nalignment and discriminative loss serves as a geometric regularizer, preventing\nthe prompt representation from collapsing onto a single class and effectively\nbridging the distribution gap between the source and test domains. Our method,\nnamed PromptSync, synchronizes the prompts for each test sample on both the\ntext and vision branches of the V-L model. In empirical evaluations on the\ndomain generalization benchmark, our method outperforms previous best methods\nby 2.33\\% in overall performance, by 1\\% in base-to-novel generalization, and\nby 2.84\\% in cross-dataset transfer tasks.\n","authors":["Anant Khandelwal"],"pdf_url":"https://arxiv.org/pdf/2404.07520v1.pdf","comment":"Accepted at CVPR 2024 LIMIT, 12 pages, 8 Tables, 2 Figures"},{"id":"http://arxiv.org/abs/2312.15918v2","updated":"2024-04-11T06:41:15Z","published":"2023-12-26T07:24:46Z","title":"Supervised Knowledge Makes Large Language Models Better In-context\n  Learners","summary":"  Large Language Models (LLMs) exhibit emerging in-context learning abilities\nthrough prompt engineering. The recent progress in large-scale generative\nmodels has further expanded their use in real-world language applications.\nHowever, the critical challenge of improving the generalizability and\nfactuality of LLMs in natural language understanding and question answering\nremains under-explored. While previous in-context learning research has focused\non enhancing models to adhere to users' specific instructions and quality\nexpectations, and to avoid undesired outputs, little to no work has explored\nthe use of task-Specific fine-tuned Language Models (SLMs) to improve LLMs'\nin-context learning during the inference stage. Our primary contribution is the\nestablishment of a simple yet effective framework that enhances the reliability\nof LLMs as it: 1) generalizes out-of-distribution data, 2) elucidates how LLMs\nbenefit from discriminative models, and 3) minimizes hallucinations in\ngenerative tasks. Using our proposed plug-in method, enhanced versions of Llama\n2 and ChatGPT surpass their original versions regarding generalizability and\nfactuality. We offer a comprehensive suite of resources, including 16 curated\ndatasets, prompts, model checkpoints, and LLM outputs across 9 distinct tasks.\nThe code and data are released at:\nhttps://github.com/YangLinyi/Supervised-Knowledge-Makes-Large-Language-Models-Better-In-context-Learners.\nOur empirical analysis sheds light on the advantages of incorporating\ndiscriminative models into LLMs and highlights the potential of our methodology\nin fostering more reliable LLMs.\n","authors":["Linyi Yang","Shuibai Zhang","Zhuohao Yu","Guangsheng Bao","Yidong Wang","Jindong Wang","Ruochen Xu","Wei Ye","Xing Xie","Weizhu Chen","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.15918v2.pdf","comment":"Accepted to ICLR 2024"},{"id":"http://arxiv.org/abs/2404.07503v1","updated":"2024-04-11T06:34:17Z","published":"2024-04-11T06:34:17Z","title":"Best Practices and Lessons Learned on Synthetic Data for Language Models","summary":"  The success of AI models relies on the availability of large, diverse, and\nhigh-quality datasets, which can be challenging to obtain due to data scarcity,\nprivacy concerns, and high costs. Synthetic data has emerged as a promising\nsolution by generating artificial data that mimics real-world patterns. This\npaper provides an overview of synthetic data research, discussing its\napplications, challenges, and future directions. We present empirical evidence\nfrom prior art to demonstrate its effectiveness and highlight the importance of\nensuring its factuality, fidelity, and unbiasedness. We emphasize the need for\nresponsible use of synthetic data to build more powerful, inclusive, and\ntrustworthy language models.\n","authors":["Ruibo Liu","Jerry Wei","Fangyu Liu","Chenglei Si","Yanzhe Zhang","Jinmeng Rao","Steven Zheng","Daiyi Peng","Diyi Yang","Denny Zhou","Andrew M. Dai"],"pdf_url":"https://arxiv.org/pdf/2404.07503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07501v1","updated":"2024-04-11T06:32:03Z","published":"2024-04-11T06:32:03Z","title":"Leveraging Data Augmentation for Process Information Extraction","summary":"  Business Process Modeling projects often require formal process models as a\ncentral component. High costs associated with the creation of such formal\nprocess models motivated many different fields of research aimed at automated\ngeneration of process models from readily available data. These include process\nmining on event logs, and generating business process models from natural\nlanguage texts. Research in the latter field is regularly faced with the\nproblem of limited data availability, hindering both evaluation and development\nof new techniques, especially learning-based ones.\n  To overcome this data scarcity issue, in this paper we investigate the\napplication of data augmentation for natural language text data. Data\naugmentation methods are well established in machine learning for creating new,\nsynthetic data without human assistance. We find that many of these methods are\napplicable to the task of business process information extraction, improving\nthe accuracy of extraction. Our study shows, that data augmentation is an\nimportant component in enabling machine learning methods for the task of\nbusiness process model generation from natural language text, where currently\nmostly rule-based systems are still state of the art. Simple data augmentation\ntechniques improved the $F_1$ score of mention extraction by 2.9 percentage\npoints, and the $F_1$ of relation extraction by $4.5$. To better understand how\ndata augmentation alters human annotated texts, we analyze the resulting text,\nvisualizing and discussing the properties of augmented textual data.\n  We make all code and experiments results publicly available.\n","authors":["Julian Neuberger","Leonie Doll","Benedict Engelmann","Lars Ackermann","Stefan Jablonski"],"pdf_url":"https://arxiv.org/pdf/2404.07501v1.pdf","comment":"Accepted at BPMDS 2024 (https://sites.google.com/view/bpmds/), to be\n  printed"},{"id":"http://arxiv.org/abs/2404.07498v1","updated":"2024-04-11T06:22:56Z","published":"2024-04-11T06:22:56Z","title":"Interactive Prompt Debugging with Sequence Salience","summary":"  We present Sequence Salience, a visual tool for interactive prompt debugging\nwith input salience methods. Sequence Salience builds on widely used salience\nmethods for text classification and single-token prediction, and extends this\nto a system tailored for debugging complex LLM prompts. Our system is\nwell-suited for long texts, and expands on previous work by 1) providing\ncontrollable aggregation of token-level salience to the word, sentence, or\nparagraph level, making salience over long inputs tractable; and 2) supporting\nrapid iteration where practitioners can act on salience results, refine\nprompts, and run salience on the new output. We include case studies showing\nhow Sequence Salience can help practitioners work with several complex\nprompting strategies, including few-shot, chain-of-thought, and constitutional\nprinciples. Sequence Salience is built on the Learning Interpretability Tool,\nan open-source platform for ML model visualizations, and code, notebooks, and\ntutorials are available at http://goo.gle/sequence-salience.\n","authors":["Ian Tenney","Ryan Mullins","Bin Du","Shree Pandya","Minsuk Kahng","Lucas Dixon"],"pdf_url":"https://arxiv.org/pdf/2404.07498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07135v2","updated":"2024-04-11T05:56:39Z","published":"2024-04-10T16:12:50Z","title":"Towards Robustness of Text-to-Visualization Translation against Lexical\n  and Phrasal Variability","summary":"  Text-to-Vis is an emerging task in the natural language processing (NLP) area\nthat aims to automatically generate data visualizations from natural language\nquestions (NLQs). Despite their progress, existing text-to-vis models often\nheavily rely on lexical matching between words in the questions and tokens in\ndata schemas. This overreliance on lexical matching may lead to a diminished\nlevel of model robustness against input variations. In this study, we\nthoroughly examine the robustness of current text-to-vis models, an area that\nhas not previously been explored. In particular, we construct the first\nrobustness dataset nvBench-Rob, which contains diverse lexical and phrasal\nvariations based on the original text-to-vis benchmark nvBench. Then, we found\nthat the performance of existing text-to-vis models on this new dataset\ndramatically drops, implying that these methods exhibit inadequate robustness\noverall. Finally, we propose a novel framework based on Retrieval-Augmented\nGeneration (RAG) technique, named GRED, specifically designed to address input\nperturbations in these two variants. The framework consists of three parts:\nNLQ-Retrieval Generator, Visualization Query-Retrieval Retuner and\nAnnotation-based Debugger, which are used to tackle the challenges posed by\nnatural language variants, programming style differences and data schema\nvariants, respectively. Extensive experimental evaluations show that, compared\nto the state-of-the-art model RGVisNet in the Text-to-Vis field, GRED performs\nbetter in terms of model robustness, with a 32% increase in accuracy on the\nproposed nvBench-Rob dataset.\n","authors":["Jinwei Lu","Yuanfeng Song","Haodi Zhang","Chen Zhang","Raymond Chi-Wing Wong"],"pdf_url":"https://arxiv.org/pdf/2404.07135v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.05494v3","updated":"2024-04-11T05:25:17Z","published":"2023-09-11T14:36:16Z","title":"CrisisTransformers: Pre-trained language models and sentence encoders\n  for crisis-related social media texts","summary":"  Social media platforms play an essential role in crisis communication, but\nanalyzing crisis-related social media texts is challenging due to their\ninformal nature. Transformer-based pre-trained models like BERT and RoBERTa\nhave shown success in various NLP tasks, but they are not tailored for\ncrisis-related texts. Furthermore, general-purpose sentence encoders are used\nto generate sentence embeddings, regardless of the textual complexities in\ncrisis-related texts. Advances in applications like text classification,\nsemantic search, and clustering contribute to the effective processing of\ncrisis-related texts, which is essential for emergency responders to gain a\ncomprehensive view of a crisis event, whether historical or real-time. To\naddress these gaps in crisis informatics literature, this study introduces\nCrisisTransformers, an ensemble of pre-trained language models and sentence\nencoders trained on an extensive corpus of over 15 billion word tokens from\ntweets associated with more than 30 crisis events, including disease outbreaks,\nnatural disasters, conflicts, and other critical incidents. We evaluate\nexisting models and CrisisTransformers on 18 crisis-specific public datasets.\nOur pre-trained models outperform strong baselines across all datasets in\nclassification tasks, and our best-performing sentence encoder improves the\nstate-of-the-art by 17.43% in sentence encoding tasks. Additionally, we\ninvestigate the impact of model initialization on convergence and evaluate the\nsignificance of domain-specific models in generating semantically meaningful\nsentence embeddings. The models are publicly available at:\nhttps://huggingface.co/crisistransformers\n","authors":["Rabindra Lamsal","Maria Rodriguez Read","Shanika Karunasekera"],"pdf_url":"https://arxiv.org/pdf/2309.05494v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05449v2","updated":"2024-04-11T05:21:00Z","published":"2024-04-08T12:31:23Z","title":"RoT: Enhancing Large Language Models with Reflection on Search Trees","summary":"  Large language models (LLMs) have demonstrated impressive capability in\nreasoning and planning when integrated with tree-search-based prompting\nmethods. However, since these methods ignore the previous search experiences,\nthey often make the same mistakes in the search process. To address this issue,\nwe introduce Reflection on search Trees (RoT), an LLM reflection framework\ndesigned to improve the performance of tree-search-based prompting methods. It\nuses a strong LLM to summarize guidelines from previous tree search experiences\nto enhance the ability of a weak LLM. The guidelines are instructions about\nsolving this task through tree search which can prevent the weak LLMs from\nmaking similar mistakes in the past search process. In addition, we proposed a\nnovel state selection method, which identifies the critical information from\nhistorical search processes to help RoT generate more specific and meaningful\nguidelines. In our extensive experiments, we find that RoT significantly\nimproves the performance of LLMs in reasoning or planning tasks with various\ntree-search-based prompting methods (e.g., BFS and MCTS). Non-tree-search-based\nprompting methods such as Chain-of-Thought (CoT) can also benefit from RoT\nguidelines since RoT can provide task-specific knowledge collected from the\nsearch experience.\n","authors":["Wenyang Hui","Chengyue Jiang","Yan Wang","Kewei Tu"],"pdf_url":"https://arxiv.org/pdf/2404.05449v2.pdf","comment":"9 pages main"},{"id":"http://arxiv.org/abs/2404.00511v3","updated":"2024-04-11T05:14:35Z","published":"2024-03-31T01:16:02Z","title":"MIPS at SemEval-2024 Task 3: Multimodal Emotion-Cause Pair Extraction in\n  Conversations with Multimodal Language Models","summary":"  This paper presents our winning submission to Subtask 2 of SemEval 2024 Task\n3 on multimodal emotion cause analysis in conversations. We propose a novel\nMultimodal Emotion Recognition and Multimodal Emotion Cause Extraction\n(MER-MCE) framework that integrates text, audio, and visual modalities using\nspecialized emotion encoders. Our approach sets itself apart from\ntop-performing teams by leveraging modality-specific features for enhanced\nemotion understanding and causality inference. Experimental evaluation\ndemonstrates the advantages of our multimodal approach, with our submission\nachieving a competitive weighted F1 score of 0.3435, ranking third with a\nmargin of only 0.0339 behind the 1st team and 0.0025 behind the 2nd team.\nProject: https://github.com/MIPS-COLT/MER-MCE.git\n","authors":["Zebang Cheng","Fuqiang Niu","Yuxiang Lin","Zhi-Qi Cheng","Bowen Zhang","Xiaojiang Peng"],"pdf_url":"https://arxiv.org/pdf/2404.00511v3.pdf","comment":"Ranked 3rd in SemEval '24 Task 3 with F1 of 0.3435, close to 1st &\n  2nd by 0.0339 & 0.0025"},{"id":"http://arxiv.org/abs/2404.07475v1","updated":"2024-04-11T05:09:03Z","published":"2024-04-11T05:09:03Z","title":"Laissez-Faire Harms: Algorithmic Biases in Generative Language Models","summary":"  The rapid deployment of generative language models (LMs) has raised concerns\nabout social biases affecting the well-being of diverse consumers. The extant\nliterature on generative LMs has primarily examined bias via explicit identity\nprompting. However, prior research on bias in earlier language-based technology\nplatforms, including search engines, has shown that discrimination can occur\neven when identity terms are not specified explicitly. Studies of bias in LM\nresponses to open-ended prompts (where identity classifications are left\nunspecified) are lacking and have not yet been grounded in end-consumer harms.\nHere, we advance studies of generative LM bias by considering a broader set of\nnatural use cases via open-ended prompting. In this \"laissez-faire\" setting, we\nfind that synthetically generated texts from five of the most pervasive LMs\n(ChatGPT3.5, ChatGPT4, Claude2.0, Llama2, and PaLM2) perpetuate harms of\nomission, subordination, and stereotyping for minoritized individuals with\nintersectional race, gender, and/or sexual orientation identities (AI/AN,\nAsian, Black, Latine, MENA, NH/PI, Female, Non-binary, Queer). We find\nwidespread evidence of bias to an extent that such individuals are hundreds to\nthousands of times more likely to encounter LM-generated outputs that portray\ntheir identities in a subordinated manner compared to representative or\nempowering portrayals. We also document a prevalence of stereotypes (e.g.\nperpetual foreigner) in LM-generated outputs that are known to trigger\npsychological harms that disproportionately affect minoritized individuals.\nThese include stereotype threat, which leads to impaired cognitive performance\nand increased negative self-perception. Our findings highlight the urgent need\nto protect consumers from discriminatory harms caused by language models and\ninvest in critical AI education programs tailored towards empowering diverse\nconsumers.\n","authors":["Evan Shieh","Faye-Marie Vassel","Cassidy Sugimoto","Thema Monroe-White"],"pdf_url":"https://arxiv.org/pdf/2404.07475v1.pdf","comment":"16 pages (44 if including supplementals), 4 figures (20 if including\n  supplementals)"},{"id":"http://arxiv.org/abs/2404.07471v1","updated":"2024-04-11T04:24:48Z","published":"2024-04-11T04:24:48Z","title":"Structure-aware Fine-tuning for Code Pre-trained Models","summary":"  Over the past few years, we have witnessed remarkable advancements in Code\nPre-trained Models (CodePTMs). These models achieved excellent representation\ncapabilities by designing structure-based pre-training tasks for code. However,\nhow to enhance the absorption of structural knowledge when fine-tuning CodePTMs\nstill remains a significant challenge. To fill this gap, in this paper, we\npresent Structure-aware Fine-tuning (SAT), a novel structure-enhanced and\nplug-and-play fine-tuning method for CodePTMs. We first propose a structure\nloss to quantify the difference between the information learned by CodePTMs and\nthe knowledge extracted from code structure. Specifically, we use the attention\nscores extracted from Transformer layer as the learned structural information,\nand the shortest path length between leaves in abstract syntax trees as the\nstructural knowledge. Subsequently, multi-task learning is introduced to\nimprove the performance of fine-tuning. Experiments conducted on four\npre-trained models and two generation tasks demonstrate the effectiveness of\nour proposed method as a plug-and-play solution. Furthermore, we observed that\nSAT can benefit CodePTMs more with limited training data.\n","authors":["Jiayi Wu","Renyu Zhu","Nuo Chen","Qiushi Sun","Xiang Li","Ming Gao"],"pdf_url":"https://arxiv.org/pdf/2404.07471v1.pdf","comment":"Accepted by COLING 2024"},{"id":"http://arxiv.org/abs/2404.07470v1","updated":"2024-04-11T04:22:15Z","published":"2024-04-11T04:22:15Z","title":"Scalable Language Model with Generalized Continual Learning","summary":"  Continual learning has gained increasing importance as it facilitates the\nacquisition and refinement of scalable knowledge and skills in language models.\nHowever, existing methods typically encounter strict limitations and challenges\nin real-world scenarios, such as reliance on experience replay, optimization\nconstraints, and inference task-ID. In this study, we introduce the Scalable\nLanguage Model (SLM) to overcome these limitations within a more challenging\nand generalized setting, representing a significant advancement toward\npractical applications for continual learning. Specifically, we propose the\nJoint Adaptive Re-Parameterization (JARe), integrated with Dynamic Task-related\nKnowledge Retrieval (DTKR), to enable adaptive adjustment of language models\nbased on specific downstream tasks. This approach leverages the task\ndistribution within the vector space, aiming to achieve a smooth and effortless\ncontinual learning process. Our method demonstrates state-of-the-art\nperformance on diverse backbones and benchmarks, achieving effective continual\nlearning in both full-set and few-shot scenarios with minimal forgetting.\nMoreover, while prior research primarily focused on a single task type such as\nclassification, our study goes beyond, with the large language model, i.e.,\nLLaMA-2, to explore the effects across diverse domains and task types, such\nthat a single language model can be decently scaled to broader applications.\n","authors":["Bohao Peng","Zhuotao Tian","Shu Liu","Mingchang Yang","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2404.07470v1.pdf","comment":"The Twelfth International Conference on Learning Representations"},{"id":"http://arxiv.org/abs/2404.07461v1","updated":"2024-04-11T03:51:29Z","published":"2024-04-11T03:51:29Z","title":"\"Confidently Nonsensical?'': A Critical Survey on the Perspectives and\n  Challenges of 'Hallucinations' in NLP","summary":"  We investigate how hallucination in large language models (LLM) is\ncharacterized in peer-reviewed literature using a critical examination of 103\npublications across NLP research. Through a comprehensive review of\nsociological and technological literature, we identify a lack of agreement with\nthe term `hallucination.' Additionally, we conduct a survey with 171\npractitioners from the field of NLP and AI to capture varying perspectives on\nhallucination. Our analysis underscores the necessity for explicit definitions\nand frameworks outlining hallucination within NLP, highlighting potential\nchallenges, and our survey inputs provide a thematic understanding of the\ninfluence and ramifications of hallucination in society.\n","authors":["Pranav Narayanan Venkit","Tatiana Chakravorti","Vipul Gupta","Heidi Biggs","Mukund Srinath","Koustava Goswami","Sarah Rajtmajer","Shomir Wilson"],"pdf_url":"https://arxiv.org/pdf/2404.07461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11193v8","updated":"2024-04-11T03:29:20Z","published":"2023-12-18T13:40:16Z","title":"Training With \"Paraphrasing the Original Text'' Improves Long-Context\n  Performance","summary":"  As Large Language Models (LLMs) continue to evolve, more are being designed\nto handle long-context inputs. Despite this advancement, many models face\nchallenges in achieving high precision on long-context tasks, often showing a\n``lost in the middle'' issue. This paper identifies the root of these issues as\na deficiency in retrieval capabilities, exacerbated by the sparsity of key\ninformation in long contexts. To tackle this challenge, we introduce a novel\napproach called ``Paraphrasing the Original Text'', aimed at augmenting LLMs'\nproficiency in extracting information from long context. This enhancement is\nachieved through a specialized supervised fine-tuning stage that incorporates\nparaphrasing information into training samples, thereby improving the model's\nretrieval capabilities for long-context scenarios. Testing on datasets like\nLongBench and NaturalQuestions Multi-document QA dataset, our method\ndemonstrated significant improvements in managing long-context tasks,\neffectively addressing the ``lost in the middle'' dilemma. Specifically, we\nobserved an average performance increase of 6.4\\% and 5.9\\% across these\ndatasets, respectively. Moreover, our approach is efficient, requiring minimal\noverhead with fine-tuning needed on just 19k samples. The model and training\ndata have been made available on\nHuggingFace(https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k).\n","authors":["Yijiong Yu"],"pdf_url":"https://arxiv.org/pdf/2312.11193v8.pdf","comment":"Chinese version of this paper can be downloaded from\n  (https://cloud.tsinghua.edu.cn/d/5894ec4442e54a6aac96/)"},{"id":"http://arxiv.org/abs/2404.07448v1","updated":"2024-04-11T03:08:53Z","published":"2024-04-11T03:08:53Z","title":"Transferable and Principled Efficiency for Open-Vocabulary Segmentation","summary":"  Recent success of pre-trained foundation vision-language models makes\nOpen-Vocabulary Segmentation (OVS) possible. Despite the promising performance,\nthis approach introduces heavy computational overheads for two challenges: 1)\nlarge model sizes of the backbone; 2) expensive costs during the fine-tuning.\nThese challenges hinder this OVS strategy from being widely applicable and\naffordable in real-world scenarios. Although traditional methods such as model\ncompression and efficient fine-tuning can address these challenges, they often\nrely on heuristics. This means that their solutions cannot be easily\ntransferred and necessitate re-training on different models, which comes at a\ncost. In the context of efficient OVS, we target achieving performance that is\ncomparable to or even better than prior OVS works based on large\nvision-language foundation models, by utilizing smaller models that incur lower\ntraining costs. The core strategy is to make our efficiency principled and thus\nseamlessly transferable from one OVS framework to others without further\ncustomization. Comprehensive experiments on diverse OVS benchmarks demonstrate\nour superior trade-off between segmentation accuracy and computation costs over\nprevious works. Our code is available on https://github.com/Xujxyang/OpenTrans\n","authors":["Jingxuan Xu","Wuyang Chen","Yao Zhao","Yunchao Wei"],"pdf_url":"https://arxiv.org/pdf/2404.07448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07439v1","updated":"2024-04-11T02:44:13Z","published":"2024-04-11T02:44:13Z","title":"Behavior Trees Enable Structured Programming of Language Model Agents","summary":"  Language models trained on internet-scale data sets have shown an impressive\nability to solve problems in Natural Language Processing and Computer Vision.\nHowever, experience is showing that these models are frequently brittle in\nunexpected ways, and require significant scaffolding to ensure that they\noperate correctly in the larger systems that comprise \"language-model agents.\"\nIn this paper, we argue that behavior trees provide a unifying framework for\ncombining language models with classical AI and traditional programming. We\nintroduce Dendron, a Python library for programming language model agents using\nbehavior trees. We demonstrate the approach embodied by Dendron in three case\nstudies: building a chat agent, a camera-based infrastructure inspection agent\nfor use on a mobile robot or vehicle, and an agent that has been built to\nsatisfy safety constraints that it did not receive through instruction tuning\nor RLHF.\n","authors":["Richard Kelley"],"pdf_url":"https://arxiv.org/pdf/2404.07439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07108v2","updated":"2024-04-11T02:36:27Z","published":"2024-04-10T15:46:08Z","title":"From Model-centered to Human-Centered: Revision Distance as a Metric for\n  Text Evaluation in LLMs-based Applications","summary":"  Evaluating large language models (LLMs) is fundamental, particularly in the\ncontext of practical applications. Conventional evaluation methods, typically\ndesigned primarily for LLM development, yield numerical scores that ignore the\nuser experience. Therefore, our study shifts the focus from model-centered to\nhuman-centered evaluation in the context of AI-powered writing assistance\napplications. Our proposed metric, termed ``Revision Distance,'' utilizes LLMs\nto suggest revision edits that mimic the human writing process. It is\ndetermined by counting the revision edits generated by LLMs. Benefiting from\nthe generated revision edit details, our metric can provide a self-explained\ntext evaluation result in a human-understandable manner beyond the\ncontext-independent score. Our results show that for the easy-writing task,\n``Revision Distance'' is consistent with established metrics (ROUGE,\nBert-score, and GPT-score), but offers more insightful, detailed feedback and\nbetter distinguishes between texts. Moreover, in the context of challenging\nacademic writing tasks, our metric still delivers reliable evaluations where\nother metrics tend to struggle. Furthermore, our metric also holds significant\npotential for scenarios lacking reference texts.\n","authors":["Yongqiang Ma","Lizhi Qing","Jiawei Liu","Yangyang Kang","Yue Zhang","Wei Lu","Xiaozhong Liu","Qikai Cheng"],"pdf_url":"https://arxiv.org/pdf/2404.07108v2.pdf","comment":"9 pages, 2 figures, under review"},{"id":"http://arxiv.org/abs/2310.06694v2","updated":"2024-04-11T01:18:06Z","published":"2023-10-10T15:13:30Z","title":"Sheared LLaMA: Accelerating Language Model Pre-training via Structured\n  Pruning","summary":"  The popularity of LLaMA (Touvron et al., 2023a;b) and other recently emerged\nmoderate-sized large language models (LLMs) highlights the potential of\nbuilding smaller yet powerful LLMs. Regardless, the cost of training such\nmodels from scratch on trillions of tokens remains high. In this work, we study\nstructured pruning as an effective means to develop smaller LLMs from\npre-trained, larger models. Our approach employs two key techniques: (1)\ntargeted structured pruning, which prunes a larger model to a specified target\nshape by removing layers, heads, and intermediate and hidden dimensions in an\nend-to-end manner, and (2) dynamic batch loading, which dynamically updates the\ncomposition of sampled data in each training batch based on varying losses\nacross different domains. We demonstrate the efficacy of our approach by\npresenting the Sheared-LLaMA series, pruning the LLaMA2-7B model down to 1.3B\nand 2.7B parameters. Sheared-LLaMA models outperform state-of-the-art\nopen-source models of equivalent sizes, such as Pythia, INCITE, OpenLLaMA and\nthe concurrent TinyLlama models, on a wide range of downstream and instruction\ntuning evaluations, while requiring only 3% of compute compared to training\nsuch models from scratch. This work provides compelling evidence that\nleveraging existing LLMs with structured pruning is a far more cost-effective\napproach for building competitive small-scale LLMs\n","authors":["Mengzhou Xia","Tianyu Gao","Zhiyuan Zeng","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2310.06694v2.pdf","comment":"The code and models are available at\n  https://github.com/princeton-nlp/LLM-Shearing"},{"id":"http://arxiv.org/abs/2404.07413v1","updated":"2024-04-11T00:52:39Z","published":"2024-04-11T00:52:39Z","title":"JetMoE: Reaching Llama2 Performance with 0.1M Dollars","summary":"  Large Language Models (LLMs) have achieved remarkable results, but their\nincreasing resource demand has become a major obstacle to the development of\npowerful and accessible super-human intelligence. This report introduces\nJetMoE-8B, a new LLM trained with less than $0.1 million, using 1.25T tokens\nfrom carefully mixed open-source corpora and 30,000 H100 GPU hours. Despite its\nlow cost, the JetMoE-8B demonstrates impressive performance, with JetMoE-8B\noutperforming the Llama2-7B model and JetMoE-8B-Chat surpassing the\nLlama2-13B-Chat model. These results suggest that LLM training can be much more\ncost-effective than generally thought. JetMoE-8B is based on an efficient\nSparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention\nand feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B\nto have 8B parameters while only activating 2B for each input token, reducing\ninference computation by about 70% compared to Llama2-7B. Moreover, JetMoE-8B\nis highly open and academia-friendly, using only public datasets and training\ncode. All training parameters and data mixtures have been detailed in this\nreport to facilitate future efforts in the development of open foundation\nmodels. This transparency aims to encourage collaboration and further\nadvancements in the field of accessible and efficient LLMs. The model weights\nare publicly available at https://github.com/myshell-ai/JetMoE.\n","authors":["Yikang Shen","Zhen Guo","Tianle Cai","Zengyi Qin"],"pdf_url":"https://arxiv.org/pdf/2404.07413v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06001v2","updated":"2024-04-11T00:17:18Z","published":"2024-04-09T04:11:25Z","title":"Privacy Preserving Prompt Engineering: A Survey","summary":"  Pre-trained language models (PLMs) have demonstrated significant proficiency\nin solving a wide range of general natural language processing (NLP) tasks.\nResearchers have observed a direct correlation between the performance of these\nmodels and their sizes. As a result, the sizes of these models have notably\nexpanded in recent years, persuading researchers to adopt the term large\nlanguage models (LLMs) to characterize the larger-sized PLMs. The size\nexpansion comes with a distinct capability called in-context learning (ICL),\nwhich represents a special form of prompting and allows the models to be\nutilized through the presentation of demonstration examples without\nmodifications to the model parameters. Although interesting, privacy concerns\nhave become a major obstacle in its widespread usage. Multiple studies have\nexamined the privacy risks linked to ICL and prompting in general, and have\ndevised techniques to alleviate these risks. Thus, there is a necessity to\norganize these mitigation techniques for the benefit of the community. This\nsurvey provides a systematic overview of the privacy protection methods\nemployed during ICL and prompting in general. We review, analyze, and compare\ndifferent methods under this paradigm. Furthermore, we provide a summary of the\nresources accessible for the development of these frameworks. Finally, we\ndiscuss the limitations of these frameworks and offer a detailed examination of\nthe promising areas that necessitate further exploration.\n","authors":["Kennedy Edemacu","Xintao Wu"],"pdf_url":"https://arxiv.org/pdf/2404.06001v2.pdf","comment":"23 pages, 8 figures"},{"id":"http://arxiv.org/abs/2404.06654v2","updated":"2024-04-11T23:53:59Z","published":"2024-04-09T23:41:27Z","title":"RULER: What's the Real Context Size of Your Long-Context Language\n  Models?","summary":"  The needle-in-a-haystack (NIAH) test, which examines the ability to retrieve\na piece of information (the \"needle\") from long distractor texts (the\n\"haystack\"), has been widely adopted to evaluate long-context language models\n(LMs). However, this simple retrieval-based test is indicative of only a\nsuperficial form of long-context understanding. To provide a more comprehensive\nevaluation of long-context LMs, we create a new synthetic benchmark RULER with\nflexible configurations for customized sequence length and task complexity.\nRULER expands upon the vanilla NIAH test to encompass variations with diverse\ntypes and quantities of needles. Moreover, RULER introduces new task categories\nmulti-hop tracing and aggregation to test behaviors beyond searching from\ncontext. We evaluate ten long-context LMs with 13 representative tasks in\nRULER. Despite achieving nearly perfect accuracy in the vanilla NIAH test, all\nmodels exhibit large performance drops as the context length increases. While\nthese models all claim context sizes of 32K tokens or greater, only four models\n(GPT-4, Command-R, Yi-34B, and Mixtral) can maintain satisfactory performance\nat the length of 32K. Our analysis of Yi-34B, which supports context length of\n200K, reveals large room for improvement as we increase input length and task\ncomplexity. We open source RULER to spur comprehensive evaluation of\nlong-context LMs.\n","authors":["Cheng-Ping Hsieh","Simeng Sun","Samuel Kriman","Shantanu Acharya","Dima Rekesh","Fei Jia","Yang Zhang","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2404.06654v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08156v1","updated":"2024-04-11T23:09:18Z","published":"2024-04-11T23:09:18Z","title":"Multimodal Contextual Dialogue Breakdown Detection for Conversational AI\n  Models","summary":"  Detecting dialogue breakdown in real time is critical for conversational AI\nsystems, because it enables taking corrective action to successfully complete a\ntask. In spoken dialog systems, this breakdown can be caused by a variety of\nunexpected situations including high levels of background noise, causing STT\nmistranscriptions, or unexpected user flows. In particular, industry settings\nlike healthcare, require high precision and high flexibility to navigate\ndifferently based on the conversation history and dialogue states. This makes\nit both more challenging and more critical to accurately detect dialog\nbreakdown. To accurately detect breakdown, we found it requires processing\naudio inputs along with downstream NLP model inferences on transcribed text in\nreal time. In this paper, we introduce a Multimodal Contextual Dialogue\nBreakdown (MultConDB) model. This model significantly outperforms other known\nbest models by achieving an F1 of 69.27.\n","authors":["Md Messal Monem Miah","Ulie Schnaithmann","Arushi Raghuvanshi","Youngseo Son"],"pdf_url":"https://arxiv.org/pdf/2404.08156v1.pdf","comment":"Published in NAACL 2024 Industry Track"},{"id":"http://arxiv.org/abs/2404.08155v1","updated":"2024-04-11T22:47:50Z","published":"2024-04-11T22:47:50Z","title":"Graph Integrated Language Transformers for Next Action Prediction in\n  Complex Phone Calls","summary":"  Current Conversational AI systems employ different machine learning\npipelines, as well as external knowledge sources and business logic to predict\nthe next action. Maintaining various components in dialogue managers' pipeline\nadds complexity in expansion and updates, increases processing time, and causes\nadditive noise through the pipeline that can lead to incorrect next action\nprediction. This paper investigates graph integration into language\ntransformers to improve understanding the relationships between humans'\nutterances, previous, and next actions without the dependency on external\nsources or components. Experimental analyses on real calls indicate that the\nproposed Graph Integrated Language Transformer models can achieve higher\nperformance compared to other production level conversational AI systems in\ndriving interactive calls with human users in real-world settings.\n","authors":["Amin Hosseiny Marani","Ulie Schnaithmann","Youngseo Son","Akil Iyer","Manas Paldhe","Arushi Raghuvanshi"],"pdf_url":"https://arxiv.org/pdf/2404.08155v1.pdf","comment":"Published in NAACL 2024 Industry Track"},{"id":"http://arxiv.org/abs/2307.16382v2","updated":"2024-04-11T22:21:09Z","published":"2023-07-31T03:17:51Z","title":"Does fine-tuning GPT-3 with the OpenAI API leak personally-identifiable\n  information?","summary":"  Machine learning practitioners often fine-tune generative pre-trained models\nlike GPT-3 to improve model performance at specific tasks. Previous works,\nhowever, suggest that fine-tuned machine learning models memorize and emit\nsensitive information from the original fine-tuning dataset. Companies such as\nOpenAI offer fine-tuning services for their models, but no prior work has\nconducted a memorization attack on any closed-source models. In this work, we\nsimulate a privacy attack on GPT-3 using OpenAI's fine-tuning API. Our\nobjective is to determine if personally identifiable information (PII) can be\nextracted from this model. We (1) explore the use of naive prompting methods on\na GPT-3 fine-tuned classification model, and (2) we design a practical word\ngeneration task called Autocomplete to investigate the extent of PII\nmemorization in fine-tuned GPT-3 within a real-world context. Our findings\nreveal that fine-tuning GPT3 for both tasks led to the model memorizing and\ndisclosing critical personally identifiable information (PII) obtained from the\nunderlying fine-tuning dataset. To encourage further research, we have made our\ncodes and datasets publicly available on GitHub at:\nhttps://github.com/albertsun1/gpt3-pii-attacks\n","authors":["Albert Yu Sun","Eliott Zemour","Arushi Saxena","Udith Vaidyanathan","Eric Lin","Christian Lau","Vaikkunth Mugunthan"],"pdf_url":"https://arxiv.org/pdf/2307.16382v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08148v1","updated":"2024-04-11T22:19:50Z","published":"2024-04-11T22:19:50Z","title":"Distilling Algorithmic Reasoning from LLMs via Explaining Solution\n  Programs","summary":"  Distilling explicit chain-of-thought reasoning paths has emerged as an\neffective method for improving the reasoning abilities of large language models\n(LLMs) across various tasks. However, when tackling complex tasks that pose\nsignificant challenges for state-of-the-art models, this technique often\nstruggles to produce effective chains of thought that lead to correct answers.\nIn this work, we propose a novel approach to distill reasoning abilities from\nLLMs by leveraging their capacity to explain solutions. We apply our method to\nsolving competitive-level programming challenges. More specifically, we employ\nan LLM to generate explanations for a set of <problem, solution-program> pairs,\nthen use <problem, explanation> pairs to fine-tune a smaller language model,\nwhich we refer to as the Reasoner, to learn algorithmic reasoning that can\ngenerate \"how-to-solve\" hints for unseen problems. Our experiments demonstrate\nthat learning from explanations enables the Reasoner to more effectively guide\nprogram implementation by a Coder, resulting in higher solve rates than strong\nchain-of-thought baselines on competitive-level programming problems. It also\noutperforms models that learn directly from <problem, solution-program> pairs.\nWe curated an additional test set in the CodeContests format, which includes\n246 more recent problems posted after the models' knowledge cutoff.\n","authors":["Jierui Li","Raymond Mooney"],"pdf_url":"https://arxiv.org/pdf/2404.08148v1.pdf","comment":"pre-print"},{"id":"http://arxiv.org/abs/2404.08134v1","updated":"2024-04-11T21:31:02Z","published":"2024-04-11T21:31:02Z","title":"Extending Translate-Train for ColBERT-X to African Language CLIR","summary":"  This paper describes the submission runs from the HLTCOE team at the CIRAL\nCLIR tasks for African languages at FIRE 2023. Our submissions use machine\ntranslation models to translate the documents and the training passages, and\nColBERT-X as the retrieval model. Additionally, we present a set of unofficial\nruns that use an alternative training procedure with a similar training\nsetting.\n","authors":["Eugene Yang","Dawn J. Lawrie","Paul McNamee","James Mayfield"],"pdf_url":"https://arxiv.org/pdf/2404.08134v1.pdf","comment":"10 pages, 2 figures. System description paper for HLTCOE's\n  participation in CIRAL@FIRE 2023"},{"id":"http://arxiv.org/abs/2404.08118v1","updated":"2024-04-11T20:46:18Z","published":"2024-04-11T20:46:18Z","title":"HLTCOE at TREC 2023 NeuCLIR Track","summary":"  The HLTCOE team applied PLAID, an mT5 reranker, and document translation to\nthe TREC 2023 NeuCLIR track. For PLAID we included a variety of models and\ntraining techniques -- the English model released with ColBERT v2,\ntranslate-train~(TT), Translate Distill~(TD) and multilingual\ntranslate-train~(MTT). TT trains a ColBERT model with English queries and\npassages automatically translated into the document language from the MS-MARCO\nv1 collection. This results in three cross-language models for the track, one\nper language. MTT creates a single model for all three document languages by\ncombining the translations of MS-MARCO passages in all three languages into\nmixed-language batches. Thus the model learns about matching queries to\npassages simultaneously in all languages. Distillation uses scores from the mT5\nmodel over non-English translated document pairs to learn how to score\nquery-document pairs. The team submitted runs to all NeuCLIR tasks: the CLIR\nand MLIR news task as well as the technical documents task.\n","authors":["Eugene Yang","Dawn Lawrie","James Mayfield"],"pdf_url":"https://arxiv.org/pdf/2404.08118v1.pdf","comment":"6 pages. Part of TREC 2023 Proceedings"},{"id":"http://arxiv.org/abs/2404.08111v1","updated":"2024-04-11T20:25:26Z","published":"2024-04-11T20:25:26Z","title":"S3Editor: A Sparse Semantic-Disentangled Self-Training Framework for\n  Face Video Editing","summary":"  Face attribute editing plays a pivotal role in various applications. However,\nexisting methods encounter challenges in achieving high-quality results while\npreserving identity, editing faithfulness, and temporal consistency. These\nchallenges are rooted in issues related to the training pipeline, including\nlimited supervision, architecture design, and optimization strategy. In this\nwork, we introduce S3Editor, a Sparse Semantic-disentangled Self-training\nframework for face video editing. S3Editor is a generic solution that\ncomprehensively addresses these challenges with three key contributions.\nFirstly, S3Editor adopts a self-training paradigm to enhance the training\nprocess through semi-supervision. Secondly, we propose a semantic disentangled\narchitecture with a dynamic routing mechanism that accommodates diverse editing\nrequirements. Thirdly, we present a structured sparse optimization schema that\nidentifies and deactivates malicious neurons to further disentangle impacts\nfrom untarget attributes. S3Editor is model-agnostic and compatible with\nvarious editing approaches. Our extensive qualitative and quantitative results\naffirm that our approach significantly enhances identity preservation, editing\nfidelity, as well as temporal consistency.\n","authors":["Guangzhi Wang","Tianyi Chen","Kamran Ghasedi","HsiangTao Wu","Tianyu Ding","Chris Nuesmeyer","Ilya Zharkov","Mohan Kankanhalli","Luming Liang"],"pdf_url":"https://arxiv.org/pdf/2404.08111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.07430v5","updated":"2024-04-11T19:47:18Z","published":"2023-09-14T05:15:01Z","title":"Adapted Large Language Models Can Outperform Medical Experts in Clinical\n  Text Summarization","summary":"  Analyzing vast textual data and summarizing key information from electronic\nhealth records imposes a substantial burden on how clinicians allocate their\ntime. Although large language models (LLMs) have shown promise in natural\nlanguage processing (NLP), their effectiveness on a diverse range of clinical\nsummarization tasks remains unproven. In this study, we apply adaptation\nmethods to eight LLMs, spanning four distinct clinical summarization tasks:\nradiology reports, patient questions, progress notes, and doctor-patient\ndialogue. Quantitative assessments with syntactic, semantic, and conceptual NLP\nmetrics reveal trade-offs between models and adaptation methods. A clinical\nreader study with ten physicians evaluates summary completeness, correctness,\nand conciseness; in a majority of cases, summaries from our best adapted LLMs\nare either equivalent (45%) or superior (36%) compared to summaries from\nmedical experts. The ensuing safety analysis highlights challenges faced by\nboth LLMs and medical experts, as we connect errors to potential medical harm\nand categorize types of fabricated information. Our research provides evidence\nof LLMs outperforming medical experts in clinical text summarization across\nmultiple tasks. This suggests that integrating LLMs into clinical workflows\ncould alleviate documentation burden, allowing clinicians to focus more on\npatient care.\n","authors":["Dave Van Veen","Cara Van Uden","Louis Blankemeier","Jean-Benoit Delbrouck","Asad Aali","Christian Bluethgen","Anuj Pareek","Malgorzata Polacin","Eduardo Pontes Reis","Anna Seehofnerova","Nidhi Rohatgi","Poonam Hosamani","William Collins","Neera Ahuja","Curtis P. Langlotz","Jason Hom","Sergios Gatidis","John Pauly","Akshay S. Chaudhari"],"pdf_url":"https://arxiv.org/pdf/2309.07430v5.pdf","comment":"27 pages, 19 figures"},{"id":"http://arxiv.org/abs/2404.08092v1","updated":"2024-04-11T19:15:32Z","published":"2024-04-11T19:15:32Z","title":"Data-Augmentation-Based Dialectal Adaptation for LLMs","summary":"  This report presents GMUNLP's participation to the Dialect-Copa shared task\nat VarDial 2024, which focuses on evaluating the commonsense reasoning\ncapabilities of large language models (LLMs) on South Slavic micro-dialects.\nThe task aims to assess how well LLMs can handle non-standard dialectal\nvarieties, as their performance on standard languages is already\nwell-established. We propose an approach that combines the strengths of\ndifferent types of language models and leverages data augmentation techniques\nto improve task performance on three South Slavic dialects: Chakavian,\nCherkano, and Torlak. We conduct experiments using a language-family-focused\nencoder-based model (BERTi\\'c) and a domain-agnostic multilingual model\n(AYA-101). Our results demonstrate that the proposed data augmentation\ntechniques lead to substantial performance gains across all three test datasets\nin the open-source model category. This work highlights the practical utility\nof data augmentation and the potential of LLMs in handling non-standard\ndialectal varieties, contributing to the broader goal of advancing natural\nlanguage understanding in low-resource and dialectal settings.\nCode:https://github.com/ffaisal93/dialect_copa\n","authors":["Fahim Faisal","Antonios Anastasopoulos"],"pdf_url":"https://arxiv.org/pdf/2404.08092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08080v1","updated":"2024-04-11T18:35:49Z","published":"2024-04-11T18:35:49Z","title":"Variance-reduced Zeroth-Order Methods for Fine-Tuning Language Models","summary":"  Fine-tuning language models (LMs) has demonstrated success in a wide array of\ndownstream tasks. However, as LMs are scaled up, the memory requirements for\nbackpropagation become prohibitively high. Zeroth-order (ZO) optimization\nmethods can leverage memory-efficient forward passes to estimate gradients.\nMore recently, MeZO, an adaptation of ZO-SGD, has been shown to consistently\noutperform zero-shot and in-context learning when combined with suitable task\nprompts. In this work, we couple ZO methods with variance reduction techniques\nto enhance stability and convergence for inference-based LM fine-tuning. We\nintroduce Memory-Efficient Zeroth-Order Stochastic Variance-Reduced Gradient\n(MeZO-SVRG) and demonstrate its efficacy across multiple LM fine-tuning tasks,\neliminating the reliance on task-specific prompts. Evaluated across a range of\nboth masked and autoregressive LMs on benchmark GLUE tasks, MeZO-SVRG\noutperforms MeZO with up to 20% increase in test accuracies in both full- and\npartial-parameter fine-tuning settings. MeZO-SVRG benefits from reduced\ncomputation time as it often surpasses MeZO's peak test accuracy with a\n$2\\times$ reduction in GPU-hours. MeZO-SVRG significantly reduces the required\nmemory footprint compared to first-order SGD, i.e. by $2\\times$ for\nautoregressive models. Our experiments highlight that MeZO-SVRG's memory\nsavings progressively improve compared to SGD with larger batch sizes.\n","authors":["Tanmay Gautam","Youngsuk Park","Hao Zhou","Parameswaran Raman","Wooseok Ha"],"pdf_url":"https://arxiv.org/pdf/2404.08080v1.pdf","comment":"29 pages, 25 tables, 9 figures"},{"id":"http://arxiv.org/abs/2404.08078v1","updated":"2024-04-11T18:34:11Z","published":"2024-04-11T18:34:11Z","title":"SQBC: Active Learning using LLM-Generated Synthetic Data for Stance\n  Detection in Online Political Discussions","summary":"  Stance detection is an important task for many applications that analyse or\nsupport online political discussions. Common approaches include fine-tuning\ntransformer based models. However, these models require a large amount of\nlabelled data, which might not be available. In this work, we present two\ndifferent ways to leverage LLM-generated synthetic data to train and improve\nstance detection agents for online political discussions: first, we show that\naugmenting a small fine-tuning dataset with synthetic data can improve the\nperformance of the stance detection model. Second, we propose a new active\nlearning method called SQBC based on the \"Query-by-Comittee\" approach. The key\nidea is to use LLM-generated synthetic data as an oracle to identify the most\ninformative unlabelled samples, that are selected for manual labelling.\nComprehensive experiments show that both ideas can improve the stance detection\nperformance. Curiously, we observed that fine-tuning on actively selected\nsamples can exceed the performance of using the full dataset.\n","authors":["Stefan Sylvius Wagner","Maike Behrendt","Marc Ziegele","Stefan Harmeling"],"pdf_url":"https://arxiv.org/pdf/2404.08078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15744v2","updated":"2024-04-11T18:28:16Z","published":"2024-03-23T07:16:23Z","title":"On the Fragility of Active Learners","summary":"  Active learning (AL) techniques aim to maximally utilize a labeling budget by\niteratively selecting instances that are most likely to improve prediction\naccuracy. However, their benefit compared to random sampling has not been\nconsistent across various setups, e.g., different datasets, classifiers. In\nthis empirical study, we examine how a combination of different factors might\nobscure any gains from an AL technique. Focusing on text classification, we\nrigorously evaluate AL techniques over around 1000 experiments that vary wrt\nthe dataset, batch size, text representation and the classifier. We show that\nAL is only effective in a narrow set of circumstances. We also address the\nproblem of using metrics that are better aligned with real world expectations.\nThe impact of this study is in its insights for a practitioner: (a) the choice\nof text representation and classifier is as important as that of an AL\ntechnique, (b) choice of the right metric is critical in assessment of the\nlatter, and, finally, (c) reported AL results must be holistically interpreted,\naccounting for variables other than just the query strategy.\n","authors":["Abhishek Ghose","Emma Nguyen"],"pdf_url":"https://arxiv.org/pdf/2403.15744v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08066v1","updated":"2024-04-11T18:12:12Z","published":"2024-04-11T18:12:12Z","title":"MSciNLI: A Diverse Benchmark for Scientific Natural Language Inference","summary":"  The task of scientific Natural Language Inference (NLI) involves predicting\nthe semantic relation between two sentences extracted from research articles.\nThis task was recently proposed along with a new dataset called SciNLI derived\nfrom papers published in the computational linguistics domain. In this paper,\nwe aim to introduce diversity in the scientific NLI task and present MSciNLI, a\ndataset containing 132,320 sentence pairs extracted from five new scientific\ndomains. The availability of multiple domains makes it possible to study domain\nshift for scientific NLI. We establish strong baselines on MSciNLI by\nfine-tuning Pre-trained Language Models (PLMs) and prompting Large Language\nModels (LLMs). The highest Macro F1 scores of PLM and LLM baselines are 77.21%\nand 51.77%, respectively, illustrating that MSciNLI is challenging for both\ntypes of models. Furthermore, we show that domain shift degrades the\nperformance of scientific NLI models which demonstrates the diverse\ncharacteristics of different domains in our dataset. Finally, we use both\nscientific NLI datasets in an intermediate task transfer learning setting and\nshow that they can improve the performance of downstream tasks in the\nscientific domain. We make our dataset and code available on Github.\n","authors":["Mobashir Sadat","Cornelia Caragea"],"pdf_url":"https://arxiv.org/pdf/2404.08066v1.pdf","comment":"Accepted to the NAACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2310.07923v5","updated":"2024-04-11T18:03:53Z","published":"2023-10-11T22:35:18Z","title":"The Expressive Power of Transformers with Chain of Thought","summary":"  Recent theoretical work has identified surprisingly simple reasoning\nproblems, such as checking if two nodes in a graph are connected or simulating\nfinite-state machines, that are provably unsolvable by standard transformers\nthat answer immediately after reading their input. However, in practice,\ntransformers' reasoning can be improved by allowing them to use a \"chain of\nthought\" or \"scratchpad\", i.e., generate and condition on a sequence of\nintermediate tokens before answering. Motivated by this, we ask: Does such\nintermediate generation fundamentally extend the computational power of a\ndecoder-only transformer? We show that the answer is yes, but the amount of\nincrease depends crucially on the amount of intermediate generation. For\ninstance, we find that transformer decoders with a logarithmic number of\ndecoding steps (w.r.t. the input length) push the limits of standard\ntransformers only slightly, while a linear number of decoding steps, assuming\nprojected pre-norm (a slight generalization of standard pre-norm), adds a clear\nnew ability (under standard complexity conjectures): recognizing all regular\nlanguages. Our results also imply that linear steps keep transformer decoders\nwithin context-sensitive languages, and polynomial steps with generalized\npre-norm make them recognize exactly the class of polynomial-time solvable\nproblems -- the first exact characterization of a type of transformers in terms\nof standard complexity classes. Together, this provides a nuanced framework for\nunderstanding how the length of a transformer's chain of thought or scratchpad\nimpacts its reasoning power.\n","authors":["William Merrill","Ashish Sabharwal"],"pdf_url":"https://arxiv.org/pdf/2310.07923v5.pdf","comment":"9-page preprint. ICLR camera ready posted April 11"},{"id":"http://arxiv.org/abs/2404.08020v1","updated":"2024-04-11T05:53:38Z","published":"2024-04-11T05:53:38Z","title":"Augmenting Knowledge Graph Hierarchies Using Neural Transformers","summary":"  Knowledge graphs are useful tools to organize, recommend and sort data.\nHierarchies in knowledge graphs provide significant benefit in improving\nunderstanding and compartmentalization of the data within a knowledge graph.\nThis work leverages large language models to generate and augment hierarchies\nin an existing knowledge graph. For small (<100,000 node) domain-specific KGs,\nwe find that a combination of few-shot prompting with one-shot generation works\nwell, while larger KG may require cyclical generation. We present techniques\nfor augmenting hierarchies, which led to coverage increase by 98% for intents\nand 99% for colors in our knowledge graph.\n","authors":["Sanat Sharma","Mayank Poddar","Jayant Kumar","Kosta Blank","Tracy King"],"pdf_url":"https://arxiv.org/pdf/2404.08020v1.pdf","comment":"European Conference on Information Retrieval 2024"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2404.07992v1","updated":"2024-04-11T17:59:59Z","published":"2024-04-11T17:59:59Z","title":"GoMVS: Geometrically Consistent Cost Aggregation for Multi-View Stereo","summary":"  Matching cost aggregation plays a fundamental role in learning-based\nmulti-view stereo networks. However, directly aggregating adjacent costs can\nlead to suboptimal results due to local geometric inconsistency. Related\nmethods either seek selective aggregation or improve aggregated depth in the 2D\nspace, both are unable to handle geometric inconsistency in the cost volume\neffectively. In this paper, we propose GoMVS to aggregate geometrically\nconsistent costs, yielding better utilization of adjacent geometries. More\nspecifically, we correspond and propagate adjacent costs to the reference pixel\nby leveraging the local geometric smoothness in conjunction with surface\nnormals. We achieve this by the geometric consistent propagation (GCP) module.\nIt computes the correspondence from the adjacent depth hypothesis space to the\nreference depth space using surface normals, then uses the correspondence to\npropagate adjacent costs to the reference geometry, followed by a convolution\nfor aggregation. Our method achieves new state-of-the-art performance on DTU,\nTanks & Temple, and ETH3D datasets. Notably, our method ranks 1st on the Tanks\n& Temple Advanced benchmark.\n","authors":["Jiang Wu","Rui Li","Haofei Xu","Wenxun Zhao","Yu Zhu","Jinqiu Sun","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.07992v1.pdf","comment":"CVPR 2024. Project page: https://wuuu3511.github.io/gomvs/ Code:\n  https://github.com/Wuuu3511/GoMVS"},{"id":"http://arxiv.org/abs/2404.07993v1","updated":"2024-04-11T17:59:59Z","published":"2024-04-11T17:59:59Z","title":"Connecting NeRFs, Images, and Text","summary":"  Neural Radiance Fields (NeRFs) have emerged as a standard framework for\nrepresenting 3D scenes and objects, introducing a novel data type for\ninformation exchange and storage. Concurrently, significant progress has been\nmade in multimodal representation learning for text and image data. This paper\nexplores a novel research direction that aims to connect the NeRF modality with\nother modalities, similar to established methodologies for images and text. To\nthis end, we propose a simple framework that exploits pre-trained models for\nNeRF representations alongside multimodal models for text and image processing.\nOur framework learns a bidirectional mapping between NeRF embeddings and those\nobtained from corresponding images and text. This mapping unlocks several novel\nand useful applications, including NeRF zero-shot classification and NeRF\nretrieval from images or text.\n","authors":["Francesco Ballerini","Pierluigi Zama Ramirez","Roberto Mirabella","Samuele Salti","Luigi Di Stefano"],"pdf_url":"https://arxiv.org/pdf/2404.07993v1.pdf","comment":"Accepted at CVPRW-INRV 2024"},{"id":"http://arxiv.org/abs/2404.07991v1","updated":"2024-04-11T17:59:57Z","published":"2024-04-11T17:59:57Z","title":"GoMAvatar: Efficient Animatable Human Modeling from Monocular Video\n  Using Gaussians-on-Mesh","summary":"  We introduce GoMAvatar, a novel approach for real-time, memory-efficient,\nhigh-quality animatable human modeling. GoMAvatar takes as input a single\nmonocular video to create a digital avatar capable of re-articulation in new\nposes and real-time rendering from novel viewpoints, while seamlessly\nintegrating with rasterization-based graphics pipelines. Central to our method\nis the Gaussians-on-Mesh representation, a hybrid 3D model combining rendering\nquality and speed of Gaussian splatting with geometry modeling and\ncompatibility of deformable meshes. We assess GoMAvatar on ZJU-MoCap data and\nvarious YouTube videos. GoMAvatar matches or surpasses current monocular human\nmodeling algorithms in rendering quality and significantly outperforms them in\ncomputational efficiency (43 FPS) while being memory-efficient (3.63 MB per\nsubject).\n","authors":["Jing Wen","Xiaoming Zhao","Zhongzheng Ren","Alexander G. Schwing","Shenlong Wang"],"pdf_url":"https://arxiv.org/pdf/2404.07991v1.pdf","comment":"CVPR 2024; project page: https://wenj.github.io/GoMAvatar/"},{"id":"http://arxiv.org/abs/2404.07990v1","updated":"2024-04-11T17:59:56Z","published":"2024-04-11T17:59:56Z","title":"OpenBias: Open-set Bias Detection in Text-to-Image Generative Models","summary":"  Text-to-image generative models are becoming increasingly popular and\naccessible to the general public. As these models see large-scale deployments,\nit is necessary to deeply investigate their safety and fairness to not\ndisseminate and perpetuate any kind of biases. However, existing works focus on\ndetecting closed sets of biases defined a priori, limiting the studies to\nwell-known concepts. In this paper, we tackle the challenge of open-set bias\ndetection in text-to-image generative models presenting OpenBias, a new\npipeline that identifies and quantifies the severity of biases agnostically,\nwithout access to any precompiled set. OpenBias has three stages. In the first\nphase, we leverage a Large Language Model (LLM) to propose biases given a set\nof captions. Secondly, the target generative model produces images using the\nsame set of captions. Lastly, a Vision Question Answering model recognizes the\npresence and extent of the previously proposed biases. We study the behavior of\nStable Diffusion 1.5, 2, and XL emphasizing new biases, never investigated\nbefore. Via quantitative experiments, we demonstrate that OpenBias agrees with\ncurrent closed-set bias detection methods and human judgement.\n","authors":["Moreno D'Inc√†","Elia Peruzzo","Massimiliano Mancini","Dejia Xu","Vidit Goel","Xingqian Xu","Zhangyang Wang","Humphrey Shi","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2404.07990v1.pdf","comment":"CVPR 2024 Highlight - Code:\n  https://github.com/Picsart-AI-Research/OpenBias"},{"id":"http://arxiv.org/abs/2404.07989v1","updated":"2024-04-11T17:59:45Z","published":"2024-04-11T17:59:45Z","title":"Any2Point: Empowering Any-modality Large Models for Efficient 3D\n  Understanding","summary":"  Large foundation models have recently emerged as a prominent focus of\ninterest, attaining superior performance in widespread scenarios. Due to the\nscarcity of 3D data, many efforts have been made to adapt pre-trained\ntransformers from vision to 3D domains. However, such 2D-to-3D approaches are\nstill limited, due to the potential loss of spatial geometries and high\ncomputation cost. More importantly, their frameworks are mainly designed for 2D\nmodels, lacking a general any-to-3D paradigm. In this paper, we introduce\nAny2Point, a parameter-efficient method to empower any-modality large models\n(vision, language, audio) for 3D understanding. Given a frozen transformer from\nany source modality, we propose a 3D-to-any (1D or 2D) virtual projection\nstrategy that correlates the input 3D points to the original 1D or 2D positions\nwithin the source modality. This mechanism enables us to assign each 3D token\nwith a positional encoding paired with the pre-trained model, which avoids 3D\ngeometry loss caused by the true projection and better motivates the\ntransformer for 3D learning with 1D/2D positional priors. Then, within each\ntransformer block, we insert an any-to-3D guided adapter module for\nparameter-efficient fine-tuning. The adapter incorporates prior spatial\nknowledge from the source modality to guide the local feature aggregation of 3D\ntokens, compelling the semantic adaption of any-modality transformers. We\nconduct extensive experiments to showcase the effectiveness and efficiency of\nour method. Code and models are released at\nhttps://github.com/Ivan-Tang-3D/Any2Point.\n","authors":["Yiwen Tang","Jiaming Liu","Dong Wang","Zhigang Wang","Shanghang Zhang","Bin Zhao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2404.07989v1.pdf","comment":"Code and models are released at\n  https://github.com/Ivan-Tang-3D/Any2Point"},{"id":"http://arxiv.org/abs/2401.10222v2","updated":"2024-04-11T17:59:42Z","published":"2024-01-18T18:58:54Z","title":"Supervised Fine-tuning in turn Improves Visual Foundation Models","summary":"  Image-text training like CLIP has dominated the pretraining of vision\nfoundation models in recent years. Subsequent efforts have been made to\nintroduce region-level visual learning into CLIP's pretraining but face\nscalability challenges due to the lack of large-scale region-level datasets.\nDrawing inspiration from supervised fine-tuning (SFT) in natural language\nprocessing such as instruction tuning, we explore the potential of fine-grained\nSFT in enhancing the generation of vision foundation models after their\npretraining. Thus a two-stage method ViSFT (Vision SFT) is proposed to unleash\nthe fine-grained knowledge of vision foundation models. In ViSFT, the vision\nfoundation model is enhanced by performing visual joint learning on some\nin-domain tasks and then tested on out-of-domain benchmarks. With updating\nusing ViSFT on 8 V100 GPUs in less than 2 days, a vision transformer with over\n4.4B parameters shows improvements across various out-of-domain benchmarks\nincluding vision and vision-linguistic scenarios.\n","authors":["Xiaohu Jiang","Yixiao Ge","Yuying Ge","Dachuan Shi","Chun Yuan","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2401.10222v2.pdf","comment":"23 pages, 3 figures, Project page:\n  https://github.com/TencentARC/ViSFT/tree/main"},{"id":"http://arxiv.org/abs/2404.07988v1","updated":"2024-04-11T17:59:40Z","published":"2024-04-11T17:59:40Z","title":"QuasiSim: Parameterized Quasi-Physical Simulators for Dexterous\n  Manipulations Transfer","summary":"  We explore the dexterous manipulation transfer problem by designing\nsimulators. The task wishes to transfer human manipulations to dexterous robot\nhand simulations and is inherently difficult due to its intricate,\nhighly-constrained, and discontinuous dynamics and the need to control a\ndexterous hand with a DoF to accurately replicate human manipulations. Previous\napproaches that optimize in high-fidelity black-box simulators or a modified\none with relaxed constraints only demonstrate limited capabilities or are\nrestricted by insufficient simulation fidelity. We introduce parameterized\nquasi-physical simulators and a physics curriculum to overcome these\nlimitations. The key ideas are 1) balancing between fidelity and optimizability\nof the simulation via a curriculum of parameterized simulators, and 2) solving\nthe problem in each of the simulators from the curriculum, with properties\nranging from high task optimizability to high fidelity. We successfully enable\na dexterous hand to track complex and diverse manipulations in high-fidelity\nsimulated environments, boosting the success rate by 11\\%+ from the\nbest-performed baseline. The project website is available at\nhttps://meowuu7.github.io/QuasiSim/.\n","authors":["Xueyi Liu","Kangbo Lyu","Jieqiong Zhang","Tao Du","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2404.07988v1.pdf","comment":"Project website: https://meowuu7.github.io/QuasiSim/ Code:\n  https://github.com/Meowuu7/QuasiSim Hugging Face Demo:\n  https://huggingface.co/spaces/xymeow7/quasi-physical-sims"},{"id":"http://arxiv.org/abs/2404.07987v1","updated":"2024-04-11T17:59:09Z","published":"2024-04-11T17:59:09Z","title":"ControlNet++: Improving Conditional Controls with Efficient Consistency\n  Feedback","summary":"  To enhance the controllability of text-to-image diffusion models, existing\nefforts like ControlNet incorporated image-based conditional controls. In this\npaper, we reveal that existing methods still face significant challenges in\ngenerating images that align with the image conditional controls. To this end,\nwe propose ControlNet++, a novel approach that improves controllable generation\nby explicitly optimizing pixel-level cycle consistency between generated images\nand conditional controls. Specifically, for an input conditional control, we\nuse a pre-trained discriminative reward model to extract the corresponding\ncondition of the generated images, and then optimize the consistency loss\nbetween the input conditional control and extracted condition. A\nstraightforward implementation would be generating images from random noises\nand then calculating the consistency loss, but such an approach requires\nstoring gradients for multiple sampling timesteps, leading to considerable time\nand memory costs. To address this, we introduce an efficient reward strategy\nthat deliberately disturbs the input images by adding noise, and then uses the\nsingle-step denoised images for reward fine-tuning. This avoids the extensive\ncosts associated with image sampling, allowing for more efficient reward\nfine-tuning. Extensive experiments show that ControlNet++ significantly\nimproves controllability under various conditional controls. For example, it\nachieves improvements over ControlNet by 7.9% mIoU, 13.4% SSIM, and 7.6% RMSE,\nrespectively, for segmentation mask, line-art edge, and depth conditions.\n","authors":["Ming Li","Taojiannan Yang","Huafeng Kuang","Jie Wu","Zhaoning Wang","Xuefeng Xiao","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2404.07987v1.pdf","comment":"Project Page: https://liming-ai.github.io/ControlNet_Plus_Plus"},{"id":"http://arxiv.org/abs/2404.07985v1","updated":"2024-04-11T17:58:44Z","published":"2024-04-11T17:58:44Z","title":"WaveMo: Learning Wavefront Modulations to See Through Scattering","summary":"  Imaging through scattering media is a fundamental and pervasive challenge in\nfields ranging from medical diagnostics to astronomy. A promising strategy to\novercome this challenge is wavefront modulation, which induces measurement\ndiversity during image acquisition. Despite its importance, designing optimal\nwavefront modulations to image through scattering remains under-explored. This\npaper introduces a novel learning-based framework to address the gap. Our\napproach jointly optimizes wavefront modulations and a computationally\nlightweight feedforward \"proxy\" reconstruction network. This network is trained\nto recover scenes obscured by scattering, using measurements that are modified\nby these modulations. The learned modulations produced by our framework\ngeneralize effectively to unseen scattering scenarios and exhibit remarkable\nversatility. During deployment, the learned modulations can be decoupled from\nthe proxy network to augment other more computationally expensive restoration\nalgorithms. Through extensive experiments, we demonstrate our approach\nsignificantly advances the state of the art in imaging through scattering\nmedia. Our project webpage is at https://wavemo-2024.github.io/.\n","authors":["Mingyang Xie","Haiyun Guo","Brandon Y. Feng","Lingbo Jin","Ashok Veeraraghavan","Christopher A. Metzler"],"pdf_url":"https://arxiv.org/pdf/2404.07985v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07984v1","updated":"2024-04-11T17:58:11Z","published":"2024-04-11T17:58:11Z","title":"View Selection for 3D Captioning via Diffusion Ranking","summary":"  Scalable annotation approaches are crucial for constructing extensive 3D-text\ndatasets, facilitating a broader range of applications. However, existing\nmethods sometimes lead to the generation of hallucinated captions, compromising\ncaption quality. This paper explores the issue of hallucination in 3D object\ncaptioning, with a focus on Cap3D method, which renders 3D objects into 2D\nviews for captioning using pre-trained models. We pinpoint a major challenge:\ncertain rendered views of 3D objects are atypical, deviating from the training\ndata of standard image captioning models and causing hallucinations. To tackle\nthis, we present DiffuRank, a method that leverages a pre-trained text-to-3D\nmodel to assess the alignment between 3D objects and their 2D rendered views,\nwhere the view with high alignment closely represent the object's\ncharacteristics. By ranking all rendered views and feeding the top-ranked ones\ninto GPT4-Vision, we enhance the accuracy and detail of captions, enabling the\ncorrection of 200k captions in the Cap3D dataset and extending it to 1 million\ncaptions across Objaverse and Objaverse-XL datasets. Additionally, we showcase\nthe adaptability of DiffuRank by applying it to pre-trained text-to-image\nmodels for a Visual Question Answering task, where it outperforms the CLIP\nmodel.\n","authors":["Tiange Luo","Justin Johnson","Honglak Lee"],"pdf_url":"https://arxiv.org/pdf/2404.07984v1.pdf","comment":"Dataset link: https://huggingface.co/datasets/tiange/Cap3D"},{"id":"http://arxiv.org/abs/2404.07983v1","updated":"2024-04-11T17:58:06Z","published":"2024-04-11T17:58:06Z","title":"Two Effects, One Trigger: On the Modality Gap, Object Bias, and\n  Information Imbalance in Contrastive Vision-Language Representation Learning","summary":"  Contrastive vision-language models like CLIP have gained popularity for their\nversatile applicable learned representations in various downstream tasks.\nDespite their successes in some tasks, like zero-shot image recognition, they\nalso perform surprisingly poor on other tasks, like attribute detection.\nPrevious work has attributed these challenges to the modality gap, a separation\nof image and text in the shared representation space, and a bias towards\nobjects over other factors, such as attributes. In this work we investigate\nboth phenomena. We find that only a few embedding dimensions drive the modality\ngap. Further, we propose a measure for object bias and find that object bias\ndoes not lead to worse performance on other concepts, such as attributes. But\nwhat leads to the emergence of the modality gap and object bias? To answer this\nquestion we carefully designed an experimental setting which allows us to\ncontrol the amount of shared information between the modalities. This revealed\nthat the driving factor behind both, the modality gap and the object bias, is\nthe information imbalance between images and captions.\n","authors":["Simon Schrodi","David T. Hoffmann","Max Argus","Volker Fischer","Thomas Brox"],"pdf_url":"https://arxiv.org/pdf/2404.07983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07977v1","updated":"2024-04-11T17:57:19Z","published":"2024-04-11T17:57:19Z","title":"Gaga: Group Any Gaussians via 3D-aware Memory Bank","summary":"  We introduce Gaga, a framework that reconstructs and segments open-world 3D\nscenes by leveraging inconsistent 2D masks predicted by zero-shot segmentation\nmodels. Contrasted to prior 3D scene segmentation approaches that heavily rely\non video object tracking, Gaga utilizes spatial information and effectively\nassociates object masks across diverse camera poses. By eliminating the\nassumption of continuous view changes in training images, Gaga demonstrates\nrobustness to variations in camera poses, particularly beneficial for sparsely\nsampled images, ensuring precise mask label consistency. Furthermore, Gaga\naccommodates 2D segmentation masks from diverse sources and demonstrates robust\nperformance with different open-world zero-shot segmentation models, enhancing\nits versatility. Extensive qualitative and quantitative evaluations demonstrate\nthat Gaga performs favorably against state-of-the-art methods, emphasizing its\npotential for real-world applications such as scene understanding and\nmanipulation.\n","authors":["Weijie Lyu","Xueting Li","Abhijit Kundu","Yi-Hsuan Tsai","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2404.07977v1.pdf","comment":"Project Page: https://www.gaga.gallery"},{"id":"http://arxiv.org/abs/2404.07976v1","updated":"2024-04-11T17:56:40Z","published":"2024-04-11T17:56:40Z","title":"Self-supervised Dataset Distillation: A Good Compression Is All You Need","summary":"  Dataset distillation aims to compress information from a large-scale original\ndataset to a new compact dataset while striving to preserve the utmost degree\nof the original data informational essence. Previous studies have predominantly\nconcentrated on aligning the intermediate statistics between the original and\ndistilled data, such as weight trajectory, features, gradient, BatchNorm, etc.\nIn this work, we consider addressing this task through the new lens of model\ninformativeness in the compression stage on the original dataset pretraining.\nWe observe that with the prior state-of-the-art SRe$^2$L, as model sizes\nincrease, it becomes increasingly challenging for supervised pretrained models\nto recover learned information during data synthesis, as the channel-wise mean\nand variance inside the model are flatting and less informative. We further\nnotice that larger variances in BN statistics from self-supervised models\nenable larger loss signals to update the recovered data by gradients, enjoying\nmore informativeness during synthesis. Building on this observation, we\nintroduce SC-DD, a simple yet effective Self-supervised Compression framework\nfor Dataset Distillation that facilitates diverse information compression and\nrecovery compared to traditional supervised learning schemes, further reaps the\npotential of large pretrained models with enhanced capabilities. Extensive\nexperiments are conducted on CIFAR-100, Tiny-ImageNet and ImageNet-1K datasets\nto demonstrate the superiority of our proposed approach. The proposed SC-DD\noutperforms all previous state-of-the-art supervised dataset distillation\nmethods when employing larger models, such as SRe$^2$L, MTT, TESLA, DC, CAFE,\netc., by large margins under the same recovery and post-training budgets. Code\nis available at https://github.com/VILA-Lab/SRe2L/tree/main/SCDD/.\n","authors":["Muxin Zhou","Zeyuan Yin","Shitong Shao","Zhiqiang Shen"],"pdf_url":"https://arxiv.org/pdf/2404.07976v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07973v1","updated":"2024-04-11T17:56:05Z","published":"2024-04-11T17:56:05Z","title":"Ferret-v2: An Improved Baseline for Referring and Grounding with Large\n  Language Models","summary":"  While Ferret seamlessly integrates regional understanding into the Large\nLanguage Model (LLM) to facilitate its referring and grounding capability, it\nposes certain limitations: constrained by the pre-trained fixed visual encoder\nand failed to perform well on broader tasks. In this work, we unveil Ferret-v2,\na significant upgrade to Ferret, with three key designs. (1) Any resolution\ngrounding and referring: A flexible approach that effortlessly handles higher\nimage resolution, improving the model's ability to process and understand\nimages in greater detail. (2) Multi-granularity visual encoding: By integrating\nthe additional DINOv2 encoder, the model learns better and diverse underlying\ncontexts for global and fine-grained visual information. (3) A three-stage\ntraining paradigm: Besides image-caption alignment, an additional stage is\nproposed for high-resolution dense alignment before the final instruction\ntuning. Experiments show that Ferret-v2 provides substantial improvements over\nFerret and other state-of-the-art methods, thanks to its high-resolution\nscaling and fine-grained visual processing.\n","authors":["Haotian Zhang","Haoxuan You","Philipp Dufter","Bowen Zhang","Chen Chen","Hong-You Chen","Tsu-Jui Fu","William Yang Wang","Shih-Fu Chang","Zhe Gan","Yinfei Yang"],"pdf_url":"https://arxiv.org/pdf/2404.07973v1.pdf","comment":"Preprint. 14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2404.07949v1","updated":"2024-04-11T17:46:14Z","published":"2024-04-11T17:46:14Z","title":"Taming Stable Diffusion for Text to 360¬∞ Panorama Image Generation","summary":"  Generative models, e.g., Stable Diffusion, have enabled the creation of\nphotorealistic images from text prompts. Yet, the generation of 360-degree\npanorama images from text remains a challenge, particularly due to the dearth\nof paired text-panorama data and the domain gap between panorama and\nperspective images. In this paper, we introduce a novel dual-branch diffusion\nmodel named PanFusion to generate a 360-degree image from a text prompt. We\nleverage the stable diffusion model as one branch to provide prior knowledge in\nnatural image generation and register it to another panorama branch for\nholistic image generation. We propose a unique cross-attention mechanism with\nprojection awareness to minimize distortion during the collaborative denoising\nprocess. Our experiments validate that PanFusion surpasses existing methods\nand, thanks to its dual-branch structure, can integrate additional constraints\nlike room layout for customized panorama outputs. Code is available at\nhttps://chengzhag.github.io/publication/panfusion.\n","authors":["Cheng Zhang","Qianyi Wu","Camilo Cruz Gambardella","Xiaoshui Huang","Dinh Phung","Wanli Ouyang","Jianfei Cai"],"pdf_url":"https://arxiv.org/pdf/2404.07949v1.pdf","comment":"CVPR 2024. Project Page:\n  https://chengzhag.github.io/publication/panfusion Code:\n  https://github.com/chengzhag/PanFusion"},{"id":"http://arxiv.org/abs/2404.07933v1","updated":"2024-04-11T17:30:24Z","published":"2024-04-11T17:30:24Z","title":"Boosting Self-Supervision for Single-View Scene Completion via Knowledge\n  Distillation","summary":"  Inferring scene geometry from images via Structure from Motion is a\nlong-standing and fundamental problem in computer vision. While classical\napproaches and, more recently, depth map predictions only focus on the visible\nparts of a scene, the task of scene completion aims to reason about geometry\neven in occluded regions. With the popularity of neural radiance fields\n(NeRFs), implicit representations also became popular for scene completion by\npredicting so-called density fields. Unlike explicit approaches. e.g.\nvoxel-based methods, density fields also allow for accurate depth prediction\nand novel-view synthesis via image-based rendering. In this work, we propose to\nfuse the scene reconstruction from multiple images and distill this knowledge\ninto a more accurate single-view scene reconstruction. To this end, we propose\nMulti-View Behind the Scenes (MVBTS) to fuse density fields from multiple posed\nimages, trained fully self-supervised only from image data. Using knowledge\ndistillation, we use MVBTS to train a single-view scene completion network via\ndirect supervision called KDBTS. It achieves state-of-the-art performance on\noccupancy prediction, especially in occluded regions.\n","authors":["Keonhee Han","Dominik Muhle","Felix Wimbauer","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2404.07933v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07932v1","updated":"2024-04-11T17:29:56Z","published":"2024-04-11T17:29:56Z","title":"FusionMamba: Efficient Image Fusion with State Space Model","summary":"  Image fusion aims to generate a high-resolution multi/hyper-spectral image by\ncombining a high-resolution image with limited spectral information and a\nlow-resolution image with abundant spectral data. Current deep learning\n(DL)-based methods for image fusion primarily rely on CNNs or Transformers to\nextract features and merge different types of data. While CNNs are efficient,\ntheir receptive fields are limited, restricting their capacity to capture\nglobal context. Conversely, Transformers excel at learning global information\nbut are hindered by their quadratic complexity. Fortunately, recent\nadvancements in the State Space Model (SSM), particularly Mamba, offer a\npromising solution to this issue by enabling global awareness with linear\ncomplexity. However, there have been few attempts to explore the potential of\nSSM in information fusion, which is a crucial ability in domains like image\nfusion. Therefore, we propose FusionMamba, an innovative method for efficient\nimage fusion. Our contributions mainly focus on two aspects. Firstly,\nrecognizing that images from different sources possess distinct properties, we\nincorporate Mamba blocks into two U-shaped networks, presenting a novel\narchitecture that extracts spatial and spectral features in an efficient,\nindependent, and hierarchical manner. Secondly, to effectively combine spatial\nand spectral information, we extend the Mamba block to accommodate dual inputs.\nThis expansion leads to the creation of a new module called the FusionMamba\nblock, which outperforms existing fusion techniques such as concatenation and\ncross-attention. To validate FusionMamba's effectiveness, we conduct a series\nof experiments on five datasets related to three image fusion tasks. The\nquantitative and qualitative evaluation results demonstrate that our method\nachieves state-of-the-art (SOTA) performance, underscoring the superiority of\nFusionMamba.\n","authors":["Siran Peng","Xiangyu Zhu","Haoyu Deng","Zhen Lei","Liang-Jian Deng"],"pdf_url":"https://arxiv.org/pdf/2404.07932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07930v1","updated":"2024-04-11T17:27:39Z","published":"2024-04-11T17:27:39Z","title":"Parameter Hierarchical Optimization for Visible-Infrared Person\n  Re-Identification","summary":"  Visible-infrared person re-identification (VI-reID) aims at matching\ncross-modality pedestrian images captured by disjoint visible or infrared\ncameras. Existing methods alleviate the cross-modality discrepancies via\ndesigning different kinds of network architectures. Different from available\nmethods, in this paper, we propose a novel parameter optimizing paradigm,\nparameter hierarchical optimization (PHO) method, for the task of VI-ReID. It\nallows part of parameters to be directly optimized without any training, which\nnarrows the search space of parameters and makes the whole network more easier\nto be trained. Specifically, we first divide the parameters into different\ntypes, and then introduce a self-adaptive alignment strategy (SAS) to\nautomatically align the visible and infrared images through transformation.\nConsidering that features in different dimension have varying importance, we\ndevelop an auto-weighted alignment learning (AAL) module that can automatically\nweight features according to their importance. Importantly, in the alignment\nprocess of SAS and AAL, all the parameters are immediately optimized with\noptimization principles rather than training the whole network, which yields a\nbetter parameter training manner. Furthermore, we establish the cross-modality\nconsistent learning (CCL) loss to extract discriminative person representations\nwith translation consistency. We provide both theoretical justification and\nempirical evidence that our proposed PHO method outperform existing VI-reID\napproaches.\n","authors":["Zeng YU","Yunxiao Shi"],"pdf_url":"https://arxiv.org/pdf/2404.07930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07922v1","updated":"2024-04-11T17:09:28Z","published":"2024-04-11T17:09:28Z","title":"LaVy: Vietnamese Multimodal Large Language Model","summary":"  Large Language Models (LLMs) and Multimodal Large language models (MLLMs)\nhave taken the world by storm with impressive abilities in complex reasoning\nand linguistic comprehension. Meanwhile there are plethora of works related to\nVietnamese Large Language Models, the lack of high-quality resources in\nmultimodality limits the progress of Vietnamese MLLMs. In this paper, we\npioneer in address this by introducing LaVy, a state-of-the-art Vietnamese\nMLLM, and we also introduce LaVy-Bench benchmark designated for evaluating\nMLLMs's understanding on Vietnamese visual language tasks. All code and model\nweights are public at https://github.com/baochi0212/LaVy\n","authors":["Chi Tran","Huong Le Thanh"],"pdf_url":"https://arxiv.org/pdf/2404.07922v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2401.04716v3","updated":"2024-04-11T16:46:52Z","published":"2024-01-09T18:40:52Z","title":"Low-Resource Vision Challenges for Foundation Models","summary":"  Low-resource settings are well-established in natural language processing,\nwhere many languages lack sufficient data for deep learning at scale. However,\nlow-resource problems are under-explored in computer vision. In this paper, we\naddress this gap and explore the challenges of low-resource image tasks with\nvision foundation models. We first collect a benchmark of genuinely\nlow-resource image data, covering historic maps, circuit diagrams, and\nmechanical drawings. These low-resource settings all share three challenges:\ndata scarcity, fine-grained differences, and the distribution shift from\nnatural images to the specialized domain of interest. While existing foundation\nmodels have shown impressive generalizability, we find they cannot transfer\nwell to our low-resource tasks. To begin to tackle the challenges of\nlow-resource vision, we introduce one simple baseline per challenge.\nSpecifically, we i) enlarge the data space by generative models, ii) adopt the\nbest sub-kernels to encode local regions for fine-grained difference discovery\nand iii) learn attention for specialized domains. Experiments on our three\nlow-resource tasks demonstrate our proposals already provide a better baseline\nthan transfer learning, data augmentation, and fine-grained methods. This\nhighlights the unique characteristics and challenges of low-resource vision for\nfoundation models that warrant further investigation. Project page:\nhttps://xiaobai1217.github.io/Low-Resource-Vision/.\n","authors":["Yunhua Zhang","Hazel Doughty","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2401.04716v3.pdf","comment":"Accepted at CVPR2024"},{"id":"http://arxiv.org/abs/2401.08739v2","updated":"2024-04-11T16:35:22Z","published":"2024-01-16T18:55:22Z","title":"EgoGen: An Egocentric Synthetic Data Generator","summary":"  Understanding the world in first-person view is fundamental in Augmented\nReality (AR). This immersive perspective brings dramatic visual changes and\nunique challenges compared to third-person views. Synthetic data has empowered\nthird-person-view vision models, but its application to embodied egocentric\nperception tasks remains largely unexplored. A critical challenge lies in\nsimulating natural human movements and behaviors that effectively steer the\nembodied cameras to capture a faithful egocentric representation of the 3D\nworld. To address this challenge, we introduce EgoGen, a new synthetic data\ngenerator that can produce accurate and rich ground-truth training data for\negocentric perception tasks. At the heart of EgoGen is a novel human motion\nsynthesis model that directly leverages egocentric visual inputs of a virtual\nhuman to sense the 3D environment. Combined with collision-avoiding motion\nprimitives and a two-stage reinforcement learning approach, our motion\nsynthesis model offers a closed-loop solution where the embodied perception and\nmovement of the virtual human are seamlessly coupled. Compared to previous\nworks, our model eliminates the need for a pre-defined global path, and is\ndirectly applicable to dynamic environments. Combined with our easy-to-use and\nscalable data generation pipeline, we demonstrate EgoGen's efficacy in three\ntasks: mapping and localization for head-mounted cameras, egocentric camera\ntracking, and human mesh recovery from egocentric views. EgoGen will be fully\nopen-sourced, offering a practical solution for creating realistic egocentric\ntraining data and aiming to serve as a useful tool for egocentric computer\nvision research. Refer to our project page: https://ego-gen.github.io/.\n","authors":["Gen Li","Kaifeng Zhao","Siwei Zhang","Xiaozhong Lyu","Mihai Dusmanu","Yan Zhang","Marc Pollefeys","Siyu Tang"],"pdf_url":"https://arxiv.org/pdf/2401.08739v2.pdf","comment":"Accepted by CVPR 2024 (Oral). 23 pages, 17 figures. Project page:\n  https://ego-gen.github.io/"},{"id":"http://arxiv.org/abs/2404.07887v1","updated":"2024-04-11T16:17:36Z","published":"2024-04-11T16:17:36Z","title":"Context-aware Video Anomaly Detection in Long-Term Datasets","summary":"  Video anomaly detection research is generally evaluated on short, isolated\nbenchmark videos only a few minutes long. However, in real-world environments,\nsecurity cameras observe the same scene for months or years at a time, and the\nnotion of anomalous behavior critically depends on context, such as the time of\nday, day of week, or schedule of events. Here, we propose a context-aware video\nanomaly detection algorithm, Trinity, specifically targeted to these scenarios.\nTrinity is especially well-suited to crowded scenes in which individuals cannot\nbe easily tracked, and anomalies are due to speed, direction, or absence of\ngroup motion. Trinity is a contrastive learning framework that aims to learn\nalignments between context, appearance, and motion, and uses alignment quality\nto classify videos as normal or anomalous. We evaluate our algorithm on both\nconventional benchmarks and a public webcam-based dataset we collected that\nspans more than three months of activity.\n","authors":["Zhengye Yang","Richard Radke"],"pdf_url":"https://arxiv.org/pdf/2404.07887v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06564v2","updated":"2024-04-11T16:06:39Z","published":"2024-04-09T18:28:55Z","title":"MambaAD: Exploring State Space Models for Multi-class Unsupervised\n  Anomaly Detection","summary":"  Recent advancements in anomaly detection have seen the efficacy of CNN- and\ntransformer-based approaches. However, CNNs struggle with long-range\ndependencies, while transformers are burdened by quadratic computational\ncomplexity. Mamba-based models, with their superior long-range modeling and\nlinear efficiency, have garnered substantial attention. This study pioneers the\napplication of Mamba to multi-class unsupervised anomaly detection, presenting\nMambaAD, which consists of a pre-trained encoder and a Mamba decoder featuring\n(Locality-Enhanced State Space) LSS modules at multi-scales. The proposed LSS\nmodule, integrating parallel cascaded (Hybrid State Space) HSS blocks and\nmulti-kernel convolutions operations, effectively captures both long-range and\nlocal information. The HSS block, utilizing (Hybrid Scanning) HS encoders,\nencodes feature maps into five scanning methods and eight directions, thereby\nstrengthening global connections through the (State Space Model) SSM. The use\nof Hilbert scanning and eight directions significantly improves feature\nsequence modeling. Comprehensive experiments on six diverse anomaly detection\ndatasets and seven metrics demonstrate state-of-the-art performance,\nsubstantiating the method's effectiveness.\n","authors":["Haoyang He","Yuhu Bai","Jiangning Zhang","Qingdong He","Hongxu Chen","Zhenye Gan","Chengjie Wang","Xiangtai Li","Guanzhong Tian","Lei Xie"],"pdf_url":"https://arxiv.org/pdf/2404.06564v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07867v1","updated":"2024-04-11T16:01:00Z","published":"2024-04-11T16:01:00Z","title":"The Power of Properties: Uncovering the Influential Factors in Emotion\n  Classification","summary":"  Facial expression-based human emotion recognition is a critical research area\nin psychology and medicine. State-of-the-art classification performance is only\nreached by end-to-end trained neural networks. Nevertheless, such black-box\nmodels lack transparency in their decision-making processes, prompting efforts\nto ascertain the rules that underlie classifiers' decisions. Analyzing single\ninputs alone fails to expose systematic learned biases. These biases can be\ncharacterized as facial properties summarizing abstract information like age or\nmedical conditions. Therefore, understanding a model's prediction behavior\nrequires an analysis rooted in causality along such selected properties. We\ndemonstrate that up to 91.25% of classifier output behavior changes are\nstatistically significant concerning basic properties. Among those are age,\ngender, and facial symmetry. Furthermore, the medical usage of surface\nelectromyography significantly influences emotion prediction. We introduce a\nworkflow to evaluate explicit properties and their impact. These insights might\nhelp medical professionals select and apply classifiers regarding their\nspecialized data and properties.\n","authors":["Tim B√ºchner","Niklas Penzel","Orlando Guntinas-Lichius","Joachim Denzler"],"pdf_url":"https://arxiv.org/pdf/2404.07867v1.pdf","comment":"8 pages, 3 tables, 1 figure, accepted at ICPRAI 2024"},{"id":"http://arxiv.org/abs/2404.06177v2","updated":"2024-04-11T15:57:52Z","published":"2024-04-09T09:58:10Z","title":"Uncertainty-aware Evidential Fusion-based Learning for Semi-supervised\n  Medical Image Segmentation","summary":"  Although the existing uncertainty-based semi-supervised medical segmentation\nmethods have achieved excellent performance, they usually only consider a\nsingle uncertainty evaluation, which often fails to solve the problem related\nto credibility completely. Therefore, based on the framework of evidential deep\nlearning, this paper integrates the evidential predictive results in the\ncross-region of mixed and original samples to reallocate the confidence degree\nand uncertainty measure of each voxel, which is realized by emphasizing\nuncertain information of probability assignments fusion rule of traditional\nevidence theory. Furthermore, we design a voxel-level asymptotic learning\nstrategy by introducing information entropy to combine with the fused\nuncertainty measure to estimate voxel prediction more precisely. The model will\ngradually pay attention to the prediction results with high uncertainty in the\nlearning process, to learn the features that are difficult to master. The\nexperimental results on LA, Pancreas-CT, ACDC and TBAD datasets demonstrate the\nsuperior performance of our proposed method in comparison with the existing\nstate of the arts.\n","authors":["Yuanpeng He","Lijian Li"],"pdf_url":"https://arxiv.org/pdf/2404.06177v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07855v1","updated":"2024-04-11T15:51:52Z","published":"2024-04-11T15:51:52Z","title":"Resolve Domain Conflicts for Generalizable Remote Physiological\n  Measurement","summary":"  Remote photoplethysmography (rPPG) technology has become increasingly popular\ndue to its non-invasive monitoring of various physiological indicators, making\nit widely applicable in multimedia interaction, healthcare, and emotion\nanalysis. Existing rPPG methods utilize multiple datasets for training to\nenhance the generalizability of models. However, they often overlook the\nunderlying conflict issues across different datasets, such as (1) label\nconflict resulting from different phase delays between physiological signal\nlabels and face videos at the instance level, and (2) attribute conflict\nstemming from distribution shifts caused by head movements, illumination\nchanges, skin types, etc. To address this, we introduce the DOmain-HArmonious\nframework (DOHA). Specifically, we first propose a harmonious phase strategy to\neliminate uncertain phase delays and preserve the temporal variation of\nphysiological signals. Next, we design a harmonious hyperplane optimization\nthat reduces irrelevant attribute shifts and encourages the model's\noptimization towards a global solution that fits more valid scenarios. Our\nexperiments demonstrate that DOHA significantly improves the performance of\nexisting methods under multiple protocols. Our code is available at\nhttps://github.com/SWY666/rPPG-DOHA.\n","authors":["Weiyu Sun","Xinyu Zhang","Hao Lu","Ying Chen","Yun Ge","Xiaolin Huang","Jie Yuan","Yingcong Chen"],"pdf_url":"https://arxiv.org/pdf/2404.07855v1.pdf","comment":"Accepted by ACM MM 2023"},{"id":"http://arxiv.org/abs/2404.07850v1","updated":"2024-04-11T15:46:42Z","published":"2024-04-11T15:46:42Z","title":"MindBridge: A Cross-Subject Brain Decoding Framework","summary":"  Brain decoding, a pivotal field in neuroscience, aims to reconstruct stimuli\nfrom acquired brain signals, primarily utilizing functional magnetic resonance\nimaging (fMRI). Currently, brain decoding is confined to a\nper-subject-per-model paradigm, limiting its applicability to the same\nindividual for whom the decoding model is trained. This constraint stems from\nthree key challenges: 1) the inherent variability in input dimensions across\nsubjects due to differences in brain size; 2) the unique intrinsic neural\npatterns, influencing how different individuals perceive and process sensory\ninformation; 3) limited data availability for new subjects in real-world\nscenarios hampers the performance of decoding models. In this paper, we present\na novel approach, MindBridge, that achieves cross-subject brain decoding by\nemploying only one model. Our proposed framework establishes a generic paradigm\ncapable of addressing these challenges by introducing biological-inspired\naggregation function and novel cyclic fMRI reconstruction mechanism for\nsubject-invariant representation learning. Notably, by cycle reconstruction of\nfMRI, MindBridge can enable novel fMRI synthesis, which also can serve as\npseudo data augmentation. Within the framework, we also devise a novel\nreset-tuning method for adapting a pretrained model to a new subject.\nExperimental results demonstrate MindBridge's ability to reconstruct images for\nmultiple subjects, which is competitive with dedicated subject-specific models.\nFurthermore, with limited data for a new subject, we achieve a high level of\ndecoding accuracy, surpassing that of subject-specific models. This advancement\nin cross-subject brain decoding suggests promising directions for wider\napplications in neuroscience and indicates potential for more efficient\nutilization of limited fMRI data in real-world scenarios. Project page:\nhttps://littlepure2333.github.io/MindBridge\n","authors":["Shizun Wang","Songhua Liu","Zhenxiong Tan","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2404.07850v1.pdf","comment":"CVPR 2024 highlight. Code is available at\n  https://github.com/littlepure2333/MindBridge"},{"id":"http://arxiv.org/abs/2404.07847v1","updated":"2024-04-11T15:42:53Z","published":"2024-04-11T15:42:53Z","title":"Fuss-Free Network: A Simplified and Efficient Neural Network for Crowd\n  Counting","summary":"  In the field of crowd-counting research, many recent deep learning based\nmethods have demonstrated robust capabilities for accurately estimating crowd\nsizes. However, the enhancement in their performance often arises from an\nincrease in the complexity of the model structure. This paper introduces the\nFuss-Free Network (FFNet), a crowd counting deep learning model that is\ncharacterized by its simplicity and efficiency in terms of its structure. The\nmodel comprises only a backbone of a neural network and a multi-scale feature\nfusion structure.The multi-scale feature fusion structure is a simple\narchitecture consisting of three branches, each only equipped with a focus\ntransition module, and combines the features from these branches through the\nconcatenation operation.Our proposed crowd counting model is trained and\nevaluated on four widely used public datasets, and it achieves accuracy that is\ncomparable to that of existing complex models.The experimental results further\nindicate that excellent performance in crowd counting tasks can also be\nachieved by utilizing a simple, low-parameter, and computationally efficient\nneural network structure.\n","authors":["Lei Chen","Xingen Gao"],"pdf_url":"https://arxiv.org/pdf/2404.07847v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07846v1","updated":"2024-04-11T15:39:10Z","published":"2024-04-11T15:39:10Z","title":"TBSN: Transformer-Based Blind-Spot Network for Self-Supervised Image\n  Denoising","summary":"  Blind-spot networks (BSN) have been prevalent network architectures in\nself-supervised image denoising (SSID). Existing BSNs are mostly conducted with\nconvolution layers. Although transformers offer potential solutions to the\nlimitations of convolutions and have demonstrated success in various image\nrestoration tasks, their attention mechanisms may violate the blind-spot\nrequirement, thus restricting their applicability in SSID. In this paper, we\npresent a transformer-based blind-spot network (TBSN) by analyzing and\nredesigning the transformer operators that meet the blind-spot requirement.\nSpecifically, TBSN follows the architectural principles of dilated BSNs, and\nincorporates spatial as well as channel self-attention layers to enhance the\nnetwork capability. For spatial self-attention, an elaborate mask is applied to\nthe attention matrix to restrict its receptive field, thus mimicking the\ndilated convolution. For channel self-attention, we observe that it may leak\nthe blind-spot information when the channel number is greater than spatial size\nin the deep layers of multi-scale architectures. To eliminate this effect, we\ndivide the channel into several groups and perform channel attention\nseparately. Furthermore, we introduce a knowledge distillation strategy that\ndistills TBSN into smaller denoisers to improve computational efficiency while\nmaintaining performance. Extensive experiments on real-world image denoising\ndatasets show that TBSN largely extends the receptive field and exhibits\nfavorable performance against state-of-the-art SSID methods. The code and\npre-trained models will be publicly available at\nhttps://github.com/nagejacob/TBSN.\n","authors":["Junyi Li","Zhilu Zhang","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2404.07846v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.08890v3","updated":"2024-04-11T15:34:46Z","published":"2023-02-17T14:19:28Z","title":"Deep Learning for Event-based Vision: A Comprehensive Survey and\n  Benchmarks","summary":"  Event cameras are bio-inspired sensors that capture the per-pixel intensity\nchanges asynchronously and produce event streams encoding the time, pixel\nposition, and polarity (sign) of the intensity changes. Event cameras possess a\nmyriad of advantages over canonical frame-based cameras, such as high temporal\nresolution, high dynamic range, low latency, etc. Being capable of capturing\ninformation in challenging visual conditions, event cameras have the potential\nto overcome the limitations of frame-based cameras in the computer vision and\nrobotics community. In very recent years, deep learning (DL) has been brought\nto this emerging field and inspired active research endeavors in mining its\npotential. However, there is still a lack of taxonomies in DL techniques for\nevent-based vision. We first scrutinize the typical event representations with\nquality enhancement methods as they play a pivotal role as inputs to the DL\nmodels. We then provide a comprehensive survey of existing DL-based methods by\nstructurally grouping them into two major categories: 1) image/video\nreconstruction and restoration; 2) event-based scene understanding and 3D\nvision. We conduct benchmark experiments for the existing methods in some\nrepresentative research directions, i.e., image reconstruction, deblurring, and\nobject recognition, to identify some critical insights and problems. Finally,\nwe have discussions regarding the challenges and provide new perspectives for\ninspiring more research studies.\n","authors":["Xu Zheng","Yexin Liu","Yunfan Lu","Tongyan Hua","Tianbo Pan","Weiming Zhang","Dacheng Tao","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2302.08890v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06842v2","updated":"2024-04-11T15:28:36Z","published":"2024-04-10T09:14:28Z","title":"MoCha-Stereo: Motif Channel Attention Network for Stereo Matching","summary":"  Learning-based stereo matching techniques have made significant progress.\nHowever, existing methods inevitably lose geometrical structure information\nduring the feature channel generation process, resulting in edge detail\nmismatches. In this paper, the Motif Cha}nnel Attention Stereo Matching Network\n(MoCha-Stereo) is designed to address this problem. We provide the Motif\nChannel Correlation Volume (MCCV) to determine more accurate edge matching\ncosts. MCCV is achieved by projecting motif channels, which capture common\ngeometric structures in feature channels, onto feature maps and cost volumes.\nIn addition, edge variations in %potential feature channels of the\nreconstruction error map also affect details matching, we propose the\nReconstruction Error Motif Penalty (REMP) module to further refine the\nfull-resolution disparity estimation. REMP integrates the frequency information\nof typical channel features from the reconstruction error. MoCha-Stereo ranks\n1st on the KITTI-2015 and KITTI-2012 Reflective leaderboards. Our structure\nalso shows excellent performance in Multi-View Stereo. Code is avaliable at\nhttps://github.com/ZYangChen/MoCha-Stereo.\n","authors":["Ziyang Chen","Wei Long","He Yao","Yongjun Zhang","Bingshu Wang","Yongbin Qin","Jia Wu"],"pdf_url":"https://arxiv.org/pdf/2404.06842v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2404.07833v1","updated":"2024-04-11T15:18:34Z","published":"2024-04-11T15:18:34Z","title":"Streamlined Photoacoustic Image Processing with Foundation Models: A\n  Training-Free Solution","summary":"  Foundation models have rapidly evolved and have achieved significant\naccomplishments in computer vision tasks. Specifically, the prompt mechanism\nconveniently allows users to integrate image prior information into the model,\nmaking it possible to apply models without any training. Therefore, we propose\na method based on foundation models and zero training to solve the tasks of\nphotoacoustic (PA) image segmentation. We employed the segment anything model\n(SAM) by setting simple prompts and integrating the model's outputs with prior\nknowledge of the imaged objects to accomplish various tasks, including: (1)\nremoving the skin signal in three-dimensional PA image rendering; (2) dual\nspeed-of-sound reconstruction, and (3) segmentation of finger blood vessels.\nThrough these demonstrations, we have concluded that deep learning can be\ndirectly applied in PA imaging without the requirement for network design and\ntraining. This potentially allows for a hands-on, convenient approach to\nachieving efficient and accurate segmentation of PA images. This letter serves\nas a comprehensive tutorial, facilitating the mastery of the technique through\nthe provision of code and sample datasets.\n","authors":["Handi Deng","Yucheng Zhou","Jiaxuan Xiang","Liujie Gu","Yan Luo","Hai Feng","Mingyuan Liu","Cheng Ma"],"pdf_url":"https://arxiv.org/pdf/2404.07833v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07824v1","updated":"2024-04-11T15:09:22Z","published":"2024-04-11T15:09:22Z","title":"Heron-Bench: A Benchmark for Evaluating Vision Language Models in\n  Japanese","summary":"  Vision Language Models (VLMs) have undergone a rapid evolution, giving rise\nto significant advancements in the realm of multimodal understanding tasks.\nHowever, the majority of these models are trained and evaluated on\nEnglish-centric datasets, leaving a gap in the development and evaluation of\nVLMs for other languages, such as Japanese. This gap can be attributed to the\nlack of methodologies for constructing VLMs and the absence of benchmarks to\naccurately measure their performance. To address this issue, we introduce a\nnovel benchmark, Japanese Heron-Bench, for evaluating Japanese capabilities of\nVLMs. The Japanese Heron-Bench consists of a variety of imagequestion answer\npairs tailored to the Japanese context. Additionally, we present a baseline\nJapanese VLM that has been trained with Japanese visual instruction tuning\ndatasets. Our Heron-Bench reveals the strengths and limitations of the proposed\nVLM across various ability dimensions. Furthermore, we clarify the capability\ngap between strong closed models like GPT-4V and the baseline model, providing\nvaluable insights for future research in this domain. We release the benchmark\ndataset and training code to facilitate further developments in Japanese VLM\nresearch.\n","authors":["Yuichi Inoue","Kento Sasaki","Yuma Ochi","Kazuki Fujii","Kotaro Tanahashi","Yu Yamaguchi"],"pdf_url":"https://arxiv.org/pdf/2404.07824v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07821v1","updated":"2024-04-11T15:00:55Z","published":"2024-04-11T15:00:55Z","title":"Sparse Laneformer","summary":"  Lane detection is a fundamental task in autonomous driving, and has achieved\ngreat progress as deep learning emerges. Previous anchor-based methods often\ndesign dense anchors, which highly depend on the training dataset and remain\nfixed during inference. We analyze that dense anchors are not necessary for\nlane detection, and propose a transformer-based lane detection framework based\non a sparse anchor mechanism. To this end, we generate sparse anchors with\nposition-aware lane queries and angle queries instead of traditional explicit\nanchors. We adopt Horizontal Perceptual Attention (HPA) to aggregate the lane\nfeatures along the horizontal direction, and adopt Lane-Angle Cross Attention\n(LACA) to perform interactions between lane queries and angle queries. We also\npropose Lane Perceptual Attention (LPA) based on deformable cross attention to\nfurther refine the lane predictions. Our method, named Sparse Laneformer, is\neasy-to-implement and end-to-end trainable. Extensive experiments demonstrate\nthat Sparse Laneformer performs favorably against the state-of-the-art methods,\ne.g., surpassing Laneformer by 3.0% F1 score and O2SFormer by 0.7% F1 score\nwith fewer MACs on CULane with the same ResNet-34 backbone.\n","authors":["Ji Liu","Zifeng Zhang","Mingjie Lu","Hongyang Wei","Dong Li","Yile Xie","Jinzhang Peng","Lu Tian","Ashish Sirasao","Emad Barsoum"],"pdf_url":"https://arxiv.org/pdf/2404.07821v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07807v1","updated":"2024-04-11T14:51:12Z","published":"2024-04-11T14:51:12Z","title":"Voice-Assisted Real-Time Traffic Sign Recognition System Using\n  Convolutional Neural Network","summary":"  Traffic signs are important in communicating information to drivers. Thus,\ncomprehension of traffic signs is essential for road safety and ignorance may\nresult in road accidents. Traffic sign detection has been a research spotlight\nover the past few decades. Real-time and accurate detections are the\npreliminaries of robust traffic sign detection system which is yet to be\nachieved. This study presents a voice-assisted real-time traffic sign\nrecognition system which is capable of assisting drivers. This system functions\nunder two subsystems. Initially, the detection and recognition of the traffic\nsigns are carried out using a trained Convolutional Neural Network (CNN). After\nrecognizing the specific traffic sign, it is narrated to the driver as a voice\nmessage using a text-to-speech engine. An efficient CNN model for a benchmark\ndataset is developed for real-time detection and recognition using Deep\nLearning techniques. The advantage of this system is that even if the driver\nmisses a traffic sign, or does not look at the traffic sign, or is unable to\ncomprehend the sign, the system detects it and narrates it to the driver. A\nsystem of this type is also important in the development of autonomous\nvehicles.\n","authors":["Mayura Manawadu","Udaya Wijenayake"],"pdf_url":"https://arxiv.org/pdf/2404.07807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07794v1","updated":"2024-04-11T14:35:59Z","published":"2024-04-11T14:35:59Z","title":"DGMamba: Domain Generalization via Generalized State Space Model","summary":"  Domain generalization~(DG) aims at solving distribution shift problems in\nvarious scenes. Existing approaches are based on Convolution Neural Networks\n(CNNs) or Vision Transformers (ViTs), which suffer from limited receptive\nfields or quadratic complexities issues. Mamba, as an emerging state space\nmodel (SSM), possesses superior linear complexity and global receptive fields.\nDespite this, it can hardly be applied to DG to address distribution shifts,\ndue to the hidden state issues and inappropriate scan mechanisms. In this\npaper, we propose a novel framework for DG, named DGMamba, that excels in\nstrong generalizability toward unseen domains and meanwhile has the advantages\nof global receptive fields, and efficient linear complexity. Our DGMamba\ncompromises two core components: Hidden State Suppressing~(HSS) and\nSemantic-aware Patch refining~(SPR). In particular, HSS is introduced to\nmitigate the influence of hidden states associated with domain-specific\nfeatures during output prediction. SPR strives to encourage the model to\nconcentrate more on objects rather than context, consisting of two designs:\nPrior-Free Scanning~(PFS), and Domain Context Interchange~(DCI). Concretely,\nPFS aims to shuffle the non-semantic patches within images, creating more\nflexible and effective sequences from images, and DCI is designed to regularize\nMamba with the combination of mismatched non-semantic and semantic information\nby fusing patches among domains. Extensive experiments on four commonly used DG\nbenchmarks demonstrate that the proposed DGMamba achieves remarkably superior\nresults to state-of-the-art models. The code will be made publicly available.\n","authors":["Shaocong Long","Qianyu Zhou","Xiangtai Li","Xuequan Lu","Chenhao Ying","Yuan Luo","Lizhuang Ma","Shuicheng Yan"],"pdf_url":"https://arxiv.org/pdf/2404.07794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07790v1","updated":"2024-04-11T14:31:11Z","published":"2024-04-11T14:31:11Z","title":"VIFNet: An End-to-end Visible-Infrared Fusion Network for Image Dehazing","summary":"  Image dehazing poses significant challenges in environmental perception.\nRecent research mainly focus on deep learning-based methods with single\nmodality, while they may result in severe information loss especially in\ndense-haze scenarios. The infrared image exhibits robustness to the haze,\nhowever, existing methods have primarily treated the infrared modality as\nauxiliary information, failing to fully explore its rich information in\ndehazing. To address this challenge, the key insight of this study is to design\na visible-infrared fusion network for image dehazing. In particular, we propose\na multi-scale Deep Structure Feature Extraction (DSFE) module, which\nincorporates the Channel-Pixel Attention Block (CPAB) to restore more spatial\nand marginal information within the deep structural features. Additionally, we\nintroduce an inconsistency weighted fusion strategy to merge the two modalities\nby leveraging the more reliable information. To validate this, we construct a\nvisible-infrared multimodal dataset called AirSim-VID based on the AirSim\nsimulation platform. Extensive experiments performed on challenging real and\nsimulated image datasets demonstrate that VIFNet can outperform many\nstate-of-the-art competing methods. The code and dataset are available at\nhttps://github.com/mengyu212/VIFNet_dehazing.\n","authors":["Meng Yu","Te Cui","Haoyang Lu","Yufeng Yue"],"pdf_url":"https://arxiv.org/pdf/2404.07790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07788v1","updated":"2024-04-11T14:29:30Z","published":"2024-04-11T14:29:30Z","title":"AUG: A New Dataset and An Efficient Model for Aerial Image Urban Scene\n  Graph Generation","summary":"  Scene graph generation (SGG) aims to understand the visual objects and their\nsemantic relationships from one given image. Until now, lots of SGG datasets\nwith the eyelevel view are released but the SGG dataset with the overhead view\nis scarcely studied. By contrast to the object occlusion problem in the\neyelevel view, which impedes the SGG, the overhead view provides a new\nperspective that helps to promote the SGG by providing a clear perception of\nthe spatial relationships of objects in the ground scene. To fill in the gap of\nthe overhead view dataset, this paper constructs and releases an aerial image\nurban scene graph generation (AUG) dataset. Images from the AUG dataset are\ncaptured with the low-attitude overhead view. In the AUG dataset, 25,594\nobjects, 16,970 relationships, and 27,175 attributes are manually annotated. To\navoid the local context being overwhelmed in the complex aerial urban scene,\nthis paper proposes one new locality-preserving graph convolutional network\n(LPG). Different from the traditional graph convolutional network, which has\nthe natural advantage of capturing the global context for SGG, the\nconvolutional layer in the LPG integrates the non-destructive initial features\nof the objects with dynamically updated neighborhood information to preserve\nthe local context under the premise of mining the global context. To address\nthe problem that there exists an extra-large number of potential object\nrelationship pairs but only a small part of them is meaningful in AUG, we\npropose the adaptive bounding box scaling factor for potential relationship\ndetection (ABS-PRD) to intelligently prune the meaningless relationship pairs.\nExtensive experiments on the AUG dataset show that our LPG can significantly\noutperform the state-of-the-art methods and the effectiveness of the proposed\nlocality-preserving strategy.\n","authors":["Yansheng Li","Kun Li","Yongjun Zhang","Linlin Wang","Dingwen Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.07788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07785v1","updated":"2024-04-11T14:28:04Z","published":"2024-04-11T14:28:04Z","title":"PRAM: Place Recognition Anywhere Model for Efficient Visual Localization","summary":"  Humans localize themselves efficiently in known environments by first\nrecognizing landmarks defined on certain objects and their spatial\nrelationships, and then verifying the location by aligning detailed structures\nof recognized objects with those in the memory. Inspired by this, we propose\nthe place recognition anywhere model (PRAM) to perform visual localization as\nefficiently as humans do. PRAM consists of two main components - recognition\nand registration. In detail, first of all, a self-supervised map-centric\nlandmark definition strategy is adopted, making places in either indoor or\noutdoor scenes act as unique landmarks. Then, sparse keypoints extracted from\nimages, are utilized as the input to a transformer-based deep neural network\nfor landmark recognition; these keypoints enable PRAM to recognize hundreds of\nlandmarks with high time and memory efficiency. Keypoints along with recognized\nlandmark labels are further used for registration between query images and the\n3D landmark map. Different from previous hierarchical methods, PRAM discards\nglobal and local descriptors, and reduces over 90% storage. Since PRAM utilizes\nrecognition and landmark-wise verification to replace global reference search\nand exhaustive matching respectively, it runs 2.4 times faster than prior\nstate-of-the-art approaches. Moreover, PRAM opens new directions for visual\nlocalization including multi-modality localization, map-centric feature\nlearning, and hierarchical scene coordinate regression.\n","authors":["Fei Xue","Ignas Budvytis","Roberto Cipolla"],"pdf_url":"https://arxiv.org/pdf/2404.07785v1.pdf","comment":"project page: https://feixue94.github.io/pram-project/"},{"id":"http://arxiv.org/abs/2404.04562v2","updated":"2024-04-11T14:28:00Z","published":"2024-04-06T09:03:18Z","title":"Diffusion Time-step Curriculum for One Image to 3D Generation","summary":"  Score distillation sampling~(SDS) has been widely adopted to overcome the\nabsence of unseen views in reconstructing 3D objects from a \\textbf{single}\nimage. It leverages pre-trained 2D diffusion models as teacher to guide the\nreconstruction of student 3D models. Despite their remarkable success,\nSDS-based methods often encounter geometric artifacts and texture saturation.\nWe find out the crux is the overlooked indiscriminate treatment of diffusion\ntime-steps during optimization: it unreasonably treats the student-teacher\nknowledge distillation to be equal at all time-steps and thus entangles\ncoarse-grained and fine-grained modeling. Therefore, we propose the Diffusion\nTime-step Curriculum one-image-to-3D pipeline (DTC123), which involves both the\nteacher and student models collaborating with the time-step curriculum in a\ncoarse-to-fine manner. Extensive experiments on NeRF4, RealFusion15, GSO and\nLevel50 benchmark demonstrate that DTC123 can produce multi-view consistent,\nhigh-quality, and diverse 3D assets. Codes and more generation demos will be\nreleased in https://github.com/yxymessi/DTC123.\n","authors":["Xuanyu Yi","Zike Wu","Qingshan Xu","Pan Zhou","Joo-Hwee Lim","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.04562v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2401.07782v2","updated":"2024-04-11T14:27:27Z","published":"2024-01-15T15:43:56Z","title":"Exploring Masked Autoencoders for Sensor-Agnostic Image Retrieval in\n  Remote Sensing","summary":"  Self-supervised learning through masked autoencoders (MAEs) has recently\nattracted great attention for remote sensing (RS) image representation\nlearning, and thus embodies a significant potential for content-based image\nretrieval (CBIR) from ever-growing RS image archives. However, the existing\nstudies on MAEs in RS assume that the considered RS images are acquired by a\nsingle image sensor, and thus are only suitable for uni-modal CBIR problems.\nThe effectiveness of MAEs for cross-sensor CBIR, which aims to search\nsemantically similar images across different image modalities, has not been\nexplored yet. In this paper, we take the first step to explore the\neffectiveness of MAEs for sensor-agnostic CBIR in RS. To this end, we present a\nsystematic overview on the possible adaptations of the vanilla MAE to exploit\nmasked image modeling on multi-sensor RS image archives (denoted as\ncross-sensor masked autoencoders [CSMAEs]). Based on different adjustments\napplied to the vanilla MAE, we introduce different CSMAE models. We also\nprovide an extensive experimental analysis of these CSMAE models. We finally\nderive a guideline to exploit masked image modeling for uni-modal and\ncross-modal CBIR problems in RS. The code of this work is publicly available at\nhttps://github.com/jakhac/CSMAE.\n","authors":["Jakob Hackstein","Gencer Sumbul","Kai Norman Clasen","Beg√ºm Demir"],"pdf_url":"https://arxiv.org/pdf/2401.07782v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Our code is available at https://github.com/jakhac/CSMAE"},{"id":"http://arxiv.org/abs/2309.09590v2","updated":"2024-04-11T14:24:09Z","published":"2023-09-18T08:54:29Z","title":"An Autonomous Vision-Based Algorithm for Interplanetary Navigation","summary":"  The surge of deep-space probes makes it unsustainable to navigate them with\nstandard radiometric tracking. Self-driving interplanetary satellites represent\na solution to this problem. In this work, a full vision-based navigation\nalgorithm is built by combining an orbit determination method with an image\nprocessing pipeline suitable for interplanetary transfers of autonomous\nplatforms. To increase the computational efficiency of the algorithm, a\nnon-dimensional extended Kalman filter is selected as state estimator, fed by\nthe positions of the planets extracted from deep-space images. An enhancement\nof the estimation accuracy is performed by applying an optimal strategy to\nselect the best pair of planets to track. Moreover, a novel analytical\nmeasurement model for deep-space navigation is developed providing a\nfirst-order approximation of the light-aberration and light-time effects.\nAlgorithm performance is tested on a high-fidelity, Earth--Mars interplanetary\ntransfer, showing the algorithm applicability for deep-space navigation.\n","authors":["Eleonora Andreis","Paolo Panicucci","Francesco Topputo"],"pdf_url":"https://arxiv.org/pdf/2309.09590v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18551v2","updated":"2024-04-11T14:10:43Z","published":"2024-03-27T13:31:39Z","title":"Attention Calibration for Disentangled Text-to-Image Personalization","summary":"  Recent thrilling progress in large-scale text-to-image (T2I) models has\nunlocked unprecedented synthesis quality of AI-generated content (AIGC)\nincluding image generation, 3D and video composition. Further, personalized\ntechniques enable appealing customized production of a novel concept given only\nseveral images as reference. However, an intriguing problem persists: Is it\npossible to capture multiple, novel concepts from one single reference image?\nIn this paper, we identify that existing approaches fail to preserve visual\nconsistency with the reference image and eliminate cross-influence from\nconcepts. To alleviate this, we propose an attention calibration mechanism to\nimprove the concept-level understanding of the T2I model. Specifically, we\nfirst introduce new learnable modifiers bound with classes to capture\nattributes of multiple concepts. Then, the classes are separated and\nstrengthened following the activation of the cross-attention operation,\nensuring comprehensive and self-contained concepts. Additionally, we suppress\nthe attention activation of different classes to mitigate mutual influence\namong concepts. Together, our proposed method, dubbed DisenDiff, can learn\ndisentangled multiple concepts from one single image and produce novel\ncustomized images with learned concepts. We demonstrate that our method\noutperforms the current state of the art in both qualitative and quantitative\nevaluations. More importantly, our proposed techniques are compatible with LoRA\nand inpainting pipelines, enabling more interactive experiences.\n","authors":["Yanbing Zhang","Mengping Yang","Qin Zhou","Zhe Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18551v2.pdf","comment":"CVPR 2024 (Oral)"},{"id":"http://arxiv.org/abs/2404.07773v1","updated":"2024-04-11T14:08:45Z","published":"2024-04-11T14:08:45Z","title":"ConsistencyDet: Robust Object Detector with Denoising Paradigm of\n  Consistency Model","summary":"  Object detection, a quintessential task in the realm of perceptual computing,\ncan be tackled using a generative methodology. In the present study, we\nintroduce a novel framework designed to articulate object detection as a\ndenoising diffusion process, which operates on perturbed bounding boxes of\nannotated entities. This framework, termed ConsistencyDet, leverages an\ninnovative denoising concept known as the Consistency Model. The hallmark of\nthis model is its self-consistency feature, which empowers the model to map\ndistorted information from any temporal stage back to its pristine state,\nthereby realizing a ``one-step denoising'' mechanism. Such an attribute\nmarkedly elevates the operational efficiency of the model, setting it apart\nfrom the conventional Diffusion Model. Throughout the training phase,\nConsistencyDet initiates the diffusion sequence with noise-infused boxes\nderived from the ground-truth annotations and conditions the model to perform\nthe denoising task. Subsequently, in the inference stage, the model employs a\ndenoising sampling strategy that commences with bounding boxes randomly sampled\nfrom a normal distribution. Through iterative refinement, the model transforms\nan assortment of arbitrarily generated boxes into the definitive detections.\nComprehensive evaluations employing standard benchmarks, such as MS-COCO and\nLVIS, corroborate that ConsistencyDet surpasses other leading-edge detectors in\nperformance metrics.\n","authors":["Lifan Jiang","Zhihui Wang","Changmiao Wang","Ming Li","Jiaxu Leng","Xindong Wu"],"pdf_url":"https://arxiv.org/pdf/2404.07773v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07770v1","updated":"2024-04-11T14:07:16Z","published":"2024-04-11T14:07:16Z","title":"Joint Conditional Diffusion Model for Image Restoration with Mixed\n  Degradations","summary":"  Image restoration is rather challenging in adverse weather conditions,\nespecially when multiple degradations occur simultaneously. Blind image\ndecomposition was proposed to tackle this issue, however, its effectiveness\nheavily relies on the accurate estimation of each component. Although\ndiffusion-based models exhibit strong generative abilities in image restoration\ntasks, they may generate irrelevant contents when the degraded images are\nseverely corrupted. To address these issues, we leverage physical constraints\nto guide the whole restoration process, where a mixed degradation model based\non atmosphere scattering model is constructed. Then we formulate our Joint\nConditional Diffusion Model (JCDM) by incorporating the degraded image and\ndegradation mask to provide precise guidance. To achieve better color and\ndetail recovery results, we further integrate a refinement network to\nreconstruct the restored image, where Uncertainty Estimation Block (UEB) is\nemployed to enhance the features. Extensive experiments performed on both\nmulti-weather and weather-specific datasets demonstrate the superiority of our\nmethod over state-of-the-art competing methods.\n","authors":["Yufeng Yue","Meng Yu","Luojie Yang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2404.07770v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07766v1","updated":"2024-04-11T14:05:37Z","published":"2024-04-11T14:05:37Z","title":"RMAFF-PSN: A Residual Multi-Scale Attention Feature Fusion Photometric\n  Stereo Network","summary":"  Predicting accurate normal maps of objects from two-dimensional images in\nregions of complex structure and spatial material variations is challenging\nusing photometric stereo methods due to the influence of surface reflection\nproperties caused by variations in object geometry and surface materials. To\naddress this issue, we propose a photometric stereo network called a RMAFF-PSN\nthat uses residual multiscale attentional feature fusion to handle the\n``difficult'' regions of the object. Unlike previous approaches that only use\nstacked convolutional layers to extract deep features from the input image, our\nmethod integrates feature information from different resolution stages and\nscales of the image. This approach preserves more physical information, such as\ntexture and geometry of the object in complex regions, through shallow-deep\nstage feature extraction, double branching enhancement, and attention\noptimization. To test the network structure under real-world conditions, we\npropose a new real dataset called Simple PS data, which contains multiple\nobjects with varying structures and materials. Experimental results on a\npublicly available benchmark dataset demonstrate that our method outperforms\nmost existing calibrated photometric stereo methods for the same number of\ninput images, especially in the case of highly non-convex object structures.\nOur method also obtains good results under sparse lighting conditions.\n","authors":["Kai Luo","Yakun Ju","Lin Qi","Kaixuan Wang","Junyu Dong"],"pdf_url":"https://arxiv.org/pdf/2404.07766v1.pdf","comment":"17 pages,12 figures"},{"id":"http://arxiv.org/abs/2404.07762v1","updated":"2024-04-11T14:03:16Z","published":"2024-04-11T14:03:16Z","title":"NeuroNCAP: Photorealistic Closed-loop Safety Testing for Autonomous\n  Driving","summary":"  We present a versatile NeRF-based simulator for testing autonomous driving\n(AD) software systems, designed with a focus on sensor-realistic closed-loop\nevaluation and the creation of safety-critical scenarios. The simulator learns\nfrom sequences of real-world driving sensor data and enables reconfigurations\nand renderings of new, unseen scenarios. In this work, we use our simulator to\ntest the responses of AD models to safety-critical scenarios inspired by the\nEuropean New Car Assessment Programme (Euro NCAP). Our evaluation reveals that,\nwhile state-of-the-art end-to-end planners excel in nominal driving scenarios\nin an open-loop setting, they exhibit critical flaws when navigating our\nsafety-critical scenarios in a closed-loop setting. This highlights the need\nfor advancements in the safety and real-world usability of end-to-end planners.\nBy publicly releasing our simulator and scenarios as an easy-to-run evaluation\nsuite, we invite the research community to explore, refine, and validate their\nAD models in controlled, yet highly configurable and challenging\nsensor-realistic environments. Code and instructions can be found at\nhttps://github.com/wljungbergh/NeuroNCAP\n","authors":["William Ljungbergh","Adam Tonderski","Joakim Johnander","Holger Caesar","Kalle √Östr√∂m","Michael Felsberg","Christoffer Petersson"],"pdf_url":"https://arxiv.org/pdf/2404.07762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07754v1","updated":"2024-04-11T14:00:20Z","published":"2024-04-11T14:00:20Z","title":"Generating Synthetic Satellite Imagery With Deep-Learning Text-to-Image\n  Models -- Technical Challenges and Implications for Monitoring and\n  Verification","summary":"  Novel deep-learning (DL) architectures have reached a level where they can\ngenerate digital media, including photorealistic images, that are difficult to\ndistinguish from real data. These technologies have already been used to\ngenerate training data for Machine Learning (ML) models, and large\ntext-to-image models like DALL-E 2, Imagen, and Stable Diffusion are achieving\nremarkable results in realistic high-resolution image generation. Given these\ndevelopments, issues of data authentication in monitoring and verification\ndeserve a careful and systematic analysis: How realistic are synthetic images?\nHow easily can they be generated? How useful are they for ML researchers, and\nwhat is their potential for Open Science? In this work, we use novel DL models\nto explore how synthetic satellite images can be created using conditioning\nmechanisms. We investigate the challenges of synthetic satellite image\ngeneration and evaluate the results based on authenticity and state-of-the-art\nmetrics. Furthermore, we investigate how synthetic data can alleviate the lack\nof data in the context of ML methods for remote-sensing. Finally we discuss\nimplications of synthetic satellite imagery in the context of monitoring and\nverification.\n","authors":["Tuong Vy Nguyen","Alexander Glaser","Felix Biessmann"],"pdf_url":"https://arxiv.org/pdf/2404.07754v1.pdf","comment":"https://resources.inmm.org/annual-meeting-proceedings/generating-synthetic-satellite-imagery-deep-learning-text-image-models"},{"id":"http://arxiv.org/abs/2404.07748v1","updated":"2024-04-11T13:46:05Z","published":"2024-04-11T13:46:05Z","title":"3D-CSAD: Untrained 3D Anomaly Detection for Complex Manufacturing\n  Surfaces","summary":"  The surface quality inspection of manufacturing parts based on 3D point cloud\ndata has attracted increasing attention in recent years. The reason is that the\n3D point cloud can capture the entire surface of manufacturing parts, unlike\nthe previous practices that focus on some key product characteristics. However,\nachieving accurate 3D anomaly detection is challenging, due to the complex\nsurfaces of manufacturing parts and the difficulty of collecting sufficient\nanomaly samples. To address these challenges, we propose a novel untrained\nanomaly detection method based on 3D point cloud data for complex manufacturing\nparts, which can achieve accurate anomaly detection in a single sample without\ntraining data. In the proposed framework, we transform an input sample into two\nsets of profiles along different directions. Based on one set of the profiles,\na novel segmentation module is devised to segment the complex surface into\nmultiple basic and simple components. In each component, another set of\nprofiles, which have the nature of similar shapes, can be modeled as a low-rank\nmatrix. Thus, accurate 3D anomaly detection can be achieved by using Robust\nPrincipal Component Analysis (RPCA) on these low-rank matrices. Extensive\nnumerical experiments on different types of parts show that our method achieves\npromising results compared with the benchmark methods.\n","authors":["Xuanming Cao","Chengyu Tao","Juan Du"],"pdf_url":"https://arxiv.org/pdf/2404.07748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.05539v2","updated":"2024-04-11T13:39:18Z","published":"2023-11-09T17:34:57Z","title":"A Deep Learning Method for Simultaneous Denoising and Missing Wedge\n  Reconstruction in Cryogenic Electron Tomography","summary":"  Cryogenic electron tomography is a technique for imaging biological samples\nin 3D. A microscope collects a series of 2D projections of the sample, and the\ngoal is to reconstruct the 3D density of the sample called the tomogram.\nReconstruction is difficult as the 2D projections are noisy and can not be\nrecorded from all directions, resulting in a missing wedge of information.\nTomograms conventionally reconstructed with filtered back-projection suffer\nfrom noise and strong artifacts due to the missing wedge. Here, we propose a\ndeep-learning approach for simultaneous denoising and missing wedge\nreconstruction called DeepDeWedge. The algorithm requires no ground truth data\nand is based on fitting a neural network to the 2D projections using a\nself-supervised loss. DeepDeWedge performs better than CryoCARE and IsoNet,\nwhich are state-of-the-art methods for denoising and missing wedge\nreconstruction, and similarly and, in some cases, better than the combination\nof the two methods. At the same time, DeepDeWedge is simpler than this two-step\napproach, as it does denoising and missing wedge reconstruction simultaneously\nrather than sequentially.\n","authors":["Simon Wiedemann","Reinhard Heckel"],"pdf_url":"https://arxiv.org/pdf/2311.05539v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07739v1","updated":"2024-04-11T13:37:51Z","published":"2024-04-11T13:37:51Z","title":"Exploiting Object-based and Segmentation-based Semantic Features for\n  Deep Learning-based Indoor Scene Classification","summary":"  Indoor scenes are usually characterized by scattered objects and their\nrelationships, which turns the indoor scene classification task into a\nchallenging computer vision task. Despite the significant performance boost in\nclassification tasks achieved in recent years, provided by the use of\ndeep-learning-based methods, limitations such as inter-category ambiguity and\nintra-category variation have been holding back their performance. To overcome\nsuch issues, gathering semantic information has been shown to be a promising\nsource of information towards a more complete and discriminative feature\nrepresentation of indoor scenes. Therefore, the work described in this paper\nuses both semantic information, obtained from object detection, and semantic\nsegmentation techniques. While object detection techniques provide the 2D\nlocation of objects allowing to obtain spatial distributions between objects,\nsemantic segmentation techniques provide pixel-level information that allows to\nobtain, at a pixel-level, a spatial distribution and shape-related features of\nthe segmentation categories. Hence, a novel approach that uses a semantic\nsegmentation mask to provide Hu-moments-based segmentation categories' shape\ncharacterization, designated by Segmentation-based Hu-Moments Features (SHMFs),\nis proposed. Moreover, a three-main-branch network, designated by\nGOS$^2$F$^2$App, that exploits deep-learning-based global features,\nobject-based features, and semantic segmentation-based features is also\nproposed. GOS$^2$F$^2$App was evaluated in two indoor scene benchmark datasets:\nSUN RGB-D and NYU Depth V2, where, to the best of our knowledge,\nstate-of-the-art results were achieved on both datasets, which present\nevidences of the effectiveness of the proposed approach.\n","authors":["Ricardo Pereira","Lu√≠s Garrote","Tiago Barros","Ana Lopes","Urbano J. Nunes"],"pdf_url":"https://arxiv.org/pdf/2404.07739v1.pdf","comment":"This preprint was submitted at IEEE Transactions on Image Processing"},{"id":"http://arxiv.org/abs/2404.05392v2","updated":"2024-04-11T13:36:58Z","published":"2024-04-08T10:51:29Z","title":"T-DEED: Temporal-Discriminability Enhancer Encoder-Decoder for Precise\n  Event Spotting in Sports Videos","summary":"  In this paper, we introduce T-DEED, a Temporal-Discriminability Enhancer\nEncoder-Decoder for Precise Event Spotting in sports videos. T-DEED addresses\nmultiple challenges in the task, including the need for discriminability among\nframe representations, high output temporal resolution to maintain prediction\nprecision, and the necessity to capture information at different temporal\nscales to handle events with varying dynamics. It tackles these challenges\nthrough its specifically designed architecture, featuring an encoder-decoder\nfor leveraging multiple temporal scales and achieving high output temporal\nresolution, along with temporal modules designed to increase token\ndiscriminability. Leveraging these characteristics, T-DEED achieves SOTA\nperformance on the FigureSkating and FineDiving datasets. Code is available at\nhttps://github.com/arturxe2/T-DEED.\n","authors":["Artur Xarles","Sergio Escalera","Thomas B. Moeslund","Albert Clap√©s"],"pdf_url":"https://arxiv.org/pdf/2404.05392v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07729v1","updated":"2024-04-11T13:19:46Z","published":"2024-04-11T13:19:46Z","title":"Realistic Continual Learning Approach using Pre-trained Models","summary":"  Continual learning (CL) is crucial for evaluating adaptability in learning\nsolutions to retain knowledge. Our research addresses the challenge of\ncatastrophic forgetting, where models lose proficiency in previously learned\ntasks as they acquire new ones. While numerous solutions have been proposed,\nexisting experimental setups often rely on idealized class-incremental learning\nscenarios. We introduce Realistic Continual Learning (RealCL), a novel CL\nparadigm where class distributions across tasks are random, departing from\nstructured setups.\n  We also present CLARE (Continual Learning Approach with pRE-trained models\nfor RealCL scenarios), a pre-trained model-based solution designed to integrate\nnew knowledge while preserving past learning. Our contributions include\npioneering RealCL as a generalization of traditional CL setups, proposing CLARE\nas an adaptable approach for RealCL tasks, and conducting extensive experiments\ndemonstrating its effectiveness across various RealCL scenarios. Notably, CLARE\noutperforms existing models on RealCL benchmarks, highlighting its versatility\nand robustness in unpredictable learning environments.\n","authors":["Nadia Nasri","Carlos Guti√©rrez-√Ålvarez","Sergio Lafuente-Arroyo","Saturnino Maldonado-Basc√≥n","Roberto J. L√≥pez-Sastre"],"pdf_url":"https://arxiv.org/pdf/2404.07729v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07724v1","updated":"2024-04-11T13:16:47Z","published":"2024-04-11T13:16:47Z","title":"Applying Guidance in a Limited Interval Improves Sample and Distribution\n  Quality in Diffusion Models","summary":"  Guidance is a crucial technique for extracting the best performance out of\nimage-generating diffusion models. Traditionally, a constant guidance weight\nhas been applied throughout the sampling chain of an image. We show that\nguidance is clearly harmful toward the beginning of the chain (high noise\nlevels), largely unnecessary toward the end (low noise levels), and only\nbeneficial in the middle. We thus restrict it to a specific range of noise\nlevels, improving both the inference speed and result quality. This limited\nguidance interval improves the record FID in ImageNet-512 significantly, from\n1.81 to 1.40. We show that it is quantitatively and qualitatively beneficial\nacross different sampler parameters, network architectures, and datasets,\nincluding the large-scale setting of Stable Diffusion XL. We thus suggest\nexposing the guidance interval as a hyperparameter in all diffusion models that\nuse guidance.\n","authors":["Tuomas Kynk√§√§nniemi","Miika Aittala","Tero Karras","Samuli Laine","Timo Aila","Jaakko Lehtinen"],"pdf_url":"https://arxiv.org/pdf/2404.07724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03778v2","updated":"2024-04-11T13:12:48Z","published":"2024-04-04T19:50:57Z","title":"Flattening the Parent Bias: Hierarchical Semantic Segmentation in the\n  Poincar{√©} Ball","summary":"  Hierarchy is a natural representation of semantic taxonomies, including the\nones routinely used in image segmentation. Indeed, recent work on semantic\nsegmentation reports improved accuracy from supervised training leveraging\nhierarchical label structures. Encouraged by these results, we revisit the\nfundamental assumptions behind that work. We postulate and then empirically\nverify that the reasons for the observed improvement in segmentation accuracy\nmay be entirely unrelated to the use of the semantic hierarchy. To demonstrate\nthis, we design a range of cross-domain experiments with a representative\nhierarchical approach. We find that on the new testing domains, a flat\n(non-hierarchical) segmentation network, in which the parents are inferred from\nthe children, has superior segmentation accuracy to the hierarchical approach\nacross the board. Complementing these findings and inspired by the intrinsic\nproperties of hyperbolic spaces, we study a more principled approach to\nhierarchical segmentation using the Poincar\\'e ball model. The hyperbolic\nrepresentation largely outperforms the previous (Euclidean) hierarchical\napproach as well and is on par with our flat Euclidean baseline in terms of\nsegmentation accuracy. However, it additionally exhibits surprisingly strong\ncalibration quality of the parent nodes in the semantic hierarchy, especially\non the more challenging domains. Our combined analysis suggests that the\nestablished practice of hierarchical segmentation may be limited to in-domain\nsettings, whereas flat classifiers generalize substantially better, especially\nif they are modeled in the hyperbolic space.\n","authors":["Simon Weber","Barƒ±≈ü Z√∂ng√ºr","Nikita Araslanov","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2404.03778v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16783v2","updated":"2024-04-11T13:07:43Z","published":"2023-03-29T15:19:01Z","title":"Exploring Efficient Asymmetric Blind-Spots for Self-Supervised Denoising\n  in Real-World Scenarios","summary":"  Self-supervised denoising has attracted widespread attention due to its\nability to train without clean images. However, noise in real-world scenarios\nis often spatially correlated, which causes many self-supervised algorithms\nthat assume pixel-wise independent noise to perform poorly. Recent works have\nattempted to break noise correlation with downsampling or neighborhood masking.\nHowever, denoising on downsampled subgraphs can lead to aliasing effects and\nloss of details due to a lower sampling rate. Furthermore, the neighborhood\nmasking methods either come with high computational complexity or do not\nconsider local spatial preservation during inference. Through the analysis of\nexisting methods, we point out that the key to obtaining high-quality and\ntexture-rich results in real-world self-supervised denoising tasks is to train\nat the original input resolution structure and use asymmetric operations during\ntraining and inference. Based on this, we propose Asymmetric Tunable Blind-Spot\nNetwork (AT-BSN), where the blind-spot size can be freely adjusted, thus better\nbalancing noise correlation suppression and image local spatial destruction\nduring training and inference. In addition, we regard the pre-trained AT-BSN as\na meta-teacher network capable of generating various teacher networks by\nsampling different blind-spots. We propose a blind-spot based multi-teacher\ndistillation strategy to distill a lightweight network, significantly improving\nperformance. Experimental results on multiple datasets prove that our method\nachieves state-of-the-art, and is superior to other self-supervised algorithms\nin terms of computational overhead and visual effects.\n","authors":["Shiyan Chen","Jiyuan Zhang","Zhaofei Yu","Tiejun Huang"],"pdf_url":"https://arxiv.org/pdf/2303.16783v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2404.03936v2","updated":"2024-04-11T13:02:58Z","published":"2024-04-05T07:44:17Z","title":"Deep Learning for Satellite Image Time Series Analysis: A Review","summary":"  Earth observation (EO) satellite missions have been providing detailed images\nabout the state of the Earth and its land cover for over 50 years. Long term\nmissions, such as NASA's Landsat, Terra, and Aqua satellites, and more\nrecently, the ESA's Sentinel missions, record images of the entire world every\nfew days. Although single images provide point-in-time data, repeated images of\nthe same area, or satellite image time series (SITS) provide information about\nthe changing state of vegetation and land use. These SITS are useful for\nmodeling dynamic processes and seasonal changes such as plant phenology. They\nhave potential benefits for many aspects of land and natural resource\nmanagement, including applications in agricultural, forest, water, and disaster\nmanagement, urban planning, and mining. However, the resulting satellite image\ntime series (SITS) are complex, incorporating information from the temporal,\nspatial, and spectral dimensions. Therefore, deep learning methods are often\ndeployed as they can analyze these complex relationships. This review presents\na summary of the state-of-the-art methods of modelling environmental,\nagricultural, and other Earth observation variables from SITS data using deep\nlearning methods. We aim to provide a resource for remote sensing experts\ninterested in using deep learning techniques to enhance Earth observation\nmodels with temporal information.\n","authors":["Lynn Miller","Charlotte Pelletier","Geoffrey I. Webb"],"pdf_url":"https://arxiv.org/pdf/2404.03936v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2404.07713v1","updated":"2024-04-11T12:59:38Z","published":"2024-04-11T12:59:38Z","title":"Progressive Semantic-Guided Vision Transformer for Zero-Shot Learning","summary":"  Zero-shot learning (ZSL) recognizes the unseen classes by conducting\nvisual-semantic interactions to transfer semantic knowledge from seen classes\nto unseen ones, supported by semantic information (e.g., attributes). However,\nexisting ZSL methods simply extract visual features using a pre-trained network\nbackbone (i.e., CNN or ViT), which fail to learn matched visual-semantic\ncorrespondences for representing semantic-related visual features as lacking of\nthe guidance of semantic information, resulting in undesirable visual-semantic\ninteractions. To tackle this issue, we propose a progressive semantic-guided\nvision transformer for zero-shot learning (dubbed ZSLViT). ZSLViT mainly\nconsiders two properties in the whole network: i) discover the semantic-related\nvisual representations explicitly, and ii) discard the semantic-unrelated\nvisual information. Specifically, we first introduce semantic-embedded token\nlearning to improve the visual-semantic correspondences via semantic\nenhancement and discover the semantic-related visual tokens explicitly with\nsemantic-guided token attention. Then, we fuse low semantic-visual\ncorrespondence visual tokens to discard the semantic-unrelated visual\ninformation for visual enhancement. These two operations are integrated into\nvarious encoders to progressively learn semantic-related visual representations\nfor accurate visual-semantic interactions in ZSL. The extensive experiments\nshow that our ZSLViT achieves significant performance gains on three popular\nbenchmark datasets, i.e., CUB, SUN, and AWA2.\n","authors":["Shiming Chen","Wenjin Hou","Salman Khan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2404.07713v1.pdf","comment":"Accepted to CVPR'24"},{"id":"http://arxiv.org/abs/2404.07711v1","updated":"2024-04-11T12:58:12Z","published":"2024-04-11T12:58:12Z","title":"OpenTrench3D: A Photogrammetric 3D Point Cloud Dataset for Semantic\n  Segmentation of Underground Utilities","summary":"  Identifying and classifying underground utilities is an important task for\nefficient and effective urban planning and infrastructure maintenance. We\npresent OpenTrench3D, a novel and comprehensive 3D Semantic Segmentation point\ncloud dataset, designed to advance research and development in underground\nutility surveying and mapping. OpenTrench3D covers a completely novel domain\nfor public 3D point cloud datasets and is unique in its focus, scope, and\ncost-effective capturing method. The dataset consists of 310 point clouds\ncollected across 7 distinct areas. These include 5 water utility areas and 2\ndistrict heating utility areas. The inclusion of different geographical areas\nand main utilities (water and district heating utilities) makes OpenTrench3D\nparticularly valuable for inter-domain transfer learning experiments. We\nprovide benchmark results for the dataset using three state-of-the-art semantic\nsegmentation models, PointNeXt, PointVector and PointMetaBase. Benchmarks are\nconducted by training on data from water areas, fine-tuning on district heating\narea 1 and evaluating on district heating area 2. The dataset is publicly\navailable. With OpenTrench3D, we seek to foster innovation and progress in the\nfield of 3D semantic segmentation in applications related to detection and\ndocumentation of underground utilities as well as in transfer learning methods\nin general.\n","authors":["Lasse H. Hansen","Simon B. Jensen","Mark P. Philipsen","Andreas M√∏gelmose","Lars Bodum","Thomas B. Moeslund"],"pdf_url":"https://arxiv.org/pdf/2404.07711v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07705v1","updated":"2024-04-11T12:49:56Z","published":"2024-04-11T12:49:56Z","title":"ViM-UNet: Vision Mamba for Biomedical Segmentation","summary":"  CNNs, most notably the UNet, are the default architecture for biomedical\nsegmentation. Transformer-based approaches, such as UNETR, have been proposed\nto replace them, benefiting from a global field of view, but suffering from\nlarger runtimes and higher parameter counts. The recent Vision Mamba\narchitecture offers a compelling alternative to transformers, also providing a\nglobal field of view, but at higher efficiency. Here, we introduce ViM-UNet, a\nnovel segmentation architecture based on it and compare it to UNet and UNETR\nfor two challenging microscopy instance segmentation tasks. We find that it\nperforms similarly or better than UNet, depending on the task, and outperforms\nUNETR while being more efficient. Our code is open source and documented at\nhttps://github.com/constantinpape/torch-em/blob/main/vimunet.md.\n","authors":["Anwai Archit","Constantin Pape"],"pdf_url":"https://arxiv.org/pdf/2404.07705v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07698v1","updated":"2024-04-11T12:44:15Z","published":"2024-04-11T12:44:15Z","title":"Point Cloud Geometry Scalable Coding with a Quality-Conditioned Latents\n  Probability Estimator","summary":"  The widespread usage of point clouds (PC) for immersive visual applications\nhas resulted in the use of very heterogeneous receiving conditions and devices,\nnotably in terms of network, hardware, and display capabilities. In this\nscenario, quality scalability, i.e., the ability to reconstruct a signal at\ndifferent qualities by progressively decoding a single bitstream, is a major\nrequirement that has yet to be conveniently addressed, notably in most\nlearning-based PC coding solutions. This paper proposes a quality scalability\nscheme, named Scalable Quality Hyperprior (SQH), adaptable to learning-based\nstatic point cloud geometry codecs, which uses a Quality-conditioned Latents\nProbability Estimator (QuLPE) to decode a high-quality version of a PC\nlearning-based representation, based on an available lower quality base layer.\nSQH is integrated in the future JPEG PC coding standard, allowing to create a\nlayered bitstream that can be used to progressively decode the PC geometry with\nincreasing quality and fidelity. Experimental results show that SQH offers the\nquality scalability feature with very limited or no compression performance\npenalty at all when compared with the corresponding non-scalable solution, thus\npreserving the significant compression gains over other state-of-the-art PC\ncodecs.\n","authors":["Daniele Mari","Andr√© F. R. Guarda","Nuno M. M. Rodrigues","Simone Milani","Fernando Pereira"],"pdf_url":"https://arxiv.org/pdf/2404.07698v1.pdf","comment":"Submitted at ICIP 2024"},{"id":"http://arxiv.org/abs/2404.07696v1","updated":"2024-04-11T12:42:18Z","published":"2024-04-11T12:42:18Z","title":"Flatness Improves Backbone Generalisation in Few-shot Classification","summary":"  Deployment of deep neural networks in real-world settings typically requires\nadaptation to new tasks with few examples. Few-shot classification (FSC)\nprovides a solution to this problem by leveraging pre-trained backbones for\nfast adaptation to new classes. Surprisingly, most efforts have only focused on\ndeveloping architectures for easing the adaptation to the target domain without\nconsidering the importance of backbone training for good generalisation. We\nshow that flatness-aware backbone training with vanilla fine-tuning results in\na simpler yet competitive baseline compared to the state-of-the-art. Our\nresults indicate that for in- and cross-domain FSC, backbone training is\ncrucial to achieving good generalisation across different adaptation methods.\nWe advocate more care should be taken when training these models.\n","authors":["Rui Li","Martin Trapp","Marcus Klasson","Arno Solin"],"pdf_url":"https://arxiv.org/pdf/2404.07696v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07687v1","updated":"2024-04-11T12:26:10Z","published":"2024-04-11T12:26:10Z","title":"Chaos in Motion: Unveiling Robustness in Remote Heart Rate Measurement\n  through Brain-Inspired Skin Tracking","summary":"  Heart rate is an important physiological indicator of human health status.\nExisting remote heart rate measurement methods typically involve facial\ndetection followed by signal extraction from the region of interest (ROI).\nThese SOTA methods have three serious problems: (a) inaccuracies even failures\nin detection caused by environmental influences or subject movement; (b)\nfailures for special patients such as infants and burn victims; (c) privacy\nleakage issues resulting from collecting face video. To address these issues,\nwe regard the remote heart rate measurement as the process of analyzing the\nspatiotemporal characteristics of the optical flow signal in the video. We\napply chaos theory to computer vision tasks for the first time, thus designing\na brain-inspired framework. Firstly, using an artificial primary visual cortex\nmodel to extract the skin in the videos, and then calculate heart rate by\ntime-frequency analysis on all pixels. Our method achieves Robust Skin Tracking\nfor Heart Rate measurement, called HR-RST. The experimental results show that\nHR-RST overcomes the difficulty of environmental influences and effectively\ntracks the subject movement. Moreover, the method could extend to other body\nparts. Consequently, the method can be applied to special patients and\neffectively protect individual privacy, offering an innovative solution.\n","authors":["Jie Wang","Jing Lian","Minjie Ma","Junqiang Lei","Chunbiao Li","Bin Li","Jizhao Liu"],"pdf_url":"https://arxiv.org/pdf/2404.07687v1.pdf","comment":"8 pages, 10 figures"},{"id":"http://arxiv.org/abs/2404.07686v1","updated":"2024-04-11T12:25:54Z","published":"2024-04-11T12:25:54Z","title":"Depth Estimation using Weighted-loss and Transfer Learning","summary":"  Depth estimation from 2D images is a common computer vision task that has\napplications in many fields including autonomous vehicles, scene understanding\nand robotics. The accuracy of a supervised depth estimation method mainly\nrelies on the chosen loss function, the model architecture, quality of data and\nperformance metrics. In this study, we propose a simplified and adaptable\napproach to improve depth estimation accuracy using transfer learning and an\noptimized loss function. The optimized loss function is a combination of\nweighted losses to which enhance robustness and generalization: Mean Absolute\nError (MAE), Edge Loss and Structural Similarity Index (SSIM). We use a grid\nsearch and a random search method to find optimized weights for the losses,\nwhich leads to an improved model. We explore multiple encoder-decoder-based\nmodels including DenseNet121, DenseNet169, DenseNet201, and EfficientNet for\nthe supervised depth estimation model on NYU Depth Dataset v2. We observe that\nthe EfficientNet model, pre-trained on ImageNet for classification when used as\nan encoder, with a simple upsampling decoder, gives the best results in terms\nof RSME, REL and log10: 0.386, 0.113 and 0.049, respectively. We also perform a\nqualitative analysis which illustrates that our model produces depth maps that\nclosely resemble ground truth, even in cases where the ground truth is flawed.\nThe results indicate significant improvements in accuracy and robustness, with\nEfficientNet being the most successful architecture.\n","authors":["Muhammad Adeel Hafeez","Michael G. Madden","Ganesh Sistu","Ihsan Ullah"],"pdf_url":"https://arxiv.org/pdf/2404.07686v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.09657v2","updated":"2024-04-11T12:25:45Z","published":"2022-08-20T10:59:33Z","title":"Is Medieval Distant Viewing Possible? : Extending and Enriching\n  Annotation of Legacy Image Collections using Visual Analytics","summary":"  Distant viewing approaches have typically used image datasets close to the\ncontemporary image data used to train machine learning models. To work with\nimages from other historical periods requires expert annotated data, and the\nquality of labels is crucial for the quality of results. Especially when\nworking with cultural heritage collections that contain myriad uncertainties,\nannotating data, or re-annotating, legacy data is an arduous task. In this\npaper, we describe working with two pre-annotated sets of medieval manuscript\nimages that exhibit conflicting and overlapping metadata. Since a manual\nreconciliation of the two legacy ontologies would be very expensive, we aim (1)\nto create a more uniform set of descriptive labels to serve as a \"bridge\" in\nthe combined dataset, and (2) to establish a high quality hierarchical\nclassification that can be used as a valuable input for subsequent supervised\nmachine learning. To achieve these goals, we developed visualization and\ninteraction mechanisms, enabling medievalists to combine, regularize and extend\nthe vocabulary used to describe these, and other cognate, image datasets. The\nvisual interfaces provide experts an overview of relationships in the data\ngoing beyond the sum total of the metadata. Word and image embeddings as well\nas co-occurrences of labels across the datasets, enable batch re-annotation of\nimages, recommendation of label candidates and support composing a hierarchical\nclassification of labels.\n","authors":["Christofer Meinecke","Estelle Gu√©ville","David Joseph Wrisley","Stefan J√§nicke"],"pdf_url":"https://arxiv.org/pdf/2208.09657v2.pdf","comment":"Revision after DSH Peer Review. Paper is now accepted at DSH"},{"id":"http://arxiv.org/abs/2404.07685v1","updated":"2024-04-11T12:24:47Z","published":"2024-04-11T12:24:47Z","title":"Run-time Monitoring of 3D Object Detection in Automated Driving Systems\n  Using Early Layer Neural Activation Patterns","summary":"  Monitoring the integrity of object detection for errors within the perception\nmodule of automated driving systems (ADS) is paramount for ensuring safety.\nDespite recent advancements in deep neural network (DNN)-based object\ndetectors, their susceptibility to detection errors, particularly in the\nless-explored realm of 3D object detection, remains a significant concern.\nState-of-the-art integrity monitoring (also known as introspection) mechanisms\nin 2D object detection mainly utilise the activation patterns in the final\nlayer of the DNN-based detector's backbone. However, that may not sufficiently\naddress the complexities and sparsity of data in 3D object detection. To this\nend, we conduct, in this article, an extensive investigation into the effects\nof activation patterns extracted from various layers of the backbone network\nfor introspecting the operation of 3D object detectors. Through a comparative\nanalysis using Kitti and NuScenes datasets with PointPillars and CenterPoint\ndetectors, we demonstrate that using earlier layers' activation patterns\nenhances the error detection performance of the integrity monitoring system,\nyet increases computational complexity. To address the real-time operation\nrequirements in ADS, we also introduce a novel introspection method that\ncombines activation patterns from multiple layers of the detector's backbone\nand report its performance.\n","authors":["Hakan Yekta Yatbaz","Mehrdad Dianati","Konstantinos Koufos","Roger Woodman"],"pdf_url":"https://arxiv.org/pdf/2404.07685v1.pdf","comment":"Accepted by CVPR 2024 Workshop on Safe Autonomy for All Domains\n  (SAIAD)"},{"id":"http://arxiv.org/abs/2404.07676v1","updated":"2024-04-11T12:14:48Z","published":"2024-04-11T12:14:48Z","title":"Model-based Cleaning of the QUILT-1M Pathology Dataset for\n  Text-Conditional Image Synthesis","summary":"  The QUILT-1M dataset is the first openly available dataset containing images\nharvested from various online sources. While it provides a huge data variety,\nthe image quality and composition is highly heterogeneous, impacting its\nutility for text-conditional image synthesis. We propose an automatic pipeline\nthat provides predictions of the most common impurities within the images,\ne.g., visibility of narrators, desktop environment and pathology software, or\ntext within the image. Additionally, we propose to use semantic alignment\nfiltering of the image-text pairs. Our findings demonstrate that by rigorously\nfiltering the dataset, there is a substantial enhancement of image fidelity in\ntext-to-image tasks.\n","authors":["Marc Aubreville","Jonathan Ganz","Jonas Ammeling","Christopher C. Kaltenecker","Christof A. Bertram"],"pdf_url":"https://arxiv.org/pdf/2404.07676v1.pdf","comment":"4 pages (short paper)"},{"id":"http://arxiv.org/abs/2402.13255v2","updated":"2024-04-11T12:13:27Z","published":"2024-02-20T18:59:57Z","title":"How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a Survey","summary":"  Over the past two decades, research in the field of Simultaneous Localization\nand Mapping (SLAM) has undergone a significant evolution, highlighting its\ncritical role in enabling autonomous exploration of unknown environments. This\nevolution ranges from hand-crafted methods, through the era of deep learning,\nto more recent developments focused on Neural Radiance Fields (NeRFs) and 3D\nGaussian Splatting (3DGS) representations. Recognizing the growing body of\nresearch and the absence of a comprehensive survey on the topic, this paper\naims to provide the first comprehensive overview of SLAM progress through the\nlens of the latest advancements in radiance fields. It sheds light on the\nbackground, evolutionary path, inherent strengths and limitations, and serves\nas a fundamental reference to highlight the dynamic progress and specific\nchallenges.\n","authors":["Fabio Tosi","Youmin Zhang","Ziren Gong","Erik Sandstr√∂m","Stefano Mattoccia","Martin R. Oswald","Matteo Poggi"],"pdf_url":"https://arxiv.org/pdf/2402.13255v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07671v1","updated":"2024-04-11T12:06:50Z","published":"2024-04-11T12:06:50Z","title":"Deep learning-driven pulmonary arteries and veins segmentation reveals\n  demography-associated pulmonary vasculature anatomy","summary":"  Pulmonary artery-vein segmentation is crucial for diagnosing pulmonary\ndiseases and surgical planning, and is traditionally achieved by Computed\nTomography Pulmonary Angiography (CTPA). However, concerns regarding adverse\nhealth effects from contrast agents used in CTPA have constrained its clinical\nutility. In contrast, identifying arteries and veins using non-contrast CT, a\nconventional and low-cost clinical examination routine, has long been\nconsidered impossible. Here we propose a High-abundant Pulmonary Artery-vein\nSegmentation (HiPaS) framework achieving accurate artery-vein segmentation on\nboth non-contrast CT and CTPA across various spatial resolutions. HiPaS first\nperforms spatial normalization on raw CT scans via a super-resolution module,\nand then iteratively achieves segmentation results at different branch levels\nby utilizing the low-level vessel segmentation as a prior for high-level vessel\nsegmentation. We trained and validated HiPaS on our established multi-centric\ndataset comprising 1,073 CT volumes with meticulous manual annotation. Both\nquantitative experiments and clinical evaluation demonstrated the superior\nperformance of HiPaS, achieving a dice score of 91.8% and a sensitivity of\n98.0%. Further experiments demonstrated the non-inferiority of HiPaS\nsegmentation on non-contrast CT compared to segmentation on CTPA. Employing\nHiPaS, we have conducted an anatomical study of pulmonary vasculature on 10,613\nparticipants in China (five sites), discovering a new association between\npulmonary vessel abundance and sex and age: vessel abundance is significantly\nhigher in females than in males, and slightly decreases with age, under the\ncontrolling of lung volumes (p < 0.0001). HiPaS realizing accurate artery-vein\nsegmentation delineates a promising avenue for clinical diagnosis and\nunderstanding pulmonary physiology in a non-invasive manner.\n","authors":["Yuetan Chu","Gongning Luo","Longxi Zhou","Shaodong Cao","Guolin Ma","Xianglin Meng","Juexiao Zhou","Changchun Yang","Dexuan Xie","Ricardo Henao","Xigang Xiao","Lianming Wu","Zhaowen Qiu","Xin Gao"],"pdf_url":"https://arxiv.org/pdf/2404.07671v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11111v2","updated":"2024-04-11T12:01:34Z","published":"2024-03-17T06:31:16Z","title":"3D Human Reconstruction in the Wild with Synthetic Data Using Generative\n  Models","summary":"  In this work, we show that synthetic data created by generative models is\ncomplementary to computer graphics (CG) rendered data for achieving remarkable\ngeneralization performance on diverse real-world scenes for 3D human pose and\nshape estimation (HPS). Specifically, we propose an effective approach based on\nrecent diffusion models, termed HumanWild, which can effortlessly generate\nhuman images and corresponding 3D mesh annotations. We first collect a\nlarge-scale human-centric dataset with comprehensive annotations, e.g., text\ncaptions and surface normal images. Then, we train a customized ControlNet\nmodel upon this dataset to generate diverse human images and initial\nground-truth labels. At the core of this step is that we can easily obtain\nnumerous surface normal images from a 3D human parametric model, e.g., SMPL-X,\nby rendering the 3D mesh onto the image plane. As there exists inevitable noise\nin the initial labels, we then apply an off-the-shelf foundation segmentation\nmodel, i.e., SAM, to filter negative data samples. Our data generation pipeline\nis flexible and customizable to facilitate different real-world tasks, e.g.,\nego-centric scenes and perspective-distortion scenes. The generated dataset\ncomprises 0.79M images with corresponding 3D annotations, covering versatile\nviewpoints, scenes, and human identities. We train various HPS regressors on\ntop of the generated data and evaluate them on a wide range of benchmarks\n(3DPW, RICH, EgoBody, AGORA, SSP-3D) to verify the effectiveness of the\ngenerated data. By exclusively employing generative models, we generate\nlarge-scale in-the-wild human images and high-quality annotations, eliminating\nthe need for real-world data collection.\n","authors":["Yongtao Ge","Wenjia Wang","Yongfan Chen","Hao Chen","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2403.11111v2.pdf","comment":"project page: https://yongtaoge.github.io/projects/humanwild"},{"id":"http://arxiv.org/abs/2404.07668v1","updated":"2024-04-11T12:00:13Z","published":"2024-04-11T12:00:13Z","title":"Shape Completion in the Dark: Completing Vertebrae Morphology from 3D\n  Ultrasound","summary":"  Purpose: Ultrasound (US) imaging, while advantageous for its radiation-free\nnature, is challenging to interpret due to only partially visible organs and a\nlack of complete 3D information. While performing US-based diagnosis or\ninvestigation, medical professionals therefore create a mental map of the 3D\nanatomy. In this work, we aim to replicate this process and enhance the visual\nrepresentation of anatomical structures.\n  Methods: We introduce a point-cloud-based probabilistic DL method to complete\noccluded anatomical structures through 3D shape completion and choose US-based\nspine examinations as our application. To enable training, we generate\nsynthetic 3D representations of partially occluded spinal views by mimicking US\nphysics and accounting for inherent artifacts.\n  Results: The proposed model performs consistently on synthetic and patient\ndata, with mean and median differences of 2.02 and 0.03 in CD, respectively.\nOur ablation study demonstrates the importance of US physics-based data\ngeneration, reflected in the large mean and median difference of 11.8 CD and\n9.55 CD, respectively. Additionally, we demonstrate that anatomic landmarks,\nsuch as the spinous process (with reconstruction CD of 4.73) and the facet\njoints (mean distance to GT of 4.96mm) are preserved in the 3D completion.\n  Conclusion: Our work establishes the feasibility of 3D shape completion for\nlumbar vertebrae, ensuring the preservation of level-wise characteristics and\nsuccessful generalization from synthetic to real data. The incorporation of US\nphysics contributes to more accurate patient data completions. Notably, our\nmethod preserves essential anatomic landmarks and reconstructs crucial\ninjections sites at their correct locations. The generated data and source code\nwill be made publicly available\n(https://github.com/miruna20/Shape-Completion-in-the-Dark).\n","authors":["Miruna-Alexandra Gafencu","Yordanka Velikova","Mahdi Saleh","Tamas Ungi","Nassir Navab","Thomas Wendler","Mohammad Farid Azampour"],"pdf_url":"https://arxiv.org/pdf/2404.07668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07667v1","updated":"2024-04-11T12:00:06Z","published":"2024-04-11T12:00:06Z","title":"Dealing with Subject Similarity in Differential Morphing Attack\n  Detection","summary":"  The advent of morphing attacks has posed significant security concerns for\nautomated Face Recognition systems, raising the pressing need for robust and\neffective Morphing Attack Detection (MAD) methods able to effectively address\nthis issue. In this paper, we focus on Differential MAD (D-MAD), where a\ntrusted live capture, usually representing the criminal, is compared with the\ndocument image to classify it as morphed or bona fide. We show these approaches\nbased on identity features are effective when the morphed image and the live\none are sufficiently diverse; unfortunately, the effectiveness is significantly\nreduced when the same approaches are applied to look-alike subjects or in all\nthose cases when the similarity between the two compared images is high (e.g.\ncomparison between the morphed image and the accomplice). Therefore, in this\npaper, we propose ACIdA, a modular D-MAD system, consisting of a module for the\nattempt type classification, and two modules for the identity and artifacts\nanalysis on input images. Successfully addressing this task would allow\nbroadening the D-MAD applications including, for instance, the document\nenrollment stage, which currently relies entirely on human evaluation, thus\nlimiting the possibility of releasing ID documents with manipulated images, as\nwell as the automated gates to detect both accomplices and criminals. An\nextensive cross-dataset experimental evaluation conducted on the introduced\nscenario shows that ACIdA achieves state-of-the-art results, outperforming\nliterature competitors, while maintaining good performance in traditional D-MAD\nbenchmarks.\n","authors":["Nicol√≤ Di Domenico","Guido Borghi","Annalisa Franco","Davide Maltoni"],"pdf_url":"https://arxiv.org/pdf/2404.07667v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07664v1","updated":"2024-04-11T11:55:42Z","published":"2024-04-11T11:55:42Z","title":"Finding Dino: A plug-and-play framework for unsupervised detection of\n  out-of-distribution objects using prototypes","summary":"  Detecting and localising unknown or Out-of-distribution (OOD) objects in any\nscene can be a challenging task in vision. Particularly, in safety-critical\ncases involving autonomous systems like automated vehicles or trains.\nSupervised anomaly segmentation or open-world object detection models depend on\ntraining on exhaustively annotated datasets for every domain and still struggle\nin distinguishing between background and OOD objects. In this work, we present\na plug-and-play generalised framework - PRototype-based zero-shot OOD detection\nWithout Labels (PROWL). It is an inference-based method that does not require\ntraining on the domain dataset and relies on extracting relevant features from\nself-supervised pre-trained models. PROWL can be easily adapted to detect OOD\nobjects in any operational design domain by specifying a list of known classes\nfrom this domain. PROWL, as an unsupervised method, outperforms other\nsupervised methods trained without auxiliary OOD data on the RoadAnomaly and\nRoadObstacle datasets provided in SegmentMeIfYouCan (SMIYC) benchmark. We also\ndemonstrate its suitability for other domains such as rail and maritime scenes.\n","authors":["Poulami Sinhamahapatra","Franziska Schwaiger","Shirsha Bose","Huiyu Wang","Karsten Roscher","Stephan Guennemann"],"pdf_url":"https://arxiv.org/pdf/2404.07664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03122v2","updated":"2024-04-11T11:42:13Z","published":"2024-03-05T17:07:29Z","title":"NRDF: Neural Riemannian Distance Fields for Learning Articulated Pose\n  Priors","summary":"  Faithfully modeling the space of articulations is a crucial task that allows\nrecovery and generation of realistic poses, and remains a notorious challenge.\nTo this end, we introduce Neural Riemannian Distance Fields (NRDFs),\ndata-driven priors modeling the space of plausible articulations, represented\nas the zero-level-set of a neural field in a high-dimensional\nproduct-quaternion space. To train NRDFs only on positive examples, we\nintroduce a new sampling algorithm, ensuring that the geodesic distances follow\na desired distribution, yielding a principled distance field learning paradigm.\nWe then devise a projection algorithm to map any random pose onto the level-set\nby an adaptive-step Riemannian optimizer, adhering to the product manifold of\njoint rotations at all times. NRDFs can compute the Riemannian gradient via\nbackpropagation and by mathematical analogy, are related to Riemannian flow\nmatching, a recent generative model. We conduct a comprehensive evaluation of\nNRDF against other pose priors in various downstream tasks, i.e., pose\ngeneration, image-based pose estimation, and solving inverse kinematics,\nhighlighting NRDF's superior performance. Besides humans, NRDF's versatility\nextends to hand and animal poses, as it can effectively represent any\narticulation.\n","authors":["Yannan He","Garvita Tiwari","Tolga Birdal","Jan Eric Lenssen","Gerard Pons-Moll"],"pdf_url":"https://arxiv.org/pdf/2403.03122v2.pdf","comment":"Accepted by CVPR 2024. Project page:\n  https://virtualhumans.mpi-inf.mpg.de/nrdf"},{"id":"http://arxiv.org/abs/2404.07649v1","updated":"2024-04-11T11:12:06Z","published":"2024-04-11T11:12:06Z","title":"Separated Attention: An Improved Cycle GAN Based Under Water Image\n  Enhancement Method","summary":"  In this paper we have present an improved Cycle GAN based model for under\nwater image enhancement. We have utilized the cycle consistent learning\ntechnique of the state-of-the-art Cycle GAN model with modification in the loss\nfunction in terms of depth-oriented attention which enhance the contrast of the\noverall image, keeping global content, color, local texture, and style\ninformation intact. We trained the Cycle GAN model with the modified loss\nfunctions on the benchmarked Enhancing Underwater Visual Perception (EUPV)\ndataset a large dataset including paired and unpaired sets of underwater images\n(poor and good quality) taken with seven distinct cameras in a range of\nvisibility situation during research on ocean exploration and human-robot\ncooperation. In addition, we perform qualitative and quantitative evaluation\nwhich supports the given technique applied and provided a better contrast\nenhancement model of underwater imagery. More significantly, the upgraded\nimages provide better results from conventional models and further for under\nwater navigation, pose estimation, saliency prediction, object detection and\ntracking. The results validate the appropriateness of the model for autonomous\nunderwater vehicles (AUV) in visual navigation.\n","authors":["Tashmoy Ghosh"],"pdf_url":"https://arxiv.org/pdf/2404.07649v1.pdf","comment":"9 pages, 8 figures"},{"id":"http://arxiv.org/abs/2404.07645v1","updated":"2024-04-11T11:07:57Z","published":"2024-04-11T11:07:57Z","title":"Simba: Mamba augmented U-ShiftGCN for Skeletal Action Recognition in\n  Videos","summary":"  Skeleton Action Recognition (SAR) involves identifying human actions using\nskeletal joint coordinates and their interconnections. While plain Transformers\nhave been attempted for this task, they still fall short compared to the\ncurrent leading methods, which are rooted in Graph Convolutional Networks\n(GCNs) due to the absence of structural priors. Recently, a novel selective\nstate space model, Mamba, has surfaced as a compelling alternative to the\nattention mechanism in Transformers, offering efficient modeling of long\nsequences. In this work, to the utmost extent of our awareness, we present the\nfirst SAR framework incorporating Mamba. Each fundamental block of our model\nadopts a novel U-ShiftGCN architecture with Mamba as its core component. The\nencoder segment of the U-ShiftGCN is devised to extract spatial features from\nthe skeletal data using downsampling vanilla Shift S-GCN blocks. These spatial\nfeatures then undergo intermediate temporal modeling facilitated by the Mamba\nblock before progressing to the encoder section, which comprises vanilla\nupsampling Shift S-GCN blocks. Additionally, a Shift T-GCN (ShiftTCN) temporal\nmodeling unit is employed before the exit of each fundamental block to refine\ntemporal representations. This particular integration of downsampling spatial,\nintermediate temporal, upsampling spatial, and ultimate temporal subunits\nyields promising results for skeleton action recognition. We dub the resulting\nmodel \\textbf{Simba}, which attains state-of-the-art performance across three\nwell-known benchmark skeleton action recognition datasets: NTU RGB+D, NTU RGB+D\n120, and Northwestern-UCLA. Interestingly, U-ShiftGCN (Simba without\nIntermediate Mamba Block) by itself is capable of performing reasonably well\nand surpasses our baseline.\n","authors":["Soumyabrata Chaudhuri","Saumik Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2404.07645v1.pdf","comment":"20 pages, 6 tables, 1 figure"},{"id":"http://arxiv.org/abs/2404.03425v2","updated":"2024-04-11T10:51:34Z","published":"2024-04-04T13:06:25Z","title":"ChangeMamba: Remote Sensing Change Detection with Spatio-Temporal State\n  Space Model","summary":"  Convolutional neural networks (CNN) and Transformers have made impressive\nprogress in the field of remote sensing change detection (CD). However, both\narchitectures have inherent shortcomings. Recently, the Mamba architecture,\nbased on state space models, has shown remarkable performance in a series of\nnatural language processing tasks, which can effectively compensate for the\nshortcomings of the above two architectures. In this paper, we explore for the\nfirst time the potential of the Mamba architecture for remote sensing CD tasks.\nWe tailor the corresponding frameworks, called MambaBCD, MambaSCD, and\nMambaBDA, for binary change detection (BCD), semantic change detection (SCD),\nand building damage assessment (BDA), respectively. All three frameworks adopt\nthe cutting-edge Visual Mamba architecture as the encoder, which allows full\nlearning of global spatial contextual information from the input images. For\nthe change decoder, which is available in all three architectures, we propose\nthree spatio-temporal relationship modeling mechanisms, which can be naturally\ncombined with the Mamba architecture and fully utilize its attribute to achieve\nspatio-temporal interaction of multi-temporal features, thereby obtaining\naccurate change information. On five benchmark datasets, our proposed\nframeworks outperform current CNN- and Transformer-based approaches without\nusing any complex training strategies or tricks, fully demonstrating the\npotential of the Mamba architecture in CD tasks. Specifically, we obtained\n83.11%, 88.39% and 94.19% F1 scores on the three BCD datasets SYSU, LEVIR-CD+,\nand WHU-CD; on the SCD dataset SECOND, we obtained 24.11% SeK; and on the BDA\ndataset xBD, we obtained 81.41% overall F1 score. Further experiments show that\nour architecture is quite robust to degraded data. The source code will be\navailable in https://github.com/ChenHongruixuan/MambaCD\n","authors":["Hongruixuan Chen","Jian Song","Chengxi Han","Junshi Xia","Naoto Yokoya"],"pdf_url":"https://arxiv.org/pdf/2404.03425v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16074v2","updated":"2024-04-11T10:45:05Z","published":"2023-10-24T15:16:19Z","title":"RePoseDM: Recurrent Pose Alignment and Gradient Guidance for Pose Guided\n  Image Synthesis","summary":"  Pose-guided person image synthesis task requires re-rendering a reference\nimage, which should have a photorealistic appearance and flawless pose\ntransfer. Since person images are highly structured, existing approaches\nrequire dense connections for complex deformations and occlusions because these\nare generally handled through multi-level warping and masking in latent space.\nThe feature maps generated by convolutional neural networks do not have\nequivariance, and hence multi-level warping is required to perform pose\nalignment. Inspired by the ability of the diffusion model to generate\nphotorealistic images from the given conditional guidance, we propose recurrent\npose alignment to provide pose-aligned texture features as conditional\nguidance. Due to the leakage of the source pose in conditional guidance, we\npropose gradient guidance from pose interaction fields, which output the\ndistance from the valid pose manifold given a predicted pose as input. This\nhelps in learning plausible pose transfer trajectories that result in\nphotorealism and undistorted texture details. Extensive results on two\nlarge-scale benchmarks and a user study demonstrate the ability of our proposed\napproach to generate photorealistic pose transfer under challenging scenarios.\nAdditionally, we demonstrate the efficiency of gradient guidance in pose-guided\nimage generation on the HumanArt dataset with fine-tuned stable diffusion.\n","authors":["Anant Khandelwal"],"pdf_url":"https://arxiv.org/pdf/2310.16074v2.pdf","comment":"Accepted at CVPR 2024 SyntaGen Workshop, 13 pages, 4 tables, 7\n  figures"},{"id":"http://arxiv.org/abs/2312.01919v2","updated":"2024-04-11T10:38:33Z","published":"2023-12-04T14:23:18Z","title":"COTR: Compact Occupancy TRansformer for Vision-based 3D Occupancy\n  Prediction","summary":"  The autonomous driving community has shown significant interest in 3D\noccupancy prediction, driven by its exceptional geometric perception and\ngeneral object recognition capabilities. To achieve this, current works try to\nconstruct a Tri-Perspective View (TPV) or Occupancy (OCC) representation\nextending from the Bird-Eye-View perception. However, compressed views like TPV\nrepresentation lose 3D geometry information while raw and sparse OCC\nrepresentation requires heavy but redundant computational costs. To address the\nabove limitations, we propose Compact Occupancy TRansformer (COTR), with a\ngeometry-aware occupancy encoder and a semantic-aware group decoder to\nreconstruct a compact 3D OCC representation. The occupancy encoder first\ngenerates a compact geometrical OCC feature through efficient explicit-implicit\nview transformation. Then, the occupancy decoder further enhances the semantic\ndiscriminability of the compact OCC representation by a coarse-to-fine semantic\ngrouping strategy. Empirical experiments show that there are evident\nperformance gains across multiple baselines, e.g., COTR outperforms baselines\nwith a relative improvement of 8%-15%, demonstrating the superiority of our\nmethod.\n","authors":["Qihang Ma","Xin Tan","Yanyun Qu","Lizhuang Ma","Zhizhong Zhang","Yuan Xie"],"pdf_url":"https://arxiv.org/pdf/2312.01919v2.pdf","comment":"CVPR2024. Code is available at https://github.com/NotACracker/COTR"},{"id":"http://arxiv.org/abs/2404.07626v1","updated":"2024-04-11T10:26:40Z","published":"2024-04-11T10:26:40Z","title":"Homography Guided Temporal Fusion for Road Line and Marking Segmentation","summary":"  Reliable segmentation of road lines and markings is critical to autonomous\ndriving. Our work is motivated by the observations that road lines and markings\nare (1) frequently occluded in the presence of moving vehicles, shadow, and\nglare and (2) highly structured with low intra-class shape variance and overall\nhigh appearance consistency. To solve these issues, we propose a Homography\nGuided Fusion (HomoFusion) module to exploit temporally-adjacent video frames\nfor complementary cues facilitating the correct classification of the partially\noccluded road lines or markings. To reduce computational complexity, a novel\nsurface normal estimator is proposed to establish spatial correspondences\nbetween the sampled frames, allowing the HomoFusion module to perform a\npixel-to-pixel attention mechanism in updating the representation of the\noccluded road lines or markings. Experiments on ApolloScape, a large-scale lane\nmark segmentation dataset, and ApolloScape Night with artificial simulated\nnight-time road conditions, demonstrate that our method outperforms other\nexisting SOTA lane mark segmentation models with less than 9\\% of their\nparameters and computational complexity. We show that exploiting available\ncamera intrinsic data and ground plane assumption for cross-frame\ncorrespondence can lead to a light-weight network with significantly improved\nperformances in speed and accuracy. We also prove the versatility of our\nHomoFusion approach by applying it to the problem of water puddle segmentation\nand achieving SOTA performance.\n","authors":["Shan Wang","Chuong Nguyen","Jiawei Liu","Kaihao Zhang","Wenhan Luo","Yanhao Zhang","Sundaram Muthu","Fahira Afzal Maken","Hongdong Li"],"pdf_url":"https://arxiv.org/pdf/2404.07626v1.pdf","comment":"Accepted by ICCV 2023"},{"id":"http://arxiv.org/abs/2308.15855v2","updated":"2024-04-11T10:19:41Z","published":"2023-08-30T08:44:21Z","title":"IIDM: Inter and Intra-domain Mixing for Semi-supervised Domain\n  Adaptation in Semantic Segmentation","summary":"  Despite recent advances in semantic segmentation, an inevitable challenge is\nthe performance degradation caused by the domain shift in real applications.\nCurrent dominant approach to solve this problem is unsupervised domain\nadaptation (UDA). However, the absence of labeled target data in UDA is overly\nrestrictive and limits performance. To overcome this limitation, a more\npractical scenario called semi-supervised domain adaptation (SSDA) has been\nproposed. Existing SSDA methods are derived from the UDA paradigm and primarily\nfocus on leveraging the unlabeled target data and source data. In this paper,\nwe highlight the significance of exploiting the intra-domain information\nbetween the labeled target data and unlabeled target data. Instead of solely\nusing the scarce labeled target data for supervision, we propose a novel SSDA\nframework that incorporates both Inter and Intra Domain Mixing (IIDM), where\ninter-domain mixing mitigates the source-target domain gap and intra-domain\nmixing enriches the available target domain information, and the network can\ncapture more domain-invariant features. We also explore different domain mixing\nstrategies to better exploit the target domain information. Comprehensive\nexperiments conducted on the GTA5 to Cityscapes and SYNTHIA to Cityscapes\nbenchmarks demonstrate the effectiveness of IIDM, surpassing previous methods\nby a large margin.\n","authors":["Weifu Fu","Qiang Nie","Jialin Li","Yuhuan Lin","Kai Wu","Jian Li","Yabiao Wang","Yong Liu","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2308.15855v2.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2404.07622v1","updated":"2024-04-11T10:16:44Z","published":"2024-04-11T10:16:44Z","title":"Multi-Image Visual Question Answering for Unsupervised Anomaly Detection","summary":"  Unsupervised anomaly detection enables the identification of potential\npathological areas by juxtaposing original images with their pseudo-healthy\nreconstructions generated by models trained exclusively on normal images.\nHowever, the clinical interpretation of resultant anomaly maps presents a\nchallenge due to a lack of detailed, understandable explanations. Recent\nadvancements in language models have shown the capability of mimicking\nhuman-like understanding and providing detailed descriptions. This raises an\ninteresting question: \\textit{How can language models be employed to make the\nanomaly maps more explainable?} To the best of our knowledge, we are the first\nto leverage a language model for unsupervised anomaly detection, for which we\nconstruct a dataset with different questions and answers. Additionally, we\npresent a novel multi-image visual question answering framework tailored for\nanomaly detection, incorporating diverse feature fusion strategies to enhance\nvisual knowledge extraction. Our experiments reveal that the framework,\naugmented by our new Knowledge Q-Former module, adeptly answers questions on\nthe anomaly detection dataset. Besides, integrating anomaly maps as inputs\ndistinctly aids in improving the detection of unseen pathologies.\n","authors":["Jun Li","Cosmin I. Bercea","Philip M√ºller","Lina Felsner","Suhwan Kim","Daniel Rueckert","Benedikt Wiestler","Julia A. Schnabel"],"pdf_url":"https://arxiv.org/pdf/2404.07622v1.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2404.07620v1","updated":"2024-04-11T10:14:56Z","published":"2024-04-11T10:14:56Z","title":"Diffusion Probabilistic Multi-cue Level Set for Reducing Edge\n  Uncertainty in Pancreas Segmentation","summary":"  Accurately segmenting the pancreas remains a huge challenge. Traditional\nmethods encounter difficulties in semantic localization due to the small volume\nand distorted structure of the pancreas, while deep learning methods encounter\nchallenges in obtaining accurate edges because of low contrast and organ\noverlapping. To overcome these issues, we propose a multi-cue level set method\nbased on the diffusion probabilistic model, namely Diff-mcs. Our method adopts\na coarse-to-fine segmentation strategy. We use the diffusion probabilistic\nmodel in the coarse segmentation stage, with the obtained probability\ndistribution serving as both the initial localization and prior cues for the\nlevel set method. In the fine segmentation stage, we combine the prior cues\nwith grayscale cues and texture cues to refine the edge by maximizing the\ndifference between probability distributions of the cues inside and outside the\nlevel set curve. The method is validated on three public datasets and achieves\nstate-of-the-art performance, which can obtain more accurate segmentation\nresults with lower uncertainty segmentation edges. In addition, we conduct\nablation studies and uncertainty analysis to verify that the diffusion\nprobability model provides a more appropriate initialization for the level set\nmethod. Furthermore, when combined with multiple cues, the level set method can\nbetter obtain edges and improve the overall accuracy. Our code is available at\nhttps://github.com/GOUYUEE/Diff-mcs.\n","authors":["Yue Gou","Yuming Xing","Shengzhu Shi","Zhichang Guo"],"pdf_url":"https://arxiv.org/pdf/2404.07620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18956v2","updated":"2024-04-11T10:06:10Z","published":"2024-02-29T08:51:51Z","title":"WWW: A Unified Framework for Explaining What, Where and Why of Neural\n  Networks by Interpretation of Neuron Concepts","summary":"  Recent advancements in neural networks have showcased their remarkable\ncapabilities across various domains. Despite these successes, the \"black box\"\nproblem still remains. Addressing this, we propose a novel framework, WWW, that\noffers the 'what', 'where', and 'why' of the neural network decisions in\nhuman-understandable terms. Specifically, WWW utilizes adaptive selection for\nconcept discovery, employing adaptive cosine similarity and thresholding\ntechniques to effectively explain 'what'. To address the 'where' and 'why', we\nproposed a novel combination of neuron activation maps (NAMs) with Shapley\nvalues, generating localized concept maps and heatmaps for individual inputs.\nFurthermore, WWW introduces a method for predicting uncertainty, leveraging\nheatmap similarities to estimate 'how' reliable the prediction is. Experimental\nevaluations of WWW demonstrate superior performance in both quantitative and\nqualitative metrics, outperforming existing methods in interpretability. WWW\nprovides a unified solution for explaining 'what', 'where', and 'why',\nintroducing a method for localized explanations from global interpretations and\noffering a plug-and-play solution adaptable to various architectures.\n","authors":["Yong Hyun Ahn","Hyeon Bae Kim","Seong Tae Kim"],"pdf_url":"https://arxiv.org/pdf/2402.18956v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2404.01705v2","updated":"2024-04-11T10:05:12Z","published":"2024-04-02T07:38:16Z","title":"Samba: Semantic Segmentation of Remotely Sensed Images with State Space\n  Model","summary":"  High-resolution remotely sensed images pose a challenge for commonly used\nsemantic segmentation methods such as Convolutional Neural Network (CNN) and\nVision Transformer (ViT). CNN-based methods struggle with handling such\nhigh-resolution images due to their limited receptive field, while ViT faces\nchallenges in handling long sequences. Inspired by Mamba, which adopts a State\nSpace Model (SSM) to efficiently capture global semantic information, we\npropose a semantic segmentation framework for high-resolution remotely sensed\nimages, named Samba. Samba utilizes an encoder-decoder architecture, with Samba\nblocks serving as the encoder for efficient multi-level semantic information\nextraction, and UperNet functioning as the decoder. We evaluate Samba on the\nLoveDA, ISPRS Vaihingen, and ISPRS Potsdam datasets, comparing its performance\nagainst top-performing CNN and ViT methods. The results reveal that Samba\nachieved unparalleled performance on commonly used remote sensing datasets for\nsemantic segmentation. Our proposed Samba demonstrates for the first time the\neffectiveness of SSM in semantic segmentation of remotely sensed images,\nsetting a new benchmark in performance for Mamba-based techniques in this\nspecific application. The source code and baseline implementations are\navailable at https://github.com/zhuqinfeng1999/Samba.\n","authors":["Qinfeng Zhu","Yuanzhi Cai","Yuan Fang","Yihan Yang","Cheng Chen","Lei Fan","Anh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2404.01705v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07610v1","updated":"2024-04-11T09:58:23Z","published":"2024-04-11T09:58:23Z","title":"Do You Remember? Dense Video Captioning with Cross-Modal Memory\n  Retrieval","summary":"  There has been significant attention to the research on dense video\ncaptioning, which aims to automatically localize and caption all events within\nuntrimmed video. Several studies introduce methods by designing dense video\ncaptioning as a multitasking problem of event localization and event captioning\nto consider inter-task relations. However, addressing both tasks using only\nvisual input is challenging due to the lack of semantic content. In this study,\nwe address this by proposing a novel framework inspired by the cognitive\ninformation processing of humans. Our model utilizes external memory to\nincorporate prior knowledge. The memory retrieval method is proposed with\ncross-modal video-to-text matching. To effectively incorporate retrieved text\nfeatures, the versatile encoder and the decoder with visual and textual\ncross-attention modules are designed. Comparative experiments have been\nconducted to show the effectiveness of the proposed method on ActivityNet\nCaptions and YouCook2 datasets. Experimental results show promising performance\nof our model without extensive pretraining from a large video dataset.\n","authors":["Minkuk Kim","Hyeon Bae Kim","Jinyoung Moon","Jinwoo Choi","Seong Tae Kim"],"pdf_url":"https://arxiv.org/pdf/2404.07610v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2404.07607v1","updated":"2024-04-11T09:50:05Z","published":"2024-04-11T09:50:05Z","title":"Automatic Detection of Dark Ship-to-Ship Transfers using Deep Learning\n  and Satellite Imagery","summary":"  Despite extensive research into ship detection via remote sensing, no studies\nidentify ship-to-ship transfers in satellite imagery. Given the importance of\ntransshipment in illicit shipping practices, this is a significant gap. In what\nfollows, I train a convolutional neural network to accurately detect 4\ndifferent types of cargo vessel and two different types of Ship-to-Ship\ntransfer in PlanetScope satellite imagery. I then elaborate a pipeline for the\nautomatic detection of suspected illicit ship-to-ship transfers by\ncross-referencing satellite detections with vessel borne GPS data. Finally, I\napply this method to the Kerch Strait between Ukraine and Russia to identify\nover 400 dark transshipment events since 2022.\n","authors":["Ollie Ballinger"],"pdf_url":"https://arxiv.org/pdf/2404.07607v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07605v1","updated":"2024-04-11T09:47:52Z","published":"2024-04-11T09:47:52Z","title":"Contrastive-Based Deep Embeddings for Label Noise-Resilient\n  Histopathology Image Classification","summary":"  Recent advancements in deep learning have proven highly effective in medical\nimage classification, notably within histopathology. However, noisy labels\nrepresent a critical challenge in histopathology image classification, where\naccurate annotations are vital for training robust deep learning models.\nIndeed, deep neural networks can easily overfit label noise, leading to severe\ndegradations in model performance. While numerous public pathology foundation\nmodels have emerged recently, none have evaluated their resilience to label\nnoise. Through thorough empirical analyses across multiple datasets, we exhibit\nthe label noise resilience property of embeddings extracted from foundation\nmodels trained in a self-supervised contrastive manner. We demonstrate that\ntraining with such embeddings substantially enhances label noise robustness\nwhen compared to non-contrastive-based ones as well as commonly used\nnoise-resilient methods. Our results unequivocally underline the superiority of\ncontrastive learning in effectively mitigating the label noise challenge. Code\nis publicly available at\nhttps://github.com/LucasDedieu/NoiseResilientHistopathology.\n","authors":["Lucas Dedieu","Nicolas Nerrienet","Adrien Nivaggioli","Clara Simmat","Marceau Clavel","Arnaud Gauthier","St√©phane Sockeel","R√©my Peyret"],"pdf_url":"https://arxiv.org/pdf/2404.07605v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2404.07603v1","updated":"2024-04-11T09:43:07Z","published":"2024-04-11T09:43:07Z","title":"GLID: Pre-training a Generalist Encoder-Decoder Vision Model","summary":"  This paper proposes a GeneraLIst encoder-Decoder (GLID) pre-training method\nfor better handling various downstream computer vision tasks. While\nself-supervised pre-training approaches, e.g., Masked Autoencoder, have shown\nsuccess in transfer learning, task-specific sub-architectures are still\nrequired to be appended for different downstream tasks, which cannot enjoy the\nbenefits of large-scale pre-training. GLID overcomes this challenge by allowing\nthe pre-trained generalist encoder-decoder to be fine-tuned on various vision\ntasks with minimal task-specific architecture modifications. In the GLID\ntraining scheme, pre-training pretext task and other downstream tasks are\nmodeled as \"query-to-answer\" problems, including the pre-training pretext task\nand other downstream tasks. We pre-train a task-agnostic encoder-decoder with\nquery-mask pairs. During fine-tuning, GLID maintains the pre-trained\nencoder-decoder and queries, only replacing the topmost linear transformation\nlayer with task-specific linear heads. This minimizes the pretrain-finetune\narchitecture inconsistency and enables the pre-trained model to better adapt to\ndownstream tasks. GLID achieves competitive performance on various vision\ntasks, including object detection, image segmentation, pose estimation, and\ndepth estimation, outperforming or matching specialist models such as\nMask2Former, DETR, ViTPose, and BinsFormer.\n","authors":["Jihao Liu","Jinliang Zheng","Yu Liu","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2404.07603v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2404.07602v1","updated":"2024-04-11T09:41:14Z","published":"2024-04-11T09:41:14Z","title":"Attention based End to end network for Offline Writer Identification on\n  Word level data","summary":"  Writer identification due to its widespread application in various fields has\ngained popularity over the years. In scenarios where optimum handwriting\nsamples are available, whether they be in the form of a single line, a\nsentence, or an entire page, writer identification algorithms have demonstrated\nnoteworthy levels of accuracy. However, in scenarios where only a limited\nnumber of handwritten samples are available, particularly in the form of word\nimages, there is a significant scope for improvement.\n  In this paper, we propose a writer identification system based on an\nattention-driven Convolutional Neural Network (CNN). The system is trained\nutilizing image segments, known as fragments, extracted from word images,\nemploying a pyramid-based strategy. This methodology enables the system to\ncapture a comprehensive representation of the data, encompassing both\nfine-grained details and coarse features across various levels of abstraction.\nThese extracted fragments serve as the training data for the convolutional\nnetwork, enabling it to learn a more robust representation compared to\ntraditional convolution-based networks trained on word images. Additionally,\nthe paper explores the integration of an attention mechanism to enhance the\nrepresentational power of the learned features. The efficacy of the proposed\nalgorithm is evaluated on three benchmark databases, demonstrating its\nproficiency in writer identification tasks, particularly in scenarios with\nlimited access to handwriting data.\n","authors":["Vineet Kumar","Suresh Sundaram"],"pdf_url":"https://arxiv.org/pdf/2404.07602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07600v1","updated":"2024-04-11T09:39:58Z","published":"2024-04-11T09:39:58Z","title":"Implicit and Explicit Language Guidance for Diffusion-based Visual\n  Perception","summary":"  Text-to-image diffusion models have shown powerful ability on conditional\nimage synthesis. With large-scale vision-language pre-training, diffusion\nmodels are able to generate high-quality images with rich texture and\nreasonable structure under different text prompts. However, it is an open\nproblem to adapt the pre-trained diffusion model for visual perception. In this\npaper, we propose an implicit and explicit language guidance framework for\ndiffusion-based perception, named IEDP. Our IEDP comprises of an implicit\nlanguage guidance branch and an explicit language guidance branch. The implicit\nbranch employs frozen CLIP image encoder to directly generate implicit text\nembeddings that are fed to diffusion model, without using explicit text\nprompts. The explicit branch utilizes the ground-truth labels of corresponding\nimages as text prompts to condition feature extraction of diffusion model.\nDuring training, we jointly train diffusion model by sharing the model weights\nof these two branches. As a result, implicit and explicit branches can jointly\nguide feature learning. During inference, we only employ implicit branch for\nfinal prediction, which does not require any ground-truth labels. Experiments\nare performed on two typical perception tasks, including semantic segmentation\nand depth estimation. Our IEDP achieves promising performance on both tasks.\nFor semantic segmentation, our IEDP has the mIoU score of 55.9% on AD20K\nvalidation set, which outperforms the baseline method VPD by 2.2%. For depth\nestimation, our IEDP outperforms the baseline method VPD with a relative gain\nof 10.2%.\n","authors":["Hefeng Wang","Jiale Cao","Jin Xie","Aiping Yang","Yanwei Pang"],"pdf_url":"https://arxiv.org/pdf/2404.07600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07594v1","updated":"2024-04-11T09:23:44Z","published":"2024-04-11T09:23:44Z","title":"Weakly-Supervised Learning via Multi-Lateral Decoder Branching for\n  Guidewire Segmentation in Robot-Assisted Cardiovascular Catheterization","summary":"  Although robot-assisted cardiovascular catheterization is commonly performed\nfor intervention of cardiovascular diseases, more studies are needed to support\nthe procedure with automated tool segmentation. This can aid surgeons on tool\ntracking and visualization during intervention. Learning-based segmentation has\nrecently offered state-of-the-art segmentation performances however, generating\nground-truth signals for fully-supervised methods is labor-intensive and time\nconsuming for the interventionists. In this study, a weakly-supervised learning\nmethod with multi-lateral pseudo labeling is proposed for tool segmentation in\ncardiac angiograms. The method includes a modified U-Net model with one encoder\nand multiple lateral-branched decoders that produce pseudo labels as\nsupervision signals under different perturbation. The pseudo labels are\nself-generated through a mixed loss function and shared consistency in the\ndecoders. We trained the model end-to-end with weakly-annotated data obtained\nduring robotic cardiac catheterization. Experiments with the proposed model\nshows weakly annotated data has closer performance to when fully annotated data\nis used. Compared to three existing weakly-supervised methods, our approach\nyielded higher segmentation performance across three different cardiac\nangiogram data. With ablation study, we showed consistent performance under\ndifferent parameters. Thus, we offer a less expensive method for real-time tool\nsegmentation and tracking during robot-assisted cardiac catheterization.\n","authors":["Olatunji Mumini Omisore","Toluwanimi Akinyemi","Anh Nguyen","Lei Wang"],"pdf_url":"https://arxiv.org/pdf/2404.07594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07580v1","updated":"2024-04-11T09:13:50Z","published":"2024-04-11T09:13:50Z","title":"Multi-rater Prompting for Ambiguous Medical Image Segmentation","summary":"  Multi-rater annotations commonly occur when medical images are independently\nannotated by multiple experts (raters). In this paper, we tackle two challenges\narisen in multi-rater annotations for medical image segmentation (called\nambiguous medical image segmentation): (1) How to train a deep learning model\nwhen a group of raters produces a set of diverse but plausible annotations, and\n(2) how to fine-tune the model efficiently when computation resources are not\navailable for re-training the entire model on a different dataset domain. We\npropose a multi-rater prompt-based approach to address these two challenges\naltogether. Specifically, we introduce a series of rater-aware prompts that can\nbe plugged into the U-Net model for uncertainty estimation to handle\nmulti-annotation cases. During the prompt-based fine-tuning process, only 0.3%\nof learnable parameters are required to be updated comparing to training the\nentire model. Further, in order to integrate expert consensus and disagreement,\nwe explore different multi-rater incorporation strategies and design a\nmix-training strategy for comprehensive insight learning. Extensive experiments\nverify the effectiveness of our new approach for ambiguous medical image\nsegmentation on two public datasets while alleviating the heavy burden of model\nre-training.\n","authors":["Jinhong Wang","Yi Cheng","Jintai Chen","Hongxia Xu","Danny Chen","Jian Wu"],"pdf_url":"https://arxiv.org/pdf/2404.07580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07122v2","updated":"2024-04-11T09:10:21Z","published":"2024-04-10T16:01:37Z","title":"Driver Attention Tracking and Analysis","summary":"  We propose a novel method to estimate a driver's points-of-gaze using a pair\nof ordinary cameras mounted on the windshield and dashboard of a car. This is a\nchallenging problem due to the dynamics of traffic environments with 3D scenes\nof unknown depths. This problem is further complicated by the volatile distance\nbetween the driver and the camera system. To tackle these challenges, we\ndevelop a novel convolutional network that simultaneously analyzes the image of\nthe scene and the image of the driver's face. This network has a camera\ncalibration module that can compute an embedding vector that represents the\nspatial configuration between the driver and the camera system. This\ncalibration module improves the overall network's performance, which can be\njointly trained end to end.\n  We also address the lack of annotated data for training and evaluation by\nintroducing a large-scale driving dataset with point-of-gaze annotations. This\nis an in situ dataset of real driving sessions in an urban city, containing\nsynchronized images of the driving scene as well as the face and gaze of the\ndriver. Experiments on this dataset show that the proposed method outperforms\nvarious baseline methods, having the mean prediction error of 29.69 pixels,\nwhich is relatively small compared to the $1280{\\times}720$ resolution of the\nscene camera.\n","authors":["Dat Viet Thanh Nguyen","Anh Tran","Hoai Nam Vu","Cuong Pham","Minh Hoai"],"pdf_url":"https://arxiv.org/pdf/2404.07122v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07564v1","updated":"2024-04-11T08:50:12Z","published":"2024-04-11T08:50:12Z","title":"ObjBlur: A Curriculum Learning Approach With Progressive Object-Level\n  Blurring for Improved Layout-to-Image Generation","summary":"  We present ObjBlur, a novel curriculum learning approach to improve\nlayout-to-image generation models, where the task is to produce realistic\nimages from layouts composed of boxes and labels. Our method is based on\nprogressive object-level blurring, which effectively stabilizes training and\nenhances the quality of generated images. This curriculum learning strategy\nsystematically applies varying degrees of blurring to individual objects or the\nbackground during training, starting from strong blurring to progressively\ncleaner images. Our findings reveal that this approach yields significant\nperformance improvements, stabilized training, smoother convergence, and\nreduced variance between multiple runs. Moreover, our technique demonstrates\nits versatility by being compatible with generative adversarial networks and\ndiffusion models, underlining its applicability across various generative\nmodeling paradigms. With ObjBlur, we reach new state-of-the-art results on the\ncomplex COCO and Visual Genome datasets.\n","authors":["Stanislav Frolov","Brian B. Moser","Sebastian Palacio","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2404.07564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06710v2","updated":"2024-04-11T08:40:42Z","published":"2024-04-10T03:31:32Z","title":"SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike\n  Camera","summary":"  One of the most critical factors in achieving sharp Novel View Synthesis\n(NVS) using neural field methods like Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS) is the quality of the training images. However,\nConventional RGB cameras are susceptible to motion blur. In contrast,\nneuromorphic cameras like event and spike cameras inherently capture more\ncomprehensive temporal information, which can provide a sharp representation of\nthe scene as additional training data. Recent methods have explored the\nintegration of event cameras to improve the quality of NVS. The event-RGB\napproaches have some limitations, such as high training costs and the inability\nto work effectively in the background. Instead, our study introduces a new\nmethod that uses the spike camera to overcome these limitations. By considering\ntexture reconstruction from spike streams as ground truth, we design the\nTexture from Spike (TfS) loss. Since the spike camera relies on temporal\nintegration instead of temporal differentiation used by event cameras, our\nproposed TfS loss maintains manageable training costs. It handles foreground\nobjects with backgrounds simultaneously. We also provide a real-world dataset\ncaptured with our spike-RGB camera system to facilitate future research\nendeavors. We conduct extensive experiments using synthetic and real-world\ndatasets to demonstrate that our design can enhance novel view synthesis across\nNeRF and 3DGS. The code and dataset will be made available for public access.\n","authors":["Gaole Dai","Zhenyu Wang","Qinwen Xu","Ming Lu","Wen Cheng","Baixin Shi","Shanghang Zhang","Tiejun Huang"],"pdf_url":"https://arxiv.org/pdf/2404.06710v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07556v1","updated":"2024-04-11T08:36:36Z","published":"2024-04-11T08:36:36Z","title":"Attention-Aware Laparoscopic Image Desmoking Network with Lightness\n  Embedding and Hybrid Guided Embedding","summary":"  This paper presents a novel method of smoke removal from the laparoscopic\nimages. Due to the heterogeneous nature of surgical smoke, a two-stage network\nis proposed to estimate the smoke distribution and reconstruct a clear,\nsmoke-free surgical scene. The utilization of the lightness channel plays a\npivotal role in providing vital information pertaining to smoke density. The\nreconstruction of smoke-free image is guided by a hybrid embedding, which\ncombines the estimated smoke mask with the initial image. Experimental results\ndemonstrate that the proposed method boasts a Peak Signal to Noise Ratio that\nis $2.79\\%$ higher than the state-of-the-art methods, while also exhibits a\nremarkable $38.2\\%$ reduction in run-time. Overall, the proposed method offers\ncomparable or even superior performance in terms of both smoke removal quality\nand computational efficiency when compared to existing state-of-the-art\nmethods. This work will be publicly available on\nhttp://homepage.hit.edu.cn/wpgao\n","authors":["Ziteng Liu","Jiahua Zhu","Bainan Liu","Hao Liu","Wenpeng Gao","Yili Fu"],"pdf_url":"https://arxiv.org/pdf/2404.07556v1.pdf","comment":"ISBI2024"},{"id":"http://arxiv.org/abs/2404.07554v1","updated":"2024-04-11T08:36:13Z","published":"2024-04-11T08:36:13Z","title":"CAT: Contrastive Adapter Training for Personalized Image Generation","summary":"  The emergence of various adapters, including Low-Rank Adaptation (LoRA)\napplied from the field of natural language processing, has allowed diffusion\nmodels to personalize image generation at a low cost. However, due to the\nvarious challenges including limited datasets and shortage of regularization\nand computation resources, adapter training often results in unsatisfactory\noutcomes, leading to the corruption of the backbone model's prior knowledge.\nOne of the well known phenomena is the loss of diversity in object generation,\nespecially within the same class which leads to generating almost identical\nobjects with minor variations. This poses challenges in generation\ncapabilities. To solve this issue, we present Contrastive Adapter Training\n(CAT), a simple yet effective strategy to enhance adapter training through the\napplication of CAT loss. Our approach facilitates the preservation of the base\nmodel's original knowledge when the model initiates adapters. Furthermore, we\nintroduce the Knowledge Preservation Score (KPS) to evaluate CAT's ability to\nkeep the former information. We qualitatively and quantitatively compare CAT's\nimprovement. Finally, we mention the possibility of CAT in the aspects of\nmulti-concept adapter and optimization.\n","authors":["Jae Wan Park","Sang Hyun Park","Jun Young Koh","Junha Lee","Min Song"],"pdf_url":"https://arxiv.org/pdf/2404.07554v1.pdf","comment":"CVPRW 2024"},{"id":"http://arxiv.org/abs/2404.07553v1","updated":"2024-04-11T08:35:24Z","published":"2024-04-11T08:35:24Z","title":"SFSORT: Scene Features-based Simple Online Real-Time Tracker","summary":"  This paper introduces SFSORT, the world's fastest multi-object tracking\nsystem based on experiments conducted on MOT Challenge datasets. To achieve an\naccurate and computationally efficient tracker, this paper employs a\ntracking-by-detection method, following the online real-time tracking approach\nestablished in prior literature. By introducing a novel cost function called\nthe Bounding Box Similarity Index, this work eliminates the Kalman Filter,\nleading to reduced computational requirements. Additionally, this paper\ndemonstrates the impact of scene features on enhancing object-track association\nand improving track post-processing. Using a 2.2 GHz Intel Xeon CPU, the\nproposed method achieves an HOTA of 61.7\\% with a processing speed of 2242 Hz\non the MOT17 dataset and an HOTA of 60.9\\% with a processing speed of 304 Hz on\nthe MOT20 dataset. The tracker's source code, fine-tuned object detection\nmodel, and tutorials are available at\n\\url{https://github.com/gitmehrdad/SFSORT}.\n","authors":["M. M. Morsali","Z. Sharifi","F. Fallah","S. Hashembeiki","H. Mohammadzade","S. Bagheri Shouraki"],"pdf_url":"https://arxiv.org/pdf/2404.07553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07551v1","updated":"2024-04-11T08:34:10Z","published":"2024-04-11T08:34:10Z","title":"Event-Enhanced Snapshot Compressive Videography at 10K FPS","summary":"  Video snapshot compressive imaging (SCI) encodes the target dynamic scene\ncompactly into a snapshot and reconstructs its high-speed frame sequence\nafterward, greatly reducing the required data footprint and transmission\nbandwidth as well as enabling high-speed imaging with a low frame rate\nintensity camera. In implementation, high-speed dynamics are encoded via\ntemporally varying patterns, and only frames at corresponding temporal\nintervals can be reconstructed, while the dynamics occurring between\nconsecutive frames are lost. To unlock the potential of conventional snapshot\ncompressive videography, we propose a novel hybrid \"intensity+event\" imaging\nscheme by incorporating an event camera into a video SCI setup. Our proposed\nsystem consists of a dual-path optical setup to record the coded intensity\nmeasurement and intermediate event signals simultaneously, which is compact and\nphoton-efficient by collecting the half photons discarded in conventional video\nSCI. Correspondingly, we developed a dual-branch Transformer utilizing the\nreciprocal relationship between two data modes to decode dense video frames.\nExtensive experiments on both simulated and real-captured data demonstrate our\nsuperiority to state-of-the-art video SCI and video frame interpolation (VFI)\nmethods. Benefiting from the new hybrid design leveraging both intrinsic\nredundancy in videos and the unique feature of event cameras, we achieve\nhigh-quality videography at 0.1ms time intervals with a low-cost CMOS image\nsensor working at 24 FPS.\n","authors":["Bo Zhang","Jinli Suo","Qionghai Dai"],"pdf_url":"https://arxiv.org/pdf/2404.07551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10372v2","updated":"2024-04-11T08:21:09Z","published":"2023-10-16T13:11:35Z","title":"Learning Object Permanence from Videos via Latent Imaginations","summary":"  While human infants exhibit knowledge about object permanence from two months\nof age onwards, deep-learning approaches still largely fail to recognize\nobjects' continued existence. We introduce a slot-based autoregressive deep\nlearning system, the looped location and identity tracking model Loci-Looped,\nwhich learns to adaptively fuse latent imaginations with pixel-space\nobservations into consistent latent object-specific what and where encodings\nover time. The novel loop empowers Loci-Looped to learn the physical concepts\nof object permanence, directional inertia, and object solidity through\nobservation alone. As a result, Loci-Looped tracks objects through occlusions,\nanticipates their reappearance, and shows signs of surprise and internal\nrevisions when observing implausible object behavior. Notably, Loci-Looped\noutperforms state-of-the-art baseline models in handling object occlusions and\ntemporary sensory interruptions while exhibiting more compositional,\ninterpretable internal activity patterns. Our work thus introduces the first\nself-supervised interpretable learning model that learns about object\npermanence directly from video data without supervision.\n","authors":["Manuel Traub","Frederic Becker","Sebastian Otte","Martin V. Butz"],"pdf_url":"https://arxiv.org/pdf/2310.10372v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15011v3","updated":"2024-04-11T08:16:53Z","published":"2023-11-25T12:34:02Z","title":"VSCode: General Visual Salient and Camouflaged Object Detection with 2D\n  Prompt Learning","summary":"  Salient object detection (SOD) and camouflaged object detection (COD) are\nrelated yet distinct binary mapping tasks. These tasks involve multiple\nmodalities, sharing commonalities and unique cues. Existing research often\nemploys intricate task-specific specialist models, potentially leading to\nredundancy and suboptimal results. We introduce VSCode, a generalist model with\nnovel 2D prompt learning, to jointly address four SOD tasks and three COD\ntasks. We utilize VST as the foundation model and introduce 2D prompts within\nthe encoder-decoder architecture to learn domain and task-specific knowledge on\ntwo separate dimensions. A prompt discrimination loss helps disentangle\npeculiarities to benefit model optimization. VSCode outperforms\nstate-of-the-art methods across six tasks on 26 datasets and exhibits zero-shot\ngeneralization to unseen tasks by combining 2D prompts, such as RGB-D COD.\nSource code has been available at https://github.com/Sssssuperior/VSCode.\n","authors":["Ziyang Luo","Nian Liu","Wangbo Zhao","Xuguang Yang","Dingwen Zhang","Deng-Ping Fan","Fahad Khan","Junwei Han"],"pdf_url":"https://arxiv.org/pdf/2311.15011v3.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2204.01348v2","updated":"2024-04-11T08:12:50Z","published":"2022-04-04T09:46:30Z","title":"Extended Reality for Mental Health Evaluation -A Scoping Review","summary":"  Mental health disorders are the leading cause of health-related problems\nglobally. It is projected that mental health disorders will be the leading\ncause of morbidity among adults as the incidence rates of anxiety and\ndepression grows globally. Recently, extended reality (XR), a general term\ncovering virtual reality (VR), augmented reality (AR) and mixed reality (MR),\nis paving a new way to deliver mental health care. In this paper, we conduct a\nscoping review on the development and application of XR in the area of mental\ndisorders. We performed a scoping database search to identify the relevant\nstudies indexed in Google Scholar, PubMed, and the ACM Digital Library. A\nsearch period between August 2016 and December 2023 was defined to select\narticles related to the usage of VR, AR, and MR in a mental health context. We\nidentified a total of 85 studies from 27 countries across the globe. By\nperforming data analysis, we found that most of the studies focused on\ndeveloped countries such as the US (16.47%) and Germany (12.94%). None of the\nstudies were for African countries. The majority of the articles reported that\nXR techniques led to a significant reduction in symptoms of anxiety or\ndepression. More studies were published in the year 2021, i.e., 31.76% (n =\n31). This could indicate that mental disorder intervention received a higher\nattention when COVID-19 emerged. Most studies (n = 65) focused on a population\nbetween 18 and 65 years old, only a few studies focused on teenagers (n = 2).\nAlso, more studies were done experimentally (n = 67, 78.82%) rather than by\nanalytical and modeling approaches (n = 8, 9.41%). This shows that there is a\nrapid development of XR technology for mental health care. Furthermore, these\nstudies showed that XR technology can effectively be used for evaluating mental\ndisorders in similar or better way as the conventional approaches.\n","authors":["Omisore Olatunji","Ifeanyi Odenigbo","Joseph Orji","Amelia Beltran","Nilufar Baghaei","Meier Sandra","Rita Orji"],"pdf_url":"https://arxiv.org/pdf/2204.01348v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07545v1","updated":"2024-04-11T08:12:48Z","published":"2024-04-11T08:12:48Z","title":"Stereo-LiDAR Depth Estimation with Deformable Propagation and Learned\n  Disparity-Depth Conversion","summary":"  Accurate and dense depth estimation with stereo cameras and LiDAR is an\nimportant task for automatic driving and robotic perception. While sparse hints\nfrom LiDAR points have improved cost aggregation in stereo matching, their\neffectiveness is limited by the low density and non-uniform distribution. To\naddress this issue, we propose a novel stereo-LiDAR depth estimation network\nwith Semi-Dense hint Guidance, named SDG-Depth. Our network includes a\ndeformable propagation module for generating a semi-dense hint map and a\nconfidence map by propagating sparse hints using a learned deformable window.\nThese maps then guide cost aggregation in stereo matching. To reduce the\ntriangulation error in depth recovery from disparity, especially in distant\nregions, we introduce a disparity-depth conversion module. Our method is both\naccurate and efficient. The experimental results on benchmark tests show its\nsuperior performance. Our code is available at\nhttps://github.com/SJTU-ViSYS/SDG-Depth.\n","authors":["Ang Li","Anning Hu","Wei Xi","Wenxian Yu","Danping Zou"],"pdf_url":"https://arxiv.org/pdf/2404.07545v1.pdf","comment":"Accepted in ICRA 2024. 8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2404.07543v1","updated":"2024-04-11T08:11:36Z","published":"2024-04-11T08:11:36Z","title":"Content-Adaptive Non-Local Convolution for Remote Sensing Pansharpening","summary":"  Currently, machine learning-based methods for remote sensing pansharpening\nhave progressed rapidly. However, existing pansharpening methods often do not\nfully exploit differentiating regional information in non-local spaces, thereby\nlimiting the effectiveness of the methods and resulting in redundant learning\nparameters. In this paper, we introduce a so-called content-adaptive non-local\nconvolution (CANConv), a novel method tailored for remote sensing image\npansharpening. Specifically, CANConv employs adaptive convolution, ensuring\nspatial adaptability, and incorporates non-local self-similarity through the\nsimilarity relationship partition (SRP) and the partition-wise adaptive\nconvolution (PWAC) sub-modules. Furthermore, we also propose a corresponding\nnetwork architecture, called CANNet, which mainly utilizes the multi-scale\nself-similarity. Extensive experiments demonstrate the superior performance of\nCANConv, compared with recent promising fusion methods. Besides, we\nsubstantiate the method's effectiveness through visualization, ablation\nexperiments, and comparison with existing methods on multiple test sets. The\nsource code is publicly available at https://github.com/duanyll/CANConv.\n","authors":["Yule Duan","Xiao Wu","Haoyu Deng","Liang-Jian Deng"],"pdf_url":"https://arxiv.org/pdf/2404.07543v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2310.11725v2","updated":"2024-04-11T08:11:20Z","published":"2023-10-18T05:44:49Z","title":"VST++: Efficient and Stronger Visual Saliency Transformer","summary":"  While previous CNN-based models have exhibited promising results for salient\nobject detection (SOD), their ability to explore global long-range dependencies\nis restricted. Our previous work, the Visual Saliency Transformer (VST),\naddressed this constraint from a transformer-based sequence-to-sequence\nperspective, to unify RGB and RGB-D SOD. In VST, we developed a multi-task\ntransformer decoder that concurrently predicts saliency and boundary outcomes\nin a pure transformer architecture. Moreover, we introduced a novel token\nupsampling method called reverse T2T for predicting a high-resolution saliency\nmap effortlessly within transformer-based structures. Building upon the VST\nmodel, we further propose an efficient and stronger VST version in this work,\ni.e. VST++. To mitigate the computational costs of the VST model, we propose a\nSelect-Integrate Attention (SIA) module, partitioning foreground into\nfine-grained segments and aggregating background information into a single\ncoarse-grained token. To incorporate 3D depth information with low cost, we\ndesign a novel depth position encoding method tailored for depth maps.\nFurthermore, we introduce a token-supervised prediction loss to provide\nstraightforward guidance for the task-related tokens. We evaluate our VST++\nmodel across various transformer-based backbones on RGB, RGB-D, and RGB-T SOD\nbenchmark datasets. Experimental results show that our model outperforms\nexisting methods while achieving a 25% reduction in computational costs without\nsignificant performance compromise. The demonstrated strong ability for\ngeneralization, enhanced performance, and heightened efficiency of our VST++\nmodel highlight its potential.\n","authors":["Nian Liu","Ziyang Luo","Ni Zhang","Junwei Han"],"pdf_url":"https://arxiv.org/pdf/2310.11725v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.00349v2","updated":"2024-04-11T08:08:10Z","published":"2023-01-01T05:02:46Z","title":"Towards Reliable Medical Image Segmentation by utilizing Evidential\n  Calibrated Uncertainty","summary":"  Medical image segmentation is critical for disease diagnosis and treatment\nassessment. However, concerns regarding the reliability of segmentation regions\npersist among clinicians, mainly attributed to the absence of confidence\nassessment, robustness, and calibration to accuracy. To address this, we\nintroduce DEviS, an easily implementable foundational model that seamlessly\nintegrates into various medical image segmentation networks. DEviS not only\nenhances the calibration and robustness of baseline segmentation accuracy but\nalso provides high-efficiency uncertainty estimation for reliable predictions.\nBy leveraging subjective logic theory, we explicitly model probability and\nuncertainty for the problem of medical image segmentation. Here, the Dirichlet\ndistribution parameterizes the distribution of probabilities for different\nclasses of the segmentation results. To generate calibrated predictions and\nuncertainty, we develop a trainable calibrated uncertainty penalty.\nFurthermore, DEviS incorporates an uncertainty-aware filtering module, which\nutilizes the metric of uncertainty-calibrated error to filter reliable data\nwithin the dataset. We conducted validation studies to assess both the accuracy\nand robustness of DEviS segmentation, along with evaluating the efficiency and\nreliability of uncertainty estimation. These evaluations were performed using\npublicly available datasets including ISIC2018, LiTS2017, and BraTS2019.\nAdditionally, two potential clinical trials are being conducted at Johns\nHopkins OCT, Duke-OCT-DME, and FIVES datasets to demonstrate their efficacy in\nfiltering high-quality or out-of-distribution data. Our code has been released\nin https://github.com/Cocofeat/DEviS.\n","authors":["Ke Zou","Yidi Chen","Ling Huang","Xuedong Yuan","Xiaojing Shen","Meng Wang","Rick Siow Mong Goh","Yong Liu","Huazhu Fu"],"pdf_url":"https://arxiv.org/pdf/2301.00349v2.pdf","comment":"34 pages, 11 figures"},{"id":"http://arxiv.org/abs/2306.00696v2","updated":"2024-04-11T08:03:25Z","published":"2023-06-01T14:06:48Z","title":"Analyzing the Internals of Neural Radiance Fields","summary":"  Modern Neural Radiance Fields (NeRFs) learn a mapping from position to\nvolumetric density leveraging proposal network samplers. In contrast to the\ncoarse-to-fine sampling approach with two NeRFs, this offers significant\npotential for acceleration using lower network capacity. Given that NeRFs\nutilize most of their network capacity to estimate radiance, they could store\nvaluable density information in their parameters or their deep features. To\ninvestigate this proposition, we take one step back and analyze large, trained\nReLU-MLPs used in coarse-to-fine sampling. Building on our novel activation\nvisualization method, we find that trained NeRFs, Mip-NeRFs and proposal\nnetwork samplers map samples with high density to local minima along a ray in\nactivation feature space. We show how these large MLPs can be accelerated by\ntransforming intermediate activations to a weight estimate, without any\nmodifications to the training protocol or the network architecture. With our\napproach, we can reduce the computational requirements of trained NeRFs by up\nto 50% with only a slight hit in rendering quality. Extensive experimental\nevaluation on a variety of datasets and architectures demonstrates the\neffectiveness of our approach. Consequently, our methodology provides valuable\ninsight into the inner workings of NeRFs.\n","authors":["Lukas Radl","Andreas Kurz","Michael Steiner","Markus Steinberger"],"pdf_url":"https://arxiv.org/pdf/2306.00696v2.pdf","comment":"Accepted to CVPRW'24! Project Page:\n  https://r4dl.github.io/nerfinternals/"},{"id":"http://arxiv.org/abs/2404.07537v1","updated":"2024-04-11T08:03:23Z","published":"2024-04-11T08:03:23Z","title":"How is Visual Attention Influenced by Text Guidance? Database and Model","summary":"  The analysis and prediction of visual attention have long been crucial tasks\nin the fields of computer vision and image processing. In practical\napplications, images are generally accompanied by various text descriptions,\nhowever, few studies have explored the influence of text descriptions on visual\nattention, let alone developed visual saliency prediction models considering\ntext guidance. In this paper, we conduct a comprehensive study on text-guided\nimage saliency (TIS) from both subjective and objective perspectives.\nSpecifically, we construct a TIS database named SJTU-TIS, which includes 1200\ntext-image pairs and the corresponding collected eye-tracking data. Based on\nthe established SJTU-TIS database, we analyze the influence of various text\ndescriptions on visual attention. Then, to facilitate the development of\nsaliency prediction models considering text influence, we construct a benchmark\nfor the established SJTU-TIS database using state-of-the-art saliency models.\nFinally, considering the effect of text descriptions on visual attention, while\nmost existing saliency models ignore this impact, we further propose a\ntext-guided saliency (TGSal) prediction model, which extracts and integrates\nboth image features and text features to predict the image saliency under\nvarious text-description conditions. Our proposed model significantly\noutperforms the state-of-the-art saliency models on both the SJTU-TIS database\nand the pure image saliency databases in terms of various evaluation metrics.\nThe SJTU-TIS database and the code of the proposed TGSal model will be released\nat: https://github.com/IntMeGroup/TGSal.\n","authors":["Yinan Sun","Xiongkuo Min","Huiyu Duan","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2404.07537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09107v2","updated":"2024-04-11T07:42:43Z","published":"2024-03-14T05:00:29Z","title":"S^2MVTC: a Simple yet Efficient Scalable Multi-View Tensor Clustering","summary":"  Anchor-based large-scale multi-view clustering has attracted considerable\nattention for its effectiveness in handling massive datasets. However, current\nmethods mainly seek the consensus embedding feature for clustering by exploring\nglobal correlations between anchor graphs or projection matrices.In this paper,\nwe propose a simple yet efficient scalable multi-view tensor clustering\n(S^2MVTC) approach, where our focus is on learning correlations of embedding\nfeatures within and across views. Specifically, we first construct the\nembedding feature tensor by stacking the embedding features of different views\ninto a tensor and rotating it. Additionally, we build a novel tensor\nlow-frequency approximation (TLFA) operator, which incorporates graph\nsimilarity into embedding feature learning, efficiently achieving smooth\nrepresentation of embedding features within different views. Furthermore,\nconsensus constraints are applied to embedding features to ensure inter-view\nsemantic consistency. Experimental results on six large-scale multi-view\ndatasets demonstrate that S^2MVTC significantly outperforms state-of-the-art\nalgorithms in terms of clustering performance and CPU execution time,\nespecially when handling massive data. The code of S^2MVTC is publicly\navailable at https://github.com/longzhen520/S2MVTC.\n","authors":["Zhen Long","Qiyuan Wang","Yazhou Ren","Yipeng Liu","Ce Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.09107v2.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2404.07520v1","updated":"2024-04-11T07:26:00Z","published":"2024-04-11T07:26:00Z","title":"PromptSync: Bridging Domain Gaps in Vision-Language Models through\n  Class-Aware Prototype Alignment and Discrimination","summary":"  The potential for zero-shot generalization in vision-language (V-L) models\nsuch as CLIP has spurred their widespread adoption in addressing numerous\ndownstream tasks. Previous methods have employed test-time prompt tuning to\nadapt the model to unseen domains, but they overlooked the issue of imbalanced\nclass distributions. In this study, we explicitly address this problem by\nemploying class-aware prototype alignment weighted by mean class probabilities\nobtained for the test sample and filtered augmented views. Additionally, we\nensure that the class probabilities are as accurate as possible by performing\nprototype discrimination using contrastive learning. The combination of\nalignment and discriminative loss serves as a geometric regularizer, preventing\nthe prompt representation from collapsing onto a single class and effectively\nbridging the distribution gap between the source and test domains. Our method,\nnamed PromptSync, synchronizes the prompts for each test sample on both the\ntext and vision branches of the V-L model. In empirical evaluations on the\ndomain generalization benchmark, our method outperforms previous best methods\nby 2.33\\% in overall performance, by 1\\% in base-to-novel generalization, and\nby 2.84\\% in cross-dataset transfer tasks.\n","authors":["Anant Khandelwal"],"pdf_url":"https://arxiv.org/pdf/2404.07520v1.pdf","comment":"Accepted at CVPR 2024 LIMIT, 12 pages, 8 Tables, 2 Figures"},{"id":"http://arxiv.org/abs/2404.06859v2","updated":"2024-04-11T07:24:59Z","published":"2024-04-10T09:35:36Z","title":"Multi-Label Continual Learning for the Medical Domain: A Novel Benchmark","summary":"  Multi-label image classification in dynamic environments is a problem that\nposes significant challenges. Previous studies have primarily focused on\nscenarios such as Domain Incremental Learning and Class Incremental Learning,\nwhich do not fully capture the complexity of real-world applications. In this\npaper, we study the problem of classification of medical imaging in the\nscenario termed New Instances and New Classes, which combines the challenges of\nboth new class arrivals and domain shifts in a single framework. Unlike\ntraditional scenarios, it reflects the realistic nature of CL in domains such\nas medical imaging, where updates may introduce both new classes and changes in\ndomain characteristics. To address the unique challenges posed by this complex\nscenario, we introduce a novel approach called Pseudo-Label Replay. This method\naims to mitigate forgetting while adapting to new classes and domain shifts by\ncombining the advantages of the Replay and Pseudo-Label methods and solving\ntheir limitations in the proposed scenario. We evaluate our proposed approach\non a challenging benchmark consisting of two datasets, seven tasks, and\nnineteen classes, modeling a realistic Continual Learning scenario. Our\nexperimental findings demonstrate the effectiveness of Pseudo-Label Replay in\naddressing the challenges posed by the complex scenario proposed. Our method\nsurpasses existing approaches, exhibiting superior performance while showing\nminimal forgetting.\n","authors":["Marina Ceccon","Davide Dalle Pezze","Alessandro Fabris","Gian Antonio Susto"],"pdf_url":"https://arxiv.org/pdf/2404.06859v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07518v1","updated":"2024-04-11T07:22:14Z","published":"2024-04-11T07:22:14Z","title":"Remembering Transformer for Continual Learning","summary":"  Neural networks encounter the challenge of Catastrophic Forgetting (CF) in\ncontinual learning, where new task knowledge interferes with previously learned\nknowledge. We propose Remembering Transformer, inspired by the brain's\nComplementary Learning Systems (CLS), to tackle this issue. Remembering\nTransformer employs a mixture-of-adapters and a generative model-based routing\nmechanism to alleviate CF by dynamically routing task data to relevant\nadapters. Our approach demonstrated a new SOTA performance in various vision\ncontinual learning tasks and great parameter efficiency.\n","authors":["Yuwei Sun","Jun Sakuma","Ryota Kanai"],"pdf_url":"https://arxiv.org/pdf/2404.07518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16073v2","updated":"2024-04-11T07:20:52Z","published":"2023-10-24T14:59:51Z","title":"FloCoDe: Unbiased Dynamic Scene Graph Generation with Temporal\n  Consistency and Correlation Debiasing","summary":"  Dynamic scene graph generation (SGG) from videos requires not only a\ncomprehensive understanding of objects across scenes but also a method to\ncapture the temporal motions and interactions with different objects. Moreover,\nthe long-tailed distribution of visual relationships is a crucial bottleneck\nfor most dynamic SGG methods. This is because many of them focus on capturing\nspatio-temporal context using complex architectures, leading to the generation\nof biased scene graphs. To address these challenges, we propose\n\\textsc{FloCoDe}: \\textbf{Flo}w-aware Temporal Consistency and\n\\textbf{Co}rrelation \\textbf{De}biasing with uncertainty attenuation for\nunbiased dynamic scene graphs. \\textsc{FloCoDe} employs feature warping using\nflow to detect temporally consistent objects across frames. To address the\nlong-tail issue of visual relationships, we propose correlation debiasing and a\nlabel correlation-based loss to learn unbiased relation representations for\nlong-tailed classes. Specifically, we propose to incorporate label correlations\nusing contrastive loss to capture commonly co-occurring relations, which aids\nin learning robust representations for long-tailed classes. Further, we adopt\nthe uncertainty attenuation-based classifier framework to handle noisy\nannotations in the SGG data. Extensive experimental evaluation shows a\nperformance gain as high as 4.1\\%, demonstrating the superiority of generating\nmore unbiased scene graphs.\n","authors":["Anant Khandelwal"],"pdf_url":"https://arxiv.org/pdf/2310.16073v2.pdf","comment":"Accepted at CVPR 2024 SG2RL, 11 pages, 5 tables, 4 figures"},{"id":"http://arxiv.org/abs/2404.05426v2","updated":"2024-04-11T07:12:35Z","published":"2024-04-08T11:54:49Z","title":"Test-Time Zero-Shot Temporal Action Localization","summary":"  Zero-Shot Temporal Action Localization (ZS-TAL) seeks to identify and locate\nactions in untrimmed videos unseen during training. Existing ZS-TAL methods\ninvolve fine-tuning a model on a large amount of annotated training data. While\neffective, training-based ZS-TAL approaches assume the availability of labeled\ndata for supervised learning, which can be impractical in some applications.\nFurthermore, the training process naturally induces a domain bias into the\nlearned model, which may adversely affect the model's generalization ability to\narbitrary videos. These considerations prompt us to approach the ZS-TAL problem\nfrom a radically novel perspective, relaxing the requirement for training data.\nTo this aim, we introduce a novel method that performs Test-Time adaptation for\nTemporal Action Localization (T3AL). In a nutshell, T3AL adapts a pre-trained\nVision and Language Model (VLM). T3AL operates in three steps. First, a\nvideo-level pseudo-label of the action category is computed by aggregating\ninformation from the entire video. Then, action localization is performed\nadopting a novel procedure inspired by self-supervised learning. Finally,\nframe-level textual descriptions extracted with a state-of-the-art captioning\nmodel are employed for refining the action region proposals. We validate the\neffectiveness of T3AL by conducting experiments on the THUMOS14 and the\nActivityNet-v1.3 datasets. Our results demonstrate that T3AL significantly\noutperforms zero-shot baselines based on state-of-the-art VLMs, confirming the\nbenefit of a test-time adaptation approach.\n","authors":["Benedetta Liberatori","Alessandro Conti","Paolo Rota","Yiming Wang","Elisa Ricci"],"pdf_url":"https://arxiv.org/pdf/2404.05426v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2404.07514v1","updated":"2024-04-11T07:11:43Z","published":"2024-04-11T07:11:43Z","title":"Generalization Gap in Data Augmentation: Insights from Illumination","summary":"  In the field of computer vision, data augmentation is widely used to enrich\nthe feature complexity of training datasets with deep learning techniques.\nHowever, regarding the generalization capabilities of models, the difference in\nartificial features generated by data augmentation and natural visual features\nhas not been fully revealed. This study focuses on the visual representation\nvariable 'illumination', by simulating its distribution degradation and\nexamining how data augmentation techniques enhance model performance on a\nclassification task. Our goal is to investigate the differences in\ngeneralization between models trained with augmented data and those trained\nunder real-world illumination conditions. Results indicate that after\nundergoing various data augmentation methods, model performance has been\nsignificantly improved. Yet, a noticeable generalization gap still exists after\nutilizing various data augmentation methods, emphasizing the critical role of\nfeature diversity in the training set for enhancing model generalization.\n","authors":["Jianqiang Xiao","Weiwen Guo","Junfeng Liu","Mengze Li"],"pdf_url":"https://arxiv.org/pdf/2404.07514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.01446v2","updated":"2024-04-11T06:58:18Z","published":"2024-04-01T19:33:41Z","title":"Finding Regions of Interest in Whole Slide Images Using Multiple\n  Instance Learning","summary":"  Whole Slide Images (WSI), obtained by high-resolution digital scanning of\nmicroscope slides at multiple scales, are the cornerstone of modern Digital\nPathology. However, they represent a particular challenge to\nAI-based/AI-mediated analysis because pathology labeling is typically done at\nslide-level, instead of tile-level. It is not just that medical diagnostics is\nrecorded at the specimen level, the detection of oncogene mutation is also\nexperimentally obtained, and recorded by initiatives like The Cancer Genome\nAtlas (TCGA), at the slide level. This configures a dual challenge: a)\naccurately predicting the overall cancer phenotype and b) finding out what\ncellular morphologies are associated with it at the tile level. To address\nthese challenges, a weakly supervised Multiple Instance Learning (MIL) approach\nwas explored for two prevalent cancer types, Invasive Breast Carcinoma\n(TCGA-BRCA) and Lung Squamous Cell Carcinoma (TCGA-LUSC). This approach was\nexplored for tumor detection at low magnification levels and TP53 mutations at\nvarious levels. Our results show that a novel additive implementation of MIL\nmatched the performance of reference implementation (AUC 0.96), and was only\nslightly outperformed by Attention MIL (AUC 0.97). More interestingly from the\nperspective of the molecular pathologist, these different AI architectures\nidentify distinct sensitivities to morphological features (through the\ndetection of Regions of Interest, RoI) at different amplification levels.\nTellingly, TP53 mutation was most sensitive to features at the higher\napplications where cellular morphology is resolved.\n","authors":["Martim Afonso","Praphulla M. S. Bhawsar","Monjoy Saha","Jonas S. Almeida","Arlindo L. Oliveira"],"pdf_url":"https://arxiv.org/pdf/2404.01446v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07507v1","updated":"2024-04-11T06:55:44Z","published":"2024-04-11T06:55:44Z","title":"Learning to Classify New Foods Incrementally Via Compressed Exemplars","summary":"  Food image classification systems play a crucial role in health monitoring\nand diet tracking through image-based dietary assessment techniques. However,\nexisting food recognition systems rely on static datasets characterized by a\npre-defined fixed number of food classes. This contrasts drastically with the\nreality of food consumption, which features constantly changing data.\nTherefore, food image classification systems should adapt to and manage data\nthat continuously evolves. This is where continual learning plays an important\nrole. A challenge in continual learning is catastrophic forgetting, where ML\nmodels tend to discard old knowledge upon learning new information. While\nmemory-replay algorithms have shown promise in mitigating this problem by\nstoring old data as exemplars, they are hampered by the limited capacity of\nmemory buffers, leading to an imbalance between new and previously learned\ndata. To address this, our work explores the use of neural image compression to\nextend buffer size and enhance data diversity. We introduced the concept of\ncontinuously learning a neural compression model to adaptively improve the\nquality of compressed data and optimize the bitrates per pixel (bpp) to store\nmore exemplars. Our extensive experiments, including evaluations on\nfood-specific datasets including Food-101 and VFN-74, as well as the general\ndataset ImageNet-100, demonstrate improvements in classification accuracy. This\nprogress is pivotal in advancing more realistic food recognition systems that\nare capable of adapting to continually evolving data. Moreover, the principles\nand methodologies we've developed hold promise for broader applications,\nextending their benefits to other domains of continual machine learning\nsystems.\n","authors":["Justin Yang","Zhihao Duan","Jiangpeng He","Fengqing Zhu"],"pdf_url":"https://arxiv.org/pdf/2404.07507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15430v2","updated":"2024-04-11T06:40:12Z","published":"2024-02-23T16:50:07Z","title":"Hierarchical Invariance for Robust and Interpretable Vision Tasks at\n  Larger Scales","summary":"  Developing robust and interpretable vision systems is a crucial step towards\ntrustworthy artificial intelligence. In this regard, a promising paradigm\nconsiders embedding task-required invariant structures, e.g., geometric\ninvariance, in the fundamental image representation. However, such invariant\nrepresentations typically exhibit limited discriminability, limiting their\napplications in larger-scale trustworthy vision tasks. For this open problem,\nwe conduct a systematic investigation of hierarchical invariance, exploring\nthis topic from theoretical, practical, and application perspectives. At the\ntheoretical level, we show how to construct over-complete invariants with a\nConvolutional Neural Networks (CNN)-like hierarchical architecture yet in a\nfully interpretable manner. The general blueprint, specific definitions,\ninvariant properties, and numerical implementations are provided. At the\npractical level, we discuss how to customize this theoretical framework into a\ngiven task. With the over-completeness, discriminative features w.r.t. the task\ncan be adaptively formed in a Neural Architecture Search (NAS)-like manner. We\ndemonstrate the above arguments with accuracy, invariance, and efficiency\nresults on texture, digit, and parasite classification experiments.\nFurthermore, at the application level, our representations are explored in\nreal-world forensics tasks on adversarial perturbations and Artificial\nIntelligence Generated Content (AIGC). Such applications reveal that the\nproposed strategy not only realizes the theoretically promised invariance, but\nalso exhibits competitive discriminability even in the era of deep learning.\nFor robust and interpretable vision tasks at larger scales, hierarchical\ninvariant representation can be considered as an effective alternative to\ntraditional CNN and invariants.\n","authors":["Shuren Qi","Yushu Zhang","Chao Wang","Zhihua Xia","Xiaochun Cao","Jian Weng"],"pdf_url":"https://arxiv.org/pdf/2402.15430v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07504v1","updated":"2024-04-11T06:39:53Z","published":"2024-04-11T06:39:53Z","title":"Mitigating Object Dependencies: Improving Point Cloud Self-Supervised\n  Learning through Object Exchange","summary":"  In the realm of point cloud scene understanding, particularly in indoor\nscenes, objects are arranged following human habits, resulting in objects of\ncertain semantics being closely positioned and displaying notable inter-object\ncorrelations. This can create a tendency for neural networks to exploit these\nstrong dependencies, bypassing the individual object patterns. To address this\nchallenge, we introduce a novel self-supervised learning (SSL) strategy. Our\napproach leverages both object patterns and contextual cues to produce robust\nfeatures. It begins with the formulation of an object-exchanging strategy,\nwhere pairs of objects with comparable sizes are exchanged across different\nscenes, effectively disentangling the strong contextual dependencies.\nSubsequently, we introduce a context-aware feature learning strategy, which\nencodes object patterns without relying on their specific context by\naggregating object features across various scenes. Our extensive experiments\ndemonstrate the superiority of our method over existing SSL techniques, further\nshowing its better robustness to environmental changes. Moreover, we showcase\nthe applicability of our approach by transferring pre-trained models to diverse\npoint cloud datasets.\n","authors":["Yanhao Wu","Tong Zhang","Wei Ke","Congpei Qiu","Sabine Susstrunk","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2404.07504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08801v3","updated":"2024-04-11T06:25:41Z","published":"2024-02-05T12:33:37Z","title":"CoBra: Complementary Branch Fusing Class and Semantic Knowledge for\n  Robust Weakly Supervised Semantic Segmentation","summary":"  Leveraging semantically precise pseudo masks derived from image-level class\nknowledge for segmentation, namely image-level Weakly Supervised Semantic\nSegmentation (WSSS), still remains challenging. While Class Activation Maps\n(CAMs) using CNNs have steadily been contributing to the success of WSSS, the\nresulting activation maps often narrowly focus on class-specific parts (e.g.,\nonly face of human). On the other hand, recent works based on vision\ntransformers (ViT) have shown promising results based on their self-attention\nmechanism to capture the semantic parts but fail in capturing complete\nclass-specific details (e.g., entire body parts of human but also with a dog\nnearby). In this work, we propose Complementary Branch (CoBra), a novel dual\nbranch framework consisting of two distinct architectures which provide\nvaluable complementary knowledge of class (from CNN) and semantic (from ViT) to\neach branch. In particular, we learn Class-Aware Projection (CAP) for the CNN\nbranch and Semantic-Aware Projection (SAP) for the ViT branch to explicitly\nfuse their complementary knowledge and facilitate a new type of extra\npatch-level supervision. Our model, through CoBra, fuses CNN and ViT's\ncomplementary outputs to create robust pseudo masks that integrate both class\nand semantic information effectively. Extensive experiments qualitatively and\nquantitatively investigate how CNN and ViT complement each other on the PASCAL\nVOC 2012 dataset, showing a state-of-the-art WSSS result. This includes not\nonly the masks generated by our model, but also the segmentation results\nderived from utilizing these masks as pseudo labels.\n","authors":["Woojung Han","Seil Kang","Kyobin Choo","Seong Jae Hwang"],"pdf_url":"https://arxiv.org/pdf/2403.08801v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.07868v2","updated":"2024-04-11T06:21:29Z","published":"2023-01-19T03:42:56Z","title":"MV-Adapter: Multimodal Video Transfer Learning for Video Text Retrieval","summary":"  State-of-the-art video-text retrieval (VTR) methods typically involve fully\nfine-tuning a pre-trained model (e.g. CLIP) on specific datasets. However, this\ncan result in significant storage costs in practical applications as a separate\nmodel per task must be stored. To address this issue, we present our pioneering\nwork that enables parameter-efficient VTR using a pre-trained model, with only\na small number of tunable parameters during training. Towards this goal, we\npropose a new method dubbed Multimodal Video Adapter (MV-Adapter) for\nefficiently transferring the knowledge in the pre-trained CLIP from image-text\nto video-text. Specifically, MV-Adapter utilizes bottleneck structures in both\nvideo and text branches, along with two novel components. The first is a\nTemporal Adaptation Module that is incorporated in the video branch to\nintroduce global and local temporal contexts. We also train weights\ncalibrations to adjust to dynamic variations across frames. The second is Cross\nModality Tying that generates weights for video/text branches through sharing\ncross modality factors, for better aligning between modalities. Thanks to above\ninnovations, MV-Adapter can achieve comparable or better performance than\nstandard full fine-tuning with negligible parameters overhead. Notably,\nMV-Adapter consistently outperforms various competing methods in V2T/T2V tasks\nwith large margins on five widely used VTR benchmarks (MSR-VTT, MSVD, LSMDC,\nDiDemo, and ActivityNet).\n","authors":["Xiaojie Jin","Bowen Zhang","Weibo Gong","Kai Xu","XueQing Deng","Peng Wang","Zhao Zhang","Xiaohui Shen","Jiashi Feng"],"pdf_url":"https://arxiv.org/pdf/2301.07868v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07495v1","updated":"2024-04-11T06:06:56Z","published":"2024-04-11T06:06:56Z","title":"PillarTrack: Redesigning Pillar-based Transformer Network for Single\n  Object Tracking on Point Clouds","summary":"  LiDAR-based 3D single object tracking (3D SOT) is a critical issue in\nrobotics and autonomous driving. It aims to obtain accurate 3D BBox from the\nsearch area based on similarity or motion. However, existing 3D SOT methods\nusually follow the point-based pipeline, where the sampling operation\ninevitably leads to redundant or lost information, resulting in unexpected\nperformance. To address these issues, we propose PillarTrack, a pillar-based 3D\nsingle object tracking framework. Firstly, we transform sparse point clouds\ninto dense pillars to preserve the local and global geometrics. Secondly, we\nintroduce a Pyramid-type Encoding Pillar Feature Encoder (PE-PFE) design to\nhelp the feature representation of each pillar. Thirdly, we present an\nefficient Transformer-based backbone from the perspective of modality\ndifferences. Finally, we construct our PillarTrack tracker based above designs.\nExtensive experiments on the KITTI and nuScenes dataset demonstrate the\nsuperiority of our proposed method. Notably, our method achieves\nstate-of-the-art performance on the KITTI and nuScenes dataset and enables\nreal-time tracking speed. We hope our work could encourage the community to\nrethink existing 3D SOT tracker designs.We will open source our code to the\nresearch community in https://github.com/StiphyJay/PillarTrack.\n","authors":["Weisheng Xu","Sifan Zhou","Zhihang Yuan"],"pdf_url":"https://arxiv.org/pdf/2404.07495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07487v1","updated":"2024-04-11T05:51:06Z","published":"2024-04-11T05:51:06Z","title":"Fine-Grained Side Information Guided Dual-Prompts for Zero-Shot Skeleton\n  Action Recognition","summary":"  Skeleton-based zero-shot action recognition aims to recognize unknown human\nactions based on the learned priors of the known skeleton-based actions and a\nsemantic descriptor space shared by both known and unknown categories. However,\nprevious works focus on establishing the bridges between the known skeleton\nrepresentation space and semantic descriptions space at the coarse-grained\nlevel for recognizing unknown action categories, ignoring the fine-grained\nalignment of these two spaces, resulting in suboptimal performance in\ndistinguishing high-similarity action categories. To address these challenges,\nwe propose a novel method via Side information and dual-prompts learning for\nskeleton-based zero-shot action recognition (STAR) at the fine-grained level.\nSpecifically, 1) we decompose the skeleton into several parts based on its\ntopology structure and introduce the side information concerning multi-part\ndescriptions of human body movements for alignment between the skeleton and the\nsemantic space at the fine-grained level; 2) we design the visual-attribute and\nsemantic-part prompts to improve the intra-class compactness within the\nskeleton space and inter-class separability within the semantic space,\nrespectively, to distinguish the high-similarity actions. Extensive experiments\nshow that our method achieves state-of-the-art performance in ZSL and GZSL\nsettings on NTU RGB+D, NTU RGB+D 120, and PKU-MMD datasets.\n","authors":["Yang Chen","Jingcai Guo","Tian He","Ling Wang"],"pdf_url":"https://arxiv.org/pdf/2404.07487v1.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.00644v3","updated":"2024-04-11T05:48:36Z","published":"2024-03-01T16:25:17Z","title":"Diff-Plugin: Revitalizing Details for Diffusion-based Low-level Tasks","summary":"  Diffusion models trained on large-scale datasets have achieved remarkable\nprogress in image synthesis. However, due to the randomness in the diffusion\nprocess, they often struggle with handling diverse low-level tasks that require\ndetails preservation. To overcome this limitation, we present a new Diff-Plugin\nframework to enable a single pre-trained diffusion model to generate\nhigh-fidelity results across a variety of low-level tasks. Specifically, we\nfirst propose a lightweight Task-Plugin module with a dual branch design to\nprovide task-specific priors, guiding the diffusion process in preserving image\ncontent. We then propose a Plugin-Selector that can automatically select\ndifferent Task-Plugins based on the text instruction, allowing users to edit\nimages by indicating multiple low-level tasks with natural language. We conduct\nextensive experiments on 8 low-level vision tasks. The results demonstrate the\nsuperiority of Diff-Plugin over existing methods, particularly in real-world\nscenarios. Our ablations further validate that Diff-Plugin is stable,\nschedulable, and supports robust training across different dataset sizes.\n","authors":["Yuhao Liu","Zhanghan Ke","Fang Liu","Nanxuan Zhao","Rynson W. H. Lau"],"pdf_url":"https://arxiv.org/pdf/2403.00644v3.pdf","comment":"Accepted to CVPR2024. Replaced some celebrity images to avoid\n  copyright disputes"},{"id":"http://arxiv.org/abs/2404.06351v2","updated":"2024-04-11T05:17:44Z","published":"2024-04-09T14:42:31Z","title":"HPNet: Dynamic Trajectory Forecasting with Historical Prediction\n  Attention","summary":"  Predicting the trajectories of road agents is essential for autonomous\ndriving systems. The recent mainstream methods follow a static paradigm, which\npredicts the future trajectory by using a fixed duration of historical frames.\nThese methods make the predictions independently even at adjacent time steps,\nwhich leads to potential instability and temporal inconsistency. As successive\ntime steps have largely overlapping historical frames, their forecasting should\nhave intrinsic correlation, such as overlapping predicted trajectories should\nbe consistent, or be different but share the same motion goal depending on the\nroad situation. Motivated by this, in this work, we introduce HPNet, a novel\ndynamic trajectory forecasting method. Aiming for stable and accurate\ntrajectory forecasting, our method leverages not only historical frames\nincluding maps and agent states, but also historical predictions. Specifically,\nwe newly design a Historical Prediction Attention module to automatically\nencode the dynamic relationship between successive predictions. Besides, it\nalso extends the attention range beyond the currently visible window\nbenefitting from the use of historical predictions. The proposed Historical\nPrediction Attention together with the Agent Attention and Mode Attention is\nfurther formulated as the Triple Factorized Attention module, serving as the\ncore design of HPNet.Experiments on the Argoverse and INTERACTION datasets show\nthat HPNet achieves state-of-the-art performance, and generates accurate and\nstable future trajectories. Our code are available at\nhttps://github.com/XiaolongTang23/HPNet.\n","authors":["Xiaolong Tang","Meina Kan","Shiguang Shan","Zhilong Ji","Jinfeng Bai","Xilin Chen"],"pdf_url":"https://arxiv.org/pdf/2404.06351v2.pdf","comment":"CVPR2024"},{"id":"http://arxiv.org/abs/2404.00511v3","updated":"2024-04-11T05:14:35Z","published":"2024-03-31T01:16:02Z","title":"MIPS at SemEval-2024 Task 3: Multimodal Emotion-Cause Pair Extraction in\n  Conversations with Multimodal Language Models","summary":"  This paper presents our winning submission to Subtask 2 of SemEval 2024 Task\n3 on multimodal emotion cause analysis in conversations. We propose a novel\nMultimodal Emotion Recognition and Multimodal Emotion Cause Extraction\n(MER-MCE) framework that integrates text, audio, and visual modalities using\nspecialized emotion encoders. Our approach sets itself apart from\ntop-performing teams by leveraging modality-specific features for enhanced\nemotion understanding and causality inference. Experimental evaluation\ndemonstrates the advantages of our multimodal approach, with our submission\nachieving a competitive weighted F1 score of 0.3435, ranking third with a\nmargin of only 0.0339 behind the 1st team and 0.0025 behind the 2nd team.\nProject: https://github.com/MIPS-COLT/MER-MCE.git\n","authors":["Zebang Cheng","Fuqiang Niu","Yuxiang Lin","Zhi-Qi Cheng","Bowen Zhang","Xiaojiang Peng"],"pdf_url":"https://arxiv.org/pdf/2404.00511v3.pdf","comment":"Ranked 3rd in SemEval '24 Task 3 with F1 of 0.3435, close to 1st &\n  2nd by 0.0339 & 0.0025"},{"id":"http://arxiv.org/abs/2404.07474v1","updated":"2024-04-11T04:58:18Z","published":"2024-04-11T04:58:18Z","title":"G-NeRF: Geometry-enhanced Novel View Synthesis from Single-View Images","summary":"  Novel view synthesis aims to generate new view images of a given view image\ncollection. Recent attempts address this problem relying on 3D geometry priors\n(e.g., shapes, sizes, and positions) learned from multi-view images. However,\nsuch methods encounter the following limitations: 1) they require a set of\nmulti-view images as training data for a specific scene (e.g., face, car or\nchair), which is often unavailable in many real-world scenarios; 2) they fail\nto extract the geometry priors from single-view images due to the lack of\nmulti-view supervision. In this paper, we propose a Geometry-enhanced NeRF\n(G-NeRF), which seeks to enhance the geometry priors by a geometry-guided\nmulti-view synthesis approach, followed by a depth-aware training. In the\nsynthesis process, inspired that existing 3D GAN models can unconditionally\nsynthesize high-fidelity multi-view images, we seek to adopt off-the-shelf 3D\nGAN models, such as EG3D, as a free source to provide geometry priors through\nsynthesizing multi-view data. Simultaneously, to further improve the geometry\nquality of the synthetic data, we introduce a truncation method to effectively\nsample latent codes within 3D GAN models. To tackle the absence of multi-view\nsupervision for single-view images, we design the depth-aware training\napproach, incorporating a depth-aware discriminator to guide geometry priors\nthrough depth maps. Experiments demonstrate the effectiveness of our method in\nterms of both qualitative and quantitative results.\n","authors":["Zixiong Huang","Qi Chen","Libo Sun","Yifan Yang","Naizhou Wang","Mingkui Tan","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2404.07474v1.pdf","comment":"CVPR 2024 Accepted Paper"},{"id":"http://arxiv.org/abs/2404.07473v1","updated":"2024-04-11T04:54:42Z","published":"2024-04-11T04:54:42Z","title":"LUCF-Net: Lightweight U-shaped Cascade Fusion Network for Medical Image\n  Segmentation","summary":"  In this study, the performance of existing U-shaped neural network\narchitectures was enhanced for medical image segmentation by adding\nTransformer. Although Transformer architectures are powerful at extracting\nglobal information, its ability to capture local information is limited due to\nits high complexity. To address this challenge, we proposed a new lightweight\nU-shaped cascade fusion network (LUCF-Net) for medical image segmentation. It\nutilized an asymmetrical structural design and incorporated both local and\nglobal modules to enhance its capacity for local and global modeling.\nAdditionally, a multi-layer cascade fusion decoding network was designed to\nfurther bolster the network's information fusion capabilities. Validation\nresults achieved on multi-organ datasets in CT format, cardiac segmentation\ndatasets in MRI format, and dermatology datasets in image format demonstrated\nthat the proposed model outperformed other state-of-the-art methods in handling\nlocal-global information, achieving an improvement of 1.54% in Dice coefficient\nand 2.6 mm in Hausdorff distance on multi-organ segmentation. Furthermore, as a\nnetwork that combines Convolutional Neural Network and Transformer\narchitectures, it achieves competitive segmentation performance with only 6.93\nmillion parameters and 6.6 gigabytes of floating point operations, without the\nneed of pre-training. In summary, the proposed method demonstrated enhanced\nperformance while retaining a simpler model design compared to other\nTransformer-based segmentation networks.\n","authors":["Songkai Sun","Qingshan She","Yuliang Ma","Rihui Li","Yingchun Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.07473v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06845v2","updated":"2024-04-11T04:17:13Z","published":"2024-03-11T16:03:35Z","title":"DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video\n  Generation","summary":"  World models have demonstrated superiority in autonomous driving,\nparticularly in the generation of multi-view driving videos. However,\nsignificant challenges still exist in generating customized driving videos. In\nthis paper, we propose DriveDreamer-2, which builds upon the framework of\nDriveDreamer and incorporates a Large Language Model (LLM) to generate\nuser-defined driving videos. Specifically, an LLM interface is initially\nincorporated to convert a user's query into agent trajectories. Subsequently, a\nHDMap, adhering to traffic regulations, is generated based on the trajectories.\nUltimately, we propose the Unified Multi-View Model to enhance temporal and\nspatial coherence in the generated driving videos. DriveDreamer-2 is the first\nworld model to generate customized driving videos, it can generate uncommon\ndriving videos (e.g., vehicles abruptly cut in) in a user-friendly manner.\nBesides, experimental results demonstrate that the generated videos enhance the\ntraining of driving perception methods (e.g., 3D detection and tracking).\nFurthermore, video generation quality of DriveDreamer-2 surpasses other\nstate-of-the-art methods, showcasing FID and FVD scores of 11.2 and 55.7,\nrepresenting relative improvements of 30% and 50%.\n","authors":["Guosheng Zhao","Xiaofeng Wang","Zheng Zhu","Xinze Chen","Guan Huang","Xiaoyi Bao","Xingang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.06845v2.pdf","comment":"Project Page: https://drivedreamer2.github.io"},{"id":"http://arxiv.org/abs/2404.07467v1","updated":"2024-04-11T04:14:48Z","published":"2024-04-11T04:14:48Z","title":"Trashbusters: Deep Learning Approach for Litter Detection and Tracking","summary":"  The illegal disposal of trash is a major public health and environmental\nconcern. Disposing of trash in unplanned places poses serious health and\nenvironmental risks. We should try to restrict public trash cans as much as\npossible. This research focuses on automating the penalization of litterbugs,\naddressing the persistent problem of littering in public places. Traditional\napproaches relying on manual intervention and witness reporting suffer from\ndelays, inaccuracies, and anonymity issues. To overcome these challenges, this\npaper proposes a fully automated system that utilizes surveillance cameras and\nadvanced computer vision algorithms for litter detection, object tracking, and\nface recognition. The system accurately identifies and tracks individuals\nengaged in littering activities, attaches their identities through face\nrecognition, and enables efficient enforcement of anti-littering policies. By\nreducing reliance on manual intervention, minimizing human error, and providing\nprompt identification, the proposed system offers significant advantages in\naddressing littering incidents. The primary contribution of this research lies\nin the implementation of the proposed system, leveraging advanced technologies\nto enhance surveillance operations and automate the penalization of litterbugs.\n","authors":["Kashish Jain","Manthan Juthani","Jash Jain","Anant V. Nimkar"],"pdf_url":"https://arxiv.org/pdf/2404.07467v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.10974v4","updated":"2024-04-11T04:14:33Z","published":"2023-07-20T16:00:19Z","title":"Deep Multi-Threshold Spiking-UNet for Image Processing","summary":"  U-Net, known for its simple yet efficient architecture, is widely utilized\nfor image processing tasks and is particularly suitable for deployment on\nneuromorphic chips. This paper introduces the novel concept of Spiking-UNet for\nimage processing, which combines the power of Spiking Neural Networks (SNNs)\nwith the U-Net architecture. To achieve an efficient Spiking-UNet, we face two\nprimary challenges: ensuring high-fidelity information propagation through the\nnetwork via spikes and formulating an effective training strategy. To address\nthe issue of information loss, we introduce multi-threshold spiking neurons,\nwhich improve the efficiency of information transmission within the\nSpiking-UNet. For the training strategy, we adopt a conversion and fine-tuning\npipeline that leverage pre-trained U-Net models. During the conversion process,\nsignificant variability in data distribution across different parts is observed\nwhen utilizing skip connections. Therefore, we propose a connection-wise\nnormalization method to prevent inaccurate firing rates. Furthermore, we adopt\na flow-based training method to fine-tune the converted models, reducing time\nsteps while preserving performance. Experimental results show that, on image\nsegmentation and denoising, our Spiking-UNet achieves comparable performance to\nits non-spiking counterpart, surpassing existing SNN methods. Compared with the\nconverted Spiking-UNet without fine-tuning, our Spiking-UNet reduces inference\ntime by approximately 90\\%. This research broadens the application scope of\nSNNs in image processing and is expected to inspire further exploration in the\nfield of neuromorphic engineering. The code for our Spiking-UNet implementation\nis available at https://github.com/SNNresearch/Spiking-UNet.\n","authors":["Hebei Li","Yueyi Zhang","Zhiwei Xiong","Xiaoyan Sun"],"pdf_url":"https://arxiv.org/pdf/2307.10974v4.pdf","comment":"Accepted in NeuroComputing"},{"id":"http://arxiv.org/abs/2402.16994v2","updated":"2024-04-11T03:44:49Z","published":"2024-02-26T20:00:57Z","title":"GEM3D: GEnerative Medial Abstractions for 3D Shape Synthesis","summary":"  We introduce GEM3D -- a new deep, topology-aware generative model of 3D\nshapes. The key ingredient of our method is a neural skeleton-based\nrepresentation encoding information on both shape topology and geometry.\nThrough a denoising diffusion probabilistic model, our method first generates\nskeleton-based representations following the Medial Axis Transform (MAT), then\ngenerates surfaces through a skeleton-driven neural implicit formulation. The\nneural implicit takes into account the topological and geometric information\nstored in the generated skeleton representations to yield surfaces that are\nmore topologically and geometrically accurate compared to previous neural field\nformulations. We discuss applications of our method in shape synthesis and\npoint cloud reconstruction tasks, and evaluate our method both qualitatively\nand quantitatively. We demonstrate significantly more faithful surface\nreconstruction and diverse shape generation results compared to the\nstate-of-the-art, also involving challenging scenarios of reconstructing and\nsynthesizing structurally complex, high-genus shape surfaces from Thingi10K and\nShapeNet.\n","authors":["Dmitry Petrov","Pradyumn Goyal","Vikas Thamizharasan","Vladimir G. Kim","Matheus Gadelha","Melinos Averkiou","Siddhartha Chaudhuri","Evangelos Kalogerakis"],"pdf_url":"https://arxiv.org/pdf/2402.16994v2.pdf","comment":"Webpage: https://lodurality.github.io/GEM3D/ -- Cond. accept. to\n  SIGGRAPH 2024 (conf. track) -- Changes (based on reviews): changed style to\n  sigconf; rearranged figures for readability; added missing citations; fixed\n  misaligned centers in Fig. 3; added failure cases (Fig. 10); rewrote\n  discussion; added categories averages to Tab. 8; added Tab. 10 with model\n  capacities"},{"id":"http://arxiv.org/abs/2404.07449v1","updated":"2024-04-11T03:09:34Z","published":"2024-04-11T03:09:34Z","title":"Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs","summary":"  Integration of Large Language Models (LLMs) into visual domain tasks,\nresulting in visual-LLMs (V-LLMs), has enabled exceptional performance in\nvision-language tasks, particularly for visual question answering (VQA).\nHowever, existing V-LLMs (e.g. BLIP-2, LLaVA) demonstrate weak spatial\nreasoning and localization awareness. Despite generating highly descriptive and\nelaborate textual answers, these models fail at simple tasks like\ndistinguishing a left vs right location. In this work, we explore how\nimage-space coordinate based instruction fine-tuning objectives could inject\nspatial awareness into V-LLMs. We discover optimal coordinate representations,\ndata-efficient instruction fine-tuning objectives, and pseudo-data generation\nstrategies that lead to improved spatial awareness in V-LLMs. Additionally, our\nresulting model improves VQA across image and video domains, reduces undesired\nhallucination, and generates better contextual object descriptions. Experiments\nacross 5 vision-language tasks involving 14 different datasets establish the\nclear performance improvements achieved by our proposed framework.\n","authors":["Kanchana Ranasinghe","Satya Narayan Shukla","Omid Poursaeed","Michael S. Ryoo","Tsung-Yu Lin"],"pdf_url":"https://arxiv.org/pdf/2404.07449v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07448v1","updated":"2024-04-11T03:08:53Z","published":"2024-04-11T03:08:53Z","title":"Transferable and Principled Efficiency for Open-Vocabulary Segmentation","summary":"  Recent success of pre-trained foundation vision-language models makes\nOpen-Vocabulary Segmentation (OVS) possible. Despite the promising performance,\nthis approach introduces heavy computational overheads for two challenges: 1)\nlarge model sizes of the backbone; 2) expensive costs during the fine-tuning.\nThese challenges hinder this OVS strategy from being widely applicable and\naffordable in real-world scenarios. Although traditional methods such as model\ncompression and efficient fine-tuning can address these challenges, they often\nrely on heuristics. This means that their solutions cannot be easily\ntransferred and necessitate re-training on different models, which comes at a\ncost. In the context of efficient OVS, we target achieving performance that is\ncomparable to or even better than prior OVS works based on large\nvision-language foundation models, by utilizing smaller models that incur lower\ntraining costs. The core strategy is to make our efficiency principled and thus\nseamlessly transferable from one OVS framework to others without further\ncustomization. Comprehensive experiments on diverse OVS benchmarks demonstrate\nour superior trade-off between segmentation accuracy and computation costs over\nprevious works. Our code is available on https://github.com/Xujxyang/OpenTrans\n","authors":["Jingxuan Xu","Wuyang Chen","Yao Zhao","Yunchao Wei"],"pdf_url":"https://arxiv.org/pdf/2404.07448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.16923v2","updated":"2024-04-11T03:01:41Z","published":"2024-01-30T11:46:27Z","title":"Fourier Prompt Tuning for Modality-Incomplete Scene Segmentation","summary":"  Integrating information from multiple modalities enhances the robustness of\nscene perception systems in autonomous vehicles, providing a more comprehensive\nand reliable sensory framework. However, the modality incompleteness in\nmulti-modal segmentation remains under-explored. In this work, we establish a\ntask called Modality-Incomplete Scene Segmentation (MISS), which encompasses\nboth system-level modality absence and sensor-level modality errors. To avoid\nthe predominant modality reliance in multi-modal fusion, we introduce a\nMissing-aware Modal Switch (MMS) strategy to proactively manage missing\nmodalities during training. Utilizing bit-level batch-wise sampling enhances\nthe model's performance in both complete and incomplete testing scenarios.\nFurthermore, we introduce the Fourier Prompt Tuning (FPT) method to incorporate\nrepresentative spectral information into a limited number of learnable prompts\nthat maintain robustness against all MISS scenarios. Akin to fine-tuning\neffects but with fewer tunable parameters (1.1%). Extensive experiments prove\nthe efficacy of our proposed approach, showcasing an improvement of 5.84% mIoU\nover the prior state-of-the-art parameter-efficient methods in modality\nmissing. The source code is publicly available at\nhttps://github.com/RuipingL/MISS.\n","authors":["Ruiping Liu","Jiaming Zhang","Kunyu Peng","Yufan Chen","Ke Cao","Junwei Zheng","M. Saquib Sarfraz","Kailun Yang","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2401.16923v2.pdf","comment":"Accepted to IEEE IV 2024. The source code is publicly available at\n  https://github.com/RuipingL/MISS"},{"id":"http://arxiv.org/abs/2404.07445v1","updated":"2024-04-11T03:00:00Z","published":"2024-04-11T03:00:00Z","title":"Multi-view Aggregation Network for Dichotomous Image Segmentation","summary":"  Dichotomous Image Segmentation (DIS) has recently emerged towards\nhigh-precision object segmentation from high-resolution natural images.\n  When designing an effective DIS model, the main challenge is how to balance\nthe semantic dispersion of high-resolution targets in the small receptive field\nand the loss of high-precision details in the large receptive field. Existing\nmethods rely on tedious multiple encoder-decoder streams and stages to\ngradually complete the global localization and local refinement.\n  Human visual system captures regions of interest by observing them from\nmultiple views. Inspired by it, we model DIS as a multi-view object perception\nproblem and provide a parsimonious multi-view aggregation network (MVANet),\nwhich unifies the feature fusion of the distant view and close-up view into a\nsingle stream with one encoder-decoder structure. With the help of the proposed\nmulti-view complementary localization and refinement modules, our approach\nestablished long-range, profound visual interactions across multiple views,\nallowing the features of the detailed close-up view to focus on highly slender\nstructures.Experiments on the popular DIS-5K dataset show that our MVANet\nsignificantly outperforms state-of-the-art methods in both accuracy and speed.\nThe source code and datasets will be publicly available at\n\\href{https://github.com/qianyu-dlut/MVANet}{MVANet}.\n","authors":["Qian Yu","Xiaoqi Zhao","Youwei Pang","Lihe Zhang","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2404.07445v1.pdf","comment":"Accepted by CVPR2024 as Highlight"},{"id":"http://arxiv.org/abs/2310.14576v2","updated":"2024-04-11T02:57:21Z","published":"2023-10-23T05:25:49Z","title":"Tensor Decomposition Based Attention Module for Spiking Neural Networks","summary":"  The attention mechanism has been proven to be an effective way to improve\nspiking neural network (SNN). However, based on the fact that the current SNN\ninput data flow is split into tensors to process on GPUs, none of the previous\nworks consider the properties of tensors to implement an attention module. This\ninspires us to rethink current SNN from the perspective of tensor-relevant\ntheories. Using tensor decomposition, we design the \\textit{projected full\nattention} (PFA) module, which demonstrates excellent results with linearly\ngrowing parameters. Specifically, PFA is composed by the \\textit{linear\nprojection of spike tensor} (LPST) module and \\textit{attention map composing}\n(AMC) module. In LPST, we start by compressing the original spike tensor into\nthree projected tensors using a single property-preserving strategy with\nlearnable parameters for each dimension. Then, in AMC, we exploit the inverse\nprocedure of the tensor decomposition process to combine the three tensors into\nthe attention map using a so-called connecting factor. To validate the\neffectiveness of the proposed PFA module, we integrate it into the widely used\nVGG and ResNet architectures for classification tasks. Our method achieves\nstate-of-the-art performance on both static and dynamic benchmark datasets,\nsurpassing the existing SNN models with Transformer-based and CNN-based\nbackbones.\n","authors":["Haoyu Deng","Ruijie Zhu","Xuerui Qiu","Yule Duan","Malu Zhang","Liangjian Deng"],"pdf_url":"https://arxiv.org/pdf/2310.14576v2.pdf","comment":"Accepted by Knowledge-Based Systems"},{"id":"http://arxiv.org/abs/2403.17920v2","updated":"2024-04-11T02:42:59Z","published":"2024-03-26T17:55:11Z","title":"TC4D: Trajectory-Conditioned Text-to-4D Generation","summary":"  Recent techniques for text-to-4D generation synthesize dynamic 3D scenes\nusing supervision from pre-trained text-to-video models. However, existing\nrepresentations for motion, such as deformation models or time-dependent neural\nrepresentations, are limited in the amount of motion they can generate-they\ncannot synthesize motion extending far beyond the bounding box used for volume\nrendering. The lack of a more flexible motion model contributes to the gap in\nrealism between 4D generation methods and recent, near-photorealistic video\ngeneration models. Here, we propose TC4D: trajectory-conditioned text-to-4D\ngeneration, which factors motion into global and local components. We represent\nthe global motion of a scene's bounding box using rigid transformation along a\ntrajectory parameterized by a spline. We learn local deformations that conform\nto the global trajectory using supervision from a text-to-video model. Our\napproach enables the synthesis of scenes animated along arbitrary trajectories,\ncompositional scene generation, and significant improvements to the realism and\namount of generated motion, which we evaluate qualitatively and through a user\nstudy. Video results can be viewed on our website:\nhttps://sherwinbahmani.github.io/tc4d.\n","authors":["Sherwin Bahmani","Xian Liu","Yifan Wang","Ivan Skorokhodov","Victor Rong","Ziwei Liu","Xihui Liu","Jeong Joon Park","Sergey Tulyakov","Gordon Wetzstein","Andrea Tagliasacchi","David B. Lindell"],"pdf_url":"https://arxiv.org/pdf/2403.17920v2.pdf","comment":"Project Page: https://sherwinbahmani.github.io/tc4d"},{"id":"http://arxiv.org/abs/2404.07435v1","updated":"2024-04-11T02:29:08Z","published":"2024-04-11T02:29:08Z","title":"Encoding Urban Ecologies: Automated Building Archetype Generation\n  through Self-Supervised Learning for Energy Modeling","summary":"  As the global population and urbanization expand, the building sector has\nemerged as the predominant energy consumer and carbon emission contributor. The\nneed for innovative Urban Building Energy Modeling grows, yet existing building\narchetypes often fail to capture the unique attributes of local buildings and\nthe nuanced distinctions between different cities, jeopardizing the precision\nof energy modeling. This paper presents an alternative tool employing\nself-supervised learning to distill complex geometric data into representative,\nlocale-specific archetypes. This study attempts to foster a new paradigm of\ninteraction with built environments, incorporating local parameters to conduct\nbespoke energy simulations at the community level. The catered archetypes can\naugment the precision and applicability of energy consumption modeling at\ndifferent scales across diverse building inventories. This tool provides a\npotential solution that encourages the exploration of emerging local ecologies.\nBy integrating building envelope characteristics and cultural granularity into\nthe building archetype generation process, we seek a future where architecture\nand urban design are intricately interwoven with the energy sector in shaping\nour built environments.\n","authors":["Xinwei Zhuang","Zixun Huang","Wentao Zeng","Luisa Caldas"],"pdf_url":"https://arxiv.org/pdf/2404.07435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.10853v4","updated":"2024-04-11T01:56:38Z","published":"2023-07-20T13:16:10Z","title":"Exploring Effective Priors and Efficient Models for Weakly-Supervised\n  Change Detection","summary":"  Weakly-supervised change detection (WSCD) aims to detect pixel-level changes\nwith only image-level annotations. Owing to its label efficiency, WSCD is\ndrawing increasing attention recently. However, current WSCD methods often\nencounter the challenge of change missing and fabricating, i.e., the\ninconsistency between image-level annotations and pixel-level predictions.\nSpecifically, change missing refer to the situation that the WSCD model fails\nto predict any changed pixels, even though the image-level label indicates\nchanged, and vice versa for change fabricating. To address this challenge, in\nthis work, we leverage global-scale and local-scale priors in WSCD and propose\ntwo components: a Dilated Prior (DP) decoder and a Label Gated (LG) constraint.\nThe DP decoder decodes samples with the changed image-level label, skips\nsamples with the unchanged label, and replaces them with an all-unchanged\npixel-level label. The LG constraint is derived from the correspondence between\nchanged representations and image-level labels, penalizing the model when it\nmispredicts the change status. Additionally, we develop TransWCD, a simple yet\npowerful transformer-based model, showcasing the potential of weakly-supervised\nlearning in change detection. By integrating the DP decoder and LG constraint\ninto TransWCD, we form TransWCD-DL. Our proposed TransWCD and TransWCD-DL\nachieve significant +6.33% and +9.55% F1 score improvements over the\nstate-of-the-art methods on the WHU-CD dataset, respectively. Some performance\nmetrics even exceed several fully-supervised change detection (FSCD)\ncompetitors. Code will be available at\nhttps://github.com/zhenghuizhao/TransWCD.\n","authors":["Zhenghui Zhao","Lixiang Ru","Chen Wu"],"pdf_url":"https://arxiv.org/pdf/2307.10853v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07424v1","updated":"2024-04-11T01:33:45Z","published":"2024-04-11T01:33:45Z","title":"CopilotCAD: Empowering Radiologists with Report Completion Models and\n  Quantitative Evidence from Medical Image Foundation Models","summary":"  Computer-aided diagnosis systems hold great promise to aid radiologists and\nclinicians in radiological clinical practice and enhance diagnostic accuracy\nand efficiency. However, the conventional systems primarily focus on delivering\ndiagnostic results through text report generation or medical image\nclassification, positioning them as standalone decision-makers rather than\nhelpers and ignoring radiologists' expertise. This study introduces an\ninnovative paradigm to create an assistive co-pilot system for empowering\nradiologists by leveraging Large Language Models (LLMs) and medical image\nanalysis tools. Specifically, we develop a collaborative framework to integrate\nLLMs and quantitative medical image analysis results generated by foundation\nmodels with radiologists in the loop, achieving efficient and safe generation\nof radiology reports and effective utilization of computational power of AI and\nthe expertise of medical professionals. This approach empowers radiologists to\ngenerate more precise and detailed diagnostic reports, enhancing patient\noutcomes while reducing the burnout of clinicians. Our methodology underscores\nthe potential of AI as a supportive tool in medical diagnostics, promoting a\nharmonious integration of technology and human expertise to advance the field\nof radiology.\n","authors":["Sheng Wang","Tianming Du","Katherine Fischer","Gregory E Tasian","Justin Ziemba","Joanie M Garratt","Hersh Sagreiya","Yong Fan"],"pdf_url":"https://arxiv.org/pdf/2404.07424v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07410v1","updated":"2024-04-11T00:49:38Z","published":"2024-04-11T00:49:38Z","title":"Improving Shift Invariance in Convolutional Neural Networks with\n  Translation Invariant Polyphase Sampling","summary":"  Downsampling operators break the shift invariance of convolutional neural\nnetworks (CNNs) and this affects the robustness of features learned by CNNs\nwhen dealing with even small pixel-level shift. Through a large-scale\ncorrelation analysis framework, we study shift invariance of CNNs by inspecting\nexisting downsampling operators in terms of their maximum-sampling bias (MSB),\nand find that MSB is negatively correlated with shift invariance. Based on this\ncrucial insight, we propose a learnable pooling operator called Translation\nInvariant Polyphase Sampling (TIPS) and two regularizations on the intermediate\nfeature maps of TIPS to reduce MSB and learn translation-invariant\nrepresentations. TIPS can be integrated into any CNN and can be trained\nend-to-end with marginal computational overhead. Our experiments demonstrate\nthat TIPS results in consistent performance gains in terms of accuracy, shift\nconsistency, and shift fidelity on multiple benchmarks for image classification\nand semantic segmentation compared to previous methods and also leads to\nimprovements in adversarial and distributional robustness. TIPS results in the\nlowest MSB compared to all previous methods, thus explaining our strong\nempirical results.\n","authors":["Sourajit Saha","Tejas Gokhale"],"pdf_url":"https://arxiv.org/pdf/2404.07410v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07405v1","updated":"2024-04-11T00:45:10Z","published":"2024-04-11T00:45:10Z","title":"Simplifying Two-Stage Detectors for On-Device Inference in Remote\n  Sensing","summary":"  Deep learning has been successfully applied to object detection from remotely\nsensed images. Images are typically processed on the ground rather than\non-board due to the computation power of the ground system. Such offloaded\nprocessing causes delays in acquiring target mission information, which hinders\nits application to real-time use cases. For on-device object detection,\nresearches have been conducted on designing efficient detectors or model\ncompression to reduce inference latency. However, highly accurate two-stage\ndetectors still need further exploitation for acceleration. In this paper, we\npropose a model simplification method for two-stage object detectors. Instead\nof constructing a general feature pyramid, we utilize only one feature\nextraction in the two-stage detector. To compensate for the accuracy drop, we\napply a high pass filter to the RPN's score map. Our approach is applicable to\nany two-stage detector using a feature pyramid network. In the experiments with\nstate-of-the-art two-stage detectors such as ReDet, Oriented-RCNN, and LSKNet,\nour method reduced computation costs upto 61.2% with the accuracy loss within\n2.1% on the DOTAv1.5 dataset. Source code will be released.\n","authors":["Jaemin Kang","Hoeseok Yang","Hyungshin Kim"],"pdf_url":"https://arxiv.org/pdf/2404.07405v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.10300v4","updated":"2024-04-11T00:35:04Z","published":"2023-05-17T15:37:47Z","title":"One-Prompt to Segment All Medical Images","summary":"  Large foundation models, known for their strong zero-shot generalization,\nhave excelled in visual and language applications. However, applying them to\nmedical image segmentation, a domain with diverse imaging types and target\nlabels, remains an open challenge. Current approaches, such as adapting\ninteractive segmentation models like Segment Anything Model (SAM), require user\nprompts for each sample during inference. Alternatively, transfer learning\nmethods like few/one-shot models demand labeled samples, leading to high costs.\nThis paper introduces a new paradigm toward the universal medical image\nsegmentation, termed 'One-Prompt Segmentation.' One-Prompt Segmentation\ncombines the strengths of one-shot and interactive methods. In the inference\nstage, with just \\textbf{one prompted sample}, it can adeptly handle the unseen\ntask in a single forward pass. We train One-Prompt Model on 64 open-source\nmedical datasets, accompanied by the collection of over 3,000 clinician-labeled\nprompts. Tested on 14 previously unseen datasets, the One-Prompt Model\nshowcases superior zero-shot segmentation capabilities, outperforming a wide\nrange of related methods. The code and data is released as\n\\url{https://github.com/KidsWithTokens/one-prompt}.\n","authors":["Junde Wu","Jiayuan Zhu","Yuanpei Liu","Yueming Jin","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2305.10300v4.pdf","comment":"arXiv admin note: text overlap with arXiv:2304.12620"},{"id":"http://arxiv.org/abs/2404.07399v1","updated":"2024-04-11T00:23:28Z","published":"2024-04-11T00:23:28Z","title":"Post-hurricane building damage assessment using street-view imagery and\n  structured data: A multi-modal deep learning approach","summary":"  Accurately assessing building damage is critical for disaster response and\nrecovery. However, many existing models for detecting building damage have poor\nprediction accuracy due to their limited capabilities of identifying detailed,\ncomprehensive structural and/or non-structural damage from the street-view\nimage. Additionally, these models mainly rely on the imagery data for damage\nclassification, failing to account for other critical information, such as wind\nspeed, building characteristics, evacuation zones, and distance of the building\nto the hurricane track. To address these limitations, in this study, we propose\na novel multi-modal (i.e., imagery and structured data) approach for\npost-hurricane building damage classification, named the Multi-Modal Swin\nTransformer (MMST). We empirically train and evaluate the proposed MMST using\ndata collected from the 2022 Hurricane Ian in Florida, USA. Results show that\nMMST outperforms all selected state-of-the-art benchmark models and can achieve\nan accuracy of 92.67%, which are 7.71% improvement in accuracy compared to\nVisual Geometry Group 16 (VGG-16). In addition to the street-view imagery data,\nbuilding value, building age, and wind speed are the most important predictors\nfor damage level classification. The proposed MMST can be deployed to assist in\nrapid damage assessment and guide reconnaissance efforts in future hurricanes.\n","authors":["Zhuoqun Xue","Xiaojian Zhang","David O. Prevatt","Jennifer Bridge","Susu Xu","Xilei Zhao"],"pdf_url":"https://arxiv.org/pdf/2404.07399v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07395v1","updated":"2024-04-11T00:02:57Z","published":"2024-04-11T00:02:57Z","title":"Global versus Local: Evaluating AlexNet Architectures for Tropical\n  Cyclone Intensity Estimation","summary":"  Given the destructive impacts of tropical cyclones, it is critical to have a\nreliable system for cyclone intensity detection. Various techniques are\navailable for this purpose, each with differing levels of accuracy. In this\npaper, we introduce two ensemble-based models based on AlexNet architecture to\nestimate tropical cyclone intensity using visible satellite images. The first\nmodel, trained on the entire dataset, is called the global AlexNet model. The\nsecond model is a distributed version of AlexNet in which multiple AlexNets are\ntrained separately on subsets of the training data categorized according to the\nSaffir-Simpson wind speed scale prescribed by the meterologists. We evaluated\nthe performance of both models against a deep learning benchmark model called\n\\textit{Deepti} using a publicly available cyclone image dataset. Results\nindicate that both the global model (with a root mean square error (RMSE) of\n9.03 knots) and the distributed model (with a RMSE of 9.3 knots) outperform the\nbenchmark model (with a RMSE of 13.62 knots). We provide a thorough discussion\nof our solution approach, including an explanantion of the AlexNet's\nperformance using gradient class activation maps (grad-CAM). Our proposed\nsolution strategy allows future experimentation with various deep learning\nmodels in both single and multi-channel settings.\n","authors":["Vikas Dwivedi"],"pdf_url":"https://arxiv.org/pdf/2404.07395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.04526v2","updated":"2024-04-11T23:50:32Z","published":"2023-08-08T18:41:38Z","title":"Large-Scale Multi-Hypotheses Cell Tracking Using Ultrametric Contours\n  Maps","summary":"  In this work, we describe a method for large-scale 3D cell-tracking through a\nsegmentation selection approach. The proposed method is effective at tracking\ncells across large microscopy datasets on two fronts: (i) It can solve problems\ncontaining millions of segmentation instances in terabyte-scale 3D+t datasets;\n(ii) It achieves competitive results with or without deep learning, which\nrequires 3D annotated data, that is scarce in the fluorescence microscopy\nfield. The proposed method computes cell tracks and segments using a hierarchy\nof segmentation hypotheses and selects disjoint segments by maximizing the\noverlap between adjacent frames. We show that this method achieves\nstate-of-the-art results in 3D images from the cell tracking challenge and has\na faster integer linear programming formulation. Moreover, our framework is\nflexible and supports segmentations from off-the-shelf cell segmentation models\nand can combine them into an ensemble that improves tracking. The code is\navailable https://github.com/royerlab/ultrack.\n","authors":["Jord√£o Bragantini","Merlin Lange","Lo√Øc Royer"],"pdf_url":"https://arxiv.org/pdf/2308.04526v2.pdf","comment":"13 pages, 7 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.16400v2","updated":"2024-04-11T23:38:06Z","published":"2024-03-25T03:30:37Z","title":"ASDF: Assembly State Detection Utilizing Late Fusion by Integrating 6D\n  Pose Estimation","summary":"  In medical and industrial domains, providing guidance for assembly processes\nis critical to ensure efficiency and safety. Errors in assembly can lead to\nsignificant consequences such as extended surgery times, and prolonged\nmanufacturing or maintenance times in industry. Assembly scenarios can benefit\nfrom in-situ AR visualization to provide guidance, reduce assembly times and\nminimize errors. To enable in-situ visualization 6D pose estimation can be\nleveraged. Existing 6D pose estimation techniques primarily focus on individual\nobjects and static captures. However, assembly scenarios have various dynamics\nincluding occlusion during assembly and dynamics in the assembly objects\nappearance. Existing work, combining object detection/6D pose estimation and\nassembly state detection focuses either on pure deep learning-based approaches,\nor limit the assembly state detection to building blocks. To address the\nchallenges of 6D pose estimation in combination with assembly state detection,\nour approach ASDF builds upon the strengths of YOLOv8, a real-time capable\nobject detection framework. We extend this framework, refine the object pose\nand fuse pose knowledge with network-detected pose information. Utilizing our\nlate fusion in our Pose2State module results in refined 6D pose estimation and\nassembly state detection. By combining both pose and state information, our\nPose2State module predicts the final assembly state with precision. Our\nevaluation on our ASDF dataset shows that our Pose2State module leads to an\nimproved assembly state detection and that the improvement of the assembly\nstate further leads to a more robust 6D pose estimation. Moreover, on the GBOT\ndataset, we outperform the pure deep learning-based network, and even\noutperform the hybrid and pure tracking-based approaches.\n","authors":["Hannah Schieber","Shiyu Li","Niklas Corell","Philipp Beckerle","Julian Kreimeier","Daniel Roth"],"pdf_url":"https://arxiv.org/pdf/2403.16400v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.01492v2","updated":"2024-04-11T23:09:25Z","published":"2024-04-01T21:28:50Z","title":"Modality Translation for Object Detection Adaptation Without Forgetting\n  Prior Knowledge","summary":"  A common practice in deep learning consists of training large neural networks\non massive datasets to perform accurately for different domains and tasks.\nWhile this methodology may work well in numerous application areas, it only\napplies across modalities due to a larger distribution shift in data captured\nusing different sensors. This paper focuses on the problem of adapting a large\nobject detection model to one or multiple modalities while being efficient. To\ndo so, we propose ModTr as an alternative to the common approach of fine-tuning\nlarge models. ModTr consists of adapting the input with a small transformation\nnetwork trained to minimize the detection loss directly. The original model can\ntherefore work on the translated inputs without any further change or\nfine-tuning to its parameters. Experimental results on translating from IR to\nRGB images on two well-known datasets show that this simple ModTr approach\nprovides detectors that can perform comparably or better than the standard\nfine-tuning without forgetting the original knowledge. This opens the doors to\na more flexible and efficient service-based detection pipeline in which,\ninstead of using a different detector for each modality, a unique and unaltered\nserver is constantly running, where multiple modalities with the corresponding\ntranslations can query it. Code: https://github.com/heitorrapela/ModTr.\n","authors":["Heitor Rapela Medeiros","Masih Aminbeidokhti","Fidel Guerrero Pena","David Latortue","Eric Granger","Marco Pedersoli"],"pdf_url":"https://arxiv.org/pdf/2404.01492v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12982v2","updated":"2024-04-11T22:47:39Z","published":"2023-10-19T17:59:56Z","title":"Putting the Object Back into Video Object Segmentation","summary":"  We present Cutie, a video object segmentation (VOS) network with object-level\nmemory reading, which puts the object representation from memory back into the\nvideo object segmentation result. Recent works on VOS employ bottom-up\npixel-level memory reading which struggles due to matching noise, especially in\nthe presence of distractors, resulting in lower performance in more challenging\ndata. In contrast, Cutie performs top-down object-level memory reading by\nadapting a small set of object queries. Via those, it interacts with the\nbottom-up pixel features iteratively with a query-based object transformer (qt,\nhence Cutie). The object queries act as a high-level summary of the target\nobject, while high-resolution feature maps are retained for accurate\nsegmentation. Together with foreground-background masked attention, Cutie\ncleanly separates the semantics of the foreground object from the background.\nOn the challenging MOSE dataset, Cutie improves by 8.7 J&F over XMem with a\nsimilar running time and improves by 4.2 J&F over DeAOT while being three times\nfaster. Code is available at: https://hkchengrex.github.io/Cutie\n","authors":["Ho Kei Cheng","Seoung Wug Oh","Brian Price","Joon-Young Lee","Alexander Schwing"],"pdf_url":"https://arxiv.org/pdf/2310.12982v2.pdf","comment":"CVPR 2024 Highlight. Project page: https://hkchengrex.github.io/Cutie"},{"id":"http://arxiv.org/abs/2307.15904v2","updated":"2024-04-11T22:39:15Z","published":"2023-07-29T06:23:51Z","title":"Sat2Cap: Mapping Fine-Grained Textual Descriptions from Satellite Images","summary":"  We propose a weakly supervised approach for creating maps using free-form\ntextual descriptions. We refer to this work of creating textual maps as\nzero-shot mapping. Prior works have approached mapping tasks by developing\nmodels that predict a fixed set of attributes using overhead imagery. However,\nthese models are very restrictive as they can only solve highly specific tasks\nfor which they were trained. Mapping text, on the other hand, allows us to\nsolve a large variety of mapping problems with minimal restrictions. To achieve\nthis, we train a contrastive learning framework called Sat2Cap on a new\nlarge-scale dataset with 6.1M pairs of overhead and ground-level images. For a\ngiven location and overhead image, our model predicts the expected CLIP\nembeddings of the ground-level scenery. The predicted CLIP embeddings are then\nused to learn about the textual space associated with that location. Sat2Cap is\nalso conditioned on date-time information, allowing it to model temporally\nvarying concepts over a location. Our experimental results demonstrate that our\nmodels successfully capture ground-level concepts and allow large-scale mapping\nof fine-grained textual queries. Our approach does not require any text-labeled\ndata, making the training easily scalable. The code, dataset, and models will\nbe made publicly available.\n","authors":["Aayush Dhakal","Adeel Ahmad","Subash Khanal","Srikumar Sastry","Hannah Kerner","Nathan Jacobs"],"pdf_url":"https://arxiv.org/pdf/2307.15904v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2404.08135v1","updated":"2024-04-11T21:41:55Z","published":"2024-04-11T21:41:55Z","title":"SciFlow: Empowering Lightweight Optical Flow Models with Self-Cleaning\n  Iterations","summary":"  Optical flow estimation is crucial to a variety of vision tasks. Despite\nsubstantial recent advancements, achieving real-time on-device optical flow\nestimation remains a complex challenge. First, an optical flow model must be\nsufficiently lightweight to meet computation and memory constraints to ensure\nreal-time performance on devices. Second, the necessity for real-time on-device\noperation imposes constraints that weaken the model's capacity to adequately\nhandle ambiguities in flow estimation, thereby intensifying the difficulty of\npreserving flow accuracy. This paper introduces two synergistic techniques,\nSelf-Cleaning Iteration (SCI) and Regression Focal Loss (RFL), designed to\nenhance the capabilities of optical flow models, with a focus on addressing\noptical flow regression ambiguities. These techniques prove particularly\neffective in mitigating error propagation, a prevalent issue in optical flow\nmodels that employ iterative refinement. Notably, these techniques add\nnegligible to zero overhead in model parameters and inference latency, thereby\npreserving real-time on-device efficiency. The effectiveness of our proposed\nSCI and RFL techniques, collectively referred to as SciFlow for brevity, is\ndemonstrated across two distinct lightweight optical flow model architectures\nin our experiments. Remarkably, SciFlow enables substantial reduction in error\nmetrics (EPE and Fl-all) over the baseline models by up to 6.3% and 10.5% for\nin-domain scenarios and by up to 6.2% and 13.5% for cross-domain scenarios on\nthe Sintel and KITTI 2015 datasets, respectively.\n","authors":["Jamie Menjay Lin","Jisoo Jeong","Hong Cai","Risheek Garrepalli","Kai Wang","Fatih Porikli"],"pdf_url":"https://arxiv.org/pdf/2404.08135v1.pdf","comment":"CVPRW 2024"},{"id":"http://arxiv.org/abs/2404.08127v1","updated":"2024-04-11T21:07:38Z","published":"2024-04-11T21:07:38Z","title":"Self-Supervised Learning of Color Constancy","summary":"  Color constancy (CC) describes the ability of the visual system to perceive\nan object as having a relatively constant color despite changes in lighting\nconditions. While CC and its limitations have been carefully characterized in\nhumans, it is still unclear how the visual system acquires this ability during\ndevelopment. Here, we present a first study showing that CC develops in a\nneural network trained in a self-supervised manner through an invariance\nlearning objective. During learning, objects are presented under changing\nilluminations, while the network aims to map subsequent views of the same\nobject onto close-by latent representations. This gives rise to representations\nthat are largely invariant to the illumination conditions, offering a plausible\nexample of how CC could emerge during human cognitive development via a form of\nself-supervised learning.\n","authors":["Markus R. Ernst","Francisco M. L√≥pez","Arthur Aubret","Roland W. Fleming","Jochen Triesch"],"pdf_url":"https://arxiv.org/pdf/2404.08127v1.pdf","comment":"7 pages, 5 figures, submitted to the IEEE International Conference on\n  Development and Learning (ICDL 2024)"},{"id":"http://arxiv.org/abs/2404.08111v1","updated":"2024-04-11T20:25:26Z","published":"2024-04-11T20:25:26Z","title":"S3Editor: A Sparse Semantic-Disentangled Self-Training Framework for\n  Face Video Editing","summary":"  Face attribute editing plays a pivotal role in various applications. However,\nexisting methods encounter challenges in achieving high-quality results while\npreserving identity, editing faithfulness, and temporal consistency. These\nchallenges are rooted in issues related to the training pipeline, including\nlimited supervision, architecture design, and optimization strategy. In this\nwork, we introduce S3Editor, a Sparse Semantic-disentangled Self-training\nframework for face video editing. S3Editor is a generic solution that\ncomprehensively addresses these challenges with three key contributions.\nFirstly, S3Editor adopts a self-training paradigm to enhance the training\nprocess through semi-supervision. Secondly, we propose a semantic disentangled\narchitecture with a dynamic routing mechanism that accommodates diverse editing\nrequirements. Thirdly, we present a structured sparse optimization schema that\nidentifies and deactivates malicious neurons to further disentangle impacts\nfrom untarget attributes. S3Editor is model-agnostic and compatible with\nvarious editing approaches. Our extensive qualitative and quantitative results\naffirm that our approach significantly enhances identity preservation, editing\nfidelity, as well as temporal consistency.\n","authors":["Guangzhi Wang","Tianyi Chen","Kamran Ghasedi","HsiangTao Wu","Tianyu Ding","Chris Nuesmeyer","Ilya Zharkov","Mohan Kankanhalli","Luming Liang"],"pdf_url":"https://arxiv.org/pdf/2404.08111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01001v2","updated":"2024-04-11T20:07:20Z","published":"2023-12-02T02:09:31Z","title":"Learning county from pixels: Corn yield prediction with\n  attention-weighted multiple instance learning","summary":"  Remote sensing technology has become a promising tool in yield prediction.\nMost prior work employs satellite imagery for county-level corn yield\nprediction by spatially aggregating all pixels within a county into a single\nvalue, potentially overlooking the detailed information and valuable insights\noffered by more granular data. To this end, this research examines each county\nat the pixel level and applies multiple instance learning to leverage detailed\ninformation within a county. In addition, our method addresses the \"mixed\npixel\" issue caused by the inconsistent resolution between feature datasets and\ncrop mask, which may introduce noise into the model and therefore hinder\naccurate yield prediction. Specifically, the attention mechanism is employed to\nautomatically assign weights to different pixels, which can mitigate the\ninfluence of mixed pixels. The experimental results show that the developed\nmodel outperforms four other machine learning models over the past five years\nin the U.S. corn belt and demonstrates its best performance in 2022, achieving\na coefficient of determination (R2) value of 0.84 and a root mean square error\n(RMSE) of 0.83. This paper demonstrates the advantages of our approach from\nboth spatial and temporal perspectives. Furthermore, through an in-depth study\nof the relationship between mixed pixels and attention, it is verified that our\napproach can capture critical feature information while filtering out noise\nfrom mixed pixels.\n","authors":["Xiaoyu Wang","Yuchi Ma","Qunying Huang","Zhengwei Yang","Zhou Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.01001v2.pdf","comment":"I am writing to request the withdrawal of my paper submitted to\n  arXiv. Upon further review, I have identified an error in the paper that\n  significantly affects the results and conclusions. To maintain the integrity\n  of the scientific record and prevent the dissemination of incorrect\n  information, I believe it is necessary to withdraw the paper from the archive"},{"id":"http://arxiv.org/abs/2307.03798v2","updated":"2024-04-11T19:24:50Z","published":"2023-07-07T18:54:11Z","title":"Fooling Contrastive Language-Image Pre-trained Models with\n  CLIPMasterPrints","summary":"  Models leveraging both visual and textual data such as Contrastive\nLanguage-Image Pre-training (CLIP), are the backbone of many recent advances in\nartificial intelligence. In this work, we show that despite their versatility,\nsuch models are vulnerable to what we refer to as fooling master images.\nFooling master images are capable of maximizing the confidence score of a CLIP\nmodel for a significant number of widely varying prompts, while being either\nunrecognizable or unrelated to the attacked prompts for humans. The existence\nof such images is problematic as it could be used by bad actors to maliciously\ninterfere with CLIP-trained image retrieval models in production with\ncomparably small effort as a single image can attack many different prompts. We\ndemonstrate how fooling master images for CLIP (CLIPMasterPrints) can be mined\nusing stochastic gradient descent, projected gradient descent, or blackbox\noptimization. Contrary to many common adversarial attacks, the blackbox\noptimization approach allows us to mine CLIPMasterPrints even when the weights\nof the model are not accessible. We investigate the properties of the mined\nimages, and find that images trained on a small number of image captions\ngeneralize to a much larger number of semantically related captions. We\nevaluate possible mitigation strategies, where we increase the robustness of\nthe model and introduce an approach to automatically detect CLIPMasterPrints to\nsanitize the input of vulnerable models. Finally, we find that vulnerability to\nCLIPMasterPrints is related to a modality gap in contrastive pre-trained\nmulti-modal networks. Code available at\nhttps://github.com/matfrei/CLIPMasterPrints.\n","authors":["Matthias Freiberger","Peter Kun","Christian Igel","Anders Sundnes L√∏vlie","Sebastian Risi"],"pdf_url":"https://arxiv.org/pdf/2307.03798v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.13004v3","updated":"2024-04-11T19:22:41Z","published":"2022-10-24T07:50:02Z","title":"Efficient Representation of Natural Image Patches","summary":"  Utilizing an abstract information processing model based on minimal yet\nrealistic assumptions inspired by biological systems, we study how to achieve\nthe early visual system's two ultimate objectives: efficient information\ntransmission and accurate sensor probability distribution modeling. We prove\nthat optimizing for information transmission does not guarantee optimal\nprobability distribution modeling in general. We illustrate, using a two-pixel\n(2D) system and image patches, that an efficient representation can be realized\nthrough a nonlinear population code driven by two types of biologically\nplausible loss functions that depend solely on output. After unsupervised\nlearning, our abstract information processing model bears remarkable\nresemblances to biological systems, despite not mimicking many features of real\nneurons, such as spiking activity. A preliminary comparison with a contemporary\ndeep learning model suggests that our model offers a significant efficiency\nadvantage. Our model provides novel insights into the computational theory of\nearly visual systems as well as a potential new approach to enhance the\nefficiency of deep learning models.\n","authors":["Cheng Guo"],"pdf_url":"https://arxiv.org/pdf/2210.13004v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08088v1","updated":"2024-04-11T19:06:36Z","published":"2024-04-11T19:06:36Z","title":"Visual Context-Aware Person Fall Detection","summary":"  As the global population ages, the number of fall-related incidents is on the\nrise. Effective fall detection systems, specifically in healthcare sector, are\ncrucial to mitigate the risks associated with such events. This study evaluates\nthe role of visual context, including background objects, on the accuracy of\nfall detection classifiers. We present a segmentation pipeline to\nsemi-automatically separate individuals and objects in images. Well-established\nmodels like ResNet-18, EfficientNetV2-S, and Swin-Small are trained and\nevaluated. During training, pixel-based transformations are applied to\nsegmented objects, and the models are then evaluated on raw images without\nsegmentation. Our findings highlight the significant influence of visual\ncontext on fall detection. The application of Gaussian blur to the image\nbackground notably improves the performance and generalization capabilities of\nall models. Background objects such as beds, chairs, or wheelchairs can\nchallenge fall detection systems, leading to false positive alarms. However, we\ndemonstrate that object-specific contextual transformations during training\neffectively mitigate this challenge. Further analysis using saliency maps\nsupports our observation that visual context is crucial in classification\ntasks. We create both dataset processing API and segmentation pipeline,\navailable at https://github.com/A-NGJ/image-segmentation-cli.\n","authors":["Aleksander Nagaj","Zenjie Li","Dim P. Papadopoulos","Kamal Nasrollahi"],"pdf_url":"https://arxiv.org/pdf/2404.08088v1.pdf","comment":"10 pages, 6 figures, KES IDT-24 conference"},{"id":"http://arxiv.org/abs/2404.03507v2","updated":"2024-04-11T18:54:24Z","published":"2024-04-04T15:10:24Z","title":"DQ-DETR: DETR with Dynamic Query for Tiny Object Detection","summary":"  Despite previous DETR-like methods having performed successfully in generic\nobject detection, tiny object detection is still a challenging task for them\nsince the positional information of object queries is not customized for\ndetecting tiny objects, whose scale is extraordinarily smaller than general\nobjects. Also, DETR-like methods using a fixed number of queries make them\nunsuitable for aerial datasets, which only contain tiny objects, and the\nnumbers of instances are imbalanced between different images. Thus, we present\na simple yet effective model, named DQ-DETR, which consists of three different\ncomponents: categorical counting module, counting-guided feature enhancement,\nand dynamic query selection to solve the above-mentioned problems. DQ-DETR uses\nthe prediction and density maps from the categorical counting module to\ndynamically adjust the number of object queries and improve the positional\ninformation of queries. Our model DQ-DETR outperforms previous CNN-based and\nDETR-like methods, achieving state-of-the-art mAP 30.2% on the AI-TOD-V2\ndataset, which mostly consists of tiny objects.\n","authors":["Yi-Xin Huang","Hou-I Liu","Hong-Han Shuai","Wen-Huang Cheng"],"pdf_url":"https://arxiv.org/pdf/2404.03507v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.17205v4","updated":"2024-04-11T18:48:04Z","published":"2023-12-28T18:40:31Z","title":"EFHQ: Multi-purpose ExtremePose-Face-HQ dataset","summary":"  The existing facial datasets, while having plentiful images at near frontal\nviews, lack images with extreme head poses, leading to the downgraded\nperformance of deep learning models when dealing with profile or pitched faces.\nThis work aims to address this gap by introducing a novel dataset named Extreme\nPose Face High-Quality Dataset (EFHQ), which includes a maximum of 450k\nhigh-quality images of faces at extreme poses. To produce such a massive\ndataset, we utilize a novel and meticulous dataset processing pipeline to\ncurate two publicly available datasets, VFHQ and CelebV-HQ, which contain many\nhigh-resolution face videos captured in various settings. Our dataset can\ncomplement existing datasets on various facial-related tasks, such as facial\nsynthesis with 2D/3D-aware GAN, diffusion-based text-to-image face generation,\nand face reenactment. Specifically, training with EFHQ helps models generalize\nwell across diverse poses, significantly improving performance in scenarios\ninvolving extreme views, confirmed by extensive experiments. Additionally, we\nutilize EFHQ to define a challenging cross-view face verification benchmark, in\nwhich the performance of SOTA face recognition models drops 5-37% compared to\nfrontal-to-frontal scenarios, aiming to stimulate studies on face recognition\nunder severe pose conditions in the wild.\n","authors":["Trung Tuan Dao","Duc Hong Vu","Cuong Pham","Anh Tran"],"pdf_url":"https://arxiv.org/pdf/2312.17205v4.pdf","comment":"Project Page: https://bomcon123456.github.io/efhq/"},{"id":"http://arxiv.org/abs/2404.08081v1","updated":"2024-04-11T18:42:14Z","published":"2024-04-11T18:42:14Z","title":"Real-Time Detection and Analysis of Vehicles and Pedestrians using Deep\n  Learning","summary":"  Computer vision, particularly vehicle and pedestrian identification is\ncritical to the evolution of autonomous driving, artificial intelligence, and\nvideo surveillance. Current traffic monitoring systems confront major\ndifficulty in recognizing small objects and pedestrians effectively in\nreal-time, posing a serious risk to public safety and contributing to traffic\ninefficiency. Recognizing these difficulties, our project focuses on the\ncreation and validation of an advanced deep-learning framework capable of\nprocessing complex visual input for precise, real-time recognition of cars and\npeople in a variety of environmental situations. On a dataset representing\ncomplicated urban settings, we trained and evaluated different versions of the\nYOLOv8 and RT-DETR models. The YOLOv8 Large version proved to be the most\neffective, especially in pedestrian recognition, with great precision and\nrobustness. The results, which include Mean Average Precision and recall rates,\ndemonstrate the model's ability to dramatically improve traffic monitoring and\nsafety. This study makes an important addition to real-time, reliable detection\nin computer vision, establishing new benchmarks for traffic management systems.\n","authors":["Md Nahid Sadik","Tahmim Hossain","Faisal Sayeed"],"pdf_url":"https://arxiv.org/pdf/2404.08081v1.pdf","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2404.08079v1","updated":"2024-04-11T18:34:29Z","published":"2024-04-11T18:34:29Z","title":"DIMAT: Decentralized Iterative Merging-And-Training for Deep Learning\n  Models","summary":"  Recent advances in decentralized deep learning algorithms have demonstrated\ncutting-edge performance on various tasks with large pre-trained models.\nHowever, a pivotal prerequisite for achieving this level of competitiveness is\nthe significant communication and computation overheads when updating these\nmodels, which prohibits the applications of them to real-world scenarios. To\naddress this issue, drawing inspiration from advanced model merging techniques\nwithout requiring additional training, we introduce the Decentralized Iterative\nMerging-And-Training (DIMAT) paradigm--a novel decentralized deep learning\nframework. Within DIMAT, each agent is trained on their local data and\nperiodically merged with their neighboring agents using advanced model merging\ntechniques like activation matching until convergence is achieved. DIMAT\nprovably converges with the best available rate for nonconvex functions with\nvarious first-order methods, while yielding tighter error bounds compared to\nthe popular existing approaches. We conduct a comprehensive empirical analysis\nto validate DIMAT's superiority over baselines across diverse computer vision\ntasks sourced from multiple datasets. Empirical results validate our\ntheoretical claims by showing that DIMAT attains faster and higher initial gain\nin accuracy with independent and identically distributed (IID) and non-IID\ndata, incurring lower communication overhead. This DIMAT paradigm presents a\nnew opportunity for the future decentralized learning, enhancing its\nadaptability to real-world with sparse and light-weight communication and\ncomputation.\n","authors":["Nastaran Saadati","Minh Pham","Nasla Saleem","Joshua R. Waite","Aditya Balu","Zhanhong Jiang","Chinmay Hegde","Soumik Sarkar"],"pdf_url":"https://arxiv.org/pdf/2404.08079v1.pdf","comment":"CVPR 2024 accepted paper, 22 pages, 12 figures"},{"id":"http://arxiv.org/abs/2404.02059v2","updated":"2024-04-11T18:29:01Z","published":"2024-04-02T15:58:36Z","title":"IISAN: Efficiently Adapting Multimodal Representation for Sequential\n  Recommendation with Decoupled PEFT","summary":"  Multimodal foundation models are transformative in sequential recommender\nsystems, leveraging powerful representation learning capabilities. While\nParameter-efficient Fine-tuning (PEFT) is commonly used to adapt foundation\nmodels for recommendation tasks, most research prioritizes parameter\nefficiency, often overlooking critical factors like GPU memory efficiency and\ntraining speed. Addressing this gap, our paper introduces IISAN (Intra- and\nInter-modal Side Adapted Network for Multimodal Representation), a simple\nplug-and-play architecture using a Decoupled PEFT structure and exploiting both\nintra- and inter-modal adaptation.\n  IISAN matches the performance of full fine-tuning (FFT) and state-of-the-art\nPEFT. More importantly, it significantly reduces GPU memory usage - from 47GB\nto just 3GB for multimodal sequential recommendation tasks. Additionally, it\naccelerates training time per epoch from 443s to 22s compared to FFT. This is\nalso a notable improvement over the Adapter and LoRA, which require 37-39 GB\nGPU memory and 350-380 seconds per epoch for training.\n  Furthermore, we propose a new composite efficiency metric, TPME\n(Training-time, Parameter, and GPU Memory Efficiency) to alleviate the\nprevalent misconception that \"parameter efficiency represents overall\nefficiency\". TPME provides more comprehensive insights into practical\nefficiency comparisons between different methods. Besides, we give an\naccessible efficiency analysis of all PEFT and FFT approaches, which\ndemonstrate the superiority of IISAN. We release our codes and other materials\nat https://github.com/GAIR-Lab/IISAN.\n","authors":["Junchen Fu","Xuri Ge","Xin Xin","Alexandros Karatzoglou","Ioannis Arapakis","Jie Wang","Joemon M. Jose"],"pdf_url":"https://arxiv.org/pdf/2404.02059v2.pdf","comment":"Accepted by SIGIR2024"},{"id":"http://arxiv.org/abs/2404.08031v1","updated":"2024-04-11T17:59:52Z","published":"2024-04-11T17:59:52Z","title":"Latent Guard: a Safety Framework for Text-to-image Generation","summary":"  With the ability to generate high-quality images, text-to-image (T2I) models\ncan be exploited for creating inappropriate content. To prevent misuse,\nexisting safety measures are either based on text blacklists, which can be\neasily circumvented, or harmful content classification, requiring large\ndatasets for training and offering low flexibility. Hence, we propose Latent\nGuard, a framework designed to improve safety measures in text-to-image\ngeneration. Inspired by blacklist-based approaches, Latent Guard learns a\nlatent space on top of the T2I model's text encoder, where it is possible to\ncheck the presence of harmful concepts in the input text embeddings. Our\nproposed framework is composed of a data generation pipeline specific to the\ntask using large language models, ad-hoc architectural components, and a\ncontrastive learning strategy to benefit from the generated data. The\neffectiveness of our method is verified on three datasets and against four\nbaselines. Code and data will be shared at\nhttps://github.com/rt219/LatentGuard.\n","authors":["Runtao Liu","Ashkan Khakzar","Jindong Gu","Qifeng Chen","Philip Torr","Fabio Pizzati"],"pdf_url":"https://arxiv.org/pdf/2404.08031v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2404.08030v1","updated":"2024-04-11T17:59:43Z","published":"2024-04-11T17:59:43Z","title":"Rethinking Artistic Copyright Infringements in the Era of Text-to-Image\n  Generative Models","summary":"  Recent text-to-image generative models such as Stable Diffusion are extremely\nadept at mimicking and generating copyrighted content, raising concerns amongst\nartists that their unique styles may be improperly copied. Understanding how\ngenerative models copy \"artistic style\" is more complex than duplicating a\nsingle image, as style is comprised by a set of elements (or signature) that\nfrequently co-occurs across a body of work, where each individual work may vary\nsignificantly. In our paper, we first reformulate the problem of \"artistic\ncopyright infringement\" to a classification problem over image sets, instead of\nprobing image-wise similarities. We then introduce ArtSavant, a practical\n(i.e., efficient and easy to understand) tool to (i) determine the unique style\nof an artist by comparing it to a reference dataset of works from 372 artists\ncurated from WikiArt, and (ii) recognize if the identified style reappears in\ngenerated images. We leverage two complementary methods to perform artistic\nstyle classification over image sets, includingTagMatch, which is a novel\ninherently interpretable and attributable method, making it more suitable for\nbroader use by non-technical stake holders (artists, lawyers, judges, etc).\nLeveraging ArtSavant, we then perform a large-scale empirical study to provide\nquantitative insight on the prevalence of artistic style copying across 3\npopular text-to-image generative models. Namely, amongst a dataset of prolific\nartists (including many famous ones), only 20% of them appear to have their\nstyles be at a risk of copying via simple prompting of today's popular\ntext-to-image generative models.\n","authors":["Mazda Moayeri","Samyadeep Basu","Sriram Balasubramanian","Priyatham Kattakinda","Atoosa Chengini","Robert Brauneis","Soheil Feizi"],"pdf_url":"https://arxiv.org/pdf/2404.08030v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2404.07981v1","updated":"2024-04-11T17:57:32Z","published":"2024-04-11T17:57:32Z","title":"Manipulating Large Language Models to Increase Product Visibility","summary":"  Large language models (LLMs) are increasingly being integrated into search\nengines to provide natural language responses tailored to user queries.\nCustomers and end-users are also becoming more dependent on these models for\nquick and easy purchase decisions. In this work, we investigate whether\nrecommendations from LLMs can be manipulated to enhance a product's visibility.\nWe demonstrate that adding a strategic text sequence (STS) -- a carefully\ncrafted message -- to a product's information page can significantly increase\nits likelihood of being listed as the LLM's top recommendation. To understand\nthe impact of STS, we use a catalog of fictitious coffee machines and analyze\nits effect on two target products: one that seldom appears in the LLM's\nrecommendations and another that usually ranks second. We observe that the\nstrategic text sequence significantly enhances the visibility of both products\nby increasing their chances of appearing as the top recommendation. This\nability to manipulate LLM-generated search responses provides vendors with a\nconsiderable competitive advantage and has the potential to disrupt fair market\ncompetition. Just as search engine optimization (SEO) revolutionized how\nwebpages are customized to rank higher in search engine results, influencing\nLLM recommendations could profoundly impact content optimization for AI-driven\nsearch services. Code for our experiments is available at\nhttps://github.com/aounon/llm-rank-optimizer.\n","authors":["Aounon Kumar","Himabindu Lakkaraju"],"pdf_url":"https://arxiv.org/pdf/2404.07981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07896v1","updated":"2024-04-11T16:31:35Z","published":"2024-04-11T16:31:35Z","title":"Auditing health-related recommendations in social media: A Case Study of\n  Abortion on YouTube","summary":"  Recommendation algorithms (RS) used by social media, like YouTube,\nsignificantly shape our information consumption across various domains,\nespecially in healthcare. Hence, algorithmic auditing becomes crucial to\nuncover their potential bias and misinformation, particularly in the context of\ncontroversial topics like abortion. We introduce a simple yet effective sock\npuppet auditing approach to investigate how YouTube recommends abortion-related\nvideos to individuals with different backgrounds. Our framework allows for\nefficient auditing of RS, regardless of the complexity of the underlying\nalgorithms\n","authors":["Mohammed Lahsaini","Mohamed Lechiakh","Alexandre Maurer"],"pdf_url":"https://arxiv.org/pdf/2404.07896v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06912v2","updated":"2024-04-11T13:00:18Z","published":"2024-04-10T11:04:24Z","title":"Set-Encoder: Permutation-Invariant Inter-Passage Attention for Listwise\n  Passage Re-Ranking with Cross-Encoders","summary":"  Cross-encoders are effective passage re-rankers. But when re-ranking multiple\npassages at once, existing cross-encoders inefficiently optimize the output\nranking over several input permutations, as their passage interactions are not\npermutation-invariant. Moreover, their high memory footprint constrains the\nnumber of passages during listwise training. To tackle these issues, we propose\nthe Set-Encoder, a new cross-encoder architecture that (1) introduces\ninter-passage attention with parallel passage processing to ensure permutation\ninvariance between input passages, and that (2) uses fused-attention kernels to\nenable training with more passages at a time. In experiments on TREC Deep\nLearning and TIREx, the Set-Encoder is more effective than previous\ncross-encoders with a similar number of parameters. Compared to larger models,\nthe Set-Encoder is more efficient and either on par or even more effective.\n","authors":["Ferdinand Schlatt","Maik Fr√∂be","Harrisen Scells","Shengyao Zhuang","Bevan Koopman","Guido Zuccon","Benno Stein","Martin Potthast","Matthias Hagen"],"pdf_url":"https://arxiv.org/pdf/2404.06912v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07581v1","updated":"2024-04-11T09:13:52Z","published":"2024-04-11T09:13:52Z","title":"M-scan: A Multi-Scenario Causal-driven Adaptive Network for\n  Recommendation","summary":"  We primarily focus on the field of multi-scenario recommendation, which poses\na significant challenge in effectively leveraging data from different scenarios\nto enhance predictions in scenarios with limited data. Current mainstream\nefforts mainly center around innovative model network architectures, with the\naim of enabling the network to implicitly acquire knowledge from diverse\nscenarios. However, the uncertainty of implicit learning in networks arises\nfrom the absence of explicit modeling, leading to not only difficulty in\ntraining but also incomplete user representation and suboptimal performance.\nFurthermore, through causal graph analysis, we have discovered that the\nscenario itself directly influences click behavior, yet existing approaches\ndirectly incorporate data from other scenarios during the training of the\ncurrent scenario, leading to prediction biases when they directly utilize click\nbehaviors from other scenarios to train models. To address these problems, we\npropose the Multi-Scenario Causal-driven Adaptive Network M-scan). This model\nincorporates a Scenario-Aware Co-Attention mechanism that explicitly extracts\nuser interests from other scenarios that align with the current scenario.\nAdditionally, it employs a Scenario Bias Eliminator module utilizing causal\ncounterfactual inference to mitigate biases introduced by data from other\nscenarios. Extensive experiments on two public datasets demonstrate the\nefficacy of our M-scan compared to the existing baseline models.\n","authors":["Jiachen Zhu","Yichao Wang","Jianghao Lin","Jiarui Qin","Ruiming Tang","Weinan Zhang","Yong Yu"],"pdf_url":"https://arxiv.org/pdf/2404.07581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07499v1","updated":"2024-04-11T06:22:56Z","published":"2024-04-11T06:22:56Z","title":"Can Large Language Models Assess Serendipity in Recommender Systems?","summary":"  Serendipity-oriented recommender systems aim to counteract\nover-specialization in user preferences. However, evaluating a user's\nserendipitous response towards a recommended item can be challenging because of\nits emotional nature. In this study, we address this issue by leveraging the\nrich knowledge of large language models (LLMs), which can perform a variety of\ntasks. First, this study explored the alignment between serendipitous\nevaluations made by LLMs and those made by humans. In this investigation, a\nbinary classification task was given to the LLMs to predict whether a user\nwould find the recommended item serendipitously. The predictive performances of\nthree LLMs on a benchmark dataset in which humans assigned the ground truth of\nserendipitous items were measured. The experimental findings reveal that\nLLM-based assessment methods did not have a very high agreement rate with human\nassessments. However, they performed as well as or better than the baseline\nmethods. Further validation results indicate that the number of user rating\nhistories provided to LLM prompts should be carefully chosen to avoid both\ninsufficient and excessive inputs and that the output of LLMs that show high\nclassification performance is difficult to interpret.\n","authors":["Yu Tokutake","Kazushi Okamoto"],"pdf_url":"https://arxiv.org/pdf/2404.07499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07494v1","updated":"2024-04-11T06:05:40Z","published":"2024-04-11T06:05:40Z","title":"Adaptive Fair Representation Learning for Personalized Fairness in\n  Recommendations via Information Alignment","summary":"  Personalized fairness in recommendations has been attracting increasing\nattention from researchers. The existing works often treat a fairness\nrequirement, represented as a collection of sensitive attributes, as a\nhyper-parameter, and pursue extreme fairness by completely removing information\nof sensitive attributes from the learned fair embedding, which suffer from two\nchallenges: huge training cost incurred by the explosion of attribute\ncombinations, and the suboptimal trade-off between fairness and accuracy. In\nthis paper, we propose a novel Adaptive Fair Representation Learning (AFRL)\nmodel, which achieves a real personalized fairness due to its advantage of\ntraining only one model to adaptively serve different fairness requirements\nduring inference phase. Particularly, AFRL treats fairness requirements as\ninputs and can learn an attribute-specific embedding for each attribute from\nthe unfair user embedding, which endows AFRL with the adaptability during\ninference phase to determine the non-sensitive attributes under the guidance of\nthe user's unique fairness requirement. To achieve a better trade-off between\nfairness and accuracy in recommendations, AFRL conducts a novel Information\nAlignment to exactly preserve discriminative information of non-sensitive\nattributes and incorporate a debiased collaborative embedding into the fair\nembedding to capture attribute-independent collaborative signals, without loss\nof fairness. Finally, the extensive experiments conducted on real datasets\ntogether with the sound theoretical analysis demonstrate the superiority of\nAFRL.\n","authors":["XInyu Zhu","Lilin Zhang","Ning Yang"],"pdf_url":"https://arxiv.org/pdf/2404.07494v1.pdf","comment":"This paper has been accepted by SIGIR'24"},{"id":"http://arxiv.org/abs/2306.11134v2","updated":"2024-04-11T02:42:07Z","published":"2023-06-19T19:38:29Z","title":"OpenP5: An Open-Source Platform for Developing, Training, and Evaluating\n  LLM-based Recommender Systems","summary":"  In recent years, the integration of Large Language Models (LLMs) into\nrecommender systems has garnered interest among both practitioners and\nresearchers. Despite this interest, the field is still emerging, and the lack\nof open-source R&D platforms may impede the exploration of LLM-based\nrecommendations. This paper introduces OpenP5, an open-source platform designed\nas a resource to facilitate the development, training, and evaluation of\nLLM-based generative recommender systems for research purposes. The platform is\nimplemented using encoder-decoder LLMs (e.g., T5) and decoder-only LLMs (e.g.,\nLlama-2) across 10 widely recognized public datasets, catering to two\nfundamental recommendation tasks: sequential and straightforward\nrecommendations. Recognizing the crucial role of item IDs in LLM-based\nrecommendations, we have also incorporated three item indexing methods within\nthe OpenP5 platform: random indexing, sequential indexing and collaborative\nindexing. Built on the Transformers library, the platform facilitates easy\ncustomization of LLM-based recommendations for users. OpenP5 boasts a range of\nfeatures including extensible data processing, task-centric optimization,\ncomprehensive datasets and checkpoints, efficient acceleration, and\nstandardized evaluations, making it a valuable tool for the implementation and\nevaluation of LLM-based recommender systems. The open-source code and\npre-trained checkpoints for the OpenP5 library are publicly available at\nhttps://github.com/agiresearch/OpenP5.\n","authors":["Shuyuan Xu","Wenyue Hua","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2306.11134v2.pdf","comment":"In SIGIR 2024 Resource & Reproducibility Track"},{"id":"http://arxiv.org/abs/2404.07108v2","updated":"2024-04-11T02:36:27Z","published":"2024-04-10T15:46:08Z","title":"From Model-centered to Human-Centered: Revision Distance as a Metric for\n  Text Evaluation in LLMs-based Applications","summary":"  Evaluating large language models (LLMs) is fundamental, particularly in the\ncontext of practical applications. Conventional evaluation methods, typically\ndesigned primarily for LLM development, yield numerical scores that ignore the\nuser experience. Therefore, our study shifts the focus from model-centered to\nhuman-centered evaluation in the context of AI-powered writing assistance\napplications. Our proposed metric, termed ``Revision Distance,'' utilizes LLMs\nto suggest revision edits that mimic the human writing process. It is\ndetermined by counting the revision edits generated by LLMs. Benefiting from\nthe generated revision edit details, our metric can provide a self-explained\ntext evaluation result in a human-understandable manner beyond the\ncontext-independent score. Our results show that for the easy-writing task,\n``Revision Distance'' is consistent with established metrics (ROUGE,\nBert-score, and GPT-score), but offers more insightful, detailed feedback and\nbetter distinguishes between texts. Moreover, in the context of challenging\nacademic writing tasks, our metric still delivers reliable evaluations where\nother metrics tend to struggle. Furthermore, our metric also holds significant\npotential for scenarios lacking reference texts.\n","authors":["Yongqiang Ma","Lizhi Qing","Jiawei Liu","Yangyang Kang","Yue Zhang","Wei Lu","Xiaozhong Liu","Qikai Cheng"],"pdf_url":"https://arxiv.org/pdf/2404.07108v2.pdf","comment":"9 pages, 2 figures, under review"},{"id":"http://arxiv.org/abs/2404.01574v2","updated":"2024-04-11T02:00:12Z","published":"2024-04-02T02:08:29Z","title":"Multi-granular Adversarial Attacks against Black-box Neural Ranking\n  Models","summary":"  Adversarial ranking attacks have gained increasing attention due to their\nsuccess in probing vulnerabilities, and, hence, enhancing the robustness, of\nneural ranking models. Conventional attack methods employ perturbations at a\nsingle granularity, e.g., word or sentence level, to target documents. However,\nlimiting perturbations to a single level of granularity may reduce the\nflexibility of adversarial examples, thereby diminishing the potential threat\nof the attack. Therefore, we focus on generating high-quality adversarial\nexamples by incorporating multi-granular perturbations. Achieving this\nobjective involves tackling a combinatorial explosion problem, which requires\nidentifying an optimal combination of perturbations across all possible levels\nof granularity, positions, and textual pieces. To address this challenge, we\ntransform the multi-granular adversarial attack into a sequential\ndecision-making process, where perturbations in the next attack step build on\nthe perturbed document in the current attack step. Since the attack process can\nonly access the final state without direct intermediate signals, we use\nreinforcement learning to perform multi-granular attacks. During the\nreinforcement learning process, two agents work cooperatively to identify\nmulti-granular vulnerabilities as attack targets and organize perturbation\ncandidates into a final perturbation sequence. Experimental results show that\nour attack method surpasses prevailing baselines in both attack effectiveness\nand imperceptibility.\n","authors":["Yu-An Liu","Ruqing Zhang","Jiafeng Guo","Maarten de Rijke","Yixing Fan","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2404.01574v2.pdf","comment":"Accepted by SIGIR2024"},{"id":"http://arxiv.org/abs/2404.08137v1","updated":"2024-04-11T21:48:54Z","published":"2024-04-11T21:48:54Z","title":"Generative Information Retrieval Evaluation","summary":"  In this chapter, we consider generative information retrieval evaluation from\ntwo distinct but interrelated perspectives. First, large language models (LLMs)\nthemselves are rapidly becoming tools for evaluation, with current research\nindicating that LLMs may be superior to crowdsource workers and other paid\nassessors on basic relevance judgement tasks. We review past and ongoing\nrelated research, including speculation on the future of shared task\ninitiatives, such as TREC, and a discussion on the continuing need for human\nassessments. Second, we consider the evaluation of emerging LLM-based\ngenerative information retrieval (GenIR) systems, including retrieval augmented\ngeneration (RAG) systems. We consider approaches that focus both on the\nend-to-end evaluation of GenIR systems and on the evaluation of a retrieval\ncomponent as an element in a RAG system. Going forward, we expect the\nevaluation of GenIR systems to be at least partially based on LLM-based\nassessment, creating an apparent circularity, with a system seemingly\nevaluating its own output. We resolve this apparent circularity in two ways: 1)\nby viewing LLM-based assessment as a form of \"slow search\", where a slower IR\nsystem is used for evaluation and training of a faster production IR system;\nand 2) by recognizing a continuing need to ground evaluation in human\nassessment, even if the characteristics of that human assessment must change.\n","authors":["Marwah Alaofi","Negar Arabzadeh","Charles L. A. Clarke","Mark Sanderson"],"pdf_url":"https://arxiv.org/pdf/2404.08137v1.pdf","comment":"Draft of a chapter intended to appear in a forthcoming book on\n  generative information retrieval, co-edited by Chirag Shah and Ryen White"},{"id":"http://arxiv.org/abs/2404.08134v1","updated":"2024-04-11T21:31:02Z","published":"2024-04-11T21:31:02Z","title":"Extending Translate-Train for ColBERT-X to African Language CLIR","summary":"  This paper describes the submission runs from the HLTCOE team at the CIRAL\nCLIR tasks for African languages at FIRE 2023. Our submissions use machine\ntranslation models to translate the documents and the training passages, and\nColBERT-X as the retrieval model. Additionally, we present a set of unofficial\nruns that use an alternative training procedure with a similar training\nsetting.\n","authors":["Eugene Yang","Dawn J. Lawrie","Paul McNamee","James Mayfield"],"pdf_url":"https://arxiv.org/pdf/2404.08134v1.pdf","comment":"10 pages, 2 figures. System description paper for HLTCOE's\n  participation in CIRAL@FIRE 2023"},{"id":"http://arxiv.org/abs/2404.08118v1","updated":"2024-04-11T20:46:18Z","published":"2024-04-11T20:46:18Z","title":"HLTCOE at TREC 2023 NeuCLIR Track","summary":"  The HLTCOE team applied PLAID, an mT5 reranker, and document translation to\nthe TREC 2023 NeuCLIR track. For PLAID we included a variety of models and\ntraining techniques -- the English model released with ColBERT v2,\ntranslate-train~(TT), Translate Distill~(TD) and multilingual\ntranslate-train~(MTT). TT trains a ColBERT model with English queries and\npassages automatically translated into the document language from the MS-MARCO\nv1 collection. This results in three cross-language models for the track, one\nper language. MTT creates a single model for all three document languages by\ncombining the translations of MS-MARCO passages in all three languages into\nmixed-language batches. Thus the model learns about matching queries to\npassages simultaneously in all languages. Distillation uses scores from the mT5\nmodel over non-English translated document pairs to learn how to score\nquery-document pairs. The team submitted runs to all NeuCLIR tasks: the CLIR\nand MLIR news task as well as the technical documents task.\n","authors":["Eugene Yang","Dawn Lawrie","James Mayfield"],"pdf_url":"https://arxiv.org/pdf/2404.08118v1.pdf","comment":"6 pages. Part of TREC 2023 Proceedings"},{"id":"http://arxiv.org/abs/2404.02059v2","updated":"2024-04-11T18:29:01Z","published":"2024-04-02T15:58:36Z","title":"IISAN: Efficiently Adapting Multimodal Representation for Sequential\n  Recommendation with Decoupled PEFT","summary":"  Multimodal foundation models are transformative in sequential recommender\nsystems, leveraging powerful representation learning capabilities. While\nParameter-efficient Fine-tuning (PEFT) is commonly used to adapt foundation\nmodels for recommendation tasks, most research prioritizes parameter\nefficiency, often overlooking critical factors like GPU memory efficiency and\ntraining speed. Addressing this gap, our paper introduces IISAN (Intra- and\nInter-modal Side Adapted Network for Multimodal Representation), a simple\nplug-and-play architecture using a Decoupled PEFT structure and exploiting both\nintra- and inter-modal adaptation.\n  IISAN matches the performance of full fine-tuning (FFT) and state-of-the-art\nPEFT. More importantly, it significantly reduces GPU memory usage - from 47GB\nto just 3GB for multimodal sequential recommendation tasks. Additionally, it\naccelerates training time per epoch from 443s to 22s compared to FFT. This is\nalso a notable improvement over the Adapter and LoRA, which require 37-39 GB\nGPU memory and 350-380 seconds per epoch for training.\n  Furthermore, we propose a new composite efficiency metric, TPME\n(Training-time, Parameter, and GPU Memory Efficiency) to alleviate the\nprevalent misconception that \"parameter efficiency represents overall\nefficiency\". TPME provides more comprehensive insights into practical\nefficiency comparisons between different methods. Besides, we give an\naccessible efficiency analysis of all PEFT and FFT approaches, which\ndemonstrate the superiority of IISAN. We release our codes and other materials\nat https://github.com/GAIR-Lab/IISAN.\n","authors":["Junchen Fu","Xuri Ge","Xin Xin","Alexandros Karatzoglou","Ioannis Arapakis","Jie Wang","Joemon M. Jose"],"pdf_url":"https://arxiv.org/pdf/2404.02059v2.pdf","comment":"Accepted by SIGIR2024"},{"id":"http://arxiv.org/abs/2404.08071v1","updated":"2024-04-11T18:22:04Z","published":"2024-04-11T18:22:04Z","title":"Overview of the TREC 2023 NeuCLIR Track","summary":"  The principal goal of the TREC Neural Cross-Language Information Retrieval\n(NeuCLIR) track is to study the impact of neural approaches to cross-language\ninformation retrieval. The track has created four collections, large\ncollections of Chinese, Persian, and Russian newswire and a smaller collection\nof Chinese scientific abstracts. The principal tasks are ranked retrieval of\nnews in one of the three languages, using English topics. Results for a\nmultilingual task, also with English topics but with documents from all three\nnewswire collections, are also reported. New in this second year of the track\nis a pilot technical documents CLIR task for ranked retrieval of Chinese\ntechnical documents using English topics. A total of 220 runs across all tasks\nwere submitted by six participating teams and, as baselines, by track\ncoordinators. Task descriptions and results are presented.\n","authors":["Dawn Lawrie","Sean MacAvaney","James Mayfield","Paul McNamee","Douglas W. Oard","Luca Soldaini","Eugene Yang"],"pdf_url":"https://arxiv.org/pdf/2404.08071v1.pdf","comment":"27 pages, 17 figures. Part of the TREC 2023 Proceedings"},{"id":"http://arxiv.org/abs/2404.08020v1","updated":"2024-04-11T05:53:38Z","published":"2024-04-11T05:53:38Z","title":"Augmenting Knowledge Graph Hierarchies Using Neural Transformers","summary":"  Knowledge graphs are useful tools to organize, recommend and sort data.\nHierarchies in knowledge graphs provide significant benefit in improving\nunderstanding and compartmentalization of the data within a knowledge graph.\nThis work leverages large language models to generate and augment hierarchies\nin an existing knowledge graph. For small (<100,000 node) domain-specific KGs,\nwe find that a combination of few-shot prompting with one-shot generation works\nwell, while larger KG may require cyclical generation. We present techniques\nfor augmenting hierarchies, which led to coverage increase by 98% for intents\nand 99% for colors in our knowledge graph.\n","authors":["Sanat Sharma","Mayank Poddar","Jayant Kumar","Kosta Blank","Tracy King"],"pdf_url":"https://arxiv.org/pdf/2404.08020v1.pdf","comment":"European Conference on Information Retrieval 2024"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2404.07989v1","updated":"2024-04-11T17:59:45Z","published":"2024-04-11T17:59:45Z","title":"Any2Point: Empowering Any-modality Large Models for Efficient 3D\n  Understanding","summary":"  Large foundation models have recently emerged as a prominent focus of\ninterest, attaining superior performance in widespread scenarios. Due to the\nscarcity of 3D data, many efforts have been made to adapt pre-trained\ntransformers from vision to 3D domains. However, such 2D-to-3D approaches are\nstill limited, due to the potential loss of spatial geometries and high\ncomputation cost. More importantly, their frameworks are mainly designed for 2D\nmodels, lacking a general any-to-3D paradigm. In this paper, we introduce\nAny2Point, a parameter-efficient method to empower any-modality large models\n(vision, language, audio) for 3D understanding. Given a frozen transformer from\nany source modality, we propose a 3D-to-any (1D or 2D) virtual projection\nstrategy that correlates the input 3D points to the original 1D or 2D positions\nwithin the source modality. This mechanism enables us to assign each 3D token\nwith a positional encoding paired with the pre-trained model, which avoids 3D\ngeometry loss caused by the true projection and better motivates the\ntransformer for 3D learning with 1D/2D positional priors. Then, within each\ntransformer block, we insert an any-to-3D guided adapter module for\nparameter-efficient fine-tuning. The adapter incorporates prior spatial\nknowledge from the source modality to guide the local feature aggregation of 3D\ntokens, compelling the semantic adaption of any-modality transformers. We\nconduct extensive experiments to showcase the effectiveness and efficiency of\nour method. Code and models are released at\nhttps://github.com/Ivan-Tang-3D/Any2Point.\n","authors":["Yiwen Tang","Jiaming Liu","Dong Wang","Zhigang Wang","Shanghang Zhang","Bin Zhao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2404.07989v1.pdf","comment":"Code and models are released at\n  https://github.com/Ivan-Tang-3D/Any2Point"},{"id":"http://arxiv.org/abs/2404.07987v1","updated":"2024-04-11T17:59:09Z","published":"2024-04-11T17:59:09Z","title":"ControlNet++: Improving Conditional Controls with Efficient Consistency\n  Feedback","summary":"  To enhance the controllability of text-to-image diffusion models, existing\nefforts like ControlNet incorporated image-based conditional controls. In this\npaper, we reveal that existing methods still face significant challenges in\ngenerating images that align with the image conditional controls. To this end,\nwe propose ControlNet++, a novel approach that improves controllable generation\nby explicitly optimizing pixel-level cycle consistency between generated images\nand conditional controls. Specifically, for an input conditional control, we\nuse a pre-trained discriminative reward model to extract the corresponding\ncondition of the generated images, and then optimize the consistency loss\nbetween the input conditional control and extracted condition. A\nstraightforward implementation would be generating images from random noises\nand then calculating the consistency loss, but such an approach requires\nstoring gradients for multiple sampling timesteps, leading to considerable time\nand memory costs. To address this, we introduce an efficient reward strategy\nthat deliberately disturbs the input images by adding noise, and then uses the\nsingle-step denoised images for reward fine-tuning. This avoids the extensive\ncosts associated with image sampling, allowing for more efficient reward\nfine-tuning. Extensive experiments show that ControlNet++ significantly\nimproves controllability under various conditional controls. For example, it\nachieves improvements over ControlNet by 7.9% mIoU, 13.4% SSIM, and 7.6% RMSE,\nrespectively, for segmentation mask, line-art edge, and depth conditions.\n","authors":["Ming Li","Taojiannan Yang","Huafeng Kuang","Jie Wu","Zhaoning Wang","Xuefeng Xiao","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2404.07987v1.pdf","comment":"Project Page: https://liming-ai.github.io/ControlNet_Plus_Plus"},{"id":"http://arxiv.org/abs/2404.07181v2","updated":"2024-04-11T17:58:47Z","published":"2024-04-10T17:31:49Z","title":"BAMBOO: a predictive and transferable machine learning force field\n  framework for liquid electrolyte development","summary":"  Despite the widespread applications of machine learning force field (MLFF) on\nsolids and small molecules, there is a notable gap in applying MLFF to complex\nliquid electrolytes. In this work, we introduce BAMBOO (ByteDance AI Molecular\nSimulation Booster), a novel framework for molecular dynamics (MD) simulations,\nwith a demonstration of its capabilities in the context of liquid electrolytes\nfor lithium batteries. We design a physics-inspired graph equivariant\ntransformer architecture as the backbone of BAMBOO to learn from quantum\nmechanical simulations. Additionally, we pioneer an ensemble knowledge\ndistillation approach and apply it on MLFFs to improve the stability of MD\nsimulations. Finally, we propose the density alignment algorithm to align\nBAMBOO with experimental measurements. BAMBOO demonstrates state-of-the-art\naccuracy in predicting key electrolyte properties such as density, viscosity,\nand ionic conductivity across various solvents and salt combinations. Our\ncurrent model, trained on more than 15 chemical species, achieves the average\ndensity error of 0.01 g/cm$^3$ on various compositions compared with\nexperimental data. Moreover, our model demonstrates transferability to\nmolecules not included in the quantum mechanical dataset. We envision this work\nas paving the way to a \"universal MLFF\" capable of simulating properties of\ncommon organic liquids.\n","authors":["Sheng Gong","Yumin Zhang","Zhenliang Mu","Zhichen Pu","Hongyi Wang","Zhiao Yu","Mengyi Chen","Tianze Zheng","Zhi Wang","Lifei Chen","Xiaojie Wu","Shaochen Shi","Weihao Gao","Wen Yan","Liang Xiang"],"pdf_url":"https://arxiv.org/pdf/2404.07181v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07983v1","updated":"2024-04-11T17:58:06Z","published":"2024-04-11T17:58:06Z","title":"Two Effects, One Trigger: On the Modality Gap, Object Bias, and\n  Information Imbalance in Contrastive Vision-Language Representation Learning","summary":"  Contrastive vision-language models like CLIP have gained popularity for their\nversatile applicable learned representations in various downstream tasks.\nDespite their successes in some tasks, like zero-shot image recognition, they\nalso perform surprisingly poor on other tasks, like attribute detection.\nPrevious work has attributed these challenges to the modality gap, a separation\nof image and text in the shared representation space, and a bias towards\nobjects over other factors, such as attributes. In this work we investigate\nboth phenomena. We find that only a few embedding dimensions drive the modality\ngap. Further, we propose a measure for object bias and find that object bias\ndoes not lead to worse performance on other concepts, such as attributes. But\nwhat leads to the emergence of the modality gap and object bias? To answer this\nquestion we carefully designed an experimental setting which allows us to\ncontrol the amount of shared information between the modalities. This revealed\nthat the driving factor behind both, the modality gap and the object bias, is\nthe information imbalance between images and captions.\n","authors":["Simon Schrodi","David T. Hoffmann","Max Argus","Volker Fischer","Thomas Brox"],"pdf_url":"https://arxiv.org/pdf/2404.07983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07982v1","updated":"2024-04-11T17:58:05Z","published":"2024-04-11T17:58:05Z","title":"Language Imbalance Can Boost Cross-lingual Generalisation","summary":"  Multilinguality is crucial for extending recent advancements in language\nmodelling to diverse linguistic communities. To maintain high performance while\nrepresenting multiple languages, multilingual models ideally align\nrepresentations, allowing what is learned in one language to generalise to\nothers. Prior research has emphasised the importance of parallel data and\nshared vocabulary elements as key factors for such alignment. In this study, we\ninvestigate an unintuitive novel driver of cross-lingual generalisation:\nlanguage imbalance. In controlled experiments on perfectly equivalent cloned\nlanguages, we observe that the existence of a predominant language during\ntraining boosts the performance of less frequent languages and leads to\nstronger alignment of model representations across languages. Furthermore, we\nfind that this trend is amplified with scale: with large enough models or long\nenough training, we observe that bilingual training data with a 90/10 language\nsplit yields better performance on both languages than a balanced 50/50 split.\nBuilding on these insights, we design training schemes that can improve\nperformance in all cloned languages, even without altering the training data.\nAs we extend our analysis to real languages, we find that infrequent languages\nstill benefit from frequent ones, yet whether language imbalance causes\ncross-lingual generalisation there is not conclusive.\n","authors":["Anton Sch√§fer","Shauli Ravfogel","Thomas Hofmann","Tiago Pimentel","Imanol Schlag"],"pdf_url":"https://arxiv.org/pdf/2404.07982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07979v1","updated":"2024-04-11T17:57:22Z","published":"2024-04-11T17:57:22Z","title":"LLoCO: Learning Long Contexts Offline","summary":"  Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose a\nnovel approach to address this problem by learning contexts offline through\ncontext compression and in-domain parameter-efficient finetuning. Our method\nenables an LLM to create a concise representation of the original context and\nefficiently retrieve relevant information to answer questions accurately. We\nintroduce LLoCO, a technique that combines context compression, retrieval, and\nparameter-efficient finetuning using LoRA. Our approach extends the effective\ncontext window of a 4k token LLaMA2-7B model to handle up to 128k tokens. We\nevaluate our approach on several long-context question-answering datasets,\ndemonstrating that LLoCO significantly outperforms in-context learning while\nusing $30\\times$ fewer tokens during inference. LLoCO achieves up to\n$7.62\\times$ speed-up and substantially reduces the cost of long document\nquestion answering, making it a promising solution for efficient long context\nprocessing. Our code is publicly available at\nhttps://github.com/jeffreysijuntan/lloco.\n","authors":["Sijun Tan","Xiuyu Li","Shishir Patil","Ziyang Wu","Tianjun Zhang","Kurt Keutzer","Joseph E. Gonzalez","Raluca Ada Popa"],"pdf_url":"https://arxiv.org/pdf/2404.07979v1.pdf","comment":"The first two authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2404.07970v1","updated":"2024-04-11T17:55:05Z","published":"2024-04-11T17:55:05Z","title":"Differentiable All-pole Filters for Time-varying Audio Systems","summary":"  Infinite impulse response filters are an essential building block of many\ntime-varying audio systems, such as audio effects and synthesisers. However,\ntheir recursive structure impedes end-to-end training of these systems using\nautomatic differentiation. Although non-recursive filter approximations like\nfrequency sampling and frame-based processing have been proposed and widely\nused in previous works, they cannot accurately reflect the gradient of the\noriginal system. We alleviate this difficulty by re-expressing a time-varying\nall-pole filter to backpropagate the gradients through itself, so the filter\nimplementation is not bound to the technical limitations of automatic\ndifferentiation frameworks. This implementation can be employed within any\naudio system containing filters with poles for efficient gradient evaluation.\nWe demonstrate its training efficiency and expressive capabilities for\nmodelling real-world dynamic audio systems on a phaser, time-varying\nsubtractive synthesiser, and feed-forward compressor. We make our code\navailable and provide the trained audio effect and synth models in a VST plugin\nat https://christhetree.github.io/all_pole_filters/.\n","authors":["Chin-Yun Yu","Christopher Mitcheltree","Alistair Carson","Stefan Bilbao","Joshua D. Reiss","Gy√∂rgy Fazekas"],"pdf_url":"https://arxiv.org/pdf/2404.07970v1.pdf","comment":"Submitted to DAFx 2024"},{"id":"http://arxiv.org/abs/2404.06737v2","updated":"2024-04-11T17:54:13Z","published":"2024-04-10T04:55:57Z","title":"Disguised Copyright Infringement of Latent Diffusion Models","summary":"  Copyright infringement may occur when a generative model produces samples\nsubstantially similar to some copyrighted data that it had access to during the\ntraining phase. The notion of access usually refers to including copyrighted\nsamples directly in the training dataset, which one may inspect to identify an\ninfringement. We argue that such visual auditing largely overlooks a concealed\ncopyright infringement, where one constructs a disguise that looks drastically\ndifferent from the copyrighted sample yet still induces the effect of training\nLatent Diffusion Models on it. Such disguises only require indirect access to\nthe copyrighted material and cannot be visually distinguished, thus easily\ncircumventing the current auditing tools. In this paper, we provide a better\nunderstanding of such disguised copyright infringement by uncovering the\ndisguises generation algorithm, the revelation of the disguises, and\nimportantly, how to detect them to augment the existing toolbox. Additionally,\nwe introduce a broader notion of acknowledgment for comprehending such indirect\naccess.\n","authors":["Yiwei Lu","Matthew Y. R. Yang","Zuoqiu Liu","Gautam Kamath","Yaoliang Yu"],"pdf_url":"https://arxiv.org/pdf/2404.06737v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07956v1","updated":"2024-04-11T17:49:15Z","published":"2024-04-11T17:49:15Z","title":"Lyapunov-stable Neural Control for State and Output Feedback: A Novel\n  Formulation for Efficient Synthesis and Verification","summary":"  Learning-based neural network (NN) control policies have shown impressive\nempirical performance in a wide range of tasks in robotics and control.\nHowever, formal (Lyapunov) stability guarantees over the region-of-attraction\n(ROA) for NN controllers with nonlinear dynamical systems are challenging to\nobtain, and most existing approaches rely on expensive solvers such as\nsums-of-squares (SOS), mixed-integer programming (MIP), or satisfiability\nmodulo theories (SMT). In this paper, we demonstrate a new framework for\nlearning NN controllers together with Lyapunov certificates using fast\nempirical falsification and strategic regularizations. We propose a novel\nformulation that defines a larger verifiable region-of-attraction (ROA) than\nshown in the literature, and refines the conventional restrictive constraints\non Lyapunov derivatives to focus only on certifiable ROAs. The Lyapunov\ncondition is rigorously verified post-hoc using branch-and-bound with scalable\nlinear bound propagation-based NN verification techniques. The approach is\nefficient and flexible, and the full training and verification procedure is\naccelerated on GPUs without relying on expensive solvers for SOS, MIP, nor SMT.\nThe flexibility and efficiency of our framework allow us to demonstrate\nLyapunov-stable output feedback control with synthesized NN-based controllers\nand NN-based observers with formal stability guarantees, for the first time in\nliterature. Source code at\nhttps://github.com/Verified-Intelligence/Lyapunov_Stable_NN_Controllers.\n","authors":["Lujie Yang","Hongkai Dai","Zhouxing Shi","Cho-Jui Hsieh","Russ Tedrake","Huan Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.07956v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.13356v4","updated":"2024-04-11T17:46:31Z","published":"2023-02-26T17:22:40Z","title":"Performance is not enough: the story told by a Rashomon quartet","summary":"  The usual goal of supervised learning is to find the best model, the one that\noptimizes a particular performance measure. However, what if the explanation\nprovided by this model is completely different from another model and different\nagain from another model despite all having similarly good fit statistics? Is\nit possible that the equally effective models put the spotlight on different\nrelationships in the data? Inspired by Anscombe's quartet, this paper\nintroduces a Rashomon Quartet, i.e. a set of four models built on a synthetic\ndataset which have practically identical predictive performance. However, the\nvisual exploration reveals distinct explanations of the relations in the data.\nThis illustrative example aims to encourage the use of methods for model\nvisualization to compare predictive models beyond their performance.\n","authors":["Przemyslaw Biecek","Hubert Baniecki","Mateusz Krzyzinski","Dianne Cook"],"pdf_url":"https://arxiv.org/pdf/2302.13356v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16770v2","updated":"2024-04-11T17:40:57Z","published":"2024-02-26T17:39:23Z","title":"Neural population geometry and optimal coding of tasks with shared\n  latent structure","summary":"  Humans and animals can recognize latent structures in their environment and\napply this information to efficiently navigate the world. However, it remains\nunclear what aspects of neural activity contribute to these computational\ncapabilities. Here, we develop an analytical theory linking the geometry of a\nneural population's activity to the generalization performance of a linear\nreadout on a set of tasks that depend on a common latent structure. We show\nthat four geometric measures of the activity determine performance across\ntasks. Using this theory, we find that experimentally observed disentangled\nrepresentations naturally emerge as an optimal solution to the multi-task\nlearning problem. When data is scarce, these optimal neural codes compress less\ninformative latent variables, and when data is abundant, they expand these\nvariables in the state space. We validate our theory using macaque ventral\nstream recordings. Our results therefore tie population geometry to multi-task\nlearning.\n","authors":["Albert J. Wakhloo","Will Slatton","SueYeon Chung"],"pdf_url":"https://arxiv.org/pdf/2402.16770v2.pdf","comment":"26 Pages and 7 figures in main text. 20 Pages and 7 figures in\n  supplemental material"},{"id":"http://arxiv.org/abs/2404.07937v1","updated":"2024-04-11T17:36:28Z","published":"2024-04-11T17:36:28Z","title":"Rate-Optimal Non-Asymptotics for the Quadratic Prediction Error Method","summary":"  We study the quadratic prediction error method -- i.e., nonlinear least\nsquares -- for a class of time-varying parametric predictor models satisfying a\ncertain identifiability condition. While this method is known to asymptotically\nachieve the optimal rate for a wide range of problems, there have been no\nnon-asymptotic results matching these optimal rates outside of a select few,\ntypically linear, model classes. By leveraging modern tools from learning with\ndependent data, we provide the first rate-optimal non-asymptotic analysis of\nthis method for our more general setting of nonlinearly parametrized model\nclasses. Moreover, we show that our results can be applied to a particular\nclass of identifiable AutoRegressive Moving Average (ARMA) models, resulting in\nthe first optimal non-asymptotic rates for identification of ARMA models.\n","authors":["Charis Stamouli","Ingvar Ziemann","George J. Pappas"],"pdf_url":"https://arxiv.org/pdf/2404.07937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.01177v2","updated":"2024-04-11T17:23:42Z","published":"2023-07-03T17:40:58Z","title":"Neural Hilbert Ladders: Multi-Layer Neural Networks in Function Space","summary":"  To characterize the function space explored by neural networks (NNs) is an\nimportant aspect of learning theory. In this work, noticing that a multi-layer\nNN generates implicitly a hierarchy of reproducing kernel Hilbert spaces\n(RKHSs) - named a neural Hilbert ladder (NHL) - we define the function space as\nan infinite union of RKHSs, which generalizes the existing Barron space theory\nof two-layer NNs. We then establish several theoretical properties of the new\nspace. First, we prove a correspondence between functions expressed by L-layer\nNNs and those belonging to L-level NHLs. Second, we prove generalization\nguarantees for learning an NHL with a controlled complexity measure. Third, we\nderive a non-Markovian dynamics of random fields that governs the evolution of\nthe NHL which is induced by the training of multi-layer NNs in an\ninfinite-width mean-field limit. Fourth, we show examples of depth separation\nin NHLs under the ReLU activation function. Finally, we perform numerical\nexperiments to illustrate the feature learning aspect of NN training through\nthe lens of NHLs.\n","authors":["Zhengdao Chen"],"pdf_url":"https://arxiv.org/pdf/2307.01177v2.pdf","comment":"65 pages, 3 figures. Published by the Journal of Machine Learning\n  Research and presented partially at the 40th International Conference on\n  Machine Learning (ICML 2023)"},{"id":"http://arxiv.org/abs/2404.07924v1","updated":"2024-04-11T17:10:57Z","published":"2024-04-11T17:10:57Z","title":"A Parsimonious Setup for Streamflow Forecasting using CNN-LSTM","summary":"  Significant strides have been made in advancing streamflow predictions,\nnotably with the introduction of cutting-edge machine-learning models.\nPredominantly, Long Short-Term Memories (LSTMs) and Convolution Neural Networks\n(CNNs) have been widely employed in this domain. While LSTMs are applicable in\nboth rainfall-runoff and time series settings, CNN-LSTMs have primarily been\nutilized in rainfall-runoff scenarios. In this study, we extend the application\nof CNN-LSTMs to time series settings, leveraging lagged streamflow data in\nconjunction with precipitation and temperature data to predict streamflow. Our\nresults show a substantial improvement in predictive performance in 21 out of\n32 HUC8 basins in Nebraska, showcasing noteworthy increases in the Kling-Gupta\nEfficiency (KGE) values. These results highlight the effectiveness of CNN-LSTMs\nin time series settings, particularly for spatiotemporal hydrological modeling,\nfor more accurate and robust streamflow predictions.\n","authors":["Sudan Pokharel","Tirthankar Roy"],"pdf_url":"https://arxiv.org/pdf/2404.07924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07922v1","updated":"2024-04-11T17:09:28Z","published":"2024-04-11T17:09:28Z","title":"LaVy: Vietnamese Multimodal Large Language Model","summary":"  Large Language Models (LLMs) and Multimodal Large language models (MLLMs)\nhave taken the world by storm with impressive abilities in complex reasoning\nand linguistic comprehension. Meanwhile there are plethora of works related to\nVietnamese Large Language Models, the lack of high-quality resources in\nmultimodality limits the progress of Vietnamese MLLMs. In this paper, we\npioneer in address this by introducing LaVy, a state-of-the-art Vietnamese\nMLLM, and we also introduce LaVy-Bench benchmark designated for evaluating\nMLLMs's understanding on Vietnamese visual language tasks. All code and model\nweights are public at https://github.com/baochi0212/LaVy\n","authors":["Chi Tran","Huong Le Thanh"],"pdf_url":"https://arxiv.org/pdf/2404.07922v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2404.07919v1","updated":"2024-04-11T17:04:55Z","published":"2024-04-11T17:04:55Z","title":"Low-rank Adaptation for Spatio-Temporal Forecasting","summary":"  Spatio-temporal forecasting is crucial in real-world dynamic systems,\npredicting future changes using historical data from diverse locations.\nExisting methods often prioritize the development of intricate neural networks\nto capture the complex dependencies of the data, yet their accuracy fails to\nshow sustained improvement. Besides, these methods also overlook node\nheterogeneity, hindering customized prediction modules from handling diverse\nregional nodes effectively. In this paper, our goal is not to propose a new\nmodel but to present a novel low-rank adaptation framework as an off-the-shelf\nplugin for existing spatial-temporal prediction models, termed ST-LoRA, which\nalleviates the aforementioned problems through node-level adjustments.\nSpecifically, we first tailor a node adaptive low-rank layer comprising\nmultiple trainable low-rank matrices. Additionally, we devise a multi-layer\nresidual fusion stacking module, injecting the low-rank adapters into predictor\nmodules of various models. Across six real-world traffic datasets and six\ndifferent types of spatio-temporal prediction models, our approach minimally\nincreases the parameters and training time of the original models by less than\n4%, still achieving consistent and sustained performance enhancement.\n","authors":["Weilin Ruan","Wei Chen","Xilin Dang","Jianxiang Zhou","Weichuang Li","Xu Liu","Yuxuan Liang"],"pdf_url":"https://arxiv.org/pdf/2404.07919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.00445v2","updated":"2024-04-11T16:49:57Z","published":"2023-11-01T11:13:06Z","title":"A Systematic Comparison of Syllogistic Reasoning in Humans and Language\n  Models","summary":"  A central component of rational behavior is logical inference: the process of\ndetermining which conclusions follow from a set of premises. Psychologists have\ndocumented several ways in which humans' inferences deviate from the rules of\nlogic. Do language models, which are trained on text generated by humans,\nreplicate such human biases, or are they able to overcome them? Focusing on the\ncase of syllogisms -- inferences from two simple premises -- we show that,\nwithin the PaLM2 family of transformer language models, larger models are more\nlogical than smaller ones, and also more logical than humans. At the same time,\neven the largest models make systematic errors, some of which mirror human\nreasoning biases: they show sensitivity to the (irrelevant) ordering of the\nvariables in the syllogism, and draw confident but incorrect inferences from\nparticular syllogisms (syllogistic fallacies). Overall, we find that language\nmodels often mimic the human biases included in their training data, but are\nable to overcome them in some cases.\n","authors":["Tiwalayo Eisape","MH Tessler","Ishita Dasgupta","Fei Sha","Sjoerd van Steenkiste","Tal Linzen"],"pdf_url":"https://arxiv.org/pdf/2311.00445v2.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.15304v2","updated":"2024-04-11T16:39:54Z","published":"2024-03-22T15:54:30Z","title":"KTbench: A Novel Data Leakage-Free Framework for Knowledge Tracing","summary":"  Knowledge Tracing (KT) is concerned with predicting students' future\nperformance on learning items in intelligent tutoring systems. Learning items\nare tagged with skill labels called knowledge concepts (KCs). Many KT models\nexpand the sequence of item-student interactions into KC-student interactions\nby replacing learning items with their constituting KCs. This often results in\na longer sequence length. This approach addresses the issue of sparse\nitem-student interactions and minimises model parameters. However, two problems\nhave been identified with such models.\n  The first problem is the model's ability to learn correlations between KCs\nbelonging to the same item, which can result in the leakage of ground truth\nlabels and hinder performance. This problem can lead to a significant decrease\nin performance on datasets with a higher number of KCs per item. The second\nproblem is that the available benchmark implementations ignore accounting for\nchanges in sequence length when expanding KCs, leading to different models\nbeing tested with varying sequence lengths but still compared against the same\nbenchmark.\n  To address these problems, we introduce a general masking framework that\nmitigates the first problem and enhances the performance of such KT models\nwhile preserving the original model architecture without significant\nalterations. Additionally, we introduce KTbench, an open-source benchmark\nlibrary designed to ensure the reproducibility of this work while mitigating\nthe second problem.\n","authors":["Yahya Badran","Christine Preisach"],"pdf_url":"https://arxiv.org/pdf/2403.15304v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2404.07898v1","updated":"2024-04-11T16:37:01Z","published":"2024-04-11T16:37:01Z","title":"Anomaly Detection in Power Grids via Context-Agnostic Learning","summary":"  An important tool grid operators use to safeguard against failures, whether\nnaturally occurring or malicious, involves detecting anomalies in the power\nsystem SCADA data. In this paper, we aim to solve a real-time anomaly detection\nproblem. Given time-series measurement values coming from a fixed set of\nsensors on the grid, can we identify anomalies in the network topology or\nmeasurement data? Existing methods, primarily optimization-based, mostly use\nonly a single snapshot of the measurement values and do not scale well with the\nnetwork size. Recent data-driven ML techniques have shown promise by using a\ncombination of current and historical data for anomaly detection but generally\ndo not consider physical attributes like the impact of topology or\nload/generation changes on sensor measurements and thus cannot accommodate\nregular context-variability in the historical data. To address this gap, we\npropose a novel context-aware anomaly detection algorithm, GridCAL, that\nconsiders the effect of regular topology and load/generation changes. This\nalgorithm converts the real-time power flow measurements to context-agnostic\nvalues, which allows us to analyze measurement coming from different grid\ncontexts in an aggregate fashion, enabling us to derive a unified statistical\nmodel that becomes the basis of anomaly detection. Through numerical\nsimulations on networks up to 2383 nodes, we show that our approach is\naccurate, outperforming state-of-the-art approaches, and is computationally\nefficient.\n","authors":["SangWoo Park","Amritanshu Pandey"],"pdf_url":"https://arxiv.org/pdf/2404.07898v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11321v2","updated":"2024-04-11T16:32:05Z","published":"2023-11-19T13:31:30Z","title":"Bounds on Representation-Induced Confounding Bias for Treatment Effect\n  Estimation","summary":"  State-of-the-art methods for conditional average treatment effect (CATE)\nestimation make widespread use of representation learning. Here, the idea is to\nreduce the variance of the low-sample CATE estimation by a (potentially\nconstrained) low-dimensional representation. However, low-dimensional\nrepresentations can lose information about the observed confounders and thus\nlead to bias, because of which the validity of representation learning for CATE\nestimation is typically violated. In this paper, we propose a new,\nrepresentation-agnostic refutation framework for estimating bounds on the\nrepresentation-induced confounding bias that comes from dimensionality\nreduction (or other constraints on the representations) in CATE estimation.\nFirst, we establish theoretically under which conditions CATE is\nnon-identifiable given low-dimensional (constrained) representations. Second,\nas our remedy, we propose a neural refutation framework which performs partial\nidentification of CATE or, equivalently, aims at estimating lower and upper\nbounds of the representation-induced confounding bias. We demonstrate the\neffectiveness of our bounds in a series of experiments. In sum, our refutation\nframework is of direct relevance in practice where the validity of CATE\nestimation is of importance.\n","authors":["Valentyn Melnychuk","Dennis Frauen","Stefan Feuerriegel"],"pdf_url":"https://arxiv.org/pdf/2311.11321v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02391v4","updated":"2024-04-11T16:29:12Z","published":"2023-10-03T19:24:24Z","title":"SE(3)-Stochastic Flow Matching for Protein Backbone Generation","summary":"  The computational design of novel protein structures has the potential to\nimpact numerous scientific disciplines greatly. Toward this goal, we introduce\nFoldFlow, a series of novel generative models of increasing modeling power\nbased on the flow-matching paradigm over $3\\mathrm{D}$ rigid motions -- i.e.\nthe group $\\text{SE}(3)$ -- enabling accurate modeling of protein backbones. We\nfirst introduce FoldFlow-Base, a simulation-free approach to learning\ndeterministic continuous-time dynamics and matching invariant target\ndistributions on $\\text{SE}(3)$. We next accelerate training by incorporating\nRiemannian optimal transport to create FoldFlow-OT, leading to the construction\nof both more simple and stable flows. Finally, we design FoldFlow-SFM, coupling\nboth Riemannian OT and simulation-free training to learn stochastic\ncontinuous-time dynamics over $\\text{SE}(3)$. Our family of FoldFlow,\ngenerative models offers several key advantages over previous approaches to the\ngenerative modeling of proteins: they are more stable and faster to train than\ndiffusion-based approaches, and our models enjoy the ability to map any\ninvariant source distribution to any invariant target distribution over\n$\\text{SE}(3)$. Empirically, we validate FoldFlow, on protein backbone\ngeneration of up to $300$ amino acids leading to high-quality designable,\ndiverse, and novel samples.\n","authors":["Avishek Joey Bose","Tara Akhound-Sadegh","Guillaume Huguet","Kilian Fatras","Jarrid Rector-Brooks","Cheng-Hao Liu","Andrei Cristian Nica","Maksym Korablyov","Michael Bronstein","Alexander Tong"],"pdf_url":"https://arxiv.org/pdf/2310.02391v4.pdf","comment":"ICLR 2024 Spotlight"},{"id":"http://arxiv.org/abs/2311.10421v2","updated":"2024-04-11T16:28:22Z","published":"2023-11-17T09:54:35Z","title":"Is Your Anomaly Detector Ready for Change? Adapting AIOps Solutions to\n  the Real World","summary":"  Anomaly detection techniques are essential in automating the monitoring of IT\nsystems and operations. These techniques imply that machine learning algorithms\nare trained on operational data corresponding to a specific period of time and\nthat they are continuously evaluated on newly emerging data. Operational data\nis constantly changing over time, which affects the performance of deployed\nanomaly detection models. Therefore, continuous model maintenance is required\nto preserve the performance of anomaly detectors over time. In this work, we\nanalyze two different anomaly detection model maintenance techniques in terms\nof the model update frequency, namely blind model retraining and informed model\nretraining. We further investigate the effects of updating the model by\nretraining it on all the available data (full-history approach) and only the\nnewest data (sliding window approach). Moreover, we investigate whether a data\nchange monitoring tool is capable of determining when the anomaly detection\nmodel needs to be updated through retraining.\n","authors":["Lorena Poenaru-Olaru","Natalia Karpova","Luis Cruz","Jan Rellermeyer","Arie van Deursen"],"pdf_url":"https://arxiv.org/pdf/2311.10421v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10199v2","updated":"2024-04-11T16:22:54Z","published":"2023-12-15T20:55:05Z","title":"Automatic nonlinear MPC approximation with closed-loop guarantees","summary":"  Safety guarantees are vital in many control applications, such as robotics.\nModel predictive control (MPC) provides a constructive framework for\ncontrolling safety-critical systems, but is limited by its computational\ncomplexity. We address this problem by presenting a novel algorithm that\nautomatically computes an explicit approximation to nonlinear MPC schemes while\nretaining closed-loop guarantees. Specifically, the problem can be reduced to a\nfunction approximation problem, which we then tackle by proposing ALKIA-X, the\nAdaptive and Localized Kernel Interpolation Algorithm with eXtrapolated\nreproducing kernel Hilbert space norm. ALKIA-X is a non-iterative algorithm\nthat ensures numerically well-conditioned computations, a fast-to-evaluate\napproximating function, and the guaranteed satisfaction of any desired bound on\nthe approximation error. Hence, ALKIA-X automatically computes an explicit\nfunction that approximates the MPC, yielding a controller suitable for\nsafety-critical systems and high sampling rates. We apply ALKIA-X to\napproximate two nonlinear MPC schemes, demonstrating reduced computational\ndemand and applicability to realistic problems.\n","authors":["Abdullah Tokmak","Christian Fiedler","Melanie N. Zeilinger","Sebastian Trimpe","Johannes K√∂hler"],"pdf_url":"https://arxiv.org/pdf/2312.10199v2.pdf","comment":"Submitted to IEEE Transactions on Automatic Control. Compared to the\n  previously uploaded version, this version contains an additional numerical\n  example"},{"id":"http://arxiv.org/abs/2310.06110v3","updated":"2024-04-11T16:15:34Z","published":"2023-10-09T19:33:21Z","title":"Grokking as the Transition from Lazy to Rich Training Dynamics","summary":"  We propose that the grokking phenomenon, where the train loss of a neural\nnetwork decreases much earlier than its test loss, can arise due to a neural\nnetwork transitioning from lazy training dynamics to a rich, feature learning\nregime. To illustrate this mechanism, we study the simple setting of vanilla\ngradient descent on a polynomial regression problem with a two layer neural\nnetwork which exhibits grokking without regularization in a way that cannot be\nexplained by existing theories. We identify sufficient statistics for the test\nloss of such a network, and tracking these over training reveals that grokking\narises in this setting when the network first attempts to fit a kernel\nregression solution with its initial features, followed by late-time feature\nlearning where a generalizing solution is identified after train loss is\nalready low. We find that the key determinants of grokking are the rate of\nfeature learning -- which can be controlled precisely by parameters that scale\nthe network output -- and the alignment of the initial features with the target\nfunction $y(x)$. We argue this delayed generalization arises when (1) the top\neigenvectors of the initial neural tangent kernel and the task labels $y(x)$\nare misaligned, but (2) the dataset size is large enough so that it is possible\nfor the network to generalize eventually, but not so large that train loss\nperfectly tracks test loss at all epochs, and (3) the network begins training\nin the lazy regime so does not learn features immediately. We conclude with\nevidence that this transition from lazy (linear model) to rich training\n(feature learning) can control grokking in more general settings, like on\nMNIST, one-layer Transformers, and student-teacher networks.\n","authors":["Tanishq Kumar","Blake Bordelon","Samuel J. Gershman","Cengiz Pehlevan"],"pdf_url":"https://arxiv.org/pdf/2310.06110v3.pdf","comment":"Adding new experiments on higher degree Hermite polynomials,\n  multi-index targets, removed DMFT analysis from this version"},{"id":"http://arxiv.org/abs/2404.04979v2","updated":"2024-04-11T16:11:33Z","published":"2024-04-07T14:47:07Z","title":"CAVIAR: Categorical-Variable Embeddings for Accurate and Robust\n  Inference","summary":"  Social science research often hinges on the relationship between categorical\nvariables and outcomes. We introduce CAVIAR, a novel method for embedding\ncategorical variables that assume values in a high-dimensional ambient space\nbut are sampled from an underlying manifold. Our theoretical and numerical\nanalyses outline challenges posed by such categorical variables in causal\ninference. Specifically, dynamically varying and sparse levels can lead to\nviolations of the Donsker conditions and a failure of the estimation\nfunctionals to converge to a tight Gaussian process. Traditional approaches,\nincluding the exclusion of rare categorical levels and principled variable\nselection models like LASSO, fall short. CAVIAR embeds the data into a\nlower-dimensional global coordinate system. The mapping can be derived from\nboth structured and unstructured data, and ensures stable and robust estimates\nthrough dimensionality reduction. In a dataset of direct-to-consumer apparel\nsales, we illustrate how high-dimensional categorical variables, such as zip\ncodes, can be succinctly represented, facilitating inference and analysis.\n","authors":["Anirban Mukherjee","Hannah Hanwen Chang"],"pdf_url":"https://arxiv.org/pdf/2404.04979v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13989v2","updated":"2024-04-11T16:07:30Z","published":"2024-02-21T18:19:20Z","title":"FedADMM-InSa: An Inexact and Self-Adaptive ADMM for Federated Learning","summary":"  Federated learning (FL) is a promising framework for learning from\ndistributed data while maintaining privacy. The development of efficient FL\nalgorithms encounters various challenges, including heterogeneous data and\nsystems, limited communication capacities, and constrained local computational\nresources. Recently developed FedADMM methods show great resilience to both\ndata and system heterogeneity. However, they still suffer from performance\ndeterioration if the hyperparameters are not carefully tuned. To address this\nissue, we propose an inexact and self-adaptive FedADMM algorithm, termed\nFedADMM-InSa. First, we design an inexactness criterion for the clients' local\nupdates to eliminate the need for empirically setting the local training\naccuracy. This inexactness criterion can be assessed by each client\nindependently based on its unique condition, thereby reducing the local\ncomputational cost and mitigating the undesirable straggle effect. The\nconvergence of the resulting inexact ADMM is proved under the assumption of\nstrongly convex loss functions. Additionally, we present a self-adaptive scheme\nthat dynamically adjusts each client's penalty parameter, enhancing algorithm\nrobustness by mitigating the need for empirical penalty parameter choices for\neach client. Extensive numerical experiments on both synthetic and real-world\ndatasets are conducted. As validated by some numerical tests, our proposed\nalgorithm can reduce the clients' local computational load significantly and\nalso accelerate the learning process compared to the vanilla FedADMM.\n","authors":["Yongcun Song","Ziqi Wang","Enrique Zuazua"],"pdf_url":"https://arxiv.org/pdf/2402.13989v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07864v1","updated":"2024-04-11T15:57:12Z","published":"2024-04-11T15:57:12Z","title":"Inferring Change Points in High-Dimensional Linear Regression via\n  Approximate Message Passing","summary":"  We consider the problem of localizing change points in high-dimensional\nlinear regression. We propose an Approximate Message Passing (AMP) algorithm\nfor estimating both the signals and the change point locations. Assuming\nGaussian covariates, we give an exact asymptotic characterization of its\nestimation performance in the limit where the number of samples grows\nproportionally to the signal dimension. Our algorithm can be tailored to\nexploit any prior information on the signal, noise, and change points. It also\nenables uncertainty quantification in the form of an efficiently computable\napproximate posterior distribution, whose asymptotic form we characterize\nexactly. We validate our theory via numerical experiments, and demonstrate the\nfavorable performance of our estimators on both synthetic data and images.\n","authors":["Gabriel Arpino","Xiaoqi Liu","Ramji Venkataramanan"],"pdf_url":"https://arxiv.org/pdf/2404.07864v1.pdf","comment":"24 pages, 8 figures"},{"id":"http://arxiv.org/abs/2404.07860v1","updated":"2024-04-11T15:54:20Z","published":"2024-04-11T15:54:20Z","title":"Streaming detection of significant delay changes in public transport\n  systems","summary":"  Public transport systems are expected to reduce pollution and contribute to\nsustainable development. However, disruptions in public transport such as\ndelays may negatively affect mobility choices. To quantify delays, aggregated\ndata from vehicle locations systems are frequently used. However, delays\nobserved at individual stops are caused inter alia by fluctuations in running\ntimes and propagation of delays occurring in other locations. Hence, in this\nwork, we propose both the method detecting significant delays and reference\narchitecture, relying on stream processing engines, in which the method is\nimplemented. The method can complement the calculation of delays defined as\ndeviation from schedules. This provides both online rather than batch\nidentification of significant and repetitive delays, and resilience to the\nlimited quality of location data. The method we propose can be used with\ndifferent change detectors, such as ADWIN, applied to location data stream\nshuffled to individual edges of a transport graph. It can detect in an online\nmanner at which edges statistically significant delays are observed and at\nwhich edges delays arise and are reduced. Detections can be used to model\nmobility choices and quantify the impact of repetitive rather than random\ndisruptions on feasible trips with multimodal trip modelling engines. The\nevaluation performed with the public transport data of over 2000 vehicles\nconfirms the merits of the method and reveals that a limited-size subgraph of a\ntransport system graph causes statistically significant delays\n","authors":["Przemys≈Çaw Wrona","Maciej Grzenda","Marcin Luckner"],"pdf_url":"https://arxiv.org/pdf/2404.07860v1.pdf","comment":"This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Computational Science - ICCS 2022. Lecture Notes in Computer\n  Science, vol 13353. Springer, Cham, and is available online at\n  https://doi.org/10.1007/978-3-031-08760-8_41"},{"id":"http://arxiv.org/abs/2404.07849v1","updated":"2024-04-11T15:43:11Z","published":"2024-04-11T15:43:11Z","title":"Overparameterized Multiple Linear Regression as Hyper-Curve Fitting","summary":"  The paper shows that the application of the fixed-effect multiple linear\nregression model to an overparameterized dataset is equivalent to fitting the\ndata with a hyper-curve parameterized by a single scalar parameter. This\nequivalence allows for a predictor-focused approach, where each predictor is\ndescribed by a function of the chosen parameter. It is proven that a linear\nmodel will produce exact predictions even in the presence of nonlinear\ndependencies that violate the model assumptions. Parameterization in terms of\nthe dependent variable and the monomial basis in the predictor function space\nare applied here to both synthetic and experimental data. The hyper-curve\napproach is especially suited for the regularization of problems with noise in\npredictor variables and can be used to remove noisy and \"improper\" predictors\nfrom the model.\n","authors":["E. Atza","N. Budko"],"pdf_url":"https://arxiv.org/pdf/2404.07849v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07840v1","updated":"2024-04-11T15:27:56Z","published":"2024-04-11T15:27:56Z","title":"On Training Data Influence of GPT Models","summary":"  Amidst the rapid advancements in generative language models, the\ninvestigation of how training data shapes the performance of GPT models is\nstill emerging. This paper presents GPTfluence, a novel approach that leverages\na featurized simulation to assess the impact of training examples on the\ntraining dynamics of GPT models. Our approach not only traces the influence of\nindividual training instances on performance trajectories, such as loss and\nother key metrics, on targeted test points but also enables a comprehensive\ncomparison with existing methods across various training scenarios in GPT\nmodels, ranging from 14 million to 2.8 billion parameters, across a range of\ndownstream tasks. Contrary to earlier methods that struggle with generalization\nto new data, GPTfluence introduces a parameterized simulation of training\ndynamics, demonstrating robust generalization capabilities to unseen training\ndata. This adaptability is evident across both fine-tuning and\ninstruction-tuning scenarios, spanning tasks in natural language understanding\nand generation. We will make our code and data publicly available.\n","authors":["Qingyi Liu","Yekun Chai","Shuohuan Wang","Yu Sun","Keze Wang","Hua Wu"],"pdf_url":"https://arxiv.org/pdf/2404.07840v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07839v1","updated":"2024-04-11T15:27:22Z","published":"2024-04-11T15:27:22Z","title":"RecurrentGemma: Moving Past Transformers for Efficient Open Language\n  Models","summary":"  We introduce RecurrentGemma, an open language model which uses Google's novel\nGriffin architecture. Griffin combines linear recurrences with local attention\nto achieve excellent performance on language. It has a fixed-sized state, which\nreduces memory use and enables efficient inference on long sequences. We\nprovide a pre-trained model with 2B non-embedding parameters, and an\ninstruction tuned variant. Both models achieve comparable performance to\nGemma-2B despite being trained on fewer tokens.\n","authors":["Aleksandar Botev","Soham De","Samuel L Smith","Anushan Fernando","George-Cristian Muraru","Ruba Haroun","Leonard Berrada","Razvan Pascanu","Pier Giuseppe Sessa","Robert Dadashi","L√©onard Hussenot","Johan Ferret","Sertan Girgin","Olivier Bachem","Alek Andreev","Kathleen Kenealy","Thomas Mesnard","Cassidy Hardin","Surya Bhupatiraju","Shreya Pathak","Laurent Sifre","Morgane Rivi√®re","Mihir Sanjay Kale","Juliette Love","Pouya Tafti","Armand Joulin","Noah Fiedel","Evan Senter","Yutian Chen","Srivatsan Srinivasan","Guillaume Desjardins","David Budden","Arnaud Doucet","Sharad Vikram","Adam Paszke","Trevor Gale","Sebastian Borgeaud","Charlie Chen","Andy Brock","Antonia Paterson","Jenny Brennan","Meg Risdal","Raj Gundluru","Nesh Devanathan","Paul Mooney","Nilay Chauhan","Phil Culliton","Luiz GUStavo Martins","Elisa Bandy","David Huntsperger","Glenn Cameron","Arthur Zucker","Tris Warkentin","Ludovic Peran","Minh Giang","Zoubin Ghahramani","Cl√©ment Farabet","Koray Kavukcuoglu","Demis Hassabis","Raia Hadsell","Yee Whye Teh","Nando de Frietas"],"pdf_url":"https://arxiv.org/pdf/2404.07839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07833v1","updated":"2024-04-11T15:18:34Z","published":"2024-04-11T15:18:34Z","title":"Streamlined Photoacoustic Image Processing with Foundation Models: A\n  Training-Free Solution","summary":"  Foundation models have rapidly evolved and have achieved significant\naccomplishments in computer vision tasks. Specifically, the prompt mechanism\nconveniently allows users to integrate image prior information into the model,\nmaking it possible to apply models without any training. Therefore, we propose\na method based on foundation models and zero training to solve the tasks of\nphotoacoustic (PA) image segmentation. We employed the segment anything model\n(SAM) by setting simple prompts and integrating the model's outputs with prior\nknowledge of the imaged objects to accomplish various tasks, including: (1)\nremoving the skin signal in three-dimensional PA image rendering; (2) dual\nspeed-of-sound reconstruction, and (3) segmentation of finger blood vessels.\nThrough these demonstrations, we have concluded that deep learning can be\ndirectly applied in PA imaging without the requirement for network design and\ntraining. This potentially allows for a hands-on, convenient approach to\nachieving efficient and accurate segmentation of PA images. This letter serves\nas a comprehensive tutorial, facilitating the mastery of the technique through\nthe provision of code and sample datasets.\n","authors":["Handi Deng","Yucheng Zhou","Jiaxuan Xiang","Liujie Gu","Yan Luo","Hai Feng","Mingyuan Liu","Cheng Ma"],"pdf_url":"https://arxiv.org/pdf/2404.07833v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07826v1","updated":"2024-04-11T15:09:49Z","published":"2024-04-11T15:09:49Z","title":"On the Sample Efficiency of Abstractions and Potential-Based Reward\n  Shaping in Reinforcement Learning","summary":"  The use of Potential Based Reward Shaping (PBRS) has shown great promise in\nthe ongoing research effort to tackle sample inefficiency in Reinforcement\nLearning (RL). However, the choice of the potential function is critical for\nthis technique to be effective. Additionally, RL techniques are usually\nconstrained to use a finite horizon for computational limitations. This\nintroduces a bias when using PBRS, thus adding an additional layer of\ncomplexity. In this paper, we leverage abstractions to automatically produce a\n\"good\" potential function. We analyse the bias induced by finite horizons in\nthe context of PBRS producing novel insights. Finally, to asses sample\nefficiency and performance impact, we evaluate our approach on four\nenvironments including a goal-oriented navigation task and three Arcade\nLearning Environments (ALE) games demonstrating that we can reach the same\nlevel of performance as CNN-based solutions with a simple fully-connected\nnetwork.\n","authors":["Giuseppe Canonaco","Leo Ardon","Alberto Pozanco","Daniel Borrajo"],"pdf_url":"https://arxiv.org/pdf/2404.07826v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07817v1","updated":"2024-04-11T14:59:49Z","published":"2024-04-11T14:59:49Z","title":"Calibration of Continual Learning Models","summary":"  Continual Learning (CL) focuses on maximizing the predictive performance of a\nmodel across a non-stationary stream of data. Unfortunately, CL models tend to\nforget previous knowledge, thus often underperforming when compared with an\noffline model trained jointly on the entire data stream. Given that any CL\nmodel will eventually make mistakes, it is of crucial importance to build\ncalibrated CL models: models that can reliably tell their confidence when\nmaking a prediction. Model calibration is an active research topic in machine\nlearning, yet to be properly investigated in CL. We provide the first empirical\nstudy of the behavior of calibration approaches in CL, showing that CL\nstrategies do not inherently learn calibrated models. To mitigate this issue,\nwe design a continual calibration approach that improves the performance of\npost-processing calibration methods over a wide range of different benchmarks\nand CL strategies. CL does not necessarily need perfect predictive models, but\nrather it can benefit from reliable predictive models. We believe our study on\ncontinual calibration represents a first step towards this direction.\n","authors":["Lanpei Li","Elia Piccoli","Andrea Cossu","Davide Bacciu","Vincenzo Lomonaco"],"pdf_url":"https://arxiv.org/pdf/2404.07817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08156v4","updated":"2024-04-11T14:59:39Z","published":"2024-02-13T01:38:01Z","title":"Group Decision-Making among Privacy-Aware Agents","summary":"  How can individuals exchange information to learn from each other despite\ntheir privacy needs and security concerns? For example, consider individuals\ndeliberating a contentious topic and being concerned about divulging their\nprivate experiences. Preserving individual privacy and enabling efficient\nsocial learning are both important desiderata but seem fundamentally at odds\nwith each other and very hard to reconcile. We do so by controlling information\nleakage using rigorous statistical guarantees that are based on differential\nprivacy (DP). Our agents use log-linear rules to update their beliefs after\ncommunicating with their neighbors. Adding DP randomization noise to beliefs\nprovides communicating agents with plausible deniability with regard to their\nprivate information and their network neighborhoods. We consider two learning\nenvironments one for distributed maximum-likelihood estimation given a finite\nnumber of private signals and another for online learning from an infinite,\nintermittent signal stream. Noisy information aggregation in the finite case\nleads to interesting tradeoffs between rejecting low-quality states and making\nsure all high-quality states are accepted in the algorithm output. Our results\nflesh out the nature of the trade-offs in both cases between the quality of the\ngroup decision outcomes, learning accuracy, communication cost, and the level\nof privacy protections that the agents are afforded.\n","authors":["Marios Papachristou","M. Amin Rahimian"],"pdf_url":"https://arxiv.org/pdf/2402.08156v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07815v1","updated":"2024-04-11T14:58:19Z","published":"2024-04-11T14:58:19Z","title":"Post-Hoc Reversal: Are We Selecting Models Prematurely?","summary":"  Trained models are often composed with post-hoc transforms such as\ntemperature scaling (TS), ensembling and stochastic weight averaging (SWA) to\nimprove performance, robustness, uncertainty estimation, etc. However, such\ntransforms are typically applied only after the base models have already been\nfinalized by standard means. In this paper, we challenge this practice with an\nextensive empirical study. In particular, we demonstrate a phenomenon that we\ncall post-hoc reversal, where performance trends are reversed after applying\nthese post-hoc transforms. This phenomenon is especially prominent in\nhigh-noise settings. For example, while base models overfit badly early in\ntraining, both conventional ensembling and SWA favor base models trained for\nmore epochs. Post-hoc reversal can also suppress the appearance of double\ndescent and mitigate mismatches between test loss and test error seen in base\nmodels. Based on our findings, we propose post-hoc selection, a simple\ntechnique whereby post-hoc metrics inform model development decisions such as\nearly stopping, checkpointing, and broader hyperparameter choices. Our\nexperimental analyses span real-world vision, language, tabular and graph\ndatasets from domains like satellite imaging, language modeling, census\nprediction and social network analysis. On an LLM instruction tuning dataset,\npost-hoc selection results in > 1.5x MMLU improvement compared to naive\nselection. Code is available at\nhttps://github.com/rishabh-ranjan/post-hoc-reversal.\n","authors":["Rishabh Ranjan","Saurabh Garg","Mrigank Raman","Carlos Guestrin","Zachary Chase Lipton"],"pdf_url":"https://arxiv.org/pdf/2404.07815v1.pdf","comment":"9 pages + references + appendix, 7 figures"},{"id":"http://arxiv.org/abs/2404.06405v2","updated":"2024-04-11T14:37:29Z","published":"2024-04-09T15:54:00Z","title":"Wu's Method can Boost Symbolic AI to Rival Silver Medalists and\n  AlphaGeometry to Outperform Gold Medalists at IMO Geometry","summary":"  Proving geometric theorems constitutes a hallmark of visual reasoning\ncombining both intuitive and logical skills. Therefore, automated theorem\nproving of Olympiad-level geometry problems is considered a notable milestone\nin human-level automated reasoning. The introduction of AlphaGeometry, a\nneuro-symbolic model trained with 100 million synthetic samples, marked a major\nbreakthrough. It solved 25 of 30 International Mathematical Olympiad (IMO)\nproblems whereas the reported baseline based on Wu's method solved only ten. In\nthis note, we revisit the IMO-AG-30 Challenge introduced with AlphaGeometry,\nand find that Wu's method is surprisingly strong. Wu's method alone can solve\n15 problems, and some of them are not solved by any of the other methods. This\nleads to two key findings: (i) Combining Wu's method with the classic synthetic\nmethods of deductive databases and angle, ratio, and distance chasing solves 21\nout of 30 methods by just using a CPU-only laptop with a time limit of 5\nminutes per problem. Essentially, this classic method solves just 4 problems\nless than AlphaGeometry and establishes the first fully symbolic baseline\nstrong enough to rival the performance of an IMO silver medalist. (ii) Wu's\nmethod even solves 2 of the 5 problems that AlphaGeometry failed to solve.\nThus, by combining AlphaGeometry with Wu's method we set a new state-of-the-art\nfor automated theorem proving on IMO-AG-30, solving 27 out of 30 problems, the\nfirst AI method which outperforms an IMO gold medalist.\n","authors":["Shiven Sinha","Ameya Prabhu","Ponnurangam Kumaraguru","Siddharth Bhat","Matthias Bethge"],"pdf_url":"https://arxiv.org/pdf/2404.06405v2.pdf","comment":"Work in Progress. Released for wider feedback"},{"id":"http://arxiv.org/abs/2404.07792v1","updated":"2024-04-11T14:35:23Z","published":"2024-04-11T14:35:23Z","title":"Nostra Domina at EvaLatin 2024: Improving Latin Polarity Detection\n  through Data Augmentation","summary":"  This paper describes submissions from the team Nostra Domina to the EvaLatin\n2024 shared task of emotion polarity detection. Given the low-resource\nenvironment of Latin and the complexity of sentiment in rhetorical genres like\npoetry, we augmented the available data through automatic polarity annotation.\nWe present two methods for doing so on the basis of the $k$-means algorithm,\nand we employ a variety of Latin large language models (LLMs) in a neural\narchitecture to better capture the underlying contextual sentiment\nrepresentations. Our best approach achieved the second highest macro-averaged\nMacro-$F_1$ score on the shared task's test set.\n","authors":["Stephen Bothwell","Abigail Swenor","David Chiang"],"pdf_url":"https://arxiv.org/pdf/2404.07792v1.pdf","comment":"Proceedings of the Third Workshop on Language Technologies for\n  Historical and Ancient Languages"},{"id":"http://arxiv.org/abs/2311.10792v3","updated":"2024-04-11T14:24:24Z","published":"2023-11-17T02:30:19Z","title":"Enhancing Data Efficiency and Feature Identification for Lithium-Ion\n  Battery Lifespan Prediction by Deciphering Interpretation of Temporal\n  Patterns and Cyclic Variability Using Attention-Based Models","summary":"  Accurately predicting the lifespan of lithium-ion batteries is crucial for\noptimizing operational strategies and mitigating risks. While numerous studies\nhave aimed at predicting battery lifespan, few have examined the\ninterpretability of their models or how such insights could improve\npredictions. Addressing this gap, we introduce three innovative models that\nintegrate shallow attention layers into a foundational model from our previous\nwork, which combined elements of recurrent and convolutional neural networks.\nUtilizing a well-known public dataset, we showcase our methodology's\neffectiveness. Temporal attention is applied to identify critical timesteps and\nhighlight differences among test cell batches, particularly underscoring the\nsignificance of the \"rest\" phase. Furthermore, by applying cyclic attention via\nself-attention to context vectors, our approach effectively identifies key\ncycles, enabling us to strategically decrease the input size for quicker\npredictions. Employing both single- and multi-head attention mechanisms, we\nhave systematically minimized the required input from 100 to 50 and then to 30\ncycles, refining this process based on cyclic attention scores. Our refined\nmodel exhibits strong regression capabilities, accurately forecasting the\ninitiation of rapid capacity fade with an average deviation of only 58 cycles\nby analyzing just the initial 30 cycles of easily accessible input data.\n","authors":["Jaewook Lee","Seongmin Heo","Jay H. Lee"],"pdf_url":"https://arxiv.org/pdf/2311.10792v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07083v2","updated":"2024-04-11T14:21:32Z","published":"2024-04-10T15:16:04Z","title":"Minimizing Chebyshev Prototype Risk Magically Mitigates the Perils of\n  Overfitting","summary":"  Overparameterized deep neural networks (DNNs), if not sufficiently\nregularized, are susceptible to overfitting their training examples and not\ngeneralizing well to test data. To discourage overfitting, researchers have\ndeveloped multicomponent loss functions that reduce intra-class feature\ncorrelation and maximize inter-class feature distance in one or more layers of\nthe network. By analyzing the penultimate feature layer activations output by a\nDNN's feature extraction section prior to the linear classifier, we find that\nmodified forms of the intra-class feature covariance and inter-class prototype\nseparation are key components of a fundamental Chebyshev upper bound on the\nprobability of misclassification, which we designate the Chebyshev Prototype\nRisk (CPR). While previous approaches' covariance loss terms scale\nquadratically with the number of network features, our CPR bound indicates that\nan approximate covariance loss in log-linear time is sufficient to reduce the\nbound and is scalable to large architectures. We implement the terms of the CPR\nbound into our Explicit CPR (exCPR) loss function and observe from empirical\nresults on multiple datasets and network architectures that our training\nalgorithm reduces overfitting and improves upon previous approaches in many\nsettings. Our code is available at\nhttps://github.com/Deano1718/Regularization_exCPR .\n","authors":["Nathaniel Dean","Dilip Sarkar"],"pdf_url":"https://arxiv.org/pdf/2404.07083v2.pdf","comment":"17 pages, 2 figures"},{"id":"http://arxiv.org/abs/2404.07776v1","updated":"2024-04-11T14:13:53Z","published":"2024-04-11T14:13:53Z","title":"Unsupervised Concept Drift Detection based on Parallel Activations of\n  Neural Network","summary":"  Practical applications of artificial intelligence increasingly often have to\ndeal with the streaming properties of real data, which, considering the time\nfactor, are subject to phenomena such as periodicity and more or less chaotic\ndegeneration - resulting directly in the concept drifts. The modern concept\ndrift detectors almost always assume immediate access to labels, which due to\ntheir cost, limited availability and possible delay has been shown to be\nunrealistic. This work proposes an unsupervised Parallel Activations Drift\nDetector, utilizing the outputs of an untrained neural network, presenting its\nkey design elements, intuitions about processing properties, and a pool of\ncomputer experiments demonstrating its competitiveness with state-of-the-art\nmethods.\n","authors":["Joanna Komorniczak","Pawe≈Ç Ksieniewicz"],"pdf_url":"https://arxiv.org/pdf/2404.07776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07775v1","updated":"2024-04-11T14:13:44Z","published":"2024-04-11T14:13:44Z","title":"Discourse-Aware In-Context Learning for Temporal Expression\n  Normalization","summary":"  Temporal expression (TE) normalization is a well-studied problem. However,\nthe predominately used rule-based systems are highly restricted to specific\nsettings, and upcoming machine learning approaches suffer from a lack of\nlabeled data. In this work, we explore the feasibility of proprietary and\nopen-source large language models (LLMs) for TE normalization using in-context\nlearning to inject task, document, and example information into the model. We\nexplore various sample selection strategies to retrieve the most relevant set\nof examples. By using a window-based prompt design approach, we can perform TE\nnormalization across sentences, while leveraging the LLM knowledge without\ntraining the model. Our experiments show competitive results to models designed\nfor this task. In particular, our method achieves large performance\nimprovements for non-standard settings by dynamically including relevant\nexamples during inference.\n","authors":["Akash Kumar Gautam","Lukas Lange","Jannik Str√∂tgen"],"pdf_url":"https://arxiv.org/pdf/2404.07775v1.pdf","comment":"Accepted at NAACL 2024"},{"id":"http://arxiv.org/abs/2404.07774v1","updated":"2024-04-11T14:09:41Z","published":"2024-04-11T14:09:41Z","title":"Sketch-Plan-Generalize: Continual Few-Shot Learning of Inductively\n  Generalizable Spatial Concepts for Language-Guided Robot Manipulation","summary":"  Our goal is to build embodied agents that can learn inductively generalizable\nspatial concepts in a continual manner, e.g, constructing a tower of a given\nheight. Existing work suffers from certain limitations (a) (Liang et al., 2023)\nand their multi-modal extensions, rely heavily on prior knowledge and are not\ngrounded in the demonstrations (b) (Liu et al., 2023) lack the ability to\ngeneralize due to their purely neural approach. A key challenge is to achieve a\nfine balance between symbolic representations which have the capability to\ngeneralize, and neural representations that are physically grounded. In\nresponse, we propose a neuro-symbolic approach by expressing inductive concepts\nas symbolic compositions over grounded neural concepts. Our key insight is to\ndecompose the concept learning problem into the following steps 1) Sketch:\nGetting a programmatic representation for the given instruction 2) Plan:\nPerform Model-Based RL over the sequence of grounded neural action concepts to\nlearn a grounded plan 3) Generalize: Abstract out a generic (lifted) Python\nprogram to facilitate generalizability. Continual learning is achieved by\ninterspersing learning of grounded neural concepts with higher level symbolic\nconstructs. Our experiments demonstrate that our approach significantly\noutperforms existing baselines in terms of its ability to learn novel concepts\nand generalize inductively.\n","authors":["Namasivayam Kalithasan","Sachit Sachdeva","Himanshu Gaurav Singh","Divyanshu Aggarwal","Gurarmaan Singh Panjeta","Vishal Bindal","Arnav Tuli","Rohan Paul","Parag Singla"],"pdf_url":"https://arxiv.org/pdf/2404.07774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07771v1","updated":"2024-04-11T14:07:25Z","published":"2024-04-11T14:07:25Z","title":"An Overview of Diffusion Models: Applications, Guided Generation,\n  Statistical Rates and Optimization","summary":"  Diffusion models, a powerful and universal generative AI technology, have\nachieved tremendous success in computer vision, audio, reinforcement learning,\nand computational biology. In these applications, diffusion models provide\nflexible high-dimensional data modeling, and act as a sampler for generating\nnew samples under active guidance towards task-desired properties. Despite the\nsignificant empirical success, theory of diffusion models is very limited,\npotentially slowing down principled methodological innovations for further\nharnessing and improving diffusion models. In this paper, we review emerging\napplications of diffusion models, understanding their sample generation under\nvarious controls. Next, we overview the existing theories of diffusion models,\ncovering their statistical properties and sampling capabilities. We adopt a\nprogressive routine, beginning with unconditional diffusion models and\nconnecting to conditional counterparts. Further, we review a new avenue in\nhigh-dimensional structured optimization through conditional diffusion models,\nwhere searching for solutions is reformulated as a conditional sampling problem\nand solved by diffusion models. Lastly, we discuss future directions about\ndiffusion models. The purpose of this paper is to provide a well-rounded\ntheoretical exposure for stimulating forward-looking theories and methods of\ndiffusion models.\n","authors":["Minshuo Chen","Song Mei","Jianqing Fan","Mengdi Wang"],"pdf_url":"https://arxiv.org/pdf/2404.07771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07765v1","updated":"2024-04-11T14:04:36Z","published":"2024-04-11T14:04:36Z","title":"AnnoCTR: A Dataset for Detecting and Linking Entities, Tactics, and\n  Techniques in Cyber Threat Reports","summary":"  Monitoring the threat landscape to be aware of actual or potential attacks is\nof utmost importance to cybersecurity professionals. Information about cyber\nthreats is typically distributed using natural language reports. Natural\nlanguage processing can help with managing this large amount of unstructured\ninformation, yet to date, the topic has received little attention. With this\npaper, we present AnnoCTR, a new CC-BY-SA-licensed dataset of cyber threat\nreports. The reports have been annotated by a domain expert with named\nentities, temporal expressions, and cybersecurity-specific concepts including\nimplicitly mentioned techniques and tactics. Entities and concepts are linked\nto Wikipedia and the MITRE ATT&CK knowledge base, the most widely-used taxonomy\nfor classifying types of attacks. Prior datasets linking to MITRE ATT&CK either\nprovide a single label per document or annotate sentences out-of-context; our\ndataset annotates entire documents in a much finer-grained way. In an\nexperimental study, we model the annotations of our dataset using\nstate-of-the-art neural models. In our few-shot scenario, we find that for\nidentifying the MITRE ATT&CK concepts that are mentioned explicitly or\nimplicitly in a text, concept descriptions from MITRE ATT&CK are an effective\nsource for training data augmentation.\n","authors":["Lukas Lange","Marc M√ºller","Ghazaleh Haratinezhad Torbati","Dragan Milchevski","Patrick Grau","Subhash Pujari","Annemarie Friedrich"],"pdf_url":"https://arxiv.org/pdf/2404.07765v1.pdf","comment":"Accepted at LREC-COLING 2024. Corpus available at\n  https://github.com/boschresearch/anno-ctr-lrec-coling-2024"},{"id":"http://arxiv.org/abs/2404.07754v1","updated":"2024-04-11T14:00:20Z","published":"2024-04-11T14:00:20Z","title":"Generating Synthetic Satellite Imagery With Deep-Learning Text-to-Image\n  Models -- Technical Challenges and Implications for Monitoring and\n  Verification","summary":"  Novel deep-learning (DL) architectures have reached a level where they can\ngenerate digital media, including photorealistic images, that are difficult to\ndistinguish from real data. These technologies have already been used to\ngenerate training data for Machine Learning (ML) models, and large\ntext-to-image models like DALL-E 2, Imagen, and Stable Diffusion are achieving\nremarkable results in realistic high-resolution image generation. Given these\ndevelopments, issues of data authentication in monitoring and verification\ndeserve a careful and systematic analysis: How realistic are synthetic images?\nHow easily can they be generated? How useful are they for ML researchers, and\nwhat is their potential for Open Science? In this work, we use novel DL models\nto explore how synthetic satellite images can be created using conditioning\nmechanisms. We investigate the challenges of synthetic satellite image\ngeneration and evaluate the results based on authenticity and state-of-the-art\nmetrics. Furthermore, we investigate how synthetic data can alleviate the lack\nof data in the context of ML methods for remote-sensing. Finally we discuss\nimplications of synthetic satellite imagery in the context of monitoring and\nverification.\n","authors":["Tuong Vy Nguyen","Alexander Glaser","Felix Biessmann"],"pdf_url":"https://arxiv.org/pdf/2404.07754v1.pdf","comment":"https://resources.inmm.org/annual-meeting-proceedings/generating-synthetic-satellite-imagery-deep-learning-text-image-models"},{"id":"http://arxiv.org/abs/2404.07753v1","updated":"2024-04-11T13:54:15Z","published":"2024-04-11T13:54:15Z","title":"Mitigating Vulnerable Road Users Occlusion Risk Via Collective\n  Perception: An Empirical Analysis","summary":"  Recent reports from the World Health Organization highlight that Vulnerable\nRoad Users (VRUs) have been involved in over half of the road fatalities in\nrecent years, with occlusion risk - a scenario where VRUs are hidden from\ndrivers' view by obstacles like parked vehicles - being a critical contributing\nfactor. To address this, we present a novel algorithm that quantifies occlusion\nrisk based on the dynamics of both vehicles and VRUs. This algorithm has\nundergone testing and evaluation using a real-world dataset from German\nintersections. Additionally, we introduce the concept of Maximum Tracking Loss\n(MTL), which measures the longest consecutive duration a VRU remains untracked\nby any vehicle in a given scenario. Our study extends to examining the role of\nthe Collective Perception Service (CPS) in VRU safety. CPS enhances safety by\nenabling vehicles to share sensor information, thereby potentially reducing\nocclusion risks. Our analysis reveals that a 25% market penetration of\nCPS-equipped vehicles can substantially diminish occlusion risks and\nsignificantly curtail MTL. These findings demonstrate how various scenarios\npose different levels of risk to VRUs and how the deployment of Collective\nPerception can markedly improve their safety. Furthermore, they underline the\nefficacy of our proposed metrics to capture occlusion risk as a safety factor.\n","authors":["Vincent Albert Wolff","Edmir Xhoxhi"],"pdf_url":"https://arxiv.org/pdf/2404.07753v1.pdf","comment":"Accepted for 35th IEEE Intelligent Vehicles Symposium 2024"},{"id":"http://arxiv.org/abs/2404.07748v1","updated":"2024-04-11T13:46:05Z","published":"2024-04-11T13:46:05Z","title":"3D-CSAD: Untrained 3D Anomaly Detection for Complex Manufacturing\n  Surfaces","summary":"  The surface quality inspection of manufacturing parts based on 3D point cloud\ndata has attracted increasing attention in recent years. The reason is that the\n3D point cloud can capture the entire surface of manufacturing parts, unlike\nthe previous practices that focus on some key product characteristics. However,\nachieving accurate 3D anomaly detection is challenging, due to the complex\nsurfaces of manufacturing parts and the difficulty of collecting sufficient\nanomaly samples. To address these challenges, we propose a novel untrained\nanomaly detection method based on 3D point cloud data for complex manufacturing\nparts, which can achieve accurate anomaly detection in a single sample without\ntraining data. In the proposed framework, we transform an input sample into two\nsets of profiles along different directions. Based on one set of the profiles,\na novel segmentation module is devised to segment the complex surface into\nmultiple basic and simple components. In each component, another set of\nprofiles, which have the nature of similar shapes, can be modeled as a low-rank\nmatrix. Thus, accurate 3D anomaly detection can be achieved by using Robust\nPrincipal Component Analysis (RPCA) on these low-rank matrices. Extensive\nnumerical experiments on different types of parts show that our method achieves\npromising results compared with the benchmark methods.\n","authors":["Xuanming Cao","Chengyu Tao","Juan Du"],"pdf_url":"https://arxiv.org/pdf/2404.07748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.05539v2","updated":"2024-04-11T13:39:18Z","published":"2023-11-09T17:34:57Z","title":"A Deep Learning Method for Simultaneous Denoising and Missing Wedge\n  Reconstruction in Cryogenic Electron Tomography","summary":"  Cryogenic electron tomography is a technique for imaging biological samples\nin 3D. A microscope collects a series of 2D projections of the sample, and the\ngoal is to reconstruct the 3D density of the sample called the tomogram.\nReconstruction is difficult as the 2D projections are noisy and can not be\nrecorded from all directions, resulting in a missing wedge of information.\nTomograms conventionally reconstructed with filtered back-projection suffer\nfrom noise and strong artifacts due to the missing wedge. Here, we propose a\ndeep-learning approach for simultaneous denoising and missing wedge\nreconstruction called DeepDeWedge. The algorithm requires no ground truth data\nand is based on fitting a neural network to the 2D projections using a\nself-supervised loss. DeepDeWedge performs better than CryoCARE and IsoNet,\nwhich are state-of-the-art methods for denoising and missing wedge\nreconstruction, and similarly and, in some cases, better than the combination\nof the two methods. At the same time, DeepDeWedge is simpler than this two-step\napproach, as it does denoising and missing wedge reconstruction simultaneously\nrather than sequentially.\n","authors":["Simon Wiedemann","Reinhard Heckel"],"pdf_url":"https://arxiv.org/pdf/2311.05539v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06831v2","updated":"2024-04-11T13:38:13Z","published":"2024-04-10T08:47:57Z","title":"Optimal Regret with Limited Adaptivity for Generalized Linear Contextual\n  Bandits","summary":"  We study the generalized linear contextual bandit problem within the\nrequirements of limited adaptivity. In this paper, we present two algorithms,\nB-GLinCB and RS-GLinCB, that address, respectively, two prevalent limited\nadaptivity models: batch learning with stochastic contexts and rare policy\nswitches with adversarial contexts. For both these models, we establish\nessentially tight regret bounds. Notably, in the obtained bounds, we manage to\neliminate a dependence on a key parameter $\\kappa$, which captures the\nnon-linearity of the underlying reward model. For our batch learning algorithm\nB-GLinCB, with $\\Omega\\left( \\log{\\log T} \\right)$ batches, the regret scales\nas $\\tilde{O}(\\sqrt{T})$. Further, we establish that our rarely switching\nalgorithm RS-GLinCB updates its policy at most $\\tilde{O}(\\log^2 T)$ times and\nachieves a regret of $\\tilde{O}(\\sqrt{T})$. Our approach for removing the\ndependence on $\\kappa$ for generalized linear contextual bandits might be of\nindependent interest.\n","authors":["Ayush Sawarni","Nirjhar Das","Siddharth Barman","Gaurav Sinha"],"pdf_url":"https://arxiv.org/pdf/2404.06831v2.pdf","comment":"31 pages"},{"id":"http://arxiv.org/abs/2404.07738v1","updated":"2024-04-11T13:36:29Z","published":"2024-04-11T13:36:29Z","title":"ResearchAgent: Iterative Research Idea Generation over Scientific\n  Literature with Large Language Models","summary":"  Scientific Research, vital for improving human life, is hindered by its\ninherent complexity, slow pace, and the need for specialized experts. To\nenhance its productivity, we propose a ResearchAgent, a large language\nmodel-powered research idea writing agent, which automatically generates\nproblems, methods, and experiment designs while iteratively refining them based\non scientific literature. Specifically, starting with a core paper as the\nprimary focus to generate ideas, our ResearchAgent is augmented not only with\nrelevant publications through connecting information over an academic graph but\nalso entities retrieved from an entity-centric knowledge store based on their\nunderlying concepts, mined and shared across numerous papers. In addition,\nmirroring the human approach to iteratively improving ideas with peer\ndiscussions, we leverage multiple ReviewingAgents that provide reviews and\nfeedback iteratively. Further, they are instantiated with human\npreference-aligned large language models whose criteria for evaluation are\nderived from actual human judgments. We experimentally validate our\nResearchAgent on scientific publications across multiple disciplines,\nshowcasing its effectiveness in generating novel, clear, and valid research\nideas based on human and model-based evaluation results.\n","authors":["Jinheon Baek","Sujay Kumar Jauhar","Silviu Cucerzan","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2404.07738v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07732v1","updated":"2024-04-11T13:25:35Z","published":"2024-04-11T13:25:35Z","title":"Monte Carlo Tree Search with Boltzmann Exploration","summary":"  Monte-Carlo Tree Search (MCTS) methods, such as Upper Confidence Bound\napplied to Trees (UCT), are instrumental to automated planning techniques.\nHowever, UCT can be slow to explore an optimal action when it initially appears\ninferior to other actions. Maximum ENtropy Tree-Search (MENTS) incorporates the\nmaximum entropy principle into an MCTS approach, utilising Boltzmann policies\nto sample actions, naturally encouraging more exploration. In this paper, we\nhighlight a major limitation of MENTS: optimal actions for the maximum entropy\nobjective do not necessarily correspond to optimal actions for the original\nobjective. We introduce two algorithms, Boltzmann Tree Search (BTS) and\nDecaying ENtropy Tree-Search (DENTS), that address these limitations and\npreserve the benefits of Boltzmann policies, such as allowing actions to be\nsampled faster by using the Alias method. Our empirical analysis shows that our\nalgorithms show consistent high performance across several benchmark domains,\nincluding the game of Go.\n","authors":["Michael Painter","Mohamed Baioumy","Nick Hawes","Bruno Lacerda"],"pdf_url":"https://arxiv.org/pdf/2404.07732v1.pdf","comment":"Camera ready version of NeurIPS2023 paper"},{"id":"http://arxiv.org/abs/2404.07729v1","updated":"2024-04-11T13:19:46Z","published":"2024-04-11T13:19:46Z","title":"Realistic Continual Learning Approach using Pre-trained Models","summary":"  Continual learning (CL) is crucial for evaluating adaptability in learning\nsolutions to retain knowledge. Our research addresses the challenge of\ncatastrophic forgetting, where models lose proficiency in previously learned\ntasks as they acquire new ones. While numerous solutions have been proposed,\nexisting experimental setups often rely on idealized class-incremental learning\nscenarios. We introduce Realistic Continual Learning (RealCL), a novel CL\nparadigm where class distributions across tasks are random, departing from\nstructured setups.\n  We also present CLARE (Continual Learning Approach with pRE-trained models\nfor RealCL scenarios), a pre-trained model-based solution designed to integrate\nnew knowledge while preserving past learning. Our contributions include\npioneering RealCL as a generalization of traditional CL setups, proposing CLARE\nas an adaptable approach for RealCL tasks, and conducting extensive experiments\ndemonstrating its effectiveness across various RealCL scenarios. Notably, CLARE\noutperforms existing models on RealCL benchmarks, highlighting its versatility\nand robustness in unpredictable learning environments.\n","authors":["Nadia Nasri","Carlos Guti√©rrez-√Ålvarez","Sergio Lafuente-Arroyo","Saturnino Maldonado-Basc√≥n","Roberto J. L√≥pez-Sastre"],"pdf_url":"https://arxiv.org/pdf/2404.07729v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07724v1","updated":"2024-04-11T13:16:47Z","published":"2024-04-11T13:16:47Z","title":"Applying Guidance in a Limited Interval Improves Sample and Distribution\n  Quality in Diffusion Models","summary":"  Guidance is a crucial technique for extracting the best performance out of\nimage-generating diffusion models. Traditionally, a constant guidance weight\nhas been applied throughout the sampling chain of an image. We show that\nguidance is clearly harmful toward the beginning of the chain (high noise\nlevels), largely unnecessary toward the end (low noise levels), and only\nbeneficial in the middle. We thus restrict it to a specific range of noise\nlevels, improving both the inference speed and result quality. This limited\nguidance interval improves the record FID in ImageNet-512 significantly, from\n1.81 to 1.40. We show that it is quantitatively and qualitatively beneficial\nacross different sampler parameters, network architectures, and datasets,\nincluding the large-scale setting of Stable Diffusion XL. We thus suggest\nexposing the guidance interval as a hyperparameter in all diffusion models that\nuse guidance.\n","authors":["Tuomas Kynk√§√§nniemi","Miika Aittala","Tero Karras","Samuli Laine","Timo Aila","Jaakko Lehtinen"],"pdf_url":"https://arxiv.org/pdf/2404.07724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.04067v2","updated":"2024-04-11T13:10:30Z","published":"2024-04-05T12:51:37Z","title":"CLUE: A Clinical Language Understanding Evaluation for LLMs","summary":"  Large Language Models (LLMs) have shown the potential to significantly\ncontribute to patient care, diagnostics, and administrative processes. Emerging\nbiomedical LLMs address healthcare-specific challenges, including privacy\ndemands and computational constraints. However, evaluation of these models has\nprimarily been limited to non-clinical tasks, which do not reflect the\ncomplexity of practical clinical applications. Additionally, there has been no\nthorough comparison between biomedical and general-domain LLMs for clinical\ntasks. To fill this gap, we present the Clinical Language Understanding\nEvaluation (CLUE), a benchmark tailored to evaluate LLMs on real-world clinical\ntasks. CLUE includes two novel datasets derived from MIMIC IV discharge letters\nand four existing tasks designed to test the practical applicability of LLMs in\nhealthcare settings. Our evaluation covers several biomedical and general\ndomain LLMs, providing insights into their clinical performance and\napplicability. CLUE represents a step towards a standardized approach to\nevaluating and developing LLMs in healthcare to align future model development\nwith the real-world needs of clinical application. We publish our evaluation\nand data generation scripts: https://github.com/TIO-IKIM/CLUE.\n","authors":["Amin Dada","Marie Bauer","Amanda Butler Contreras","Osman Alperen Kora≈ü","Constantin Marc Seibold","Kaleb E Smith","Jens Kleesiek"],"pdf_url":"https://arxiv.org/pdf/2404.04067v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.15948v2","updated":"2024-04-11T13:07:04Z","published":"2024-01-29T08:13:51Z","title":"AdvNF: Reducing Mode Collapse in Conditional Normalising Flows using\n  Adversarial Learning","summary":"  Deep generative models complement Markov-chain-Monte-Carlo methods for\nefficiently sampling from high-dimensional distributions. Among these methods,\nexplicit generators, such as Normalising Flows (NFs), in combination with the\nMetropolis Hastings algorithm have been extensively applied to get unbiased\nsamples from target distributions. We systematically study central problems in\nconditional NFs, such as high variance, mode collapse and data efficiency. We\npropose adversarial training for NFs to ameliorate these problems. Experiments\nare conducted with low-dimensional synthetic datasets and XY spin models in two\nspatial dimensions.\n","authors":["Vikas Kanaujia","Mathias S. Scheurer","Vipul Arora"],"pdf_url":"https://arxiv.org/pdf/2401.15948v2.pdf","comment":"29 pages, submitted to Scipost Physics"},{"id":"http://arxiv.org/abs/2404.03936v2","updated":"2024-04-11T13:02:58Z","published":"2024-04-05T07:44:17Z","title":"Deep Learning for Satellite Image Time Series Analysis: A Review","summary":"  Earth observation (EO) satellite missions have been providing detailed images\nabout the state of the Earth and its land cover for over 50 years. Long term\nmissions, such as NASA's Landsat, Terra, and Aqua satellites, and more\nrecently, the ESA's Sentinel missions, record images of the entire world every\nfew days. Although single images provide point-in-time data, repeated images of\nthe same area, or satellite image time series (SITS) provide information about\nthe changing state of vegetation and land use. These SITS are useful for\nmodeling dynamic processes and seasonal changes such as plant phenology. They\nhave potential benefits for many aspects of land and natural resource\nmanagement, including applications in agricultural, forest, water, and disaster\nmanagement, urban planning, and mining. However, the resulting satellite image\ntime series (SITS) are complex, incorporating information from the temporal,\nspatial, and spectral dimensions. Therefore, deep learning methods are often\ndeployed as they can analyze these complex relationships. This review presents\na summary of the state-of-the-art methods of modelling environmental,\nagricultural, and other Earth observation variables from SITS data using deep\nlearning methods. We aim to provide a resource for remote sensing experts\ninterested in using deep learning techniques to enhance Earth observation\nmodels with temporal information.\n","authors":["Lynn Miller","Charlotte Pelletier","Geoffrey I. Webb"],"pdf_url":"https://arxiv.org/pdf/2404.03936v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2404.07713v1","updated":"2024-04-11T12:59:38Z","published":"2024-04-11T12:59:38Z","title":"Progressive Semantic-Guided Vision Transformer for Zero-Shot Learning","summary":"  Zero-shot learning (ZSL) recognizes the unseen classes by conducting\nvisual-semantic interactions to transfer semantic knowledge from seen classes\nto unseen ones, supported by semantic information (e.g., attributes). However,\nexisting ZSL methods simply extract visual features using a pre-trained network\nbackbone (i.e., CNN or ViT), which fail to learn matched visual-semantic\ncorrespondences for representing semantic-related visual features as lacking of\nthe guidance of semantic information, resulting in undesirable visual-semantic\ninteractions. To tackle this issue, we propose a progressive semantic-guided\nvision transformer for zero-shot learning (dubbed ZSLViT). ZSLViT mainly\nconsiders two properties in the whole network: i) discover the semantic-related\nvisual representations explicitly, and ii) discard the semantic-unrelated\nvisual information. Specifically, we first introduce semantic-embedded token\nlearning to improve the visual-semantic correspondences via semantic\nenhancement and discover the semantic-related visual tokens explicitly with\nsemantic-guided token attention. Then, we fuse low semantic-visual\ncorrespondence visual tokens to discard the semantic-unrelated visual\ninformation for visual enhancement. These two operations are integrated into\nvarious encoders to progressively learn semantic-related visual representations\nfor accurate visual-semantic interactions in ZSL. The extensive experiments\nshow that our ZSLViT achieves significant performance gains on three popular\nbenchmark datasets, i.e., CUB, SUN, and AWA2.\n","authors":["Shiming Chen","Wenjin Hou","Salman Khan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2404.07713v1.pdf","comment":"Accepted to CVPR'24"},{"id":"http://arxiv.org/abs/2404.07703v1","updated":"2024-04-11T12:49:30Z","published":"2024-04-11T12:49:30Z","title":"Learning Hamiltonian Dynamics with Reproducing Kernel Hilbert Spaces and\n  Random Features","summary":"  A method for learning Hamiltonian dynamics from a limited and noisy dataset\nis proposed. The method learns a Hamiltonian vector field on a reproducing\nkernel Hilbert space (RKHS) of inherently Hamiltonian vector fields, and in\nparticular, odd Hamiltonian vector fields. This is done with a symplectic\nkernel, and it is shown how the kernel can be modified to an odd symplectic\nkernel to impose the odd symmetry. A random feature approximation is developed\nfor the proposed kernel to reduce the problem size. This includes random\nfeature approximations for odd kernels. The performance of the method is\nvalidated in simulations for three Hamiltonian systems. It is demonstrated that\nthe use of an odd symplectic kernel improves prediction accuracy, and that the\nlearned vector fields are Hamiltonian and exhibit the imposed odd symmetry\ncharacteristics.\n","authors":["Torbj√∏rn Smith","Olav Egeland"],"pdf_url":"https://arxiv.org/pdf/2404.07703v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2312.09734"},{"id":"http://arxiv.org/abs/2404.07698v1","updated":"2024-04-11T12:44:15Z","published":"2024-04-11T12:44:15Z","title":"Point Cloud Geometry Scalable Coding with a Quality-Conditioned Latents\n  Probability Estimator","summary":"  The widespread usage of point clouds (PC) for immersive visual applications\nhas resulted in the use of very heterogeneous receiving conditions and devices,\nnotably in terms of network, hardware, and display capabilities. In this\nscenario, quality scalability, i.e., the ability to reconstruct a signal at\ndifferent qualities by progressively decoding a single bitstream, is a major\nrequirement that has yet to be conveniently addressed, notably in most\nlearning-based PC coding solutions. This paper proposes a quality scalability\nscheme, named Scalable Quality Hyperprior (SQH), adaptable to learning-based\nstatic point cloud geometry codecs, which uses a Quality-conditioned Latents\nProbability Estimator (QuLPE) to decode a high-quality version of a PC\nlearning-based representation, based on an available lower quality base layer.\nSQH is integrated in the future JPEG PC coding standard, allowing to create a\nlayered bitstream that can be used to progressively decode the PC geometry with\nincreasing quality and fidelity. Experimental results show that SQH offers the\nquality scalability feature with very limited or no compression performance\npenalty at all when compared with the corresponding non-scalable solution, thus\npreserving the significant compression gains over other state-of-the-art PC\ncodecs.\n","authors":["Daniele Mari","Andr√© F. R. Guarda","Nuno M. M. Rodrigues","Simone Milani","Fernando Pereira"],"pdf_url":"https://arxiv.org/pdf/2404.07698v1.pdf","comment":"Submitted at ICIP 2024"},{"id":"http://arxiv.org/abs/2404.07696v1","updated":"2024-04-11T12:42:18Z","published":"2024-04-11T12:42:18Z","title":"Flatness Improves Backbone Generalisation in Few-shot Classification","summary":"  Deployment of deep neural networks in real-world settings typically requires\nadaptation to new tasks with few examples. Few-shot classification (FSC)\nprovides a solution to this problem by leveraging pre-trained backbones for\nfast adaptation to new classes. Surprisingly, most efforts have only focused on\ndeveloping architectures for easing the adaptation to the target domain without\nconsidering the importance of backbone training for good generalisation. We\nshow that flatness-aware backbone training with vanilla fine-tuning results in\na simpler yet competitive baseline compared to the state-of-the-art. Our\nresults indicate that for in- and cross-domain FSC, backbone training is\ncrucial to achieving good generalisation across different adaptation methods.\nWe advocate more care should be taken when training these models.\n","authors":["Rui Li","Martin Trapp","Marcus Klasson","Arno Solin"],"pdf_url":"https://arxiv.org/pdf/2404.07696v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07673v1","updated":"2024-04-11T12:09:47Z","published":"2024-04-11T12:09:47Z","title":"Curated Datasets and Neural Models for Machine Translation of Informal\n  Registers between Mayan and Spanish Vernaculars","summary":"  The Mayan languages comprise a language family with an ancient history,\nmillions of speakers, and immense cultural value, that, nevertheless, remains\nseverely underrepresented in terms of resources and global exposure. In this\npaper we develop, curate, and publicly release a set of corpora in several\nMayan languages spoken in Guatemala and Southern Mexico, which we call MayanV.\nThe datasets are parallel with Spanish, the dominant language of the region,\nand are taken from official native sources focused on representing informal,\nday-to-day, and non-domain-specific language. As such, and according to our\ndialectometric analysis, they differ in register from most other available\nresources. Additionally, we present neural machine translation models, trained\non as many resources and Mayan languages as possible, and evaluated exclusively\non our datasets. We observe lexical divergences between the dialects of Spanish\nin our resources and the more widespread written standard of Spanish, and that\nresources other than the ones we present do not seem to improve translation\nperformance, indicating that many such resources may not accurately capture\ncommon, real-life language usage. The MayanV dataset is available at\nhttps://github.com/transducens/mayanv.\n","authors":["Andr√©s Lou","Juan Antonio P√©rez-Ortiz","Felipe S√°nchez-Mart√≠nez","V√≠ctor M. S√°nchez-Cartagena"],"pdf_url":"https://arxiv.org/pdf/2404.07673v1.pdf","comment":"13 pages, 3 figures, 8 tables, Submitted to NAACL 2024"},{"id":"http://arxiv.org/abs/2404.07663v1","updated":"2024-04-11T11:53:14Z","published":"2024-04-11T11:53:14Z","title":"Interactive Ontology Matching with Cost-Efficient Learning","summary":"  The creation of high-quality ontologies is crucial for data integration and\nknowledge-based reasoning, specifically in the context of the rising data\neconomy. However, automatic ontology matchers are often bound to the heuristics\nthey are based on, leaving many matches unidentified. Interactive ontology\nmatching systems involving human experts have been introduced, but they do not\nsolve the fundamental issue of flexibly finding additional matches outside the\nscope of the implemented heuristics, even though this is highly demanded in\nindustrial settings. Active machine learning methods appear to be a promising\npath towards a flexible interactive ontology matcher. However, off-the-shelf\nactive learning mechanisms suffer from low query efficiency due to extreme\nclass imbalance, resulting in a last-mile problem where high human effort is\nrequired to identify the remaining matches.\n  To address the last-mile problem, this work introduces DualLoop, an active\nlearning method tailored to ontology matching. DualLoop offers three main\ncontributions: (1) an ensemble of tunable heuristic matchers, (2) a short-term\nlearner with a novel query strategy adapted to highly imbalanced data, and (3)\nlong-term learners to explore potential matches by creating and tuning new\nheuristics. We evaluated DualLoop on three datasets of varying sizes and\ndomains. Compared to existing active learning methods, we consistently achieved\nbetter F1 scores and recall, reducing the expected query cost spent on finding\n90% of all matches by over 50%. Compared to traditional interactive ontology\nmatchers, we are able to find additional, last-mile matches. Finally, we detail\nthe successful deployment of our approach within an actual product and report\nits operational performance results within the Architecture, Engineering, and\nConstruction (AEC) industry sector, showcasing its practical value and\nefficiency.\n","authors":["Bin Cheng","Jonathan F√ºrst","Tobias Jacobs","Celia Garrido-Hidalgo"],"pdf_url":"https://arxiv.org/pdf/2404.07663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07662v1","updated":"2024-04-11T11:51:46Z","published":"2024-04-11T11:51:46Z","title":"PINNACLE: PINN Adaptive ColLocation and Experimental points selection","summary":"  Physics-Informed Neural Networks (PINNs), which incorporate PDEs as soft\nconstraints, train with a composite loss function that contains multiple\ntraining point types: different types of collocation points chosen during\ntraining to enforce each PDE and initial/boundary conditions, and experimental\npoints which are usually costly to obtain via experiments or simulations.\nTraining PINNs using this loss function is challenging as it typically requires\nselecting large numbers of points of different types, each with different\ntraining dynamics. Unlike past works that focused on the selection of either\ncollocation or experimental points, this work introduces PINN Adaptive\nColLocation and Experimental points selection (PINNACLE), the first algorithm\nthat jointly optimizes the selection of all training point types, while\nautomatically adjusting the proportion of collocation point types as training\nprogresses. PINNACLE uses information on the interaction among training point\ntypes, which had not been considered before, based on an analysis of PINN\ntraining dynamics via the Neural Tangent Kernel (NTK). We theoretically show\nthat the criterion used by PINNACLE is related to the PINN generalization\nerror, and empirically demonstrate that PINNACLE is able to outperform existing\npoint selection methods for forward, inverse, and transfer learning problems.\n","authors":["Gregory Kang Ruey Lau","Apivich Hemachandra","See-Kiong Ng","Bryan Kian Hsiang Low"],"pdf_url":"https://arxiv.org/pdf/2404.07662v1.pdf","comment":"Accepted to 12th International Conference on Learning Representations\n  (ICLR 2024), 36 pages"},{"id":"http://arxiv.org/abs/2404.07661v1","updated":"2024-04-11T11:50:05Z","published":"2024-04-11T11:50:05Z","title":"Robust performance metrics for imbalanced classification problems","summary":"  We show that established performance metrics in binary classification, such\nas the F-score, the Jaccard similarity coefficient or Matthews' correlation\ncoefficient (MCC), are not robust to class imbalance in the sense that if the\nproportion of the minority class tends to $0$, the true positive rate (TPR) of\nthe Bayes classifier under these metrics tends to $0$ as well. Thus, in\nimbalanced classification problems, these metrics favour classifiers which\nignore the minority class. To alleviate this issue we introduce robust\nmodifications of the F-score and the MCC for which, even in strongly imbalanced\nsettings, the TPR is bounded away from $0$. We numerically illustrate the\nbehaviour of the various performance metrics in simulations as well as on a\ncredit default data set. We also discuss connections to the ROC and\nprecision-recall curves and give recommendations on how to combine their usage\nwith performance metrics.\n","authors":["Hajo Holzmann","Bernhard Klar"],"pdf_url":"https://arxiv.org/pdf/2404.07661v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.01712v2","updated":"2024-04-11T11:42:34Z","published":"2024-04-02T07:54:18Z","title":"Efficient Online Unlearning via Hessian-Free Recollection of Individual\n  Data Statistics","summary":"  Machine unlearning strives to uphold the data owners' right to be forgotten\nby enabling models to selectively forget specific data. Recent methods suggest\nthat one approach of data forgetting is by precomputing and storing statistics\ncarrying second-order information to improve computational and memory\nefficiency. However, they rely on restrictive assumptions and the\ncomputation/storage suffer from the curse of model parameter dimensionality,\nmaking it challenging to apply to most deep neural networks. In this work, we\npropose a Hessian-free online unlearning method. We propose to maintain a\nstatistical vector for each data point, computed through affine stochastic\nrecursion approximation of the difference between retrained and learned models.\nOur proposed algorithm achieves near-instantaneous online unlearning as it only\nrequires a vector addition operation. Based on the strategy that recollecting\nstatistics for forgetting data, the proposed method significantly reduces the\nunlearning runtime. Experimental studies demonstrate that the proposed scheme\nsurpasses existing results by orders of magnitude in terms of time and memory\ncosts, while also enhancing accuracy.\n","authors":["Xinbao Qiao","Meng Zhang","Ming Tang","Ermin Wei"],"pdf_url":"https://arxiv.org/pdf/2404.01712v2.pdf","comment":"25 pages, 8 figures"},{"id":"http://arxiv.org/abs/2312.01957v3","updated":"2024-04-11T10:54:19Z","published":"2023-12-04T15:16:12Z","title":"Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian\n  Perspective","summary":"  This paper proposes an interpretation of RLAIF as Bayesian inference by\nintroducing distilled Self-Critique (dSC), which refines the outputs of a LLM\nthrough a Gibbs sampler that is later distilled into a fine-tuned model. Only\nrequiring synthetic data, dSC is exercised in experiments regarding safety,\nsentiment, and privacy control, showing it can be a viable and cheap\nalternative to align LLMs. Code released at\n\\url{https://github.com/vicgalle/distilled-self-critique}.\n","authors":["Victor Gallego"],"pdf_url":"https://arxiv.org/pdf/2312.01957v3.pdf","comment":"Accepted to ICLR 2024 (TinyPapers track)"},{"id":"http://arxiv.org/abs/2310.11389v2","updated":"2024-04-11T10:18:34Z","published":"2023-10-17T16:35:39Z","title":"Risk Estimation in a Markov Cost Process: Lower and Upper Bounds","summary":"  We tackle the problem of estimating risk measures of the infinite-horizon\ndiscounted cost within a Markov cost process. The risk measures we study\ninclude variance, Value-at-Risk (VaR), and Conditional Value-at-Risk (CVaR).\nFirst, we show that estimating any of these risk measures with\n$\\epsilon$-accuracy, either in expected or high-probability sense, requires at\nleast $\\Omega(1/\\epsilon^2)$ samples. Then, using a truncation scheme, we\nderive an upper bound for the CVaR and variance estimation. This bound matches\nour lower bound up to logarithmic factors. Finally, we discuss an extension of\nour estimation scheme that covers more general risk measures satisfying a\ncertain continuity criterion, e.g., spectral risk measures, utility-based\nshortfall risk. To the best of our knowledge, our work is the first to provide\nlower and upper bounds for estimating any risk measure beyond the mean within a\nMarkovian setting. Our lower bounds also extend to the infinite-horizon\ndiscounted costs' mean. Even in that case, our lower bound of\n$\\Omega(1/\\epsilon^2) $ improves upon the existing $\\Omega(1/\\epsilon)$ bound\n[13].\n","authors":["Gugan Thoppe","L. A. Prashanth","Sanjay Bhat"],"pdf_url":"https://arxiv.org/pdf/2310.11389v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19852v2","updated":"2024-04-11T10:10:03Z","published":"2024-03-28T21:54:48Z","title":"A Review of Graph Neural Networks in Epidemic Modeling","summary":"  Since the onset of the COVID-19 pandemic, there has been a growing interest\nin studying epidemiological models. Traditional mechanistic models\nmathematically describe the transmission mechanisms of infectious diseases.\nHowever, they often fall short when confronted with the growing challenges of\ntoday. Consequently, Graph Neural Networks (GNNs) have emerged as a\nprogressively popular tool in epidemic research. In this paper, we endeavor to\nfurnish a comprehensive review of GNNs in epidemic tasks and highlight\npotential future directions. To accomplish this objective, we introduce\nhierarchical taxonomies for both epidemic tasks and methodologies, offering a\ntrajectory of development within this domain. For epidemic tasks, we establish\na taxonomy akin to those typically employed within the epidemic domain. For\nmethodology, we categorize existing work into \\textit{Neural Models} and\n\\textit{Hybrid Models}. Following this, we perform an exhaustive and systematic\nexamination of the methodologies, encompassing both the tasks and their\ntechnical details. Furthermore, we discuss the limitations of existing methods\nfrom diverse perspectives and systematically propose future research\ndirections. This survey aims to bridge literature gaps and promote the\nprogression of this promising field. We hope that it will facilitate synergies\nbetween the communities of GNNs and epidemiology, and contribute to their\ncollective progress.\n","authors":["Zewen Liu","Guancheng Wan","B. Aditya Prakash","Max S. Y. Lau","Wei Jin"],"pdf_url":"https://arxiv.org/pdf/2403.19852v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07613v1","updated":"2024-04-11T10:01:32Z","published":"2024-04-11T10:01:32Z","title":"Medical mT5: An Open-Source Multilingual Text-to-Text LLM for The\n  Medical Domain","summary":"  Research on language technology for the development of medical applications\nis currently a hot topic in Natural Language Understanding and Generation.\nThus, a number of large language models (LLMs) have recently been adapted to\nthe medical domain, so that they can be used as a tool for mediating in\nhuman-AI interaction. While these LLMs display competitive performance on\nautomated medical texts benchmarks, they have been pre-trained and evaluated\nwith a focus on a single language (English mostly). This is particularly true\nof text-to-text models, which typically require large amounts of\ndomain-specific pre-training data, often not easily accessible for many\nlanguages. In this paper, we address these shortcomings by compiling, to the\nbest of our knowledge, the largest multilingual corpus for the medical domain\nin four languages, namely English, French, Italian and Spanish. This new corpus\nhas been used to train Medical mT5, the first open-source text-to-text\nmultilingual model for the medical domain. Additionally, we present two new\nevaluation benchmarks for all four languages with the aim of facilitating\nmultilingual research in this domain. A comprehensive evaluation shows that\nMedical mT5 outperforms both encoders and similarly sized text-to-text models\nfor the Spanish, French, and Italian benchmarks, while being competitive with\ncurrent state-of-the-art LLMs in English.\n","authors":["Iker Garc√≠a-Ferrero","Rodrigo Agerri","Aitziber Atutxa Salazar","Elena Cabrio","Iker de la Iglesia","Alberto Lavelli","Bernardo Magnini","Benjamin Molinet","Johana Ramirez-Romero","German Rigau","Jose Maria Villa-Gonzalez","Serena Villata","Andrea Zaninello"],"pdf_url":"https://arxiv.org/pdf/2404.07613v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.16099v2","updated":"2024-04-11T09:58:17Z","published":"2024-03-24T11:29:55Z","title":"A Multi-Label Dataset of French Fake News: Human and Machine Insights","summary":"  We present a corpus of 100 documents, OBSINFOX, selected from 17 sources of\nFrench press considered unreliable by expert agencies, annotated using 11\nlabels by 8 annotators. By collecting more labels than usual, by more\nannotators than is typically done, we can identify features that humans\nconsider as characteristic of fake news, and compare them to the predictions of\nautomated classifiers. We present a topic and genre analysis using Gate Cloud,\nindicative of the prevalence of satire-like text in the corpus. We then use the\nsubjectivity analyzer VAGO, and a neural version of it, to clarify the link\nbetween ascriptions of the label Subjective and ascriptions of the label Fake\nNews. The annotated dataset is available online at the following url:\nhttps://github.com/obs-info/obsinfox\n  Keywords: Fake News, Multi-Labels, Subjectivity, Vagueness, Detail, Opinion,\nExaggeration, French Press\n","authors":["Benjamin Icard","Fran√ßois Maine","Morgane Casanova","G√©raud Faye","Julien Chanson","Guillaume Gadek","Ghislain Atemezing","Fran√ßois Bancilhon","Paul √âgr√©"],"pdf_url":"https://arxiv.org/pdf/2403.16099v2.pdf","comment":"Paper to appear in the Proceedings of the 2024 Joint International\n  Conference on Computational Linguistics, Language Resources and Evaluation\n  (LREC-COLING 2024)"},{"id":"http://arxiv.org/abs/2304.13653v2","updated":"2024-04-11T09:50:07Z","published":"2023-04-26T16:25:54Z","title":"Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement\n  Learning","summary":"  We investigate whether Deep Reinforcement Learning (Deep RL) is able to\nsynthesize sophisticated and safe movement skills for a low-cost, miniature\nhumanoid robot that can be composed into complex behavioral strategies in\ndynamic environments. We used Deep RL to train a humanoid robot with 20\nactuated joints to play a simplified one-versus-one (1v1) soccer game. The\nresulting agent exhibits robust and dynamic movement skills such as rapid fall\nrecovery, walking, turning, kicking and more; and it transitions between them\nin a smooth, stable, and efficient manner. The agent's locomotion and tactical\nbehavior adapts to specific game contexts in a way that would be impractical to\nmanually design. The agent also developed a basic strategic understanding of\nthe game, and learned, for instance, to anticipate ball movements and to block\nopponent shots. Our agent was trained in simulation and transferred to real\nrobots zero-shot. We found that a combination of sufficiently high-frequency\ncontrol, targeted dynamics randomization, and perturbations during training in\nsimulation enabled good-quality transfer. Although the robots are inherently\nfragile, basic regularization of the behavior during training led the robots to\nlearn safe and effective movements while still performing in a dynamic and\nagile way -- well beyond what is intuitively expected from the robot. Indeed,\nin experiments, they walked 181% faster, turned 302% faster, took 63% less time\nto get up, and kicked a ball 34% faster than a scripted baseline, while\nefficiently combining the skills to achieve the longer term objectives.\n","authors":["Tuomas Haarnoja","Ben Moran","Guy Lever","Sandy H. Huang","Dhruva Tirumala","Jan Humplik","Markus Wulfmeier","Saran Tunyasuvunakool","Noah Y. Siegel","Roland Hafner","Michael Bloesch","Kristian Hartikainen","Arunkumar Byravan","Leonard Hasenclever","Yuval Tassa","Fereshteh Sadeghi","Nathan Batchelor","Federico Casarini","Stefano Saliceti","Charles Game","Neil Sreendra","Kushal Patel","Marlon Gwira","Andrea Huber","Nicole Hurley","Francesco Nori","Raia Hadsell","Nicolas Heess"],"pdf_url":"https://arxiv.org/pdf/2304.13653v2.pdf","comment":"Project website: https://sites.google.com/view/op3-soccer"},{"id":"http://arxiv.org/abs/2404.05538v2","updated":"2024-04-11T09:45:13Z","published":"2024-04-08T14:06:52Z","title":"Cell-Free Multi-User MIMO Equalization via In-Context Learning","summary":"  Large pre-trained sequence models, such as transformers, excel as few-shot\nlearners capable of in-context learning (ICL). In ICL, a model is trained to\nadapt its operation to a new task based on limited contextual information,\ntypically in the form of a few training examples for the given task. Previous\nwork has explored the use of ICL for channel equalization in single-user\nmulti-input and multiple-output (MIMO) systems. In this work, we demonstrate\nthat ICL can be also used to tackle the problem of multi-user equalization in\ncell-free MIMO systems with limited fronthaul capacity. In this scenario, a\ntask is defined by channel statistics, signal-to-noise ratio, and modulation\nschemes. The context encompasses the users' pilot sequences, the corresponding\nquantized received signals, and the current received data signal. Different\nprompt design strategies are proposed and evaluated that encompass also\nlarge-scale fading and modulation information. Experiments demonstrate that\nICL-based equalization provides estimates with lower mean squared error as\ncompared to the linear minimum mean squared error equalizer, especially in the\npresence of limited fronthaul capacity and pilot contamination.\n","authors":["Matteo Zecchin","Kai Yu","Osvaldo Simeone"],"pdf_url":"https://arxiv.org/pdf/2404.05538v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07602v1","updated":"2024-04-11T09:41:14Z","published":"2024-04-11T09:41:14Z","title":"Attention based End to end network for Offline Writer Identification on\n  Word level data","summary":"  Writer identification due to its widespread application in various fields has\ngained popularity over the years. In scenarios where optimum handwriting\nsamples are available, whether they be in the form of a single line, a\nsentence, or an entire page, writer identification algorithms have demonstrated\nnoteworthy levels of accuracy. However, in scenarios where only a limited\nnumber of handwritten samples are available, particularly in the form of word\nimages, there is a significant scope for improvement.\n  In this paper, we propose a writer identification system based on an\nattention-driven Convolutional Neural Network (CNN). The system is trained\nutilizing image segments, known as fragments, extracted from word images,\nemploying a pyramid-based strategy. This methodology enables the system to\ncapture a comprehensive representation of the data, encompassing both\nfine-grained details and coarse features across various levels of abstraction.\nThese extracted fragments serve as the training data for the convolutional\nnetwork, enabling it to learn a more robust representation compared to\ntraditional convolution-based networks trained on word images. Additionally,\nthe paper explores the integration of an attention mechanism to enhance the\nrepresentational power of the learned features. The efficacy of the proposed\nalgorithm is evaluated on three benchmark databases, demonstrating its\nproficiency in writer identification tasks, particularly in scenarios with\nlimited access to handwriting data.\n","authors":["Vineet Kumar","Suresh Sundaram"],"pdf_url":"https://arxiv.org/pdf/2404.07602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06313v2","updated":"2024-04-11T09:27:12Z","published":"2024-04-09T13:47:37Z","title":"On adversarial training and the 1 Nearest Neighbor classifier","summary":"  The ability to fool deep learning classifiers with tiny perturbations of the\ninput has lead to the development of adversarial training in which the loss\nwith respect to adversarial examples is minimized in addition to the training\nexamples. While adversarial training improves the robustness of the learned\nclassifiers, the procedure is computationally expensive, sensitive to\nhyperparameters and may still leave the classifier vulnerable to other types of\nsmall perturbations. In this paper we analyze the adversarial robustness of the\n1 Nearest Neighbor (1NN) classifier and compare its performance to adversarial\ntraining. We prove that under reasonable assumptions, the 1 NN classifier will\nbe robust to {\\em any} small image perturbation of the training images and will\ngive high adversarial accuracy on test images as the number of training\nexamples goes to infinity. In experiments with 45 different binary image\nclassification problems taken from CIFAR10, we find that 1NN outperform TRADES\n(a powerful adversarial training algorithm) in terms of average adversarial\naccuracy. In additional experiments with 69 pretrained robust models for\nCIFAR10, we find that 1NN outperforms almost all of them in terms of robustness\nto perturbations that are only slightly different from those seen during\ntraining. Taken together, our results suggest that modern adversarial training\nmethods still fall short of the robustness of the simple 1NN classifier. our\ncode can be found at\nhttps://github.com/amirhagai/On-Adversarial-Training-And-The-1-Nearest-Neighbor-Classifier\n","authors":["Amir Hagai","Yair Weiss"],"pdf_url":"https://arxiv.org/pdf/2404.06313v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07594v1","updated":"2024-04-11T09:23:44Z","published":"2024-04-11T09:23:44Z","title":"Weakly-Supervised Learning via Multi-Lateral Decoder Branching for\n  Guidewire Segmentation in Robot-Assisted Cardiovascular Catheterization","summary":"  Although robot-assisted cardiovascular catheterization is commonly performed\nfor intervention of cardiovascular diseases, more studies are needed to support\nthe procedure with automated tool segmentation. This can aid surgeons on tool\ntracking and visualization during intervention. Learning-based segmentation has\nrecently offered state-of-the-art segmentation performances however, generating\nground-truth signals for fully-supervised methods is labor-intensive and time\nconsuming for the interventionists. In this study, a weakly-supervised learning\nmethod with multi-lateral pseudo labeling is proposed for tool segmentation in\ncardiac angiograms. The method includes a modified U-Net model with one encoder\nand multiple lateral-branched decoders that produce pseudo labels as\nsupervision signals under different perturbation. The pseudo labels are\nself-generated through a mixed loss function and shared consistency in the\ndecoders. We trained the model end-to-end with weakly-annotated data obtained\nduring robotic cardiac catheterization. Experiments with the proposed model\nshows weakly annotated data has closer performance to when fully annotated data\nis used. Compared to three existing weakly-supervised methods, our approach\nyielded higher segmentation performance across three different cardiac\nangiogram data. With ablation study, we showed consistent performance under\ndifferent parameters. Thus, we offer a less expensive method for real-time tool\nsegmentation and tracking during robot-assisted cardiac catheterization.\n","authors":["Olatunji Mumini Omisore","Toluwanimi Akinyemi","Anh Nguyen","Lei Wang"],"pdf_url":"https://arxiv.org/pdf/2404.07594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07593v1","updated":"2024-04-11T09:23:36Z","published":"2024-04-11T09:23:36Z","title":"Diffusion posterior sampling for simulation-based inference in tall data\n  settings","summary":"  Determining which parameters of a non-linear model could best describe a set\nof experimental data is a fundamental problem in science and it has gained much\ntraction lately with the rise of complex large-scale simulators (a.k.a.\nblack-box simulators). The likelihood of such models is typically intractable,\nwhich is why classical MCMC methods can not be used. Simulation-based inference\n(SBI) stands out in this context by only requiring a dataset of simulations to\ntrain deep generative models capable of approximating the posterior\ndistribution that relates input parameters to a given observation. In this\nwork, we consider a tall data extension in which multiple observations are\navailable and one wishes to leverage their shared information to better infer\nthe parameters of the model. The method we propose is built upon recent\ndevelopments from the flourishing score-based diffusion literature and allows\nus to estimate the tall data posterior distribution simply using information\nfrom the score network trained on individual observations. We compare our\nmethod to recently proposed competing approaches on various numerical\nexperiments and demonstrate its superiority in terms of numerical stability and\ncomputational cost.\n","authors":["Julia Linhart","Gabriel Victorino Cardoso","Alexandre Gramfort","Sylvain Le Corff","Pedro L. C. Rodrigues"],"pdf_url":"https://arxiv.org/pdf/2404.07593v1.pdf","comment":"38 pages, 20 figures, 3 tables, 11 appendices"},{"id":"http://arxiv.org/abs/2311.09852v4","updated":"2024-04-11T09:17:41Z","published":"2023-11-16T12:28:31Z","title":"Short vs. Long-term Coordination of Drones: When Distributed\n  Optimization Meets Deep Reinforcement Learning","summary":"  Swarms of autonomous interactive drones, with the support of recharging\ntechnology, can provide compelling sensing capabilities in Smart Cities, such\nas traffic monitoring and disaster response. This paper aims to deliver a novel\ncoordination solution for the cost-effective navigation, sensing, and\nrecharging of drones. Existing approaches, such as deep reinforcement learning\n(DRL), offer long-term adaptability, but lack energy efficiency, resilience,\nand flexibility in dynamic environments. Therefore, this paper proposes a novel\napproach where each drone independently determines its flying direction and\nrecharging place using DRL, while adapting navigation and sensing through\ndistributed optimization, which improves energy-efficiency during sensing\ntasks. Furthermore, drones efficiently exchange information while retaining\ndecision-making autonomy via a structured tree communication model. Extensive\nexperimentation with datasets generated from realistic urban mobility\nunderscores an outstanding performance of the proposed solution compared to\nstate-of-the-art methods. Significant new insights show that long-term methods\noptimize scarce drone resource for traffic management, while the integration of\nshort-term methods is crucial for advising on charging policies and maintaining\nbattery safety.\n","authors":["Chuhao Qin","Evangelos Pournaras"],"pdf_url":"https://arxiv.org/pdf/2311.09852v4.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible. 12 pages, 13 figures"},{"id":"http://arxiv.org/abs/2404.07577v1","updated":"2024-04-11T09:08:45Z","published":"2024-04-11T09:08:45Z","title":"Generating Comprehensive Lithium Battery Charging Data with Generative\n  AI","summary":"  In optimizing performance and extending the lifespan of lithium batteries,\naccurate state prediction is pivotal. Traditional regression and classification\nmethods have achieved some success in battery state prediction. However, the\nefficacy of these data-driven approaches heavily relies on the availability and\nquality of public datasets. Additionally, generating electrochemical data\npredominantly through battery experiments is a lengthy and costly process,\nmaking it challenging to acquire high-quality electrochemical data. This\ndifficulty, coupled with data incompleteness, significantly impacts prediction\naccuracy. Addressing these challenges, this study introduces the End of Life\n(EOL) and Equivalent Cycle Life (ECL) as conditions for generative AI models.\nBy integrating an embedding layer into the CVAE model, we developed the Refined\nConditional Variational Autoencoder (RCVAE). Through preprocessing data into a\nquasi-video format, our study achieves an integrated synthesis of\nelectrochemical data, including voltage, current, temperature, and charging\ncapacity, which is then processed by the RCVAE model. Coupled with customized\ntraining and inference algorithms, this model can generate specific\nelectrochemical data for EOL and ECL under supervised conditions. This method\nprovides users with a comprehensive electrochemical dataset, pioneering a new\nresearch domain for the artificial synthesis of lithium battery data.\nFurthermore, based on the detailed synthetic data, various battery state\nindicators can be calculated, offering new perspectives and possibilities for\nlithium battery performance prediction.\n","authors":["Lidang Jiang","Changyan Hu","Sibei Ji","Hang Zhao","Junxiong Chen","Ge He"],"pdf_url":"https://arxiv.org/pdf/2404.07577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07569v1","updated":"2024-04-11T08:57:48Z","published":"2024-04-11T08:57:48Z","title":"Can Vehicle Motion Planning Generalize to Realistic Long-tail Scenarios?","summary":"  Real-world autonomous driving systems must make safe decisions in the face of\nrare and diverse traffic scenarios. Current state-of-the-art planners are\nmostly evaluated on real-world datasets like nuScenes (open-loop) or nuPlan\n(closed-loop). In particular, nuPlan seems to be an expressive evaluation\nmethod since it is based on real-world data and closed-loop, yet it mostly\ncovers basic driving scenarios. This makes it difficult to judge a planner's\ncapabilities to generalize to rarely-seen situations. Therefore, we propose a\nnovel closed-loop benchmark interPlan containing several edge cases and\nchallenging driving scenarios. We assess existing state-of-the-art planners on\nour benchmark and show that neither rule-based nor learning-based planners can\nsafely navigate the interPlan scenarios.\n  A recently evolving direction is the usage of foundation models like large\nlanguage models (LLM) to handle generalization. We evaluate an LLM-only planner\nand introduce a novel hybrid planner that combines an LLM-based behavior\nplanner with a rule-based motion planner that achieves state-of-the-art\nperformance on our benchmark.\n","authors":["Marcel Hallgarten","Julian Zapata","Martin Stoll","Katrin Renz","Andreas Zell"],"pdf_url":"https://arxiv.org/pdf/2404.07569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07559v1","updated":"2024-04-11T08:42:51Z","published":"2024-04-11T08:42:51Z","title":"Differentially Private Reinforcement Learning with Self-Play","summary":"  We study the problem of multi-agent reinforcement learning (multi-agent RL)\nwith differential privacy (DP) constraints. This is well-motivated by various\nreal-world applications involving sensitive data, where it is critical to\nprotect users' private information. We first extend the definitions of Joint DP\n(JDP) and Local DP (LDP) to two-player zero-sum episodic Markov Games, where\nboth definitions ensure trajectory-wise privacy protection. Then we design a\nprovably efficient algorithm based on optimistic Nash value iteration and\nprivatization of Bernstein-type bonuses. The algorithm is able to satisfy JDP\nand LDP requirements when instantiated with appropriate privacy mechanisms.\nFurthermore, for both notions of DP, our regret bound generalizes the best\nknown result under the single-agent RL case, while our regret could also reduce\nto the best known result for multi-agent RL without privacy constraints. To the\nbest of our knowledge, these are the first line of results towards\nunderstanding trajectory-wise privacy protection in multi-agent RL.\n","authors":["Dan Qiao","Yu-Xiang Wang"],"pdf_url":"https://arxiv.org/pdf/2404.07559v1.pdf","comment":"32 pages"},{"id":"http://arxiv.org/abs/2404.05985v2","updated":"2024-04-11T08:21:27Z","published":"2024-04-09T03:36:39Z","title":"Boosting Digital Safeguards: Blending Cryptography and Steganography","summary":"  In today's digital age, the internet is essential for communication and the\nsharing of information, creating a critical need for sophisticated data\nsecurity measures to prevent unauthorized access and exploitation. Cryptography\nencrypts messages into a cipher text that is incomprehensible to unauthorized\nreaders, thus safeguarding data during its transmission. Steganography, on the\nother hand, originates from the Greek term for \"covered writing\" and involves\nthe art of hiding data within another medium, thereby facilitating covert\ncommunication by making the message invisible. This proposed approach takes\nadvantage of the latest advancements in Artificial Intelligence (AI) and Deep\nLearning (DL), especially through the application of Generative Adversarial\nNetworks (GANs), to improve upon traditional steganographic methods. By\nembedding encrypted data within another medium, our method ensures that the\ncommunication remains hidden from prying eyes. The application of GANs enables\na smart, secure system that utilizes the inherent sensitivity of neural\nnetworks to slight alterations in data, enhancing the protection against\ndetection. By merging the encryption techniques of cryptography with the hiding\ncapabilities of steganography, and augmenting these with the strengths of AI,\nwe introduce a comprehensive security system designed to maintain both the\nprivacy and integrity of information. This system is crafted not just to\nprevent unauthorized access or modification of data, but also to keep the\nexistence of the data hidden. This fusion of technologies tackles the core\nchallenges of data security in the current era of open digital communication,\npresenting an advanced solution with the potential to transform the landscape\nof information security.\n","authors":["Anamitra Maiti","Subham Laha","Rishav Upadhaya","Soumyajit Biswas","Vikas Chaudhary","Biplab Kar","Nikhil Kumar","Jaydip Sen"],"pdf_url":"https://arxiv.org/pdf/2404.05985v2.pdf","comment":"This report pertains to the Capstone Project done by Group 3 of the\n  Fall batch of 2023 students at Praxis Tech School, Kolkata, India. The\n  reports consists of 36 pages and it includes 11 figures and 5 tables"},{"id":"http://arxiv.org/abs/2302.02670v2","updated":"2024-04-11T08:14:05Z","published":"2023-02-06T10:18:03Z","title":"Random Forests for time-fixed and time-dependent predictors: The\n  DynForest R package","summary":"  The R package DynForest implements random forests for predicting a\ncontinuous, a categorical or a (multiple causes) time-to-event outcome based on\ntime-fixed and time-dependent predictors. The main originality of DynForest is\nthat it handles time-dependent predictors that can be endogeneous (i.e.,\nimpacted by the outcome process), measured with error and measured at\nsubject-specific times. At each recursive step of the tree building process,\nthe time-dependent predictors are internally summarized into individual\nfeatures on which the split can be done. This is achieved using flexible linear\nmixed models (thanks to the R package lcmm) which specification is\npre-specified by the user. DynForest returns the mean for continuous outcome,\nthe category with a majority vote for categorical outcome or the cumulative\nincidence function over time for survival outcome. DynForest also computes\nvariable importance and minimal depth to inform on the most predictive\nvariables or groups of variables. This paper aims to guide the user with\nstep-by-step examples for fitting random forests using DynForest.\n","authors":["Anthony Devaux","C√©cile Proust-Lima","Robin Genuer"],"pdf_url":"https://arxiv.org/pdf/2302.02670v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02004v2","updated":"2024-04-11T07:54:55Z","published":"2024-03-04T12:57:26Z","title":"Error bounds for particle gradient descent, and extensions of the\n  log-Sobolev and Talagrand inequalities","summary":"  We prove non-asymptotic error bounds for particle gradient descent\n(PGD)~(Kuntz et al., 2023), a recently introduced algorithm for maximum\nlikelihood estimation of large latent variable models obtained by discretizing\na gradient flow of the free energy. We begin by showing that, for models\nsatisfying a condition generalizing both the log-Sobolev and the\nPolyak--{\\L}ojasiewicz inequalities (LSI and P{\\L}I, respectively), the flow\nconverges exponentially fast to the set of minimizers of the free energy. We\nachieve this by extending a result well-known in the optimal transport\nliterature (that the LSI implies the Talagrand inequality) and its counterpart\nin the optimization literature (that the P{\\L}I implies the so-called quadratic\ngrowth condition), and applying it to our new setting. We also generalize the\nBakry--\\'Emery Theorem and show that the LSI/P{\\L}I generalization holds for\nmodels with strongly concave log-likelihoods. For such models, we further\ncontrol PGD's discretization error, obtaining non-asymptotic error bounds.\nWhile we are motivated by the study of PGD, we believe that the inequalities\nand results we extend may be of independent interest.\n","authors":["Rocco Caprio","Juan Kuntz","Samuel Power","Adam M. Johansen"],"pdf_url":"https://arxiv.org/pdf/2403.02004v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07533v1","updated":"2024-04-11T07:54:14Z","published":"2024-04-11T07:54:14Z","title":"IITP-VDLand: A Comprehensive Dataset on Decentraland Parcels","summary":"  This paper presents IITP-VDLand, a comprehensive dataset of Decentraland\nparcels sourced from diverse platforms. Unlike existing datasets which have\nlimited attributes and records, IITP-VDLand offers a rich array of attributes,\nencompassing parcel characteristics, trading history, past activities,\ntransactions, and social media interactions. Alongside, we introduce a key\nattribute in the dataset, namely Rarity score, which measures the uniqueness of\neach parcel within the virtual world. Addressing the significant challenge\nposed by the dispersed nature of this data across various sources, we employ a\nsystematic approach, utilizing both available APIs and custom scripts, to\ngather it. Subsequently, we meticulously curate and organize the information\ninto four distinct segments: (1) Characteristics Data-Fragment, (2) OpenSea\nTrading History Data-Fragment, (3) Ethereum Activity Transactions\nData-Fragment, and (4) Social Media Data-Fragment. We envisage that this\ndataset would serve as a robust resource for training machine- and\ndeep-learning models specifically designed to address real-world challenges\nwithin the domain of Decentraland parcels. The performance benchmarking of more\nthan 20 state-of-the-art price prediction models on our dataset yields\npromising results, achieving a maximum R2 score of 0.8251 and an accuracy of\n74.23% in case of Extra Trees Regressor and Classifier. The key findings reveal\nthat the ensemble models performs better than both deep learning and linear\nmodels for our dataset. We observe a significant impact of coordinates,\ngeographical proximity, rarity score, and few other economic indicators on the\nprediction of parcel prices.\n","authors":["Ankit K. Bhagat","Dipika Jha","Raju Halder","Rajendra N. Paramanik","Chandra M. Kumar"],"pdf_url":"https://arxiv.org/pdf/2404.07533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07532v1","updated":"2024-04-11T07:51:30Z","published":"2024-04-11T07:51:30Z","title":"Bayesian Federated Model Compression for Communication and Computation\n  Efficiency","summary":"  In this paper, we investigate Bayesian model compression in federated\nlearning (FL) to construct sparse models that can achieve both communication\nand computation efficiencies. We propose a decentralized Turbo variational\nBayesian inference (D-Turbo-VBI) FL framework where we firstly propose a\nhierarchical sparse prior to promote a clustered sparse structure in the weight\nmatrix. Then, by carefully integrating message passing and VBI with a\ndecentralized turbo framework, we propose the D-Turbo-VBI algorithm which can\n(i) reduce both upstream and downstream communication overhead during federated\ntraining, and (ii) reduce the computational complexity during local inference.\nAdditionally, we establish the convergence property for thr proposed\nD-Turbo-VBI algorithm. Simulation results show the significant gain of our\nproposed algorithm over the baselines in reducing communication overhead during\nfederated training and computational complexity of final model.\n","authors":["Chengyu Xia","Danny H. K. Tsang","Vincent K. N. Lau"],"pdf_url":"https://arxiv.org/pdf/2404.07532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09107v2","updated":"2024-04-11T07:42:43Z","published":"2024-03-14T05:00:29Z","title":"S^2MVTC: a Simple yet Efficient Scalable Multi-View Tensor Clustering","summary":"  Anchor-based large-scale multi-view clustering has attracted considerable\nattention for its effectiveness in handling massive datasets. However, current\nmethods mainly seek the consensus embedding feature for clustering by exploring\nglobal correlations between anchor graphs or projection matrices.In this paper,\nwe propose a simple yet efficient scalable multi-view tensor clustering\n(S^2MVTC) approach, where our focus is on learning correlations of embedding\nfeatures within and across views. Specifically, we first construct the\nembedding feature tensor by stacking the embedding features of different views\ninto a tensor and rotating it. Additionally, we build a novel tensor\nlow-frequency approximation (TLFA) operator, which incorporates graph\nsimilarity into embedding feature learning, efficiently achieving smooth\nrepresentation of embedding features within different views. Furthermore,\nconsensus constraints are applied to embedding features to ensure inter-view\nsemantic consistency. Experimental results on six large-scale multi-view\ndatasets demonstrate that S^2MVTC significantly outperforms state-of-the-art\nalgorithms in terms of clustering performance and CPU execution time,\nespecially when handling massive data. The code of S^2MVTC is publicly\navailable at https://github.com/longzhen520/S2MVTC.\n","authors":["Zhen Long","Qiyuan Wang","Yazhou Ren","Yipeng Liu","Ce Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.09107v2.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2404.07525v1","updated":"2024-04-11T07:38:50Z","published":"2024-04-11T07:38:50Z","title":"Enhancing Policy Gradient with the Polyak Step-Size Adaption","summary":"  Policy gradient is a widely utilized and foundational algorithm in the field\nof reinforcement learning (RL). Renowned for its convergence guarantees and\nstability compared to other RL algorithms, its practical application is often\nhindered by sensitivity to hyper-parameters, particularly the step-size. In\nthis paper, we introduce the integration of the Polyak step-size in RL, which\nautomatically adjusts the step-size without prior knowledge. To adapt this\nmethod to RL settings, we address several issues, including unknown f* in the\nPolyak step-size. Additionally, we showcase the performance of the Polyak\nstep-size in RL through experiments, demonstrating faster convergence and the\nattainment of more stable policies.\n","authors":["Yunxiang Li","Rui Yuan","Chen Fan","Mark Schmidt","Samuel Horv√°th","Robert M. Gower","Martin Tak√°ƒç"],"pdf_url":"https://arxiv.org/pdf/2404.07525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07523v1","updated":"2024-04-11T07:36:00Z","published":"2024-04-11T07:36:00Z","title":"GNN-based Probabilistic Supply and Inventory Predictions in Supply Chain\n  Networks","summary":"  Successful supply chain optimization must mitigate imbalances between supply\nand demand over time. While accurate demand prediction is essential for supply\nplanning, it alone does not suffice. The key to successful supply planning for\noptimal and viable execution lies in maximizing predictability for both demand\nand supply throughout an execution horizon. Therefore, enhancing the accuracy\nof supply predictions is imperative to create an attainable supply plan that\nmatches demand without overstocking or understocking. However, in complex\nsupply chain networks with numerous nodes and edges, accurate supply\npredictions are challenging due to dynamic node interactions, cascading supply\ndelays, resource availability, production and logistic capabilities.\nConsequently, supply executions often deviate from their initial plans. To\naddress this, we present the Graph-based Supply Prediction (GSP) probabilistic\nmodel. Our attention-based graph neural network (GNN) model predicts supplies,\ninventory, and imbalances using graph-structured historical data, demand\nforecasting, and original supply plan inputs. The experiments, conducted using\nhistorical data from a global consumer goods company's large-scale supply\nchain, demonstrate that GSP significantly improves supply and inventory\nprediction accuracy, potentially offering supply plan corrections to optimize\nexecutions.\n","authors":["Hyung-il Ahn","Young Chol Song","Santiago Olivar","Hershel Mehta","Naveen Tewari"],"pdf_url":"https://arxiv.org/pdf/2404.07523v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07518v1","updated":"2024-04-11T07:22:14Z","published":"2024-04-11T07:22:14Z","title":"Remembering Transformer for Continual Learning","summary":"  Neural networks encounter the challenge of Catastrophic Forgetting (CF) in\ncontinual learning, where new task knowledge interferes with previously learned\nknowledge. We propose Remembering Transformer, inspired by the brain's\nComplementary Learning Systems (CLS), to tackle this issue. Remembering\nTransformer employs a mixture-of-adapters and a generative model-based routing\nmechanism to alleviate CF by dynamically routing task data to relevant\nadapters. Our approach demonstrated a new SOTA performance in various vision\ncontinual learning tasks and great parameter efficiency.\n","authors":["Yuwei Sun","Jun Sakuma","Ryota Kanai"],"pdf_url":"https://arxiv.org/pdf/2404.07518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07511v1","updated":"2024-04-11T07:06:58Z","published":"2024-04-11T07:06:58Z","title":"Generative Probabilistic Planning for Optimizing Supply Chain Networks","summary":"  Supply chain networks in enterprises are typically composed of complex\ntopological graphs involving various types of nodes and edges, accommodating\nnumerous products with considerable demand and supply variability. However, as\nsupply chain networks expand in size and complexity, traditional supply chain\nplanning methods (e.g., those found in heuristic rule-based and operations\nresearch-based systems) tend to become locally optimal or lack computational\nscalability, resulting in substantial imbalances between supply and demand\nacross nodes in the network. This paper introduces a novel Generative AI\ntechnique, which we call Generative Probabilistic Planning (GPP). GPP generates\ndynamic supply action plans that are globally optimized across all network\nnodes over the time horizon for changing objectives like maximizing profits or\nservice levels, factoring in time-varying probabilistic demand, lead time, and\nproduction conditions. GPP leverages attention-based graph neural networks\n(GNN), offline deep reinforcement learning (Offline RL), and policy simulations\nto train generative policy models and create optimal plans through\nprobabilistic simulations, effectively accounting for various uncertainties.\nOur experiments using historical data from a global consumer goods company with\ncomplex supply chain networks demonstrate that GPP accomplishes\nobjective-adaptable, probabilistically resilient, and dynamic planning for\nsupply chain networks, leading to significant improvements in performance and\nprofitability for enterprises. Our work plays a pivotal role in shaping the\ntrajectory of AI adoption within the supply chain domain.\n","authors":["Hyung-il Ahn","Santiago Olivar","Hershel Mehta","Young Chol Song"],"pdf_url":"https://arxiv.org/pdf/2404.07511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15430v2","updated":"2024-04-11T06:40:12Z","published":"2024-02-23T16:50:07Z","title":"Hierarchical Invariance for Robust and Interpretable Vision Tasks at\n  Larger Scales","summary":"  Developing robust and interpretable vision systems is a crucial step towards\ntrustworthy artificial intelligence. In this regard, a promising paradigm\nconsiders embedding task-required invariant structures, e.g., geometric\ninvariance, in the fundamental image representation. However, such invariant\nrepresentations typically exhibit limited discriminability, limiting their\napplications in larger-scale trustworthy vision tasks. For this open problem,\nwe conduct a systematic investigation of hierarchical invariance, exploring\nthis topic from theoretical, practical, and application perspectives. At the\ntheoretical level, we show how to construct over-complete invariants with a\nConvolutional Neural Networks (CNN)-like hierarchical architecture yet in a\nfully interpretable manner. The general blueprint, specific definitions,\ninvariant properties, and numerical implementations are provided. At the\npractical level, we discuss how to customize this theoretical framework into a\ngiven task. With the over-completeness, discriminative features w.r.t. the task\ncan be adaptively formed in a Neural Architecture Search (NAS)-like manner. We\ndemonstrate the above arguments with accuracy, invariance, and efficiency\nresults on texture, digit, and parasite classification experiments.\nFurthermore, at the application level, our representations are explored in\nreal-world forensics tasks on adversarial perturbations and Artificial\nIntelligence Generated Content (AIGC). Such applications reveal that the\nproposed strategy not only realizes the theoretically promised invariance, but\nalso exhibits competitive discriminability even in the era of deep learning.\nFor robust and interpretable vision tasks at larger scales, hierarchical\ninvariant representation can be considered as an effective alternative to\ntraditional CNN and invariants.\n","authors":["Shuren Qi","Yushu Zhang","Chao Wang","Zhihua Xia","Xiaochun Cao","Jian Weng"],"pdf_url":"https://arxiv.org/pdf/2402.15430v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07502v1","updated":"2024-04-11T06:33:19Z","published":"2024-04-11T06:33:19Z","title":"Generating Counterfactual Explanations Using Cardinality Constraints","summary":"  Providing explanations about how machine learning algorithms work and/or make\nparticular predictions is one of the main tools that can be used to improve\ntheir trusworthiness, fairness and robustness. Among the most intuitive type of\nexplanations are counterfactuals, which are examples that differ from a given\npoint only in the prediction target and some set of features, presenting which\nfeatures need to be changed in the original example to flip the prediction for\nthat example. However, such counterfactuals can have many different features\nthan the original example, making their interpretation difficult. In this\npaper, we propose to explicitly add a cardinality constraint to counterfactual\ngeneration limiting how many features can be different from the original\nexample, thus providing more interpretable and easily understantable\ncounterfactuals.\n","authors":["Rub√©n Ruiz-Torrubiano"],"pdf_url":"https://arxiv.org/pdf/2404.07502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07498v1","updated":"2024-04-11T06:22:56Z","published":"2024-04-11T06:22:56Z","title":"Interactive Prompt Debugging with Sequence Salience","summary":"  We present Sequence Salience, a visual tool for interactive prompt debugging\nwith input salience methods. Sequence Salience builds on widely used salience\nmethods for text classification and single-token prediction, and extends this\nto a system tailored for debugging complex LLM prompts. Our system is\nwell-suited for long texts, and expands on previous work by 1) providing\ncontrollable aggregation of token-level salience to the word, sentence, or\nparagraph level, making salience over long inputs tractable; and 2) supporting\nrapid iteration where practitioners can act on salience results, refine\nprompts, and run salience on the new output. We include case studies showing\nhow Sequence Salience can help practitioners work with several complex\nprompting strategies, including few-shot, chain-of-thought, and constitutional\nprinciples. Sequence Salience is built on the Learning Interpretability Tool,\nan open-source platform for ML model visualizations, and code, notebooks, and\ntutorials are available at http://goo.gle/sequence-salience.\n","authors":["Ian Tenney","Ryan Mullins","Bin Du","Shree Pandya","Minsuk Kahng","Lucas Dixon"],"pdf_url":"https://arxiv.org/pdf/2404.07498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16646v2","updated":"2024-04-11T06:08:45Z","published":"2023-10-25T13:55:14Z","title":"Model predictive control-based value estimation for efficient\n  reinforcement learning","summary":"  Reinforcement learning suffers from limitations in real practices primarily\ndue to the number of required interactions with virtual environments. It\nresults in a challenging problem because we are implausible to obtain a local\noptimal strategy with only a few attempts for many learning methods. Hereby, we\ndesign an improved reinforcement learning method based on model predictive\ncontrol that models the environment through a data-driven approach. Based on\nthe learned environment model, it performs multi-step prediction to estimate\nthe value function and optimize the policy. The method demonstrates higher\nlearning efficiency, faster convergent speed of strategies tending to the local\noptimal value, and less sample capacity space required by experience replay\nbuffers. Experimental results, both in classic databases and in a dynamic\nobstacle avoidance scenario for an unmanned aerial vehicle, validate the\nproposed approaches.\n","authors":["Qizhen Wu","Kexin Liu","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2310.16646v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07493v1","updated":"2024-04-11T06:04:06Z","published":"2024-04-11T06:04:06Z","title":"Characterizing the Influence of Topology on Graph Learning Tasks","summary":"  Graph neural networks (GNN) have achieved remarkable success in a wide range\nof tasks by encoding features combined with topology to create effective\nrepresentations. However, the fundamental problem of understanding and\nanalyzing how graph topology influences the performance of learning models on\ndownstream tasks has not yet been well understood. In this paper, we propose a\nmetric, TopoInf, which characterizes the influence of graph topology by\nmeasuring the level of compatibility between the topological information of\ngraph data and downstream task objectives. We provide analysis based on the\ndecoupled GNNs on the contextual stochastic block model to demonstrate the\neffectiveness of the metric. Through extensive experiments, we demonstrate that\nTopoInf is an effective metric for measuring topological influence on\ncorresponding tasks and can be further leveraged to enhance graph learning.\n","authors":["Kailong Wu","Yule Xie","Jiaxin Ding","Yuxiang Ren","Luoyi Fu","Xinbing Wang","Chenghu Zhou"],"pdf_url":"https://arxiv.org/pdf/2404.07493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02476v2","updated":"2024-04-11T06:00:27Z","published":"2024-04-03T05:32:10Z","title":"Deep Reinforcement Learning for Traveling Purchaser Problems","summary":"  The traveling purchaser problem (TPP) is an important combinatorial\noptimization problem with broad applications. Due to the coupling between\nrouting and purchasing, existing works on TPPs commonly address route\nconstruction and purchase planning simultaneously, which, however, leads to\nexact methods with high computational cost and heuristics with sophisticated\ndesign but limited performance. In sharp contrast, we propose a novel approach\nbased on deep reinforcement learning (DRL), which addresses route construction\nand purchase planning separately, while evaluating and optimizing the solution\nfrom a global perspective. The key components of our approach include a\nbipartite graph representation for TPPs to capture the market-product\nrelations, and a policy network that extracts information from the bipartite\ngraph and uses it to sequentially construct the route. One significant benefit\nof our framework is that we can efficiently construct the route using the\npolicy network, and once the route is determined, the associated purchasing\nplan can be easily derived through linear programming, while, leveraging DRL,\nwe can train the policy network to optimize the global solution objective.\nFurthermore, by introducing a meta-learning strategy, the policy network can be\ntrained stably on large-sized TPP instances, and generalize well across\ninstances of varying sizes and distributions, even to much larger instances\nthat are never seen during training. Experiments on various synthetic TPP\ninstances and the TPPLIB benchmark demonstrate that our DRL-based approach can\nsignificantly outperform well-established TPP heuristics, reducing the\noptimality gap by 40%-90%, and also showing an advantage in runtime, especially\non large-sized instances.\n","authors":["Haofeng Yuan","Rongping Zhu","Wanlu Yang","Shiji Song","Keyou You","Yuli Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.02476v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10839v2","updated":"2024-04-11T05:46:09Z","published":"2022-07-22T02:06:53Z","title":"Robust Knowledge Adaptation for Dynamic Graph Neural Networks","summary":"  Graph structured data often possess dynamic characters in nature. Recent\nyears have witnessed the increasing attentions paid to dynamic graph neural\nnetworks for modelling graph data. However, almost all existing approaches\noperate under the assumption that, upon the establishment of a new link, the\nembeddings of the neighboring nodes should undergo updates to learn temporal\ndynamics. Nevertheless, these approaches face the following limitation: If the\nnode introduced by a new connection contains noisy information, propagating its\nknowledge to other nodes becomes unreliable and may even lead to the collapse\nof the model. In this paper, we propose Ada-DyGNN: a robust knowledge\nAdaptation framework via reinforcement learning for Dynamic Graph Neural\nNetworks. In contrast to previous approaches, which update the embeddings of\nthe neighbor nodes immediately after adding a new link, Ada-DyGNN adaptively\ndetermines which nodes should be updated. Considering that the decision to\nupdate the embedding of one neighbor node can significantly impact other\nneighbor nodes, we conceptualize the node update selection as a sequence\ndecision problem and employ reinforcement learning to address it effectively.\nBy this means, we can adaptively propagate knowledge to other nodes for\nlearning robust node embedding representations. To the best of our knowledge,\nour approach constitutes the first attempt to explore robust knowledge\nadaptation via reinforcement learning specifically tailored for dynamic graph\nneural networks. Extensive experiments on three benchmark datasets demonstrate\nthat Ada-DyGNN achieves the state-of-the-art performance. In addition, we\nconduct experiments by introducing different degrees of noise into the dataset,\nquantitatively and qualitatively illustrating the robustness of Ada-DyGNN.\n","authors":["Hanjie Li","Changsheng Li","Kaituo Feng","Ye Yuan","Guoren Wang","Hongyuan Zha"],"pdf_url":"https://arxiv.org/pdf/2207.10839v2.pdf","comment":"14 pages, 6 figures"},{"id":"http://arxiv.org/abs/2404.07475v1","updated":"2024-04-11T05:09:03Z","published":"2024-04-11T05:09:03Z","title":"Laissez-Faire Harms: Algorithmic Biases in Generative Language Models","summary":"  The rapid deployment of generative language models (LMs) has raised concerns\nabout social biases affecting the well-being of diverse consumers. The extant\nliterature on generative LMs has primarily examined bias via explicit identity\nprompting. However, prior research on bias in earlier language-based technology\nplatforms, including search engines, has shown that discrimination can occur\neven when identity terms are not specified explicitly. Studies of bias in LM\nresponses to open-ended prompts (where identity classifications are left\nunspecified) are lacking and have not yet been grounded in end-consumer harms.\nHere, we advance studies of generative LM bias by considering a broader set of\nnatural use cases via open-ended prompting. In this \"laissez-faire\" setting, we\nfind that synthetically generated texts from five of the most pervasive LMs\n(ChatGPT3.5, ChatGPT4, Claude2.0, Llama2, and PaLM2) perpetuate harms of\nomission, subordination, and stereotyping for minoritized individuals with\nintersectional race, gender, and/or sexual orientation identities (AI/AN,\nAsian, Black, Latine, MENA, NH/PI, Female, Non-binary, Queer). We find\nwidespread evidence of bias to an extent that such individuals are hundreds to\nthousands of times more likely to encounter LM-generated outputs that portray\ntheir identities in a subordinated manner compared to representative or\nempowering portrayals. We also document a prevalence of stereotypes (e.g.\nperpetual foreigner) in LM-generated outputs that are known to trigger\npsychological harms that disproportionately affect minoritized individuals.\nThese include stereotype threat, which leads to impaired cognitive performance\nand increased negative self-perception. Our findings highlight the urgent need\nto protect consumers from discriminatory harms caused by language models and\ninvest in critical AI education programs tailored towards empowering diverse\nconsumers.\n","authors":["Evan Shieh","Faye-Marie Vassel","Cassidy Sugimoto","Thema Monroe-White"],"pdf_url":"https://arxiv.org/pdf/2404.07475v1.pdf","comment":"16 pages (44 if including supplementals), 4 figures (20 if including\n  supplementals)"},{"id":"http://arxiv.org/abs/2301.12987v4","updated":"2024-04-11T05:02:10Z","published":"2023-01-30T15:29:40Z","title":"The Optimal Choice of Hypothesis Is the Weakest, Not the Shortest","summary":"  If $A$ and $B$ are sets such that $A \\subset B$, generalisation may be\nunderstood as the inference from $A$ of a hypothesis sufficient to construct\n$B$. One might infer any number of hypotheses from $A$, yet only some of those\nmay generalise to $B$. How can one know which are likely to generalise? One\nstrategy is to choose the shortest, equating the ability to compress\ninformation with the ability to generalise (a proxy for intelligence). We\nexamine this in the context of a mathematical formalism of enactive cognition.\nWe show that compression is neither necessary nor sufficient to maximise\nperformance (measured in terms of the probability of a hypothesis\ngeneralising). We formulate a proxy unrelated to length or simplicity, called\nweakness. We show that if tasks are uniformly distributed, then there is no\nchoice of proxy that performs at least as well as weakness maximisation in all\ntasks while performing strictly better in at least one. In experiments\ncomparing maximum weakness and minimum description length in the context of\nbinary arithmetic, the former generalised at between $1.1$ and $5$ times the\nrate of the latter. We argue this demonstrates that weakness is a far better\nproxy, and explains why Deepmind's Apperception Engine is able to generalise\neffectively.\n","authors":["Michael Timothy Bennett"],"pdf_url":"https://arxiv.org/pdf/2301.12987v4.pdf","comment":"Published at the 16th Conference on Artificial General Intelligence,\n  Stockholm, 2023"},{"id":"http://arxiv.org/abs/2402.08170v3","updated":"2024-04-11T05:01:12Z","published":"2024-02-13T02:03:26Z","title":"LLaGA: Large Language and Graph Assistant","summary":"  Graph Neural Networks (GNNs) have empowered the advance in graph-structured\ndata analysis. Recently, the rise of Large Language Models (LLMs) like GPT-4\nhas heralded a new era in deep learning. However, their application to graph\ndata poses distinct challenges due to the inherent difficulty of translating\ngraph structures to language. To this end, we introduce the Large Language and\nGraph Assistant (LLaGA), an innovative model that effectively integrates LLM\ncapabilities to handle the complexities of graph-structured data. LLaGA retains\nthe general-purpose nature of LLMs while adapting graph data into a format\ncompatible with LLM input. LLaGA achieves this by reorganizing graph nodes to\nstructure-aware sequences and then mapping these into the token embedding space\nthrough a versatile projector. LLaGA excels in versatility, generalizability\nand interpretability, allowing it to perform consistently well across different\ndatasets and tasks, extend its ability to unseen datasets or tasks, and provide\nexplanations for graphs. Our extensive experiments across popular graph\nbenchmarks show that LLaGA delivers outstanding performance across four\ndatasets and three tasks using one single model, surpassing state-of-the-art\ngraph models in both supervised and zero-shot scenarios. Our code is available\nat \\url{https://github.com/VITA-Group/LLaGA}.\n","authors":["Runjin Chen","Tong Zhao","Ajay Jaiswal","Neil Shah","Zhangyang Wang"],"pdf_url":"https://arxiv.org/pdf/2402.08170v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07473v1","updated":"2024-04-11T04:54:42Z","published":"2024-04-11T04:54:42Z","title":"LUCF-Net: Lightweight U-shaped Cascade Fusion Network for Medical Image\n  Segmentation","summary":"  In this study, the performance of existing U-shaped neural network\narchitectures was enhanced for medical image segmentation by adding\nTransformer. Although Transformer architectures are powerful at extracting\nglobal information, its ability to capture local information is limited due to\nits high complexity. To address this challenge, we proposed a new lightweight\nU-shaped cascade fusion network (LUCF-Net) for medical image segmentation. It\nutilized an asymmetrical structural design and incorporated both local and\nglobal modules to enhance its capacity for local and global modeling.\nAdditionally, a multi-layer cascade fusion decoding network was designed to\nfurther bolster the network's information fusion capabilities. Validation\nresults achieved on multi-organ datasets in CT format, cardiac segmentation\ndatasets in MRI format, and dermatology datasets in image format demonstrated\nthat the proposed model outperformed other state-of-the-art methods in handling\nlocal-global information, achieving an improvement of 1.54% in Dice coefficient\nand 2.6 mm in Hausdorff distance on multi-organ segmentation. Furthermore, as a\nnetwork that combines Convolutional Neural Network and Transformer\narchitectures, it achieves competitive segmentation performance with only 6.93\nmillion parameters and 6.6 gigabytes of floating point operations, without the\nneed of pre-training. In summary, the proposed method demonstrated enhanced\nperformance while retaining a simpler model design compared to other\nTransformer-based segmentation networks.\n","authors":["Songkai Sun","Qingshan She","Yuliang Ma","Rihui Li","Yingchun Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.07473v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17840v2","updated":"2024-04-11T04:23:42Z","published":"2023-11-29T17:42:05Z","title":"A quasi-polynomial time algorithm for Multi-Dimensional Scaling via LP\n  hierarchies","summary":"  Multi-dimensional Scaling (MDS) is a family of methods for embedding an\n$n$-point metric into low-dimensional Euclidean space. We study the\nKamada-Kawai formulation of MDS: given a set of non-negative dissimilarities\n$\\{d_{i,j}\\}_{i , j \\in [n]}$ over $n$ points, the goal is to find an embedding\n$\\{x_1,\\dots,x_n\\} \\in \\mathbb{R}^k$ that minimizes \\[\\text{OPT} = \\min_{x}\n\\mathbb{E}_{i,j \\in [n]} \\left[ \\left(1-\\frac{\\|x_i - x_j\\|}{d_{i,j}}\\right)^2\n\\right] \\]\n  Kamada-Kawai provides a more relaxed measure of the quality of a\nlow-dimensional metric embedding than the traditional bi-Lipschitz-ness measure\nstudied in theoretical computer science; this is advantageous because strong\nhardness-of-approximation results are known for the latter, Kamada-Kawai admits\nnontrivial approximation algorithms. Despite its popularity, our theoretical\nunderstanding of MDS is limited. Recently, Demaine, Hesterberg, Koehler, Lynch,\nand Urschel (arXiv:2109.11505) gave the first approximation algorithm with\nprovable guarantees for Kamada-Kawai in the constant-$k$ regime, with cost\n$\\text{OPT} +\\epsilon$ in $n^2 2^{\\text{poly}(\\Delta/\\epsilon)}$ time, where\n$\\Delta$ is the aspect ratio of the input. In this work, we give the first\napproximation algorithm for MDS with quasi-polynomial dependency on $\\Delta$:\nwe achieve a solution with cost $\\tilde{O}(\\log\n\\Delta)\\text{OPT}^{\\Omega(1)}+\\epsilon$ in time\n$n^{O(1)}2^{\\text{poly}(\\log(\\Delta)/\\epsilon)}$.\n  Our approach is based on a novel analysis of a conditioning-based rounding\nscheme for the Sherali-Adams LP Hierarchy. Crucially, our analysis exploits the\ngeometry of low-dimensional Euclidean space, allowing us to avoid an\nexponential dependence on the aspect ratio. We believe our geometry-aware\ntreatment of the Sherali-Adams Hierarchy is an important step towards\ndeveloping general-purpose techniques for efficient metric optimization\nalgorithms.\n","authors":["Ainesh Bakshi","Vincent Cohen-Addad","Samuel B. Hopkins","Rajesh Jayaram","Silvio Lattanzi"],"pdf_url":"https://arxiv.org/pdf/2311.17840v2.pdf","comment":"Extended exposition"},{"id":"http://arxiv.org/abs/2404.07465v1","updated":"2024-04-11T04:02:20Z","published":"2024-04-11T04:02:20Z","title":"Leveraging Domain-Unlabeled Data in Offline Reinforcement Learning\n  across Two Domains","summary":"  In this paper, we investigate an offline reinforcement learning (RL) problem\nwhere datasets are collected from two domains. In this scenario, having\ndatasets with domain labels facilitates efficient policy training. However, in\npractice, the task of assigning domain labels can be resource-intensive or\ninfeasible at a large scale, leading to a prevalence of domain-unlabeled data.\nTo formalize this challenge, we introduce a novel offline RL problem setting\nnamed Positive-Unlabeled Offline RL (PUORL), which incorporates\ndomain-unlabeled data. To address PUORL, we develop an offline RL algorithm\nutilizing positive-unlabeled learning to predict the domain labels of\ndomain-unlabeled data, enabling the integration of this data into policy\ntraining. Our experiments show the effectiveness of our method in accurately\nidentifying domains and learning policies that outperform baselines in the\nPUORL setting, highlighting its capability to leverage domain-unlabeled data\neffectively.\n","authors":["Soichiro Nishimori","Xin-Qiang Cai","Johannes Ackermann","Masashi Sugiyama"],"pdf_url":"https://arxiv.org/pdf/2404.07465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16994v2","updated":"2024-04-11T03:44:49Z","published":"2024-02-26T20:00:57Z","title":"GEM3D: GEnerative Medial Abstractions for 3D Shape Synthesis","summary":"  We introduce GEM3D -- a new deep, topology-aware generative model of 3D\nshapes. The key ingredient of our method is a neural skeleton-based\nrepresentation encoding information on both shape topology and geometry.\nThrough a denoising diffusion probabilistic model, our method first generates\nskeleton-based representations following the Medial Axis Transform (MAT), then\ngenerates surfaces through a skeleton-driven neural implicit formulation. The\nneural implicit takes into account the topological and geometric information\nstored in the generated skeleton representations to yield surfaces that are\nmore topologically and geometrically accurate compared to previous neural field\nformulations. We discuss applications of our method in shape synthesis and\npoint cloud reconstruction tasks, and evaluate our method both qualitatively\nand quantitatively. We demonstrate significantly more faithful surface\nreconstruction and diverse shape generation results compared to the\nstate-of-the-art, also involving challenging scenarios of reconstructing and\nsynthesizing structurally complex, high-genus shape surfaces from Thingi10K and\nShapeNet.\n","authors":["Dmitry Petrov","Pradyumn Goyal","Vikas Thamizharasan","Vladimir G. Kim","Matheus Gadelha","Melinos Averkiou","Siddhartha Chaudhuri","Evangelos Kalogerakis"],"pdf_url":"https://arxiv.org/pdf/2402.16994v2.pdf","comment":"Webpage: https://lodurality.github.io/GEM3D/ -- Cond. accept. to\n  SIGGRAPH 2024 (conf. track) -- Changes (based on reviews): changed style to\n  sigconf; rearranged figures for readability; added missing citations; fixed\n  misaligned centers in Fig. 3; added failure cases (Fig. 10); rewrote\n  discussion; added categories averages to Tab. 8; added Tab. 10 with model\n  capacities"},{"id":"http://arxiv.org/abs/2404.07454v1","updated":"2024-04-11T03:23:15Z","published":"2024-04-11T03:23:15Z","title":"Representation Learning of Tangled Key-Value Sequence Data for Early\n  Classification","summary":"  Key-value sequence data has become ubiquitous and naturally appears in a\nvariety of real-world applications, ranging from the user-product purchasing\nsequences in e-commerce, to network packet sequences forwarded by routers in\nnetworking. Classifying these key-value sequences is important in many\nscenarios such as user profiling and malicious applications identification. In\nmany time-sensitive scenarios, besides the requirement of classifying a\nkey-value sequence accurately, it is also desired to classify a key-value\nsequence early, in order to respond fast. However, these two goals are\nconflicting in nature, and it is challenging to achieve them simultaneously. In\nthis work, we formulate a novel tangled key-value sequence early classification\nproblem, where a tangled key-value sequence is a mixture of several concurrent\nkey-value sequences with different keys. The goal is to classify each\nindividual key-value sequence sharing a same key both accurately and early. To\naddress this problem, we propose a novel method, i.e., Key-Value sequence Early\nCo-classification (KVEC), which leverages both inner- and inter-correlations of\nitems in a tangled key-value sequence through key correlation and value\ncorrelation to learn a better sequence representation. Meanwhile, a time-aware\nhalting policy decides when to stop the ongoing key-value sequence and classify\nit based on current sequence representation. Experiments on both real-world and\nsynthetic datasets demonstrate that our method outperforms the state-of-the-art\nbaselines significantly. KVEC improves the prediction accuracy by up to $4.7 -\n17.5\\%$ under the same prediction earliness condition, and improves the\nharmonic mean of accuracy and earliness by up to $3.7 - 14.0\\%$.\n","authors":["Tao Duan","Junzhou Zhao","Shuo Zhang","Jing Tao","Pinghui Wang"],"pdf_url":"https://arxiv.org/pdf/2404.07454v1.pdf","comment":"12 pages, 31 figures, Accepted by ICDE2024"},{"id":"http://arxiv.org/abs/2404.07452v1","updated":"2024-04-11T03:14:50Z","published":"2024-04-11T03:14:50Z","title":"RiskLabs: Predicting Financial Risk Using Large Language Model Based on\n  Multi-Sources Data","summary":"  The integration of Artificial Intelligence (AI) techniques, particularly\nlarge language models (LLMs), in finance has garnered increasing academic\nattention. Despite progress, existing studies predominantly focus on tasks like\nfinancial text summarization, question-answering (Q$\\&$A), and stock movement\nprediction (binary classification), with a notable gap in the application of\nLLMs for financial risk prediction. Addressing this gap, in this paper, we\nintroduce \\textbf{RiskLabs}, a novel framework that leverages LLMs to analyze\nand predict financial risks. RiskLabs uniquely combines different types of\nfinancial data, including textual and vocal information from Earnings\nConference Calls (ECCs), market-related time series data, and contextual news\ndata surrounding ECC release dates. Our approach involves a multi-stage\nprocess: initially extracting and analyzing ECC data using LLMs, followed by\ngathering and processing time-series data before the ECC dates to model and\nunderstand risk over different timeframes. Using multimodal fusion techniques,\nRiskLabs amalgamates these varied data features for comprehensive multi-task\nfinancial risk prediction. Empirical experiment results demonstrate RiskLab's\neffectiveness in forecasting both volatility and variance in financial markets.\nThrough comparative experiments, we demonstrate how different data sources\ncontribute to financial risk assessment and discuss the critical role of LLMs\nin this context. Our findings not only contribute to the AI in finance\napplication but also open new avenues for applying LLMs in financial risk\nassessment.\n","authors":["Yupeng Cao","Zhi Chen","Qingyun Pei","Fabrizio Dimino","Lorenzo Ausiello","Prashant Kumar","K. P. Subbalakshmi","Papa Momar Ndiaye"],"pdf_url":"https://arxiv.org/pdf/2404.07452v1.pdf","comment":"24 pages, 7 figures, 5 tables, 1 algorithm"},{"id":"http://arxiv.org/abs/2404.07446v1","updated":"2024-04-11T03:02:06Z","published":"2024-04-11T03:02:06Z","title":"Graph Attention Network for Lane-Wise and Topology-Invariant\n  Intersection Traffic Simulation","summary":"  Traffic congestion has significant economic, environmental, and social\nramifications. Intersection traffic flow dynamics are influenced by numerous\nfactors. While microscopic traffic simulators are valuable tools, they are\ncomputationally intensive and challenging to calibrate. Moreover, existing\nmachine-learning approaches struggle to provide lane-specific waveforms or\nadapt to intersection topology and traffic patterns. In this study, we propose\ntwo efficient and accurate \"Digital Twin\" models for intersections, leveraging\nGraph Attention Neural Networks (GAT). These attentional graph auto-encoder\ndigital twins capture temporal, spatial, and contextual aspects of traffic\nwithin intersections, incorporating various influential factors such as\nhigh-resolution loop detector waveforms, signal state records, driving\nbehaviors, and turning-movement counts. Trained on diverse counterfactual\nscenarios across multiple intersections, our models generalize well, enabling\nthe estimation of detailed traffic waveforms for any intersection approach and\nexit lanes. Multi-scale error metrics demonstrate that our models perform\ncomparably to microsimulations. The primary application of our study lies in\ntraffic signal optimization, a pivotal area in transportation systems research.\nThese lightweight digital twins can seamlessly integrate into corridor and\nnetwork signal timing optimization frameworks. Furthermore, our study's\napplications extend to lane reconfiguration, driving behavior analysis, and\nfacilitating informed decisions regarding intersection safety and efficiency\nenhancements. A promising avenue for future research involves extending this\napproach to urban freeway corridors and integrating it with measures of\neffectiveness metrics.\n","authors":["Nooshin Yousefzadeh","Rahul Sengupta","Yashaswi Karnati","Anand Rangarajan","Sanjay Ranka"],"pdf_url":"https://arxiv.org/pdf/2404.07446v1.pdf","comment":"T-TIS Journal, 12 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2404.03372v2","updated":"2024-04-11T02:59:07Z","published":"2024-04-04T11:16:16Z","title":"Elementary Analysis of Policy Gradient Methods","summary":"  Projected policy gradient under the simplex parameterization, policy gradient\nand natural policy gradient under the softmax parameterization, are fundamental\nalgorithms in reinforcement learning. There have been a flurry of recent\nactivities in studying these algorithms from the theoretical aspect. Despite\nthis, their convergence behavior is still not fully understood, even given the\naccess to exact policy evaluations. In this paper, we focus on the discounted\nMDP setting and conduct a systematic study of the aforementioned policy\noptimization methods. Several novel results are presented, including 1) global\nlinear convergence of projected policy gradient for any constant step size, 2)\nsublinear convergence of softmax policy gradient for any constant step size, 3)\nglobal linear convergence of softmax natural policy gradient for any constant\nstep size, 4) global linear convergence of entropy regularized softmax policy\ngradient for a wider range of constant step sizes than existing result, 5)\ntight local linear convergence rate of entropy regularized natural policy\ngradient, and 6) a new and concise local quadratic convergence rate of soft\npolicy iteration without the assumption on the stationary distribution under\nthe optimal policy. New and elementary analysis techniques have been developed\nto establish these results.\n","authors":["Jiacai Liu","Wenye Li","Ke Wei"],"pdf_url":"https://arxiv.org/pdf/2404.03372v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14576v2","updated":"2024-04-11T02:57:21Z","published":"2023-10-23T05:25:49Z","title":"Tensor Decomposition Based Attention Module for Spiking Neural Networks","summary":"  The attention mechanism has been proven to be an effective way to improve\nspiking neural network (SNN). However, based on the fact that the current SNN\ninput data flow is split into tensors to process on GPUs, none of the previous\nworks consider the properties of tensors to implement an attention module. This\ninspires us to rethink current SNN from the perspective of tensor-relevant\ntheories. Using tensor decomposition, we design the \\textit{projected full\nattention} (PFA) module, which demonstrates excellent results with linearly\ngrowing parameters. Specifically, PFA is composed by the \\textit{linear\nprojection of spike tensor} (LPST) module and \\textit{attention map composing}\n(AMC) module. In LPST, we start by compressing the original spike tensor into\nthree projected tensors using a single property-preserving strategy with\nlearnable parameters for each dimension. Then, in AMC, we exploit the inverse\nprocedure of the tensor decomposition process to combine the three tensors into\nthe attention map using a so-called connecting factor. To validate the\neffectiveness of the proposed PFA module, we integrate it into the widely used\nVGG and ResNet architectures for classification tasks. Our method achieves\nstate-of-the-art performance on both static and dynamic benchmark datasets,\nsurpassing the existing SNN models with Transformer-based and CNN-based\nbackbones.\n","authors":["Haoyu Deng","Ruijie Zhu","Xuerui Qiu","Yule Duan","Malu Zhang","Liangjian Deng"],"pdf_url":"https://arxiv.org/pdf/2310.14576v2.pdf","comment":"Accepted by Knowledge-Based Systems"},{"id":"http://arxiv.org/abs/2105.15106v4","updated":"2024-04-11T02:57:17Z","published":"2021-05-06T13:05:55Z","title":"A Survey of Knowledge Tracing: Models, Variants, and Applications","summary":"  Modern online education has the capacity to provide intelligent educational\nservices by automatically analyzing substantial amounts of student behavioral\ndata. Knowledge Tracing (KT) is one of the fundamental tasks for student\nbehavioral data analysis, aiming to monitor students' evolving knowledge state\nduring their problem-solving process. In recent years, a substantial number of\nstudies have concentrated on this rapidly growing field, significantly\ncontributing to its advancements. In this survey, we will conduct a thorough\ninvestigation of these progressions. Firstly, we present three types of\nfundamental KT models with distinct technical routes. Subsequently, we review\nextensive variants of the fundamental KT models that consider more stringent\nlearning assumptions. Moreover, the development of KT cannot be separated from\nits applications, thereby we present typical KT applications in various\nscenarios. To facilitate the work of researchers and practitioners in this\nfield, we have developed two open-source algorithm libraries: EduData that\nenables the download and preprocessing of KT-related datasets, and EduKTM that\nprovides an extensible and unified implementation of existing mainstream KT\nmodels. Finally, we discuss potential directions for future research in this\nrapidly growing field. We hope that the current survey will assist both\nresearchers and practitioners in fostering the development of KT, thereby\nbenefiting a broader range of students.\n","authors":["Shuanghong Shen","Qi Liu","Zhenya Huang","Yonghe Zheng","Minghao Yin","Minjuan Wang","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2105.15106v4.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2404.07443v1","updated":"2024-04-11T02:54:17Z","published":"2024-04-11T02:54:17Z","title":"1-bit Quantized On-chip Hybrid Diffraction Neural Network Enabled by\n  Authentic All-optical Fully-connected Architecture","summary":"  Optical Diffraction Neural Networks (DNNs), a subset of Optical Neural\nNetworks (ONNs), show promise in mirroring the prowess of electronic networks.\nThis study introduces the Hybrid Diffraction Neural Network (HDNN), a novel\narchitecture that incorporates matrix multiplication into DNNs, synergizing the\nbenefits of conventional ONNs with those of DNNs to surmount the modulation\nlimitations inherent in optical diffraction neural networks. Utilizing a\nsingular phase modulation layer and an amplitude modulation layer, the trained\nneural network demonstrated remarkable accuracies of 96.39% and 89% in digit\nrecognition tasks in simulation and experiment, respectively. Additionally, we\ndevelop the Binning Design (BD) method, which effectively mitigates the\nconstraints imposed by sampling intervals on diffraction units, substantially\nstreamlining experimental procedures. Furthermore, we propose an on-chip HDNN\nthat not only employs a beam-splitting phase modulation layer for enhanced\nintegration level but also significantly relaxes device fabrication\nrequirements, replacing metasurfaces with relief surfaces designed by 1-bit\nquantization. Besides, we conceptualized an all-optical HDNN-assisted lesion\ndetection network, achieving detection outcomes that were 100% aligned with\nsimulation predictions. This work not only advances the performance of DNNs but\nalso streamlines the path towards industrial optical neural network production.\n","authors":["Yu Shao","Haiqi Gao","Yipeng Chen","Yujie liu","Junren Wen","Haidong He","Yuchuan Shao","Yueguang Zhang","Weidong Shen","Chenying Yang"],"pdf_url":"https://arxiv.org/pdf/2404.07443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.04931v2","updated":"2024-04-11T02:32:43Z","published":"2024-04-07T12:07:33Z","title":"The Sample Complexity of Gradient Descent in Stochastic Convex\n  Optimization","summary":"  We analyze the sample complexity of full-batch Gradient Descent (GD) in the\nsetup of non-smooth Stochastic Convex Optimization. We show that the\ngeneralization error of GD, with common choice of hyper-parameters, can be\n$\\tilde \\Theta(d/m + 1/\\sqrt{m})$, where $d$ is the dimension and $m$ is the\nsample size. This matches the sample complexity of \\emph{worst-case} empirical\nrisk minimizers. That means that, in contrast with other algorithms, GD has no\nadvantage over naive ERMs. Our bound follows from a new generalization bound\nthat depends on both the dimension as well as the learning rate and number of\niterations. Our bound also shows that, for general hyper-parameters, when the\ndimension is strictly larger than number of samples, $T=\\Omega(1/\\epsilon^4)$\niterations are necessary to avoid overfitting. This resolves an open problem by\nSchlisserman et al.23 and Amir er Al.21, and improves over previous lower\nbounds that demonstrated that the sample size must be at least square root of\nthe dimension.\n","authors":["Roi Livni"],"pdf_url":"https://arxiv.org/pdf/2404.04931v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07434v1","updated":"2024-04-11T02:23:30Z","published":"2024-04-11T02:23:30Z","title":"Data-Driven Portfolio Management for Motion Pictures Industry: A New\n  Data-Driven Optimization Methodology Using a Large Language Model as the\n  Expert","summary":"  Portfolio management is one of the unresponded problems of the Motion\nPictures Industry (MPI). To design an optimal portfolio for an MPI distributor,\nit is essential to predict the box office of each project. Moreover, for an\naccurate box office prediction, it is critical to consider the effect of the\ncelebrities involved in each MPI project, which was impossible with any\nprecedent expert-based method. Additionally, the asymmetric characteristic of\nMPI data decreases the performance of any predictive algorithm. In this paper,\nfirstly, the fame score of the celebrities is determined using a large language\nmodel. Then, to tackle the asymmetric character of MPI's data, projects are\nclassified. Furthermore, the box office prediction takes place for each class\nof projects. Finally, using a hybrid multi-attribute decision-making technique,\nthe preferability of each project for the distributor is calculated, and\nbenefiting from a bi-objective optimization model, the optimal portfolio is\ndesigned.\n","authors":["Mohammad Alipour-Vaezi","Kwok-Leung Tsui"],"pdf_url":"https://arxiv.org/pdf/2404.07434v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.10738v3","updated":"2024-04-11T02:21:26Z","published":"2023-05-18T06:17:50Z","title":"Deep Temporal Graph Clustering","summary":"  Deep graph clustering has recently received significant attention due to its\nability to enhance the representation learning capabilities of models in\nunsupervised scenarios. Nevertheless, deep clustering for temporal graphs,\nwhich could capture crucial dynamic interaction information, has not been fully\nexplored. It means that in many clustering-oriented real-world scenarios,\ntemporal graphs can only be processed as static graphs. This not only causes\nthe loss of dynamic information but also triggers huge computational\nconsumption. To solve the problem, we propose a general framework for deep\nTemporal Graph Clustering called TGC, which introduces deep clustering\ntechniques to suit the interaction sequence-based batch-processing pattern of\ntemporal graphs. In addition, we discuss differences between temporal graph\nclustering and static graph clustering from several levels. To verify the\nsuperiority of the proposed framework TGC, we conduct extensive experiments.\nThe experimental results show that temporal graph clustering enables more\nflexibility in finding a balance between time and space requirements, and our\nframework can effectively improve the performance of existing temporal graph\nlearning methods. The code is released:\nhttps://github.com/MGitHubL/Deep-Temporal-Graph-Clustering.\n","authors":["Meng Liu","Yue Liu","Ke Liang","Wenxuan Tu","Siwei Wang","Sihang Zhou","Xinwang Liu"],"pdf_url":"https://arxiv.org/pdf/2305.10738v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06647v2","updated":"2024-04-11T02:09:23Z","published":"2024-04-09T22:55:06Z","title":"From Protoscience to Epistemic Monoculture: How Benchmarking Set the\n  Stage for the Deep Learning Revolution","summary":"  Over the past decade, AI research has focused heavily on building ever-larger\ndeep learning models. This approach has simultaneously unlocked incredible\nachievements in science and technology, and hindered AI from overcoming\nlong-standing limitations with respect to explainability, ethical harms, and\nenvironmental efficiency. Drawing on qualitative interviews and computational\nanalyses, our three-part history of AI research traces the creation of this\n\"epistemic monoculture\" back to a radical reconceptualization of scientific\nprogress that began in the late 1980s. In the first era of AI research\n(1950s-late 1980s), researchers and patrons approached AI as a \"basic\" science\nthat would advance through autonomous exploration and organic assessments of\nprogress (e.g., peer-review, theoretical consensus). The failure of this\napproach led to a retrenchment of funding in the 1980s. Amid this \"AI Winter,\"\nan intervention by the U.S. government reoriented the field towards measurable\nprogress on tasks of military and commercial interest. A new evaluation system\ncalled \"benchmarking\" provided an objective way to quantify progress on tasks\nby focusing exclusively on increasing predictive accuracy on example datasets.\nDistilling science down to verifiable metrics clarified the roles of\nscientists, allowed the field to rapidly integrate talent, and provided clear\nsignals of significance and progress. But history has also revealed a tradeoff\nto this streamlined approach to science: the consolidation around external\ninterests and inherent conservatism of benchmarking has disincentivized\nexploration beyond scaling monoculture. In the discussion, we explain how AI's\nmonoculture offers a compelling challenge to the belief that basic,\nexploration-driven research is needed for scientific progress. Implications for\nthe spread of AI monoculture to other sciences in the era of generative AI are\nalso discussed.\n","authors":["Bernard J. Koch","David Peterson"],"pdf_url":"https://arxiv.org/pdf/2404.06647v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.01574v2","updated":"2024-04-11T02:00:12Z","published":"2024-04-02T02:08:29Z","title":"Multi-granular Adversarial Attacks against Black-box Neural Ranking\n  Models","summary":"  Adversarial ranking attacks have gained increasing attention due to their\nsuccess in probing vulnerabilities, and, hence, enhancing the robustness, of\nneural ranking models. Conventional attack methods employ perturbations at a\nsingle granularity, e.g., word or sentence level, to target documents. However,\nlimiting perturbations to a single level of granularity may reduce the\nflexibility of adversarial examples, thereby diminishing the potential threat\nof the attack. Therefore, we focus on generating high-quality adversarial\nexamples by incorporating multi-granular perturbations. Achieving this\nobjective involves tackling a combinatorial explosion problem, which requires\nidentifying an optimal combination of perturbations across all possible levels\nof granularity, positions, and textual pieces. To address this challenge, we\ntransform the multi-granular adversarial attack into a sequential\ndecision-making process, where perturbations in the next attack step build on\nthe perturbed document in the current attack step. Since the attack process can\nonly access the final state without direct intermediate signals, we use\nreinforcement learning to perform multi-granular attacks. During the\nreinforcement learning process, two agents work cooperatively to identify\nmulti-granular vulnerabilities as attack targets and organize perturbation\ncandidates into a final perturbation sequence. Experimental results show that\nour attack method surpasses prevailing baselines in both attack effectiveness\nand imperceptibility.\n","authors":["Yu-An Liu","Ruqing Zhang","Jiafeng Guo","Maarten de Rijke","Yixing Fan","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2404.01574v2.pdf","comment":"Accepted by SIGIR2024"},{"id":"http://arxiv.org/abs/2404.07428v1","updated":"2024-04-11T01:59:29Z","published":"2024-04-11T01:59:29Z","title":"AdaDemo: Data-Efficient Demonstration Expansion for Generalist Robotic\n  Agent","summary":"  Encouraged by the remarkable achievements of language and vision foundation\nmodels, developing generalist robotic agents through imitation learning, using\nlarge demonstration datasets, has become a prominent area of interest in robot\nlearning. The efficacy of imitation learning is heavily reliant on the quantity\nand quality of the demonstration datasets. In this study, we aim to scale up\ndemonstrations in a data-efficient way to facilitate the learning of generalist\nrobotic agents. We introduce AdaDemo (Adaptive Online Demonstration Expansion),\na general framework designed to improve multi-task policy learning by actively\nand continually expanding the demonstration dataset. AdaDemo strategically\ncollects new demonstrations to address the identified weakness in the existing\npolicy, ensuring data efficiency is maximized. Through a comprehensive\nevaluation on a total of 22 tasks across two robotic manipulation benchmarks\n(RLBench and Adroit), we demonstrate AdaDemo's capability to progressively\nimprove policy performance by guiding the generation of high-quality\ndemonstration datasets in a data-efficient manner.\n","authors":["Tongzhou Mu","Yijie Guo","Jie Xu","Ankit Goyal","Hao Su","Dieter Fox","Animesh Garg"],"pdf_url":"https://arxiv.org/pdf/2404.07428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02332v2","updated":"2024-04-11T01:21:03Z","published":"2024-02-04T03:54:31Z","title":"Minusformer: Improving Time Series Forecasting by Progressively Learning\n  Residuals","summary":"  In this paper, we find that ubiquitous time series (TS) forecasting models\nare prone to severe overfitting. To cope with this problem, we embrace a\nde-redundancy approach to progressively reinstate the intrinsic values of TS\nfor future intervals. Specifically, we renovate the vanilla Transformer by\nreorienting the information aggregation mechanism from addition to subtraction.\nThen, we incorporate an auxiliary output branch into each block of the original\nmodel to construct a highway leading to the ultimate prediction. The output of\nsubsequent modules in this branch will subtract the previously learned results,\nenabling the model to learn the residuals of the supervision signal, layer by\nlayer. This designing facilitates the learning-driven implicit progressive\ndecomposition of the input and output streams, empowering the model with\nheightened versatility, interpretability, and resilience against overfitting.\nSince all aggregations in the model are minus signs, which is called\nMinusformer. Extensive experiments demonstrate the proposed method outperform\nexisting state-of-the-art methods, yielding an average performance improvement\nof 11.9% across various datasets.\n","authors":["Daojun Liang","Haixia Zhang","Dongfeng Yuan","Bingzheng Zhang","Minggao Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.02332v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06694v2","updated":"2024-04-11T01:18:06Z","published":"2023-10-10T15:13:30Z","title":"Sheared LLaMA: Accelerating Language Model Pre-training via Structured\n  Pruning","summary":"  The popularity of LLaMA (Touvron et al., 2023a;b) and other recently emerged\nmoderate-sized large language models (LLMs) highlights the potential of\nbuilding smaller yet powerful LLMs. Regardless, the cost of training such\nmodels from scratch on trillions of tokens remains high. In this work, we study\nstructured pruning as an effective means to develop smaller LLMs from\npre-trained, larger models. Our approach employs two key techniques: (1)\ntargeted structured pruning, which prunes a larger model to a specified target\nshape by removing layers, heads, and intermediate and hidden dimensions in an\nend-to-end manner, and (2) dynamic batch loading, which dynamically updates the\ncomposition of sampled data in each training batch based on varying losses\nacross different domains. We demonstrate the efficacy of our approach by\npresenting the Sheared-LLaMA series, pruning the LLaMA2-7B model down to 1.3B\nand 2.7B parameters. Sheared-LLaMA models outperform state-of-the-art\nopen-source models of equivalent sizes, such as Pythia, INCITE, OpenLLaMA and\nthe concurrent TinyLlama models, on a wide range of downstream and instruction\ntuning evaluations, while requiring only 3% of compute compared to training\nsuch models from scratch. This work provides compelling evidence that\nleveraging existing LLMs with structured pruning is a far more cost-effective\napproach for building competitive small-scale LLMs\n","authors":["Mengzhou Xia","Tianyu Gao","Zhiyuan Zeng","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2310.06694v2.pdf","comment":"The code and models are available at\n  https://github.com/princeton-nlp/LLM-Shearing"},{"id":"http://arxiv.org/abs/2404.07123v2","updated":"2024-04-11T01:09:08Z","published":"2024-04-10T16:04:07Z","title":"Semantically-correlated memories in a dense associative model","summary":"  I introduce a novel associative memory model named Correlated Dense\nAssociative Memory (CDAM), which integrates both auto- and hetero-association\nin a unified framework for continuous-valued memory patterns. Employing an\narbitrary graph structure to semantically link memory patterns, CDAM is\ntheoretically and numerically analysed, revealing four distinct dynamical\nmodes: auto-association, narrow hetero-association, wide hetero-association,\nand neutral quiescence. Drawing inspiration from inhibitory modulation studies,\nI employ anti-Hebbian learning rules to control the range of\nhetero-association, extract multi-scale representations of community structures\nin graphs, and stabilise the recall of temporal sequences. Experimental\ndemonstrations showcase CDAM's efficacy in handling real-world data,\nreplicating a classical neuroscience experiment, performing image retrieval,\nand simulating arbitrary finite automata.\n","authors":["Thomas F Burns"],"pdf_url":"https://arxiv.org/pdf/2404.07123v2.pdf","comment":"35 pages, 32 figures"},{"id":"http://arxiv.org/abs/2404.07410v1","updated":"2024-04-11T00:49:38Z","published":"2024-04-11T00:49:38Z","title":"Improving Shift Invariance in Convolutional Neural Networks with\n  Translation Invariant Polyphase Sampling","summary":"  Downsampling operators break the shift invariance of convolutional neural\nnetworks (CNNs) and this affects the robustness of features learned by CNNs\nwhen dealing with even small pixel-level shift. Through a large-scale\ncorrelation analysis framework, we study shift invariance of CNNs by inspecting\nexisting downsampling operators in terms of their maximum-sampling bias (MSB),\nand find that MSB is negatively correlated with shift invariance. Based on this\ncrucial insight, we propose a learnable pooling operator called Translation\nInvariant Polyphase Sampling (TIPS) and two regularizations on the intermediate\nfeature maps of TIPS to reduce MSB and learn translation-invariant\nrepresentations. TIPS can be integrated into any CNN and can be trained\nend-to-end with marginal computational overhead. Our experiments demonstrate\nthat TIPS results in consistent performance gains in terms of accuracy, shift\nconsistency, and shift fidelity on multiple benchmarks for image classification\nand semantic segmentation compared to previous methods and also leads to\nimprovements in adversarial and distributional robustness. TIPS results in the\nlowest MSB compared to all previous methods, thus explaining our strong\nempirical results.\n","authors":["Sourajit Saha","Tejas Gokhale"],"pdf_url":"https://arxiv.org/pdf/2404.07410v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.19521v2","updated":"2024-04-11T00:38:29Z","published":"2023-05-31T03:11:15Z","title":"Incremental Randomized Smoothing Certification","summary":"  Randomized smoothing-based certification is an effective approach for\nobtaining robustness certificates of deep neural networks (DNNs) against\nadversarial attacks. This method constructs a smoothed DNN model and certifies\nits robustness through statistical sampling, but it is computationally\nexpensive, especially when certifying with a large number of samples.\nFurthermore, when the smoothed model is modified (e.g., quantized or pruned),\ncertification guarantees may not hold for the modified DNN, and recertifying\nfrom scratch can be prohibitively expensive.\n  We present the first approach for incremental robustness certification for\nrandomized smoothing, IRS. We show how to reuse the certification guarantees\nfor the original smoothed model to certify an approximated model with very few\nsamples. IRS significantly reduces the computational cost of certifying\nmodified DNNs while maintaining strong robustness guarantees. We experimentally\ndemonstrate the effectiveness of our approach, showing up to 3x certification\nspeedup over the certification that applies randomized smoothing of the\napproximate model from scratch.\n","authors":["Shubham Ugare","Tarun Suresh","Debangshu Banerjee","Gagandeep Singh","Sasa Misailovic"],"pdf_url":"https://arxiv.org/pdf/2305.19521v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2306.06611v2","updated":"2024-04-11T00:31:28Z","published":"2023-06-11T07:28:35Z","title":"Learning the Positions in CountSketch","summary":"  We consider sketching algorithms which first compress data by multiplication\nwith a random sketch matrix, and then apply the sketch to quickly solve an\noptimization problem, e.g., low-rank approximation and regression. In the\nlearning-based sketching paradigm proposed by~\\cite{indyk2019learning}, the\nsketch matrix is found by choosing a random sparse matrix, e.g., CountSketch,\nand then the values of its non-zero entries are updated by running gradient\ndescent on a training data set. Despite the growing body of work on this\nparadigm, a noticeable omission is that the locations of the non-zero entries\nof previous algorithms were fixed, and only their values were learned. In this\nwork, we propose the first learning-based algorithms that also optimize the\nlocations of the non-zero entries. Our first proposed algorithm is based on a\ngreedy algorithm. However, one drawback of the greedy algorithm is its slower\ntraining time. We fix this issue and propose approaches for learning a\nsketching matrix for both low-rank approximation and Hessian approximation for\nsecond order optimization. The latter is helpful for a range of constrained\noptimization problems, such as LASSO and matrix estimation with a nuclear norm\nconstraint. Both approaches achieve good accuracy with a fast running time.\nMoreover, our experiments suggest that our algorithm can still reduce the error\nsignificantly even if we only have a very limited number of training matrices.\n","authors":["Yi Li","Honghao Lin","Simin Liu","Ali Vakilian","David P. Woodruff"],"pdf_url":"https://arxiv.org/pdf/2306.06611v2.pdf","comment":"Corrected the proof of Theorem 5.1. arXiv admin note: text overlap\n  with arXiv:2007.09890"},{"id":"http://arxiv.org/abs/2404.07395v1","updated":"2024-04-11T00:02:57Z","published":"2024-04-11T00:02:57Z","title":"Global versus Local: Evaluating AlexNet Architectures for Tropical\n  Cyclone Intensity Estimation","summary":"  Given the destructive impacts of tropical cyclones, it is critical to have a\nreliable system for cyclone intensity detection. Various techniques are\navailable for this purpose, each with differing levels of accuracy. In this\npaper, we introduce two ensemble-based models based on AlexNet architecture to\nestimate tropical cyclone intensity using visible satellite images. The first\nmodel, trained on the entire dataset, is called the global AlexNet model. The\nsecond model is a distributed version of AlexNet in which multiple AlexNets are\ntrained separately on subsets of the training data categorized according to the\nSaffir-Simpson wind speed scale prescribed by the meterologists. We evaluated\nthe performance of both models against a deep learning benchmark model called\n\\textit{Deepti} using a publicly available cyclone image dataset. Results\nindicate that both the global model (with a root mean square error (RMSE) of\n9.03 knots) and the distributed model (with a RMSE of 9.3 knots) outperform the\nbenchmark model (with a RMSE of 13.62 knots). We provide a thorough discussion\nof our solution approach, including an explanantion of the AlexNet's\nperformance using gradient class activation maps (grad-CAM). Our proposed\nsolution strategy allows future experimentation with various deep learning\nmodels in both single and multi-channel settings.\n","authors":["Vikas Dwivedi"],"pdf_url":"https://arxiv.org/pdf/2404.07395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2010.12998v4","updated":"2024-04-11T17:05:18Z","published":"2020-10-24T22:05:41Z","title":"Demystifying Why Local Aggregation Helps: Convergence Analysis of\n  Hierarchical SGD","summary":"  Hierarchical SGD (H-SGD) has emerged as a new distributed SGD algorithm for\nmulti-level communication networks. In H-SGD, before each global aggregation,\nworkers send their updated local models to local servers for aggregations.\nDespite recent research efforts, the effect of local aggregation on global\nconvergence still lacks theoretical understanding. In this work, we first\nintroduce a new notion of \"upward\" and \"downward\" divergences. We then use it\nto conduct a novel analysis to obtain a worst-case convergence upper bound for\ntwo-level H-SGD with non-IID data, non-convex objective function, and\nstochastic gradient. By extending this result to the case with random grouping,\nwe observe that this convergence upper bound of H-SGD is between the upper\nbounds of two single-level local SGD settings, with the number of local\niterations equal to the local and global update periods in H-SGD, respectively.\nWe refer to this as the \"sandwich behavior\". Furthermore, we extend our\nanalytical approach based on \"upward\" and \"downward\" divergences to study the\nconvergence for the general case of H-SGD with more than two levels, where the\n\"sandwich behavior\" still holds. Our theoretical results provide key insights\nof why local aggregation can be beneficial in improving the convergence of\nH-SGD.\n","authors":["Jiayi Wang","Shiqiang Wang","Rong-Rong Chen","Mingyue Ji"],"pdf_url":"https://arxiv.org/pdf/2010.12998v4.pdf","comment":"36 pages, in AAAI 2022"}],"Multimedia":[{"id":"http://arxiv.org/abs/2404.07872v1","updated":"2024-04-11T16:07:11Z","published":"2024-04-11T16:07:11Z","title":"Video Compression Beyond VVC: Quantitative Analysis of Intra Coding\n  Tools in Enhanced Compression Model (ECM)","summary":"  A quantitative analysis of post-VVC luma and chroma intra tools is presented,\nfocusing on their statistical behaviors, in terms of block selection rate under\ndifferent conditions. The aim is to provide insights to the standardization\ncommunity, offering a clearer understanding of interactions between tools and\nassisting in the design of an optimal combination of these novel tools when the\nJVET enters the standardization phase. Specifically, this paper examines the\nselection rate of intra tools as function of 1) the version of the ECM, 2)\nvideo resolution, and 3) video bitrate. Additionally, tests have been conducted\non sequences beyond the JVET CTC database. The statistics show several trends\nand interactions, with various strength, between coding tools of both luma and\nchroma.\n","authors":["Mohsen Abdoli","Ramin G. Youvalari","Karam Naser","Kevin Reuz√©","Fabrice Le L√©annec"],"pdf_url":"https://arxiv.org/pdf/2404.07872v1.pdf","comment":"Submitted to IEEE ICIP 2024"},{"id":"http://arxiv.org/abs/2206.00089v4","updated":"2024-04-11T09:45:58Z","published":"2022-05-31T19:59:00Z","title":"Defining Quantum Games","summary":"  In this article, we survey the existing quantum physics related games and\nbased on them propose a definition for the concept of quantum games. We define\nquantum games as any type of rule-based games that use the principles or\nreference the theory of quantum physics or quantum phenomena through any of\nthree proposed dimensions: the perceivable dimension of quantum physics, the\ndimension of quantum technologies, and the dimension of scientific purposes\nlike citizen science or education. We also discuss the concept of quantum\ncomputer games, games on quantum computers and discuss the definitions for the\nconcept of science games. At the same time, there are various games exploring\nquantum physics and quantum computing through digital, analogue, and hybrid\nmeans with diverse incentives driving their development. As interest in games\nas educational tools for supporting quantum literacy grows, understanding the\ndiverse landscape of quantum games becomes increasingly important. We propose\nthat three dimensions of quantum games identified in this article are used for\ndesigning, analysing and defining the phenomenon of quantum games.\n","authors":["Laura Piispanen","Marcel Pfaffhauser","James Wootton","Julian Togelius","Annakaisa Kultima"],"pdf_url":"https://arxiv.org/pdf/2206.00089v4.pdf","comment":"21 pages + references, 24 pictures in 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2404.07484v1","updated":"2024-04-11T05:44:27Z","published":"2024-04-11T05:44:27Z","title":"Multimodal Emotion Recognition by Fusing Video Semantic in MOOC Learning\n  Scenarios","summary":"  In the Massive Open Online Courses (MOOC) learning scenario, the semantic\ninformation of instructional videos has a crucial impact on learners' emotional\nstate. Learners mainly acquire knowledge by watching instructional videos, and\nthe semantic information in the videos directly affects learners' emotional\nstates. However, few studies have paid attention to the potential influence of\nthe semantic information of instructional videos on learners' emotional states.\nTo deeply explore the impact of video semantic information on learners'\nemotions, this paper innovatively proposes a multimodal emotion recognition\nmethod by fusing video semantic information and physiological signals. We\ngenerate video descriptions through a pre-trained large language model (LLM) to\nobtain high-level semantic information about instructional videos. Using the\ncross-attention mechanism for modal interaction, the semantic information is\nfused with the eye movement and PhotoPlethysmoGraphy (PPG) signals to obtain\nthe features containing the critical information of the three modes. The\naccurate recognition of learners' emotional states is realized through the\nemotion classifier. The experimental results show that our method has\nsignificantly improved emotion recognition performance, providing a new\nperspective and efficient method for emotion recognition research in MOOC\nlearning scenarios. The method proposed in this paper not only contributes to a\ndeeper understanding of the impact of instructional videos on learners'\nemotional states but also provides a beneficial reference for future research\non emotion recognition in MOOC learning scenarios.\n","authors":["Yuan Zhang","Xiaomei Tao","Hanxu Ai","Tao Chen","Yanling Gan"],"pdf_url":"https://arxiv.org/pdf/2404.07484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00511v3","updated":"2024-04-11T05:14:35Z","published":"2024-03-31T01:16:02Z","title":"MIPS at SemEval-2024 Task 3: Multimodal Emotion-Cause Pair Extraction in\n  Conversations with Multimodal Language Models","summary":"  This paper presents our winning submission to Subtask 2 of SemEval 2024 Task\n3 on multimodal emotion cause analysis in conversations. We propose a novel\nMultimodal Emotion Recognition and Multimodal Emotion Cause Extraction\n(MER-MCE) framework that integrates text, audio, and visual modalities using\nspecialized emotion encoders. Our approach sets itself apart from\ntop-performing teams by leveraging modality-specific features for enhanced\nemotion understanding and causality inference. Experimental evaluation\ndemonstrates the advantages of our multimodal approach, with our submission\nachieving a competitive weighted F1 score of 0.3435, ranking third with a\nmargin of only 0.0339 behind the 1st team and 0.0025 behind the 2nd team.\nProject: https://github.com/MIPS-COLT/MER-MCE.git\n","authors":["Zebang Cheng","Fuqiang Niu","Yuxiang Lin","Zhi-Qi Cheng","Bowen Zhang","Xiaojiang Peng"],"pdf_url":"https://arxiv.org/pdf/2404.00511v3.pdf","comment":"Ranked 3rd in SemEval '24 Task 3 with F1 of 0.3435, close to 1st &\n  2nd by 0.0339 & 0.0025"}]},"2024-04-12T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2404.08634v1","updated":"2024-04-12T17:53:34Z","published":"2024-04-12T17:53:34Z","title":"Pre-training Small Base LMs with Fewer Tokens","summary":"  We study the effectiveness of a simple approach to develop a small base\nlanguage model (LM) starting from an existing large base LM: first inherit a\nfew transformer blocks from the larger LM, and then train this smaller model on\na very small subset (0.1\\%) of the raw pretraining data of the larger model. We\ncall our simple recipe Inheritune and first demonstrate it for building a small\nbase LM with 1.5B parameters using 1B tokens (and a starting few layers of\nlarger LM of 3B parameters); we do this using a single A6000 GPU for less than\nhalf a day. Across 9 diverse evaluation datasets as well as the MMLU benchmark,\nthe resulting model compares favorably to publicly available base models of\n1B-2B size, some of which have been trained using 50-1000 times more tokens.\n  We investigate Inheritune in a slightly different setting where we train\nsmall LMs utilizing larger LMs and their full pre-training dataset. Here we\nshow that smaller LMs trained utilizing some of the layers of GPT2-medium\n(355M) and GPT-2-large (770M) can effectively match the val loss of their\nbigger counterparts when trained from scratch for the same number of training\nsteps on OpenWebText dataset with 9B tokens. We analyze our recipe with\nextensive experiments and demonstrate it efficacy on diverse settings. Our code\nis available at https://github.com/sanyalsunny111/LLM-Inheritune.\n","authors":["Sunny Sanyal","Sujay Sanghavi","Alexandros G. Dimakis"],"pdf_url":"https://arxiv.org/pdf/2404.08634v1.pdf","comment":"15 pages, 6 figures, 10 tables"},{"id":"http://arxiv.org/abs/2404.08627v1","updated":"2024-04-12T17:41:05Z","published":"2024-04-12T17:41:05Z","title":"Is ChatGPT Transforming Academics' Writing Style?","summary":"  Based on one million arXiv papers submitted from May 2018 to January 2024, we\nassess the textual density of ChatGPT's writing style in their abstracts by\nmeans of a statistical analysis of word frequency changes. Our model is\ncalibrated and validated on a mixture of real abstracts and ChatGPT-modified\nabstracts (simulated data) after a careful noise analysis. We find that ChatGPT\nis having an increasing impact on arXiv abstracts, especially in the field of\ncomputer science, where the fraction of ChatGPT-revised abstracts is estimated\nto be approximately 35%, if we take the output of one of the simplest prompts,\n\"revise the following sentences\", as a baseline. We conclude with an analysis\nof both positive and negative aspects of the penetration of ChatGPT into\nacademics' writing style.\n","authors":["Mingmeng Geng","Roberto Trotta"],"pdf_url":"https://arxiv.org/pdf/2404.08627v1.pdf","comment":"15 pages, 19 figures"},{"id":"http://arxiv.org/abs/2403.15388v4","updated":"2024-04-12T17:34:29Z","published":"2024-03-22T17:59:52Z","title":"LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal\n  Models","summary":"  Large Multimodal Models (LMMs) have shown significant reasoning capabilities\nby connecting a visual encoder and a large language model. LMMs typically use a\nfixed amount of visual tokens, such as the penultimate layer features in the\nCLIP visual encoder, as the prefix content. Recent LMMs incorporate more\ncomplex visual inputs, such as high-resolution images and videos, which\nincrease the number of visual tokens significantly. However, due to the design\nof the Transformer architecture, computational costs associated with these\nmodels tend to increase quadratically with the number of input tokens. To\ntackle this problem, we explore a token reduction mechanism and find, similar\nto prior work, that many visual tokens are spatially redundant. Based on this,\nwe propose PruMerge, a novel adaptive visual token reduction approach, which\nlargely reduces the number of visual tokens while maintaining comparable model\nperformance. We first select the unpruned visual tokens based on their\nsimilarity to class tokens and spatial tokens. We then cluster the pruned\ntokens based on key similarity and merge the clustered tokens with the unpruned\ntokens to supplement their information. Empirically, when applied to LLaVA-1.5,\nour approach can compress the visual tokens by 18 times on average, and achieve\ncomparable performance across diverse visual question-answering and reasoning\ntasks. Code and checkpoints are at https://llava-prumerge.github.io/.\n","authors":["Yuzhang Shang","Mu Cai","Bingxin Xu","Yong Jae Lee","Yan Yan"],"pdf_url":"https://arxiv.org/pdf/2403.15388v4.pdf","comment":"Project page: https://llava-prumerge.github.io/"},{"id":"http://arxiv.org/abs/2404.08617v1","updated":"2024-04-12T17:27:54Z","published":"2024-04-12T17:27:54Z","title":"Synthetic Dataset Creation and Fine-Tuning of Transformer Models for\n  Question Answering in Serbian","summary":"  In this paper, we focus on generating a synthetic question answering (QA)\ndataset using an adapted Translate-Align-Retrieve method. Using this method, we\ncreated the largest Serbian QA dataset of more than 87K samples, which we name\nSQuAD-sr. To acknowledge the script duality in Serbian, we generated both\nCyrillic and Latin versions of the dataset. We investigate the dataset quality\nand use it to fine-tune several pre-trained QA models. Best results were\nobtained by fine-tuning the BERTi\\'c model on our Latin SQuAD-sr dataset,\nachieving 73.91% Exact Match and 82.97% F1 score on the benchmark XQuAD\ndataset, which we translated into Serbian for the purpose of evaluation. The\nresults show that our model exceeds zero-shot baselines, but fails to go beyond\nhuman performance. We note the advantage of using a monolingual pre-trained\nmodel over multilingual, as well as the performance increase gained by using\nLatin over Cyrillic. By performing additional analysis, we show that questions\nabout numeric values or dates are more likely to be answered correctly than\nother types of questions. Finally, we conclude that SQuAD-sr is of sufficient\nquality for fine-tuning a Serbian QA model, in the absence of a manually\ncrafted and annotated dataset.\n","authors":["Aleksa Cvetanoviƒá","Predrag Tadiƒá"],"pdf_url":"https://arxiv.org/pdf/2404.08617v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07520v2","updated":"2024-04-12T17:01:04Z","published":"2024-04-11T07:26:00Z","title":"PromptSync: Bridging Domain Gaps in Vision-Language Models through\n  Class-Aware Prototype Alignment and Discrimination","summary":"  The potential for zero-shot generalization in vision-language (V-L) models\nsuch as CLIP has spurred their widespread adoption in addressing numerous\ndownstream tasks. Previous methods have employed test-time prompt tuning to\nadapt the model to unseen domains, but they overlooked the issue of imbalanced\nclass distributions. In this study, we explicitly address this problem by\nemploying class-aware prototype alignment weighted by mean class probabilities\nobtained for the test sample and filtered augmented views. Additionally, we\nensure that the class probabilities are as accurate as possible by performing\nprototype discrimination using contrastive learning. The combination of\nalignment and discriminative loss serves as a geometric regularizer, preventing\nthe prompt representation from collapsing onto a single class and effectively\nbridging the distribution gap between the source and test domains. Our method,\nnamed PromptSync, synchronizes the prompts for each test sample on both the\ntext and vision branches of the V-L model. In empirical evaluations on the\ndomain generalization benchmark, our method outperforms previous best methods\nby 2.33% in overall performance, by 1% in base-to-novel generalization, and by\n2.84% in cross-dataset transfer tasks.\n","authors":["Anant Khandelwal"],"pdf_url":"https://arxiv.org/pdf/2404.07520v2.pdf","comment":"Accepted at CVPR 2024 LIMIT, 12 pages, 8 Tables, 2 Figures"},{"id":"http://arxiv.org/abs/2404.08579v1","updated":"2024-04-12T16:23:41Z","published":"2024-04-12T16:23:41Z","title":"Small Models Are (Still) Effective Cross-Domain Argument Extractors","summary":"  Effective ontology transfer has been a major goal of recent work on event\nargument extraction (EAE). Two methods in particular -- question answering (QA)\nand template infilling (TI) -- have emerged as promising approaches to this\nproblem. However, detailed explorations of these techniques' ability to\nactually enable this transfer are lacking. In this work, we provide such a\nstudy, exploring zero-shot transfer using both techniques on six major EAE\ndatasets at both the sentence and document levels. Further, we challenge the\ngrowing reliance on LLMs for zero-shot extraction, showing that vastly smaller\nmodels trained on an appropriate source ontology can yield zero-shot\nperformance superior to that of GPT-3.5 or GPT-4.\n","authors":["William Gantt","Aaron Steven White"],"pdf_url":"https://arxiv.org/pdf/2404.08579v1.pdf","comment":"ACL Rolling Review Short Paper"},{"id":"http://arxiv.org/abs/2401.08047v2","updated":"2024-04-12T16:13:06Z","published":"2024-01-16T02:00:17Z","title":"Incremental Extractive Opinion Summarization Using Cover Trees","summary":"  Extractive opinion summarization involves automatically producing a summary\nof text about an entity (e.g., a product's reviews) by extracting\nrepresentative sentences that capture prevalent opinions in the review set.\nTypically, in online marketplaces user reviews accumulate over time, and\nopinion summaries need to be updated periodically to provide customers with\nup-to-date information. In this work, we study the task of extractive opinion\nsummarization in an incremental setting, where the underlying review set\nevolves over time. Many of the state-of-the-art extractive opinion\nsummarization approaches are centrality-based, such as CentroidRank (Radev et\nal., 2004; Chowdhury et al., 2022). CentroidRank performs extractive\nsummarization by selecting a subset of review sentences closest to the centroid\nin the representation space as the summary. However, these methods are not\ncapable of operating efficiently in an incremental setting, where reviews\narrive one at a time. In this paper, we present an efficient algorithm for\naccurately computing the CentroidRank summaries in an incremental setting. Our\napproach, CoverSumm, relies on indexing review representations in a cover tree\nand maintaining a reservoir of candidate summary review sentences. CoverSumm's\nefficacy is supported by a theoretical and empirical analysis of running time.\nEmpirically, on a diverse collection of data (both real and synthetically\ncreated to illustrate scaling considerations), we demonstrate that CoverSumm is\nup to 36x faster than baseline methods, and capable of adapting to nuanced\nchanges in data distribution. We also conduct human evaluations of the\ngenerated summaries and find that CoverSumm is capable of producing informative\nsummaries consistent with the underlying review set.\n","authors":["Somnath Basu Roy Chowdhury","Nicholas Monath","Avinava Dubey","Manzil Zaheer","Andrew McCallum","Amr Ahmed","Snigdha Chaturvedi"],"pdf_url":"https://arxiv.org/pdf/2401.08047v2.pdf","comment":"Accepted at TMLR"},{"id":"http://arxiv.org/abs/2403.15412v2","updated":"2024-04-12T16:09:59Z","published":"2024-03-05T08:29:36Z","title":"Towards Measuring and Modeling \"Culture\" in LLMs: A Survey","summary":"  We present a survey of 39 recent papers that aim to study cultural\nrepresentation and inclusion in large language models. We observe that none of\nthe studies define \"culture,\" which is a complex, multifaceted concept;\ninstead, they probe the models on some specially designed datasets which\nrepresent certain aspects of \"culture.\" We call these aspects the proxies of\ncultures, and organize them across three dimensions of demographic, semantic\nand linguistic-cultural interaction proxies. We also categorize the probing\nmethods employed. Our analysis indicates that only certain aspects of\n\"culture,\" such as values and objectives, have been studied, leaving several\nother interesting and important facets, especially the multitude of semantic\ndomains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022),\nunexplored. Two other crucial gaps are the lack of robustness and situatedness\nof the current methods. Based on these observations, we provide several\nrecommendations for a holistic and practically useful research agenda for\nfurthering cultural inclusion in LLMs and LLM-based applications.\n","authors":["Muhammad Farid Adilazuarda","Sagnik Mukherjee","Pradhyumna Lavania","Siddhant Singh","Ashutosh Dwivedi","Alham Fikri Aji","Jacki O'Neill","Ashutosh Modi","Monojit Choudhury"],"pdf_url":"https://arxiv.org/pdf/2403.15412v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08559v1","updated":"2024-04-12T15:57:41Z","published":"2024-04-12T15:57:41Z","title":"MoPE: Mixture of Prefix Experts for Zero-Shot Dialogue State Tracking","summary":"  Zero-shot dialogue state tracking (DST) transfers knowledge to unseen\ndomains, reducing the cost of annotating new datasets. Previous zero-shot DST\nmodels mainly suffer from domain transferring and partial prediction problems.\nTo address these challenges, we propose Mixture of Prefix Experts (MoPE) to\nestablish connections between similar slots in different domains, which\nstrengthens the model transfer performance in unseen domains. Empirical results\ndemonstrate that MoPE-DST achieves the joint goal accuracy of 57.13% on\nMultiWOZ2.1 and 55.40% on SGD.\n","authors":["Tianwen Tang","Tong Zhu","Haodong Liu","Yin Bai","Jia Cheng","Wenliang Chen"],"pdf_url":"https://arxiv.org/pdf/2404.08559v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2404.08555v1","updated":"2024-04-12T15:54:15Z","published":"2024-04-12T15:54:15Z","title":"RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\n  Human Feedback for LLMs","summary":"  State-of-the-art large language models (LLMs) have become indispensable tools\nfor various tasks. However, training LLMs to serve as effective assistants for\nhumans requires careful consideration. A promising approach is reinforcement\nlearning from human feedback (RLHF), which leverages human feedback to update\nthe model in accordance with human preferences and mitigate issues like\ntoxicity and hallucinations. Yet, an understanding of RLHF for LLMs is largely\nentangled with initial design choices that popularized the method and current\nresearch focuses on augmenting those choices rather than fundamentally\nimproving the framework. In this paper, we analyze RLHF through the lens of\nreinforcement learning principles to develop an understanding of its\nfundamentals, dedicating substantial focus to the core component of RLHF -- the\nreward model. Our study investigates modeling choices, caveats of function\napproximation, and their implications on RLHF training algorithms, highlighting\nthe underlying assumptions made about the expressivity of reward. Our analysis\nimproves the understanding of the role of reward models and methods for their\ntraining, concurrently revealing limitations of the current methodology. We\ncharacterize these limitations, including incorrect generalization, model\nmisspecification, and the sparsity of feedback, along with their impact on the\nperformance of a language model. The discussion and analysis are substantiated\nby a categorical review of current literature, serving as a reference for\nresearchers and practitioners to understand the challenges of RLHF and build\nupon existing efforts.\n","authors":["Shreyas Chaudhari","Pranjal Aggarwal","Vishvak Murahari","Tanmay Rajpurohit","Ashwin Kalyan","Karthik Narasimhan","Ameet Deshpande","Bruno Castro da Silva"],"pdf_url":"https://arxiv.org/pdf/2404.08555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06499v3","updated":"2024-04-12T15:50:14Z","published":"2023-12-11T16:22:37Z","title":"TaCo: Targeted Concept Removal in Output Embeddings for NLP via\n  Information Theory and Explainability","summary":"  The fairness of Natural Language Processing (NLP) models has emerged as a\ncrucial concern. Information theory indicates that to achieve fairness, a model\nshould not be able to predict sensitive variables, such as gender, ethnicity,\nand age. However, information related to these variables often appears\nimplicitly in language, posing a challenge in identifying and mitigating biases\neffectively. To tackle this issue, we present a novel approach that operates at\nthe embedding level of an NLP model, independent of the specific architecture.\nOur method leverages insights from recent advances in XAI techniques and\nemploys an embedding transformation to eliminate implicit information from a\nselected variable. By directly manipulating the embeddings in the final layer,\nour approach enables a seamless integration into existing models without\nrequiring significant modifications or retraining. In evaluation, we show that\nthe proposed post-hoc approach significantly reduces gender-related\nassociations in NLP models while preserving the overall performance and\nfunctionality of the models. An implementation of our method is available:\nhttps://github.com/fanny-jourdan/TaCo\n","authors":["Fanny Jourdan","Louis B√©thune","Agustin Picard","Laurent Risser","Nicholas Asher"],"pdf_url":"https://arxiv.org/pdf/2312.06499v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08538v1","updated":"2024-04-12T15:32:17Z","published":"2024-04-12T15:32:17Z","title":"VertAttack: Taking advantage of Text Classifiers' horizontal vision","summary":"  Text classification systems have continuously improved in performance over\nthe years. However, nearly all current SOTA classifiers have a similar\nshortcoming, they process text in a horizontal manner. Vertically written words\nwill not be recognized by a classifier. In contrast, humans are easily able to\nrecognize and read words written both horizontally and vertically. Hence, a\nhuman adversary could write problematic words vertically and the meaning would\nstill be preserved to other humans. We simulate such an attack, VertAttack.\nVertAttack identifies which words a classifier is reliant on and then rewrites\nthose words vertically. We find that VertAttack is able to greatly drop the\naccuracy of 4 different transformer models on 5 datasets. For example, on the\nSST2 dataset, VertAttack is able to drop RoBERTa's accuracy from 94 to 13%.\nFurthermore, since VertAttack does not replace the word, meaning is easily\npreserved. We verify this via a human study and find that crowdworkers are able\nto correctly label 77% perturbed texts perturbed, compared to 81% of the\noriginal texts. We believe VertAttack offers a look into how humans might\ncircumvent classifiers in the future and thus inspire a look into more robust\nalgorithms.\n","authors":["Jonathan Rusert"],"pdf_url":"https://arxiv.org/pdf/2404.08538v1.pdf","comment":"14 pages, 4 figures, accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2404.06407v2","updated":"2024-04-12T15:02:15Z","published":"2024-04-09T15:54:16Z","title":"Rethinking How to Evaluate Language Model Jailbreak","summary":"  Large language models (LLMs) have become increasingly integrated with various\napplications. To ensure that LLMs do not generate unsafe responses, they are\naligned with safeguards that specify what content is restricted. However, such\nalignment can be bypassed to produce prohibited content using a technique\ncommonly referred to as jailbreak. Different systems have been proposed to\nperform the jailbreak automatically. These systems rely on evaluation methods\nto determine whether a jailbreak attempt is successful. However, our analysis\nreveals that current jailbreak evaluation methods have two limitations. (1)\nTheir objectives lack clarity and do not align with the goal of identifying\nunsafe responses. (2) They oversimplify the jailbreak result as a binary\noutcome, successful or not. In this paper, we propose three metrics, safeguard\nviolation, informativeness, and relative truthfulness, to evaluate language\nmodel jailbreak. Additionally, we demonstrate how these metrics correlate with\nthe goal of different malicious actors. To compute these metrics, we introduce\na multifaceted approach that extends the natural language generation evaluation\nmethod after preprocessing the response. We evaluate our metrics on a benchmark\ndataset produced from three malicious intent datasets and three jailbreak\nsystems. The benchmark dataset is labeled by three annotators. We compare our\nmultifaceted approach with three existing jailbreak evaluation methods.\nExperiments demonstrate that our multifaceted evaluation outperforms existing\nmethods, with F1 scores improving on average by 17% compared to existing\nbaselines. Our findings motivate the need to move away from the binary view of\nthe jailbreak problem and incorporate a more comprehensive evaluation to ensure\nthe safety of the language model.\n","authors":["Hongyu Cai","Arjun Arunasalam","Leo Y. Lin","Antonio Bianchi","Z. Berkay Celik"],"pdf_url":"https://arxiv.org/pdf/2404.06407v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08517v1","updated":"2024-04-12T14:55:16Z","published":"2024-04-12T14:55:16Z","title":"Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path\n  Forward","summary":"  While Large Language Models (LLMs) have seen widespread applications across\nnumerous fields, their limited interpretability poses concerns regarding their\nsafe operations from multiple aspects, e.g., truthfulness, robustness, and\nfairness. Recent research has started developing quality assurance methods for\nLLMs, introducing techniques such as offline detector-based or uncertainty\nestimation methods. However, these approaches predominantly concentrate on\npost-generation analysis, leaving the online safety analysis for LLMs during\nthe generation phase an unexplored area. To bridge this gap, we conduct in this\nwork a comprehensive evaluation of the effectiveness of existing online safety\nanalysis methods on LLMs. We begin with a pilot study that validates the\nfeasibility of detecting unsafe outputs in the early generation process.\nFollowing this, we establish the first publicly available benchmark of online\nsafety analysis for LLMs, including a broad spectrum of methods, models, tasks,\ndatasets, and evaluation metrics. Utilizing this benchmark, we extensively\nanalyze the performance of state-of-the-art online safety analysis methods on\nboth open-source and closed-source LLMs. This analysis reveals the strengths\nand weaknesses of individual methods and offers valuable insights into\nselecting the most appropriate method based on specific application scenarios\nand task requirements. Furthermore, we also explore the potential of using\nhybridization methods, i.e., combining multiple methods to derive a collective\nsafety conclusion, to enhance the efficacy of online safety analysis for LLMs.\nOur findings indicate a promising direction for the development of innovative\nand trustworthy quality assurance methodologies for LLMs, facilitating their\nreliable deployments across diverse domains.\n","authors":["Xuan Xie","Jiayang Song","Zhehua Zhou","Yuheng Huang","Da Song","Lei Ma"],"pdf_url":"https://arxiv.org/pdf/2404.08517v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.10564v3","updated":"2024-04-12T14:53:30Z","published":"2022-12-20T18:59:50Z","title":"Re-evaluating the Need for Multimodal Signals in Unsupervised Grammar\n  Induction","summary":"  Are multimodal inputs necessary for grammar induction? Recent work has shown\nthat multimodal training inputs can improve grammar induction. However, these\nimprovements are based on comparisons to weak text-only baselines that were\ntrained on relatively little textual data. To determine whether multimodal\ninputs are needed in regimes with large amounts of textual training data, we\ndesign a stronger text-only baseline, which we refer to as LC-PCFG. LC-PCFG is\na C-PFCG that incorporates em-beddings from text-only large language models\n(LLMs). We use a fixed grammar family to directly compare LC-PCFG to various\nmulti-modal grammar induction methods. We compare performance on four benchmark\ndatasets. LC-PCFG provides an up to 17% relative improvement in Corpus-F1\ncompared to state-of-the-art multimodal grammar induction methods. LC-PCFG is\nalso more computationally efficient, providing an up to 85% reduction in\nparameter count and 8.8x reduction in training time compared to multimodal\napproaches. These results suggest that multimodal inputs may not be necessary\nfor grammar induction, and emphasize the importance of strong vision-free\nbaselines for evaluating the benefit of multimodal approaches.\n","authors":["Boyi Li","Rodolfo Corona","Karttikeya Mangalam","Catherine Chen","Daniel Flaherty","Serge Belongie","Kilian Q. Weinberger","Jitendra Malik","Trevor Darrell","Dan Klein"],"pdf_url":"https://arxiv.org/pdf/2212.10564v3.pdf","comment":"NAACL Findings 2024"},{"id":"http://arxiv.org/abs/2404.08511v1","updated":"2024-04-12T14:50:41Z","published":"2024-04-12T14:50:41Z","title":"Leveraging Multi-AI Agents for Cross-Domain Knowledge Discovery","summary":"  In the rapidly evolving field of artificial intelligence, the ability to\nharness and integrate knowledge across various domains stands as a paramount\nchallenge and opportunity. This study introduces a novel approach to\ncross-domain knowledge discovery through the deployment of multi-AI agents,\neach specialized in distinct knowledge domains. These AI agents, designed to\nfunction as domain-specific experts, collaborate in a unified framework to\nsynthesize and provide comprehensive insights that transcend the limitations of\nsingle-domain expertise. By facilitating seamless interaction among these\nagents, our platform aims to leverage the unique strengths and perspectives of\neach, thereby enhancing the process of knowledge discovery and decision-making.\nWe present a comparative analysis of the different multi-agent workflow\nscenarios evaluating their performance in terms of efficiency, accuracy, and\nthe breadth of knowledge integration. Through a series of experiments involving\ncomplex, interdisciplinary queries, our findings demonstrate the superior\ncapability of domain specific multi-AI agent system in identifying and bridging\nknowledge gaps. This research not only underscores the significance of\ncollaborative AI in driving innovation but also sets the stage for future\nadvancements in AI-driven, cross-disciplinary research and application. Our\nmethods were evaluated on a small pilot data and it showed a trend we expected,\nif we increase the amount of data we custom train the agents, the trend is\nexpected to be more smooth.\n","authors":["Shiva Aryal","Tuyen Do","Bisesh Heyojoo","Sandeep Chataut","Bichar Dip Shrestha Gurung","Venkataramana Gadhamshetty","Etienne Gnimpieba"],"pdf_url":"https://arxiv.org/pdf/2404.08511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08509v1","updated":"2024-04-12T14:46:15Z","published":"2024-04-12T14:46:15Z","title":"Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\n  Prediction","summary":"  Large language models (LLMs) have been driving a new wave of interactive AI\napplications across numerous domains. However, efficiently serving LLM\ninference requests is challenging due to their unpredictable execution times\noriginating from the autoregressive nature of generative models. Existing LLM\nserving systems exploit first-come-first-serve (FCFS) scheduling, suffering\nfrom head-of-line blocking issues. To address the non-deterministic nature of\nLLMs and enable efficient interactive LLM serving, we present a speculative\nshortest-job-first (SSJF) scheduler that uses a light proxy model to predict\nLLM output sequence lengths. Our open-source SSJF implementation does not\nrequire changes to memory management or batching strategies. Evaluations on\nreal-world datasets and production workload traces show that SSJF reduces\naverage job completion times by 30.5-39.6% and increases throughput by 2.2-3.6x\ncompared to FCFS schedulers, across no batching, dynamic batching, and\ncontinuous batching settings.\n","authors":["Haoran Qiu","Weichao Mao","Archit Patke","Shengkun Cui","Saurabh Jha","Chen Wang","Hubertus Franke","Zbigniew T. Kalbarczyk","Tamer Ba≈üar","Ravishankar K. Iyer"],"pdf_url":"https://arxiv.org/pdf/2404.08509v1.pdf","comment":"Accepted at AIOps'24"},{"id":"http://arxiv.org/abs/2404.00589v2","updated":"2024-04-12T14:30:10Z","published":"2024-03-31T07:38:39Z","title":"Harnessing the Power of Large Language Model for Uncertainty Aware Graph\n  Processing","summary":"  Handling graph data is one of the most difficult tasks. Traditional\ntechniques, such as those based on geometry and matrix factorization, rely on\nassumptions about the data relations that become inadequate when handling large\nand complex graph data. On the other hand, deep learning approaches demonstrate\npromising results in handling large graph data, but they often fall short of\nproviding interpretable explanations. To equip the graph processing with both\nhigh accuracy and explainability, we introduce a novel approach that harnesses\nthe power of a large language model (LLM), enhanced by an uncertainty-aware\nmodule to provide a confidence score on the generated answer. We experiment\nwith our approach on two graph processing tasks: few-shot knowledge graph\ncompletion and graph classification. Our results demonstrate that through\nparameter efficient fine-tuning, the LLM surpasses state-of-the-art algorithms\nby a substantial margin across ten diverse benchmark datasets. Moreover, to\naddress the challenge of explainability, we propose an uncertainty estimation\nbased on perturbation, along with a calibration scheme to quantify the\nconfidence scores of the generated answers. Our confidence measure achieves an\nAUC of 0.8 or higher on seven out of the ten datasets in predicting the\ncorrectness of the answer generated by LLM.\n","authors":["Zhenyu Qian","Yiming Qian","Yuting Song","Fei Gao","Hai Jin","Chen Yu","Xia Xie"],"pdf_url":"https://arxiv.org/pdf/2404.00589v2.pdf","comment":"Because my organization does not allow members to privately upload\n  papers to arXiv, I am requesting a withdrawal of my submission"},{"id":"http://arxiv.org/abs/2404.08495v1","updated":"2024-04-12T14:25:49Z","published":"2024-04-12T14:25:49Z","title":"Dataset Reset Policy Optimization for RLHF","summary":"  Reinforcement Learning (RL) from Human Preference-based feedback is a popular\nparadigm for fine-tuning generative models, which has produced impressive\nmodels such as GPT-4 and Claude3 Opus. This framework often consists of two\nsteps: learning a reward model from an offline preference dataset followed by\nrunning online RL to optimize the learned reward model. In this work,\nleveraging the idea of reset, we propose a new RLHF algorithm with provable\nguarantees. Motivated by the fact that offline preference dataset provides\ninformative states (i.e., data that is preferred by the labelers), our new\nalgorithm, Dataset Reset Policy Optimization (DR-PO), integrates the existing\noffline preference dataset into the online policy training procedure via\ndataset reset: it directly resets the policy optimizer to the states in the\noffline dataset, instead of always starting from the initial state\ndistribution. In theory, we show that DR-PO learns to perform at least as good\nas any policy that is covered by the offline dataset under general function\napproximation with finite sample complexity. In experiments, we demonstrate\nthat on both the TL;DR summarization and the Anthropic Helpful Harmful (HH)\ndataset, the generation from DR-PO is better than that from Proximal Policy\nOptimization (PPO) and Direction Preference Optimization (DPO), under the\nmetric of GPT4 win-rate. Code for this work can be found at\nhttps://github.com/Cornell-RL/drpo.\n","authors":["Jonathan D. Chang","Wenhao Shan","Owen Oertell","Kiant√© Brantley","Dipendra Misra","Jason D. Lee","Wen Sun"],"pdf_url":"https://arxiv.org/pdf/2404.08495v1.pdf","comment":"28 pages, 6 tables, 3 Figures, 3 Algorithms"},{"id":"http://arxiv.org/abs/2404.08491v1","updated":"2024-04-12T14:19:16Z","published":"2024-04-12T14:19:16Z","title":"Mitigating Language-Level Performance Disparity in mPLMs via Teacher\n  Language Selection and Cross-lingual Self-Distillation","summary":"  Large-scale multilingual Pretrained Language Models (mPLMs) yield impressive\nperformance on cross-language tasks, yet significant performance disparities\nexist across different languages within the same mPLM. Previous studies\nendeavored to narrow these disparities by supervise fine-tuning the mPLMs with\nmultilingual data. However, obtaining labeled multilingual data is\ntime-consuming, and fine-tuning mPLM with limited labeled multilingual data\nmerely encapsulates the knowledge specific to the labeled data. Therefore, we\nintroduce ALSACE to leverage the learned knowledge from the well-performing\nlanguages to guide under-performing ones within the same mPLM, eliminating the\nneed for additional labeled multilingual data. Experiments show that ALSACE\neffectively mitigates language-level performance disparity across various mPLMs\nwhile showing the competitive performance on different multilingual NLU tasks,\nranging from full resource to limited resource settings. The code for our\napproach is available at https://github.com/pkunlp-icler/ALSACE.\n","authors":["Haozhe Zhao","Zefan Cai","Shuzheng Si","Liang Chen","Yufeng He","Kaikai An","Baobao Chang"],"pdf_url":"https://arxiv.org/pdf/2404.08491v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2404.08488v1","updated":"2024-04-12T14:10:09Z","published":"2024-04-12T14:10:09Z","title":"Thematic Analysis with Large Language Models: does it work with\n  languages other than English? A targeted test in Italian","summary":"  This paper proposes a test to perform Thematic Analysis (TA) with Large\nLanguage Model (LLM) on data which is in a different language than English.\nWhile there has been initial promising work on using pre-trained LLMs for TA on\ndata in English, we lack any tests on whether these models can reasonably\nperform the same analysis with good quality in other language. In this paper a\ntest will be proposed using an open access dataset of semi-structured\ninterviews in Italian. The test shows that a pre-trained model can perform such\na TA on the data, also using prompts in Italian. A comparative test shows the\nmodel capacity to produce themes which have a good resemblance with those\nproduced independently by human researchers. The main implication of this study\nis that pre-trained LLMs may thus be suitable to support analysis in\nmultilingual situations, so long as the language is supported by the model\nused.\n","authors":["Stefano De Paoli"],"pdf_url":"https://arxiv.org/pdf/2404.08488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08380v2","updated":"2024-04-12T14:07:38Z","published":"2023-11-14T18:43:51Z","title":"Direct Preference Optimization for Neural Machine Translation with\n  Minimum Bayes Risk Decoding","summary":"  Minimum Bayes Risk (MBR) decoding can significantly improve translation\nperformance of Multilingual Large Language Models (MLLMs). However, MBR\ndecoding is computationally expensive. We show how the recently developed\nReinforcement Learning technique, Direct Preference Optimization (DPO), can\nfine-tune MLLMs to get the gains of MBR without any additional computation in\ninference. Our method uses only a small monolingual fine-tuning set and yields\nsignificantly improved performance on multiple NMT test sets compared to MLLMs\nwithout DPO.\n","authors":["Guangyu Yang","Jinghong Chen","Weizhe Lin","Bill Byrne"],"pdf_url":"https://arxiv.org/pdf/2311.08380v2.pdf","comment":"To appear at NAACL 2024"},{"id":"http://arxiv.org/abs/2404.08480v1","updated":"2024-04-12T13:57:30Z","published":"2024-04-12T13:57:30Z","title":"Decoding AI: The inside story of data analysis in ChatGPT","summary":"  As a result of recent advancements in generative AI, the field of Data\nScience is prone to various changes. This review critically examines the Data\nAnalysis (DA) capabilities of ChatGPT assessing its performance across a wide\nrange of tasks. While DA provides researchers and practitioners with\nunprecedented analytical capabilities, it is far from being perfect, and it is\nimportant to recognize and address its limitations.\n","authors":["Ozan Evkaya","Miguel de Carvalho"],"pdf_url":"https://arxiv.org/pdf/2404.08480v1.pdf","comment":"15 pages with figures and appendix"},{"id":"http://arxiv.org/abs/2403.04643v2","updated":"2024-04-12T13:00:25Z","published":"2024-03-07T16:42:37Z","title":"QAQ: Quality Adaptive Quantization for LLM KV Cache","summary":"  The emergence of LLMs has ignited a fresh surge of breakthroughs in NLP\napplications, particularly in domains such as question-answering systems and\ntext generation. As the need for longer context grows, a significant bottleneck\nin model deployment emerges due to the linear expansion of the Key-Value (KV)\ncache with the context length. Existing methods primarily rely on various\nhypotheses, such as sorting the KV cache based on attention scores for\nreplacement or eviction, to compress the KV cache and improve model throughput.\nHowever, heuristics used by these strategies may wrongly evict essential KV\ncache, which can significantly degrade model performance. In this paper, we\npropose QAQ, a Quality Adaptive Quantization scheme for the KV cache. We\ntheoretically demonstrate that key cache and value cache exhibit distinct\nsensitivities to quantization, leading to the formulation of separate\nquantization strategies for their non-uniform quantization. Through the\nintegration of dedicated outlier handling, as well as an improved\nattention-aware approach, QAQ achieves up to 10x the compression ratio of the\nKV cache size with a neglectable impact on model performance. QAQ significantly\nreduces the practical hurdles of deploying LLMs, opening up new possibilities\nfor longer-context applications. The code is available at\ngithub.com/ClubieDong/KVCacheQuantization.\n","authors":["Shichen Dong","Wen Cheng","Jiayu Qin","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.04643v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.01210v2","updated":"2024-04-12T12:27:48Z","published":"2024-04-01T16:10:15Z","title":"AILS-NTUA at SemEval-2024 Task 6: Efficient model tuning for\n  hallucination detection and analysis","summary":"  In this paper, we present our team's submissions for SemEval-2024 Task-6 -\nSHROOM, a Shared-task on Hallucinations and Related Observable Overgeneration\nMistakes. The participants were asked to perform binary classification to\nidentify cases of fluent overgeneration hallucinations. Our experimentation\nincluded fine-tuning a pre-trained model on hallucination detection and a\nNatural Language Inference (NLI) model. The most successful strategy involved\ncreating an ensemble of these models, resulting in accuracy rates of 77.8% and\n79.9% on model-agnostic and model-aware datasets respectively, outperforming\nthe organizers' baseline and achieving notable results when contrasted with the\ntop-performing results in the competition, which reported accuracies of 84.7%\nand 81.3% correspondingly.\n","authors":["Natalia Grigoriadou","Maria Lymperaiou","Giorgos Filandrianos","Giorgos Stamou"],"pdf_url":"https://arxiv.org/pdf/2404.01210v2.pdf","comment":"SemEval-2024"},{"id":"http://arxiv.org/abs/2309.12075v3","updated":"2024-04-12T12:25:50Z","published":"2023-09-21T13:45:32Z","title":"Prompt Tuned Embedding Classification for Multi-Label Industry Sector\n  Allocation","summary":"  Prompt Tuning is emerging as a scalable and cost-effective method to\nfine-tune Pretrained Language Models (PLMs), which are often referred to as\nLarge Language Models (LLMs). This study benchmarks the performance and\ncomputational efficiency of Prompt Tuning and baselines for multi-label text\nclassification. This is applied to the challenging task of classifying\ncompanies into an investment firm's proprietary industry taxonomy, supporting\ntheir thematic investment strategy. Text-to-text classification is frequently\nreported to outperform task-specific classification heads, but has several\nlimitations when applied to a multi-label classification problem where each\nlabel consists of multiple tokens: (a) Generated labels may not match any label\nin the label taxonomy; (b) The fine-tuning process lacks permutation invariance\nand is sensitive to the order of the provided labels; (c) The model provides\nbinary decisions rather than appropriate confidence scores. Limitation (a) is\naddressed by applying constrained decoding using Trie Search, which slightly\nimproves classification performance. All limitations (a), (b), and (c) are\naddressed by replacing the PLM's language head with a classification head,\nwhich is referred to as Prompt Tuned Embedding Classification (PTEC). This\nimproves performance significantly, while also reducing computational costs\nduring inference. In our industrial application, the training data is skewed\ntowards well-known companies. We confirm that the model's performance is\nconsistent across both well-known and less-known companies. Our overall results\nindicate the continuing need to adapt state-of-the-art methods to\ndomain-specific tasks, even in the era of PLMs with strong generalization\nabilities. We release our codebase and a benchmarking dataset at\nhttps://github.com/EQTPartners/PTEC.\n","authors":["Valentin Leonhard Buchner","Lele Cao","Jan-Christoph Kalo","Vilhelm von Ehrenheim"],"pdf_url":"https://arxiv.org/pdf/2309.12075v3.pdf","comment":"Accepted by NAACL 2024 industry track (6 pages, 4 figures). Source\n  code to be found at https://github.com/EQTPartners/PTEC"},{"id":"http://arxiv.org/abs/2404.08417v1","updated":"2024-04-12T12:06:02Z","published":"2024-04-12T12:06:02Z","title":"AdapterSwap: Continuous Training of LLMs with Data Removal and\n  Access-Control Guarantees","summary":"  Large language models (LLMs) are increasingly capable of completing knowledge\nintensive tasks by recalling information from a static pretraining corpus. Here\nwe are concerned with LLMs in the context of evolving data requirements. For\ninstance: batches of new data that are introduced periodically; subsets of data\nwith user-based access controls; or requirements on dynamic removal of\ndocuments with guarantees that associated knowledge cannot be recalled. We wish\nto satisfy these requirements while at the same time ensuring a model does not\nforget old information when new data becomes available. To address these\nissues, we introduce AdapterSwap, a training and inference scheme that\norganizes knowledge from a data collection into a set of low-rank adapters,\nwhich are dynamically composed during inference. Our experiments demonstrate\nAdapterSwap's ability to support efficient continual learning, while also\nenabling organizations to have fine-grained control over data access and\ndeletion.\n","authors":["William Fleshman","Aleem Khan","Marc Marone","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2404.08417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17097v2","updated":"2024-04-12T11:37:44Z","published":"2024-02-27T00:22:18Z","title":"Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM\n  Responses","summary":"  Mitigating hallucination issues is a key challenge that must be overcome to\nreliably deploy large language models (LLMs) in real-world scenarios. Recently,\nvarious methods have been proposed to detect and revise factual errors in\nLLM-generated texts, in order to reduce hallucination. In this paper, we\npropose Re-Ex, a method for post-editing LLM-generated responses. Re-Ex\nintroduces a novel reasoning step dubbed as the factual error explanation step.\nRe-Ex revises the initial response of LLMs using 3-steps : first, external\ntools are used to retrieve the evidences of the factual errors in the initial\nLLM response; next, LLM is instructed to explain the problematic parts of the\nresponse based on the gathered evidence; finally, LLM revises the initial\nresponse using the explanations provided in the previous step. In addition to\nthe explanation step, Re-Ex also incorporates new prompting techniques to\nreduce the token count and inference time required for the response revision\nprocess. Compared with existing methods including FacTool, CoVE, and RARR,\nRe-Ex provides better detection and revision performance with less inference\ntime and fewer tokens in multiple benchmarks.\n","authors":["Juyeon Kim","Jeongeun Lee","Yoonho Chang","Chanyeol Choi","Junseong Kim","Jy-yong Sohn"],"pdf_url":"https://arxiv.org/pdf/2402.17097v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2404.08403v1","updated":"2024-04-12T11:30:16Z","published":"2024-04-12T11:30:16Z","title":"Learning representations of learning representations","summary":"  The ICLR conference is unique among the top machine learning conferences in\nthat all submitted papers are openly available. Here we present the ICLR\ndataset consisting of abstracts of all 24 thousand ICLR submissions from\n2017-2024 with meta-data, decision scores, and custom keyword-based labels. We\nfind that on this dataset, bag-of-words representation outperforms most\ndedicated sentence transformer models in terms of $k$NN classification\naccuracy, and the top performing language models barely outperform TF-IDF. We\nsee this as a challenge for the NLP community. Furthermore, we use the ICLR\ndataset to study how the field of machine learning has changed over the last\nseven years, finding some improvement in gender balance. Using a 2D embedding\nof the abstracts' texts, we describe a shift in research topics from 2017 to\n2024 and identify hedgehogs and foxes among the authors with the highest number\nof ICLR submissions.\n","authors":["Rita Gonz√°lez-M√°rquez","Dmitry Kobak"],"pdf_url":"https://arxiv.org/pdf/2404.08403v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08772v2","updated":"2024-04-12T10:50:30Z","published":"2024-01-16T19:00:10Z","title":"HuixiangDou: Overcoming Group Chat Scenarios with LLM-based Technical\n  Assistance","summary":"  In this work, we present HuixiangDou, a technical assistant powered by Large\nLanguage Models (LLM). This system is designed to assist algorithm developers\nby providing insightful responses to questions related to open-source algorithm\nprojects, such as computer vision and deep learning projects from OpenMMLab. We\nfurther explore the integration of this assistant into the group chats of\ninstant messaging (IM) tools such as WeChat and Lark. Through several iterative\nimprovements and trials, we have developed a sophisticated technical chat\nassistant capable of effectively answering users' technical questions without\ncausing message flooding. This paper's contributions include: 1) Designing an\nalgorithm pipeline specifically for group chat scenarios; 2) Verifying the\nreliable performance of text2vec in task rejection; 3) Identifying three\ncritical requirements for LLMs in technical-assistant-like products, namely\nscoring ability, In-Context Learning (ICL), and Long Context. We have made the\nsource code, android app and web service available at Github\n(https://github.com/internlm/huixiangdou), OpenXLab\n(https://openxlab.org.cn/apps/detail/tpoisonooo/huixiangdou-web) and YouTube\n(https://youtu.be/ylXrT-Tei-Y) to aid in future research and application.\nHuixiangDou is applicable to any group chat within IM tools.\n","authors":["Huanjun Kong","Songyang Zhang","Jiaying Li","Min Xiao","Jun Xu","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2401.08772v2.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2404.08382v1","updated":"2024-04-12T10:36:15Z","published":"2024-04-12T10:36:15Z","title":"Look at the Text: Instruction-Tuned Language Models are More Robust\n  Multiple Choice Selectors than You Think","summary":"  Multiple choice questions (MCQs) are commonly used to evaluate the\ncapabilities of large language models (LLMs). One common way to evaluate the\nmodel response is to rank the candidate answers based on the log probability of\nthe first token prediction. An alternative way is to examine the text output.\nPrior work has shown that first token probabilities lack robustness to changes\nin MCQ phrasing, and that first token probabilities do not match text answers\nfor instruction-tuned models. Therefore, in this paper, we investigate the\nrobustness of text answers. We show that the text answers are more robust to\nquestion perturbations than the first token probabilities, when the first token\nanswers mismatch the text answers. The difference in robustness increases as\nthe mismatch rate becomes greater. As the mismatch reaches over 50\\%, the text\nanswer is more robust to option order changes than the debiased first token\nprobabilities using state-of-the-art debiasing methods such as PriDe. Our\nfindings provide further evidence for the benefits of text answer evaluation\nover first token probability evaluation.\n","authors":["Xinpeng Wang","Chengzhi Hu","Bolei Ma","Paul R√∂ttger","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2404.08382v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.04317v3","updated":"2024-04-12T10:33:56Z","published":"2022-06-09T07:28:16Z","title":"Topic-Controllable Summarization: Topic-Aware Evaluation and Transformer\n  Methods","summary":"  Topic-controllable summarization is an emerging research area with a wide\nrange of potential applications. However, existing approaches suffer from\nsignificant limitations. For example, the majority of existing methods built\nupon recurrent architectures, which can significantly limit their performance\ncompared to more recent Transformer-based architectures, while they also\nrequire modifications to the model's architecture for controlling the topic. At\nthe same time, there is currently no established evaluation metric designed\nspecifically for topic-controllable summarization. This work proposes a new\ntopic-oriented evaluation measure to automatically evaluate the generated\nsummaries based on the topic affinity between the generated summary and the\ndesired topic. The reliability of the proposed measure is demonstrated through\nappropriately designed human evaluation. In addition, we adapt topic embeddings\nto work with powerful Transformer architectures and propose a novel and\nefficient approach for guiding the summary generation through control tokens.\nExperimental results reveal that control tokens can achieve better performance\ncompared to more complicated embedding-based approaches while also being\nsignificantly faster.\n","authors":["Tatiana Passali","Grigorios Tsoumakas"],"pdf_url":"https://arxiv.org/pdf/2206.04317v3.pdf","comment":"11 pages, 1 figure, 6 tables"},{"id":"http://arxiv.org/abs/2312.05278v2","updated":"2024-04-12T10:26:01Z","published":"2023-12-08T09:02:45Z","title":"Lyrics: Boosting Fine-grained Language-Vision Alignment and\n  Comprehension via Semantic-aware Visual Objects","summary":"  Large Vision Language Models (LVLMs) have demonstrated impressive zero-shot\ncapabilities in various vision-language dialogue scenarios. However, the\nabsence of fine-grained visual object detection hinders the model from\nunderstanding the details of images, leading to irreparable visual\nhallucinations and factual errors. In this paper, we propose Lyrics, a novel\nmulti-modal pre-training and instruction fine-tuning paradigm that bootstraps\nvision-language alignment from fine-grained cross-modal collaboration. Building\non the foundation of BLIP-2, Lyrics infuses local visual features extracted\nfrom a visual refiner that includes image tagging, object detection and\nsemantic segmentation modules into the Querying Transformer, while on the text\nside, the language inputs equip the boundary boxes and tags derived from the\nvisual refiner. We further introduce a two-stage training scheme, in which the\npre-training stage bridges the modality gap through explicit and comprehensive\nvision-language alignment targets. During the instruction fine-tuning stage, we\nintroduce semantic-aware visual feature extraction, a crucial method that\nenables the model to extract informative features from concrete visual objects.\nOur approach achieves robust performance on 13 datasets across various\nvision-language tasks, and demonstrates promising multi-modal understanding,\nperception and conversation capabilities in 11 scenario-based benchmark\ntoolkits.\n","authors":["Junyu Lu","Dixiang Zhang","Songxin Zhang","Zejian Xie","Zhuoyang Song","Cong Lin","Jiaxing Zhang","Bingyi Jing","Pingjian Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.05278v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08368v1","updated":"2024-04-12T10:12:38Z","published":"2024-04-12T10:12:38Z","title":"ASR advancements for indigenous languages: Quechua, Guarani, Bribri,\n  Kotiria, and Wa'ikhana","summary":"  Indigenous languages are a fundamental legacy in the development of human\ncommunication, embodying the unique identity and culture of local communities\nof America. The Second AmericasNLP Competition Track 1 of NeurIPS 2022 proposed\ndeveloping automatic speech recognition (ASR) systems for five indigenous\nlanguages: Quechua, Guarani, Bribri, Kotiria, and Wa'ikhana. In this paper, we\npropose a reliable ASR model for each target language by crawling speech\ncorpora spanning diverse sources and applying data augmentation methods that\nresulted in the winning approach in this competition. To achieve this, we\nsystematically investigated the impact of different hyperparameters by a\nBayesian search on the performance of the language models, specifically\nfocusing on the variants of the Wav2vec2.0 XLS-R model: 300M and 1B parameters.\nMoreover, we performed a global sensitivity analysis to assess the contribution\nof various hyperparametric configurations to the performances of our best\nmodels. Importantly, our results show that freeze fine-tuning updates and\ndropout rate are more vital parameters than the total number of epochs of lr.\nAdditionally, we liberate our best models -- with no other ASR model reported\nuntil now for two Wa'ikhana and Kotiria -- and the many experiments performed\nto pave the way to other researchers to continue improving ASR in minority\nlanguages. This insight opens up interesting avenues for future work, allowing\nfor the advancement of ASR techniques in the preservation of minority\nindigenous and acknowledging the complexities involved in this important\nendeavour.\n","authors":["Monica Romero","Sandra Gomez","Iv√°n G. Torre"],"pdf_url":"https://arxiv.org/pdf/2404.08368v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08359v1","updated":"2024-04-12T09:56:12Z","published":"2024-04-12T09:56:12Z","title":"Improving Health Question Answering with Reliable and Time-Aware\n  Evidence Retrieval","summary":"  In today's digital world, seeking answers to health questions on the Internet\nis a common practice. However, existing question answering (QA) systems often\nrely on using pre-selected and annotated evidence documents, thus making them\ninadequate for addressing novel questions. Our study focuses on the open-domain\nQA setting, where the key challenge is to first uncover relevant evidence in\nlarge knowledge bases. By utilizing the common retrieve-then-read QA pipeline\nand PubMed as a trustworthy collection of medical research documents, we answer\nhealth questions from three diverse datasets. We modify different retrieval\nsettings to observe their influence on the QA pipeline's performance, including\nthe number of retrieved documents, sentence selection process, the publication\nyear of articles, and their number of citations. Our results reveal that\ncutting down on the amount of retrieved documents and favoring more recent and\nhighly cited documents can improve the final macro F1 score up to 10%. We\ndiscuss the results, highlight interesting examples, and outline challenges for\nfuture research, like managing evidence disagreement and crafting user-friendly\nexplanations.\n","authors":["Juraj Vladika","Florian Matthes"],"pdf_url":"https://arxiv.org/pdf/2404.08359v1.pdf","comment":"Accepted to NAACL 2024 (Findings)"},{"id":"http://arxiv.org/abs/2401.03946v2","updated":"2024-04-12T09:52:05Z","published":"2024-01-08T15:05:32Z","title":"TextMachina: Seamless Generation of Machine-Generated Text Datasets","summary":"  Recent advancements in Large Language Models (LLMs) have led to high-quality\nMachine-Generated Text (MGT), giving rise to countless new use cases and\napplications. However, easy access to LLMs is posing new challenges due to\nmisuse. To address malicious usage, researchers have released datasets to\neffectively train models on MGT-related tasks. Similar strategies are used to\ncompile these datasets, but no tool currently unifies them. In this scenario,\nwe introduce TextMachina, a modular and extensible Python framework, designed\nto aid in the creation of high-quality, unbiased datasets to build robust\nmodels for MGT-related tasks such as detection, attribution, mixcase, or\nboundary detection. It provides a user-friendly pipeline that abstracts away\nthe inherent intricacies of building MGT datasets, such as LLM integrations,\nprompt templating, and bias mitigation. The quality of the datasets generated\nby TextMachina has been assessed in previous works, including shared tasks\nwhere more than one hundred teams trained robust MGT detectors.\n","authors":["Areg Mikael Sarvazyan","Jos√© √Ångel Gonz√°lez","Marc Franco-Salvador"],"pdf_url":"https://arxiv.org/pdf/2401.03946v2.pdf","comment":"14 pages, 10 figures"},{"id":"http://arxiv.org/abs/2404.08354v1","updated":"2024-04-12T09:48:58Z","published":"2024-04-12T09:48:58Z","title":"Gaining More Insight into Neural Semantic Parsing with Challenging\n  Benchmarks","summary":"  The Parallel Meaning Bank (PMB) serves as a corpus for semantic processing\nwith a focus on semantic parsing and text generation. Currently, we witness an\nexcellent performance of neural parsers and generators on the PMB. This might\nsuggest that such semantic processing tasks have by and large been solved. We\nargue that this is not the case and that performance scores from the past on\nthe PMB are inflated by non-optimal data splits and test sets that are too\neasy. In response, we introduce several changes. First, instead of the prior\nrandom split, we propose a more systematic splitting approach to improve the\nreliability of the standard test data. Second, except for the standard test\nset, we also propose two challenge sets: one with longer texts including\ndiscourse structure, and one that addresses compositional generalization. We\nevaluate five neural models for semantic parsing and meaning-to-text\ngeneration. Our results show that model performance declines (in some cases\ndramatically) on the challenge sets, revealing the limitations of neural models\nwhen confronting such challenges.\n","authors":["Xiao Zhang","Chunliu Wang","Rik van Noord","Johan Bos"],"pdf_url":"https://arxiv.org/pdf/2404.08354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16254v2","updated":"2024-04-12T09:37:37Z","published":"2023-11-27T19:02:17Z","title":"Safe-CLIP: Removing NSFW Concepts from Vision-and-Language Models","summary":"  Large-scale vision-and-language models, such as CLIP, are typically trained\non web-scale data, which can introduce inappropriate content and lead to the\ndevelopment of unsafe and biased behavior. This, in turn, hampers their\napplicability in sensitive and trustworthy contexts and could raise significant\nconcerns in their adoption. Our research introduces a novel approach to\nenhancing the safety of vision-and-language models by diminishing their\nsensitivity to NSFW (not safe for work) inputs. In particular, our methodology\nseeks to sever \"toxic\" linguistic and visual concepts, unlearning the linkage\nbetween unsafe linguistic or visual items and unsafe regions of the embedding\nspace. We show how this can be done by fine-tuning a CLIP model on synthetic\ndata obtained from a large language model trained to convert between safe and\nunsafe sentences, and a text-to-image generator. We conduct extensive\nexperiments on the resulting embedding space for cross-modal retrieval,\ntext-to-image, and image-to-text generation, where we show that our model can\nbe remarkably employed with pre-trained generative models. Our source code and\ntrained models are available at: https://github.com/aimagelab/safe-clip.\n","authors":["Samuele Poppi","Tobia Poppi","Federico Cocchi","Marcella Cornia","Lorenzo Baraldi","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2311.16254v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08345v1","updated":"2024-04-12T09:21:29Z","published":"2024-04-12T09:21:29Z","title":"FastSpell: the LangId Magic Spell","summary":"  Language identification is a crucial component in the automated production of\nlanguage resources, particularly in multilingual and big data contexts.\nHowever, commonly used language identifiers struggle to differentiate between\nsimilar or closely-related languages. This paper introduces FastSpell, a\nlanguage identifier that combines fastText (a pre-trained language identifier\ntool) and Hunspell (a spell checker) with the aim of having a refined\nsecond-opinion before deciding which language should be assigned to a text. We\nprovide a description of the FastSpell algorithm along with an explanation on\nhow to use and configure it. To that end, we motivate the need of such a tool\nand present a benchmark including some popular language identifiers evaluated\nduring the development of FastSpell. We show how FastSpell is useful not only\nto improve identification of similar languages, but also to identify new ones\nignored by other tools.\n","authors":["Marta Ba√±√≥n","Jaume Zaragoza-Bernabeu","Gema Ram√≠rez-S√°nchez","Sergio Ortiz-Rojas"],"pdf_url":"https://arxiv.org/pdf/2404.08345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02929v2","updated":"2024-04-12T09:08:30Z","published":"2024-04-02T09:54:51Z","title":"Using Large Language Models to Understand Telecom Standards","summary":"  The Third Generation Partnership Project (3GPP) has successfully introduced\nstandards for global mobility. However, the volume and complexity of these\nstandards has increased over time, thus complicating access to relevant\ninformation for vendors and service providers. Use of Generative Artificial\nIntelligence (AI) and in particular Large Language Models (LLMs), may provide\nfaster access to relevant information. In this paper, we evaluate the\ncapability of state-of-art LLMs to be used as Question Answering (QA)\nassistants for 3GPP document reference. Our contribution is threefold. First,\nwe provide a benchmark and measuring methods for evaluating performance of\nLLMs. Second, we do data preprocessing and fine-tuning for one of these LLMs\nand provide guidelines to increase accuracy of the responses that apply to all\nLLMs. Third, we provide a model of our own, TeleRoBERTa, that performs on-par\nwith foundation LLMs but with an order of magnitude less number of parameters.\nResults show that LLMs can be used as a credible reference tool on telecom\ntechnical documents, and thus have potential for a number of different\napplications from troubleshooting and maintenance, to network operations and\nsoftware product development.\n","authors":["Athanasios Karapantelakis","Mukesh Thakur","Alexandros Nikou","Farnaz Moradi","Christian Orlog","Fitsum Gaim","Henrik Holm","Doumitrou Daniil Nimara","Vincent Huang"],"pdf_url":"https://arxiv.org/pdf/2404.02929v2.pdf","comment":"Accepted to ICMLCN 2024, Stockholm, May 2024. Updating typo in\n  authors list"},{"id":"http://arxiv.org/abs/2404.08335v1","updated":"2024-04-12T09:01:14Z","published":"2024-04-12T09:01:14Z","title":"Toward a Theory of Tokenization in LLMs","summary":"  While there has been a large body of research attempting to circumvent\ntokenization for language modeling (Clark et al., 2022; Xue et al., 2022), the\ncurrent consensus is that it is a necessary initial step for designing\nstate-of-the-art performant language models. In this paper, we investigate\ntokenization from a theoretical point of view by studying the behavior of\ntransformers on simple data generating processes. When trained on data drawn\nfrom certain simple $k^{\\text{th}}$-order Markov processes for $k > 1$,\ntransformers exhibit a surprising phenomenon - in the absence of tokenization,\nthey empirically fail to learn the right distribution and predict characters\naccording to a unigram model (Makkuva et al., 2024). With the addition of\ntokenization, however, we empirically observe that transformers break through\nthis barrier and are able to model the probabilities of sequences drawn from\nthe source near-optimally, achieving small cross-entropy loss. With this\nobservation as starting point, we study the end-to-end cross-entropy loss\nachieved by transformers with and without tokenization. With the appropriate\ntokenization, we show that even the simplest unigram models (over tokens)\nlearnt by transformers are able to model the probability of sequences drawn\nfrom $k^{\\text{th}}$-order Markov sources near optimally. Our analysis provides\na justification for the use of tokenization in practice through studying the\nbehavior of transformers on Markovian data.\n","authors":["Nived Rajaraman","Jiantao Jiao","Kannan Ramchandran"],"pdf_url":"https://arxiv.org/pdf/2404.08335v1.pdf","comment":"58 pages, 10 figures"},{"id":"http://arxiv.org/abs/2404.08313v1","updated":"2024-04-12T08:17:44Z","published":"2024-04-12T08:17:44Z","title":"The Integration of Semantic and Structural Knowledge in Knowledge Graph\n  Entity Typing","summary":"  The Knowledge Graph Entity Typing (KGET) task aims to predict missing type\nannotations for entities in knowledge graphs. Recent works only utilize the\n\\textit{\\textbf{structural knowledge}} in the local neighborhood of entities,\ndisregarding \\textit{\\textbf{semantic knowledge}} in the textual\nrepresentations of entities, relations, and types that are also crucial for\ntype inference. Additionally, we observe that the interaction between semantic\nand structural knowledge can be utilized to address the false-negative problem.\nIn this paper, we propose a novel \\textbf{\\underline{S}}emantic and\n\\textbf{\\underline{S}}tructure-aware KG \\textbf{\\underline{E}}ntity\n\\textbf{\\underline{T}}yping~{(SSET)} framework, which is composed of three\nmodules. First, the \\textit{Semantic Knowledge Encoding} module encodes factual\nknowledge in the KG with a Masked Entity Typing task. Then, the\n\\textit{Structural Knowledge Aggregation} module aggregates knowledge from the\nmulti-hop neighborhood of entities to infer missing types. Finally, the\n\\textit{Unsupervised Type Re-ranking} module utilizes the inference results\nfrom the two models above to generate type predictions that are robust to\nfalse-negative samples. Extensive experiments show that SSET significantly\noutperforms existing state-of-the-art methods.\n","authors":["Muzhi Li","Minda Hu","Irwin King","Ho-fung Leung"],"pdf_url":"https://arxiv.org/pdf/2404.08313v1.pdf","comment":"Accepted in NAACL2024 main"},{"id":"http://arxiv.org/abs/2404.08309v1","updated":"2024-04-12T08:08:44Z","published":"2024-04-12T08:08:44Z","title":"Subtoxic Questions: Dive Into Attitude Change of LLM's Response in\n  Jailbreak Attempts","summary":"  As Large Language Models (LLMs) of Prompt Jailbreaking are getting more and\nmore attention, it is of great significance to raise a generalized research\nparadigm to evaluate attack strengths and a basic model to conduct subtler\nexperiments. In this paper, we propose a novel approach by focusing on a set of\ntarget questions that are inherently more sensitive to jailbreak prompts,\naiming to circumvent the limitations posed by enhanced LLM security. Through\ndesigning and analyzing these sensitive questions, this paper reveals a more\neffective method of identifying vulnerabilities in LLMs, thereby contributing\nto the advancement of LLM security. This research not only challenges existing\njailbreaking methodologies but also fortifies LLMs against potential exploits.\n","authors":["Tianyu Zhang","Zixuan Zhao","Jiaqi Huang","Jingyu Hua","Sheng Zhong"],"pdf_url":"https://arxiv.org/pdf/2404.08309v1.pdf","comment":"4 pages, 2 figures. This paper was submitted to The 7th Deep Learning\n  Security and Privacy Workshop (DLSP 2024) and was accepted as extended\n  abstract, see https://dlsp2024.ieee-security.org/"},{"id":"http://arxiv.org/abs/2404.03555v2","updated":"2024-04-12T08:05:13Z","published":"2024-04-04T16:07:06Z","title":"From News to Summaries: Building a Hungarian Corpus for Extractive and\n  Abstractive Summarization","summary":"  Training summarization models requires substantial amounts of training data.\nHowever for less resourceful languages like Hungarian, openly available models\nand datasets are notably scarce. To address this gap our paper introduces\nHunSum-2 an open-source Hungarian corpus suitable for training abstractive and\nextractive summarization models. The dataset is assembled from segments of the\nCommon Crawl corpus undergoing thorough cleaning, preprocessing and\ndeduplication. In addition to abstractive summarization we generate\nsentence-level labels for extractive summarization using sentence similarity.\nWe train baseline models for both extractive and abstractive summarization\nusing the collected dataset. To demonstrate the effectiveness of the trained\nmodels, we perform both quantitative and qualitative evaluation. Our dataset,\nmodels and code are publicly available, encouraging replication, further\nresearch, and real-world applications across various domains.\n","authors":["Botond Barta","Dorina Lakatos","Attila Nagy","Mil√°n Konor Nyist","Judit √Åcs"],"pdf_url":"https://arxiv.org/pdf/2404.03555v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06714v2","updated":"2024-04-12T06:42:12Z","published":"2024-04-10T03:46:03Z","title":"Llama-VITS: Enhancing TTS Synthesis with Semantic Awareness","summary":"  Recent advancements in Natural Language Processing (NLP) have seen\nLarge-scale Language Models (LLMs) excel at producing high-quality text for\nvarious purposes. Notably, in Text-To-Speech (TTS) systems, the integration of\nBERT for semantic token generation has underscored the importance of semantic\ncontent in producing coherent speech outputs. Despite this, the specific\nutility of LLMs in enhancing TTS synthesis remains considerably limited. This\nresearch introduces an innovative approach, Llama-VITS, which enhances TTS\nsynthesis by enriching the semantic content of text using LLM. Llama-VITS\nintegrates semantic embeddings from Llama2 with the VITS model, a leading\nend-to-end TTS framework. By leveraging Llama2 for the primary speech synthesis\nprocess, our experiments demonstrate that Llama-VITS matches the naturalness of\nthe original VITS (ORI-VITS) and those incorporate BERT (BERT-VITS), on the\nLJSpeech dataset, a substantial collection of neutral, clear speech. Moreover,\nour method significantly enhances emotive expressiveness on the EmoV_DB_bea_sem\ndataset, a curated selection of emotionally consistent speech from the EmoV_DB\ndataset, highlighting its potential to generate emotive speech.\n","authors":["Xincan Feng","Akifumi Yoshimoto"],"pdf_url":"https://arxiv.org/pdf/2404.06714v2.pdf","comment":"9 pages, 2 figures, 4 tables; accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2404.08263v1","updated":"2024-04-12T06:23:07Z","published":"2024-04-12T06:23:07Z","title":"Relational Prompt-based Pre-trained Language Models for Social Event\n  Detection","summary":"  Social Event Detection (SED) aims to identify significant events from social\nstreams, and has a wide application ranging from public opinion analysis to\nrisk management. In recent years, Graph Neural Network (GNN) based solutions\nhave achieved state-of-the-art performance. However, GNN-based methods often\nstruggle with noisy and missing edges between messages, affecting the quality\nof learned message embedding. Moreover, these methods statically initialize\nnode embedding before training, which, in turn, limits the ability to learn\nfrom message texts and relations simultaneously. In this paper, we approach\nsocial event detection from a new perspective based on Pre-trained Language\nModels (PLMs), and present RPLM_SED (Relational prompt-based Pre-trained\nLanguage Models for Social Event Detection). We first propose a new pairwise\nmessage modeling strategy to construct social messages into message pairs with\nmulti-relational sequences. Secondly, a new multi-relational prompt-based\npairwise message learning mechanism is proposed to learn more comprehensive\nmessage representation from message pairs with multi-relational prompts using\nPLMs. Thirdly, we design a new clustering constraint to optimize the encoding\nprocess by enhancing intra-cluster compactness and inter-cluster dispersion,\nmaking the message representation more distinguishable. We evaluate the\nRPLM_SED on three real-world datasets, demonstrating that the RPLM_SED model\nachieves state-of-the-art performance in offline, online, low-resource, and\nlong-tail distribution scenarios for social event detection tasks.\n","authors":["Pu Li","Xiaoyan Yu","Hao Peng","Yantuan Xian","Linqin Wang","Li Sun","Jingyun Zhang","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2404.08263v1.pdf","comment":"ACM TOIS Under Review"},{"id":"http://arxiv.org/abs/2404.08262v1","updated":"2024-04-12T06:21:48Z","published":"2024-04-12T06:21:48Z","title":"Pretraining and Updating Language- and Domain-specific Large Language\n  Model: A Case Study in Japanese Business Domain","summary":"  Several previous studies have considered language- and domain-specific large\nlanguage models (LLMs) as separate topics. This study explores the combination\nof a non-English language and a high-demand industry domain, focusing on a\nJapanese business-specific LLM. This type of a model requires expertise in the\nbusiness domain, strong language skills, and regular updates of its knowledge.\nWe trained a 13-billion-parameter LLM from scratch using a new dataset of\nbusiness texts and patents, and continually pretrained it with the latest\nbusiness documents. Further we propose a new benchmark for Japanese business\ndomain question answering (QA) and evaluate our models on it. The results show\nthat our pretrained model improves QA accuracy without losing general\nknowledge, and that continual pretraining enhances adaptation to new\ninformation. Our pretrained model and business domain benchmark are publicly\navailable.\n","authors":["Kosuke Takahashi","Takahiro Omi","Kosuke Arima","Tatsuya Ishigaki"],"pdf_url":"https://arxiv.org/pdf/2404.08262v1.pdf","comment":"9 pages. preprint of COLM2024"},{"id":"http://arxiv.org/abs/2404.08259v1","updated":"2024-04-12T06:16:26Z","published":"2024-04-12T06:16:26Z","title":"Investigating Neural Machine Translation for Low-Resource Languages:\n  Using Bavarian as a Case Study","summary":"  Machine Translation has made impressive progress in recent years offering\nclose to human-level performance on many languages, but studies have primarily\nfocused on high-resource languages with broad online presence and resources.\nWith the help of growing Large Language Models, more and more low-resource\nlanguages achieve better results through the presence of other languages.\nHowever, studies have shown that not all low-resource languages can benefit\nfrom multilingual systems, especially those with insufficient training and\nevaluation data. In this paper, we revisit state-of-the-art Neural Machine\nTranslation techniques to develop automatic translation systems between German\nand Bavarian. We investigate conditions of low-resource languages such as data\nscarcity and parameter sensitivity and focus on refined solutions that combat\nlow-resource difficulties and creative solutions such as harnessing language\nsimilarity. Our experiment entails applying Back-translation and Transfer\nLearning to automatically generate more training data and achieve higher\ntranslation performance. We demonstrate noisiness in the data and present our\napproach to carry out text preprocessing extensively. Evaluation was conducted\nusing combined metrics: BLEU, chrF and TER. Statistical significance results\nwith Bonferroni correction show surprisingly high baseline systems, and that\nBack-translation leads to significant improvement. Furthermore, we present a\nqualitative analysis of translation errors and system limitations.\n","authors":["Wan-Hua Her","Udo Kruschwitz"],"pdf_url":"https://arxiv.org/pdf/2404.08259v1.pdf","comment":"Preprint accepted at the 3rd Annual Meeting of the Special Interest\n  Group on Under-resourced Languages (SIGUL 2024)"},{"id":"http://arxiv.org/abs/2402.02619v3","updated":"2024-04-12T03:57:24Z","published":"2024-02-04T21:33:18Z","title":"Increasing Trust in Language Models through the Reuse of Verified\n  Circuits","summary":"  Language Models (LMs) are increasingly used for a wide range of prediction\ntasks, but their training can often neglect rare edge cases, reducing their\nreliability. Here, we define a stringent standard of trustworthiness whereby\nthe task algorithm and circuit implementation must be verified, accounting for\nedge cases, with no known failure modes. We show that a transformer model can\nbe trained to meet this standard if built using mathematically and logically\nspecified frameworks. In this paper, we fully verify a model for n-digit\ninteger addition. To exhibit the reusability of verified modules, we insert the\ntrained integer addition model into an untrained model and train the combined\nmodel to perform both addition and subtraction. We find extensive reuse of the\naddition circuits for both tasks, easing verification of the more complex\nsubtractor model. We discuss how inserting verified task modules into LMs can\nleverage model reuse to improve verifiability and trustworthiness of language\nmodels built using them. The reuse of verified circuits reduces the effort to\nverify more complex composite models which we believe to be a significant step\ntowards safety of language models.\n","authors":["Philip Quirke","Clement Neo","Fazl Barez"],"pdf_url":"https://arxiv.org/pdf/2402.02619v3.pdf","comment":"8 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.12416v2","updated":"2024-04-12T03:15:26Z","published":"2024-03-19T03:59:14Z","title":"Eye-gaze Guided Multi-modal Alignment Framework for Radiology","summary":"  In multi-modal frameworks, the alignment of cross-modal features presents a\nsignificant challenge. The predominant approach in multi-modal pre-training\nemphasizes either global or local alignment between modalities, utilizing\nextensive datasets. This bottom-up driven method often suffers from a lack of\ninterpretability, a critical concern in radiology. Previous studies have\nintegrated high-level labels in medical images or text, but these still rely on\nmanual annotation, a costly and labor-intensive process. Our work introduces a\nnovel approach by using eye-gaze data, collected synchronously by radiologists\nduring diagnostic evaluations. This data, indicating radiologists' focus areas,\nnaturally links chest X-rays to diagnostic texts. We propose the Eye-gaze\nGuided Multi-modal Alignment (EGMA) framework to harness eye-gaze data for\nbetter alignment of image and text features, aiming to reduce reliance on\nmanual annotations and thus cut training costs. Our model demonstrates robust\nperformance, outperforming other state-of-the-art methods in zero-shot\nclassification and retrieval tasks. The incorporation of easily-obtained\neye-gaze data during routine radiological diagnoses signifies a step towards\nminimizing manual annotation dependency. Additionally, we explore the impact of\nvarying amounts of eye-gaze data on model performance, highlighting the\nfeasibility and utility of integrating this auxiliary data into multi-modal\npre-training.\n","authors":["Chong Ma","Hanqi Jiang","Wenting Chen","Zihao Wu","Xiaowei Yu","Fang Zeng","Lei Guo","Dajiang Zhu","Tuo Zhang","Dinggang Shen","Tianming Liu","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2403.12416v2.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2404.08191v1","updated":"2024-04-12T01:44:46Z","published":"2024-04-12T01:44:46Z","title":"Measuring Cross-lingual Transfer in Bytes","summary":"  Multilingual pretraining has been a successful solution to the challenges\nposed by the lack of resources for languages. These models can transfer\nknowledge to target languages with minimal or no examples. Recent research\nsuggests that monolingual models also have a similar capability, but the\nmechanisms behind this transfer remain unclear. Some studies have explored\nfactors like language contamination and syntactic similarity. An emerging line\nof research suggests that the representations learned by language models\ncontain two components: a language-specific and a language-agnostic component.\nThe latter is responsible for transferring a more universal knowledge. However,\nthere is a lack of comprehensive exploration of these properties across diverse\ntarget languages. To investigate this hypothesis, we conducted an experiment\ninspired by the work on the Scaling Laws for Transfer. We measured the amount\nof data transferred from a source language to a target language and found that\nmodels initialized from diverse languages perform similarly to a target\nlanguage in a cross-lingual setting. This was surprising because the amount of\ndata transferred to 10 diverse target languages, such as Spanish, Korean, and\nFinnish, was quite similar. We also found evidence that this transfer is not\nrelated to language contamination or language proximity, which strengthens the\nhypothesis that the model also relies on language-agnostic knowledge. Our\nexperiments have opened up new possibilities for measuring how much data\nrepresents the language-agnostic representations learned during pretraining.\n","authors":["Leandro Rodrigues de Souza","Thales Sales Almeida","Roberto Lotufo","Rodrigo Nogueira"],"pdf_url":"https://arxiv.org/pdf/2404.08191v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2404.08189v1","updated":"2024-04-12T01:42:09Z","published":"2024-04-12T01:42:09Z","title":"Reducing hallucination in structured outputs via Retrieval-Augmented\n  Generation","summary":"  A common and fundamental limitation of Generative AI (GenAI) is its\npropensity to hallucinate. While large language models (LLM) have taken the\nworld by storm, without eliminating or at least reducing hallucinations,\nreal-world GenAI systems may face challenges in user adoption. In the process\nof deploying an enterprise application that produces workflows based on natural\nlanguage requirements, we devised a system leveraging Retrieval Augmented\nGeneration (RAG) to greatly improve the quality of the structured output that\nrepresents such workflows. Thanks to our implementation of RAG, our proposed\nsystem significantly reduces hallucinations in the output and improves the\ngeneralization of our LLM in out-of-domain settings. In addition, we show that\nusing a small, well-trained retriever encoder can reduce the size of the\naccompanying LLM, thereby making deployments of LLM-based systems less\nresource-intensive.\n","authors":["Patrice B√©chard","Orlando Marquez Ayala"],"pdf_url":"https://arxiv.org/pdf/2404.08189v1.pdf","comment":"To be presented at NAACL 2024. 11 pages and 4 figures"},{"id":"http://arxiv.org/abs/2402.01115v3","updated":"2024-04-12T01:32:32Z","published":"2024-02-02T03:15:13Z","title":"Interpretation of Intracardiac Electrograms Through Textual\n  Representations","summary":"  Understanding the irregular electrical activity of atrial fibrillation (AFib)\nhas been a key challenge in electrocardiography. For serious cases of AFib,\ncatheter ablations are performed to collect intracardiac electrograms (EGMs).\nEGMs offer intricately detailed and localized electrical activity of the heart\nand are an ideal modality for interpretable cardiac studies. Recent\nadvancements in artificial intelligence (AI) has allowed some works to utilize\ndeep learning frameworks to interpret EGMs during AFib. Additionally, language\nmodels (LMs) have shown exceptional performance in being able to generalize to\nunseen domains, especially in healthcare. In this study, we are the first to\nleverage pretrained LMs for finetuning of EGM interpolation and AFib\nclassification via masked language modeling. We formulate the EGM as a textual\nsequence and present competitive performances on AFib classification compared\nagainst other representations. Lastly, we provide a comprehensive\ninterpretability study to provide a multi-perspective intuition of the model's\nbehavior, which could greatly benefit the clinical use.\n","authors":["William Jongwon Han","Diana Gomez","Avi Alok","Chaojing Duan","Michael A. Rosenberg","Douglas Weber","Emerson Liu","Ding Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.01115v3.pdf","comment":"18 pages, 9 figures; Accepted to CHIL 2024"},{"id":"http://arxiv.org/abs/2312.17122v3","updated":"2024-04-12T01:30:55Z","published":"2023-12-28T16:59:06Z","title":"Large Language Model for Causal Decision Making","summary":"  Large Language Models (LLMs) have shown their success in language\nunderstanding and reasoning on general topics. However, their capability to\nperform inference based on user-specified structured data and knowledge in\ncorpus-rare concepts, such as causal decision-making is still limited. In this\nwork, we explore the possibility of fine-tuning an open-sourced LLM into\nLLM4Causal, which can identify the causal task, execute a corresponding\nfunction, and interpret its numerical results based on users' queries and the\nprovided dataset. Meanwhile, we propose a data generation process for more\ncontrollable GPT prompting and present two instruction-tuning datasets: (1)\nCausal-Retrieval-Bench for causal problem identification and input parameter\nextraction for causal function calling and (2) Causal-Interpret-Bench for\nin-context causal interpretation. By conducting end-to-end evaluations and two\nablation studies, we showed that LLM4Causal can deliver end-to-end solutions\nfor causal problems and provide easy-to-understand answers, which significantly\noutperforms the baselines.\n","authors":["Haitao Jiang","Lin Ge","Yuhe Gao","Jianian Wang","Rui Song"],"pdf_url":"https://arxiv.org/pdf/2312.17122v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00409v2","updated":"2024-04-12T01:09:37Z","published":"2024-03-01T09:55:18Z","title":"Provably Robust DPO: Aligning Language Models with Noisy Feedback","summary":"  Learning from preference-based feedback has recently gained traction as a\npromising approach to align language models with human interests. While these\naligned generative models have demonstrated impressive capabilities across\nvarious tasks, their dependence on high-quality human preference data poses a\nbottleneck in practical applications. Specifically, noisy (incorrect and\nambiguous) preference pairs in the dataset might restrict the language models\nfrom capturing human intent accurately. While practitioners have recently\nproposed heuristics to mitigate the effect of noisy preferences, a complete\ntheoretical understanding of their workings remain elusive.\n  In this work, we aim to bridge this gap by by introducing a general framework\nfor policy optimization in the presence of random preference flips. We focus on\nthe direct preference optimization (DPO) algorithm in particular since it\nassumes that preferences adhere to the Bradley-Terry-Luce (BTL) model, raising\nconcerns about the impact of noisy data on the learned policy. We design a\nnovel loss function, which de-bias the effect of noise on average, making a\npolicy trained by minimizing that loss robust to the noise. Under log-linear\nparameterization of the policy class and assuming good feature coverage of the\nSFT policy, we prove that the sub-optimality gap of the proposed robust DPO\n(rDPO) policy compared to the optimal policy is of the order\n$O(\\frac{1}{1-2\\epsilon}\\sqrt{\\frac{d}{n}})$, where $\\epsilon < 1/2$ is flip\nrate of labels, $d$ is policy parameter dimension and $n$ is size of dataset.\nOur experiments on IMDb sentiment generation and Anthropic's helpful-harmless\ndataset show that rDPO is robust to noise in preference labels compared to\nvanilla DPO and other heuristics proposed by practitioners.\n","authors":["Sayak Ray Chowdhury","Anush Kini","Nagarajan Natarajan"],"pdf_url":"https://arxiv.org/pdf/2403.00409v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12730v2","updated":"2024-04-12T00:53:29Z","published":"2024-02-20T05:46:29Z","title":"UMBCLU at SemEval-2024 Task 1A and 1C: Semantic Textual Relatedness with\n  and without machine translation","summary":"  The aim of SemEval-2024 Task 1, \"Semantic Textual Relatedness for African and\nAsian Languages\" is to develop models for identifying semantic textual\nrelatedness (STR) between two sentences using multiple languages (14 African\nand Asian languages) and settings (supervised, unsupervised, and\ncross-lingual). Large language models (LLMs) have shown impressive performance\non several natural language understanding tasks such as multilingual machine\ntranslation (MMT), semantic similarity (STS), and encoding sentence embeddings.\nUsing a combination of LLMs that perform well on these tasks, we developed two\nSTR models, $\\textit{TranSem}$ and $\\textit{FineSem}$, for the supervised and\ncross-lingual settings. We explore the effectiveness of several training\nmethods and the usefulness of machine translation. We find that direct\nfine-tuning on the task is comparable to using sentence embeddings and\ntranslating to English leads to better performance for some languages. In the\nsupervised setting, our model performance is better than the official baseline\nfor 3 languages with the remaining 4 performing on par. In the cross-lingual\nsetting, our model performance is better than the baseline for 3 languages\n(leading to $1^{st}$ place for Africaans and $2^{nd}$ place for Indonesian), is\non par for 2 languages and performs poorly on the remaining 7 languages. Our\ncode is publicly available at https://github.com/dipta007/SemEval24-Task8.\n","authors":["Shubhashis Roy Dipta","Sai Vallurupalli"],"pdf_url":"https://arxiv.org/pdf/2402.12730v2.pdf","comment":"Accepted at SemEval 2024 (Colocated with NAACL 2024)"},{"id":"http://arxiv.org/abs/2404.04522v2","updated":"2024-04-12T00:18:06Z","published":"2024-04-06T06:44:41Z","title":"Q-PEFT: Query-dependent Parameter Efficient Fine-tuning for Text\n  Reranking with Large Language Models","summary":"  Parameter Efficient Fine-Tuning (PEFT) methods have been extensively utilized\nin Large Language Models (LLMs) to improve the down-streaming tasks without the\ncost of fine-tuing the whole LLMs. Recent studies have shown how to effectively\nuse PEFT for fine-tuning LLMs in ranking tasks with convincing performance;\nthere are some limitations, including the learned prompt being fixed for\ndifferent documents, overfitting to specific tasks, and low adaptation ability.\nIn this paper, we introduce a query-dependent parameter efficient fine-tuning\n(Q-PEFT) approach for text reranking to leak the information of the true\nqueries to LLMs and then make the generation of true queries from input\ndocuments much easier. Specifically, we utilize the query to extract the\ntop-$k$ tokens from concatenated documents, serving as contextual clues. We\nfurther augment Q-PEFT by substituting the retrieval mechanism with a\nmulti-head attention layer to achieve end-to-end training and cover all the\ntokens in the documents, guiding the LLMs to generate more document-specific\nsynthetic queries, thereby further improving the reranking performance.\nExtensive experiments are conducted on four public datasets, demonstrating the\neffectiveness of our proposed approach.\n","authors":["Zhiyuan Peng","Xuyang Wu","Qifan Wang","Sravanthi Rajanala","Yi Fang"],"pdf_url":"https://arxiv.org/pdf/2404.04522v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.16388v2","updated":"2024-04-12T00:05:53Z","published":"2023-06-28T17:31:53Z","title":"Towards Measuring the Representation of Subjective Global Opinions in\n  Language Models","summary":"  Large language models (LLMs) may not equitably represent diverse global\nperspectives on societal issues. In this paper, we develop a quantitative\nframework to evaluate whose opinions model-generated responses are more similar\nto. We first build a dataset, GlobalOpinionQA, comprised of questions and\nanswers from cross-national surveys designed to capture diverse opinions on\nglobal issues across different countries. Next, we define a metric that\nquantifies the similarity between LLM-generated survey responses and human\nresponses, conditioned on country. With our framework, we run three experiments\non an LLM trained to be helpful, honest, and harmless with Constitutional AI.\nBy default, LLM responses tend to be more similar to the opinions of certain\npopulations, such as those from the USA, and some European and South American\ncountries, highlighting the potential for biases. When we prompt the model to\nconsider a particular country's perspective, responses shift to be more similar\nto the opinions of the prompted populations, but can reflect harmful cultural\nstereotypes. When we translate GlobalOpinionQA questions to a target language,\nthe model's responses do not necessarily become the most similar to the\nopinions of speakers of those languages. We release our dataset for others to\nuse and build on. Our data is at\nhttps://huggingface.co/datasets/Anthropic/llm_global_opinions. We also provide\nan interactive visualization at https://llmglobalvalues.anthropic.com.\n","authors":["Esin Durmus","Karina Nguyen","Thomas I. Liao","Nicholas Schiefer","Amanda Askell","Anton Bakhtin","Carol Chen","Zac Hatfield-Dodds","Danny Hernandez","Nicholas Joseph","Liane Lovitt","Sam McCandlish","Orowa Sikder","Alex Tamkin","Janel Thamkul","Jared Kaplan","Jack Clark","Deep Ganguli"],"pdf_url":"https://arxiv.org/pdf/2306.16388v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08164v1","updated":"2024-04-12T00:03:56Z","published":"2024-04-12T00:03:56Z","title":"Language Model Prompt Selection via Simulation Optimization","summary":"  With the advancement in generative language models, the selection of prompts\nhas gained significant attention in recent years. A prompt is an instruction or\ndescription provided by the user, serving as a guide for the generative\nlanguage model in content generation. Despite existing methods for prompt\nselection that are based on human labor, we consider facilitating this\nselection through simulation optimization, aiming to maximize a pre-defined\nscore for the selected prompt. Specifically, we propose a two-stage framework.\nIn the first stage, we determine a feasible set of prompts in sufficient\nnumbers, where each prompt is represented by a moderate-dimensional vector. In\nthe subsequent stage for evaluation and selection, we construct a surrogate\nmodel of the score regarding the moderate-dimensional vectors that represent\nthe prompts. We propose sequentially selecting the prompt for evaluation based\non this constructed surrogate model. We prove the consistency of the sequential\nevaluation procedure in our framework. We also conduct numerical experiments to\ndemonstrate the efficacy of our proposed framework, providing practical\ninstructions for implementation.\n","authors":["Haoting Zhang","Jinghai He","Rhonda Righter","Zeyu Zheng"],"pdf_url":"https://arxiv.org/pdf/2404.08164v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08846v1","updated":"2024-04-12T23:27:46Z","published":"2024-04-12T23:27:46Z","title":"Experimental Design for Active Transductive Inference in Large Language\n  Models","summary":"  Transduction, the ability to include query-specific examples in the prompt at\ninference time, is one of the emergent abilities of large language models\n(LLMs). In this work, we propose a framework for adaptive prompt design called\nactive transductive inference (ATI). We design the LLM prompt by adaptively\nchoosing few-shot examples for a given inference query. The examples are\ninitially unlabeled and we query the user to label the most informative ones,\nwhich maximally reduces the uncertainty in the LLM prediction. We propose two\nalgorithms, GO and SAL, which differ in how the few-shot examples are chosen.\nWe analyze these algorithms in linear models: first GO and then use its\nequivalence with SAL. We experiment with many different tasks and show that GO\nand SAL outperform other methods for choosing few-shot examples in the LLM\nprompt at inference time.\n","authors":["Subhojyoti Mukherjee","Ge Liu","Aniket Deshmukh","Anusha Lalitha","Yifei Ma","Branislav Kveton"],"pdf_url":"https://arxiv.org/pdf/2404.08846v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09578v2","updated":"2024-04-12T23:15:51Z","published":"2023-11-16T05:29:39Z","title":"Tied-Lora: Enhancing parameter efficiency of LoRA with weight tying","summary":"  We introduce Tied-LoRA, a novel paradigm leveraging weight tying and\nselective training to enhance the parameter efficiency of Low-rank Adaptation\n(LoRA). Our exploration encompasses different plausible combinations of\nparameter training and freezing, coupled with weight tying, aimed at\nidentifying the optimal trade-off between performance and the count of\ntrainable parameters. Across $5$ diverse tasks and two foundational language\nmodels with different parameter counts, our experiments provide comprehensive\ninsights into the inherent trade-offs between efficiency and performance.\n  Our findings reveal a specific Tied-LoRA configuration that distinguishes\nitself by showcasing comparable performance to LoRA across multiple tasks while\nutilizing only a fraction of the parameters employed by the standard LoRA\nmethod, particularly at elevated ranks. This underscores the efficacy of\nTied-LoRA in achieving impressive results with significantly reduced model\ncomplexity.\n","authors":["Adithya Renduchintala","Tugrul Konuk","Oleksii Kuchaiev"],"pdf_url":"https://arxiv.org/pdf/2311.09578v2.pdf","comment":"8 pages 4 figures"},{"id":"http://arxiv.org/abs/2310.18794v3","updated":"2024-04-12T23:09:52Z","published":"2023-10-28T19:42:28Z","title":"Sequence-Level Certainty Reduces Hallucination In Knowledge-Grounded\n  Dialogue Generation","summary":"  In this work, we propose sequence-level certainty as a common theme over\nhallucination in Knowledge Grounded Dialogue Generation (KGDG). We explore the\ncorrelation between the level of hallucination in model responses and two types\nof sequence-level certainty: probabilistic certainty and semantic certainty.\nEmpirical results reveal that higher levels of both types of certainty in model\nresponses are correlated with lower levels of hallucination. We further propose\nCertainty-based Response Ranking (CRR), a decoding-time hallucination\nmitigation method that samples several response candidates, ranks them based on\nsequence-level certainty, and outputs the response with the highest certainty\nlevel. Aligning with our definitions of sequence-level certainty, we design 2\ntypes of CRR approaches: Probabilistic CRR (P-CRR) and Semantic CRR (S-CRR).\nP-CRR ranks individually sampled model responses using the arithmetic mean\nlog-probability of the entire sequence. S-CRR approaches certainty estimation\nfrom meaning-space, and ranks model response candidates based on their semantic\ncertainty level as measured by an entailment-based Agreement Score (AS).\nThrough extensive experiments across 3 KGDG datasets, 3 decoding methods, and 4\nKGDG models, we validate the effectiveness of CRR for reducing hallucination in\nKGDG task.\n","authors":["Yixin Wan","Fanyou Wu","Weijie Xu","Srinivasan H. Sengamedu"],"pdf_url":"https://arxiv.org/pdf/2310.18794v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08836v1","updated":"2024-04-12T22:35:00Z","published":"2024-04-12T22:35:00Z","title":"BERT-LSH: Reducing Absolute Compute For Attention","summary":"  This study introduces a novel BERT-LSH model that incorporates Locality\nSensitive Hashing (LSH) to approximate the attention mechanism in the BERT\narchitecture. We examine the computational efficiency and performance of this\nmodel compared to a standard baseline BERT model. Our findings reveal that\nBERT-LSH significantly reduces computational demand for the self-attention\nlayer while unexpectedly outperforming the baseline model in pretraining and\nfine-tuning tasks. These results suggest that the LSH-based attention mechanism\nnot only offers computational advantages but also may enhance the model's\nability to generalize from its training data. For more information, visit our\nGitHub repository: https://github.com/leo4life2/algoml-final\n","authors":["Zezheng Li","Kingston Yip"],"pdf_url":"https://arxiv.org/pdf/2404.08836v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2404.08821v1","updated":"2024-04-12T21:35:21Z","published":"2024-04-12T21:35:21Z","title":"Constrained C-Test Generation via Mixed-Integer Programming","summary":"  This work proposes a novel method to generate C-Tests; a deviated form of\ncloze tests (a gap filling exercise) where only the last part of a word is\nturned into a gap. In contrast to previous works that only consider varying the\ngap size or gap placement to achieve locally optimal solutions, we propose a\nmixed-integer programming (MIP) approach. This allows us to consider gap size\nand placement simultaneously, achieving globally optimal solutions, and to\ndirectly integrate state-of-the-art models for gap difficulty prediction into\nthe optimization problem. A user study with 40 participants across four C-Test\ngeneration strategies (including GPT-4) shows that our approach (MIP)\nsignificantly outperforms two of the baseline strategies (based on gap\nplacement and GPT-4); and performs on-par with the third (based on gap size).\nOur analysis shows that GPT-4 still struggles to fulfill explicit constraints\nduring generation and that MIP produces C-Tests that correlate best with the\nperceived difficulty. We publish our code, model, and collected data consisting\nof 32 English C-Tests with 20 gaps each (totaling 3,200 individual gap\nresponses) under an open source license.\n","authors":["Ji-Ung Lee","Marc E. Pfetsch","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2404.08821v1.pdf","comment":"Github:\n  https://github.com/UKPLab/arxiv2024-constrained-ctest-generation"},{"id":"http://arxiv.org/abs/2404.08819v1","updated":"2024-04-12T21:30:06Z","published":"2024-04-12T21:30:06Z","title":"The Illusion of State in State-Space Models","summary":"  State-space models (SSMs) have emerged as a potential alternative\narchitecture for building large language models (LLMs) compared to the\npreviously ubiquitous transformer architecture. One theoretical weakness of\ntransformers is that they cannot express certain kinds of sequential\ncomputation and state tracking (Merrill and Sabharwal, 2023), which SSMs are\nexplicitly designed to address via their close architectural similarity to\nrecurrent neural networks (RNNs). But do SSMs truly have an advantage (over\ntransformers) in expressive power for state tracking? Surprisingly, the answer\nis no. Our analysis reveals that the expressive power of SSMs is limited very\nsimilarly to transformers: SSMs cannot express computation outside the\ncomplexity class $\\mathsf{TC}^0$. In particular, this means they cannot solve\nsimple state-tracking problems like permutation composition. It follows that\nSSMs are provably unable to accurately track chess moves with certain notation,\nevaluate code, or track entities in a long narrative. To supplement our formal\nanalysis, we report experiments showing that Mamba-style SSMs indeed struggle\nwith state tracking. Thus, despite its recurrent formulation, the \"state\" in an\nSSM is an illusion: SSMs have similar expressiveness limitations to\nnon-recurrent models like transformers, which may fundamentally limit their\nability to solve real-world state-tracking problems.\n","authors":["William Merrill","Jackson Petty","Ashish Sabharwal"],"pdf_url":"https://arxiv.org/pdf/2404.08819v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2404.08817v1","updated":"2024-04-12T21:28:18Z","published":"2024-04-12T21:28:18Z","title":"Revisiting Code Similarity Evaluation with Abstract Syntax Tree Edit\n  Distance","summary":"  This paper revisits recent code similarity evaluation metrics, particularly\nfocusing on the application of Abstract Syntax Tree (AST) editing distance in\ndiverse programming languages. In particular, we explore the usefulness of\nthese metrics and compare them to traditional sequence similarity metrics. Our\nexperiments showcase the effectiveness of AST editing distance in capturing\nintricate code structures, revealing a high correlation with established\nmetrics. Furthermore, we explore the strengths and weaknesses of AST editing\ndistance and prompt-based GPT similarity scores in comparison to BLEU score,\nexecution match, and Jaccard Similarity. We propose, optimize, and publish an\nadaptable metric that demonstrates effectiveness across all tested languages,\nrepresenting an enhanced version of Tree Similarity of Edit Distance (TSED).\n","authors":["Yewei Song","Cedric Lothritz","Daniel Tang","Tegawend√© F. Bissyand√©","Jacques Klein"],"pdf_url":"https://arxiv.org/pdf/2404.08817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07214v2","updated":"2024-04-12T21:20:37Z","published":"2024-02-20T18:57:34Z","title":"Exploring the Frontier of Vision-Language Models: A Survey of Current\n  Methodologies and Future Directions","summary":"  The advent of Large Language Models (LLMs) has significantly reshaped the\ntrajectory of the AI revolution. Nevertheless, these LLMs exhibit a notable\nlimitation, as they are primarily adept at processing textual information. To\naddress this constraint, researchers have endeavored to integrate visual\ncapabilities with LLMs, resulting in the emergence of Vision-Language Models\n(VLMs). These advanced models are instrumental in tackling more intricate tasks\nsuch as image captioning and visual question answering. In our comprehensive\nsurvey paper, we delve into the key advancements within the realm of VLMs. Our\nclassification organizes VLMs into three distinct categories: models dedicated\nto vision-language understanding, models that process multimodal inputs to\ngenerate unimodal (textual) outputs and models that both accept and produce\nmultimodal inputs and outputs.This classification is based on their respective\ncapabilities and functionalities in processing and generating various\nmodalities of data.We meticulously dissect each model, offering an extensive\nanalysis of its foundational architecture, training data sources, as well as\nits strengths and limitations wherever possible, providing readers with a\ncomprehensive understanding of its essential components. We also analyzed the\nperformance of VLMs in various benchmark datasets. By doing so, we aim to offer\na nuanced understanding of the diverse landscape of VLMs. Additionally, we\nunderscore potential avenues for future research in this dynamic domain,\nanticipating further breakthroughs and advancements.\n","authors":["Akash Ghosh","Arkadeep Acharya","Sriparna Saha","Vinija Jain","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2404.07214v2.pdf","comment":"The most extensive and up to date Survey on Visual Language Models\n  covering 76 Visual Language Models"},{"id":"http://arxiv.org/abs/2404.08816v1","updated":"2024-04-12T21:16:53Z","published":"2024-04-12T21:16:53Z","title":"Evaluating the Quality of Answers in Political Q&A Sessions with Large\n  Language Models","summary":"  This paper presents a new approach to evaluating the quality of answers in\npolitical question-and-answer sessions. We propose to measure an answer's\nquality based on the degree to which it allows us to infer the initial question\naccurately. This conception of answer quality inherently reflects their\nrelevance to initial questions. Drawing parallels with semantic search, we\nargue that this measurement approach can be operationalized by fine-tuning a\nlarge language model on the observed corpus of questions and answers without\nadditional labeled data. We showcase our measurement approach within the\ncontext of the Question Period in the Canadian House of Commons. Our approach\nyields valuable insights into the correlates of the quality of answers in the\nQuestion Period. We find that answer quality varies significantly based on the\nparty affiliation of the members of Parliament asking the questions and uncover\na meaningful correlation between answer quality and the topics of the\nquestions.\n","authors":["R. Michael Alvarez","Jacob Morrier"],"pdf_url":"https://arxiv.org/pdf/2404.08816v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09182v2","updated":"2024-04-12T21:06:43Z","published":"2023-11-15T18:23:17Z","title":"ContraDoc: Understanding Self-Contradictions in Documents with Large\n  Language Models","summary":"  In recent times, large language models (LLMs) have shown impressive\nperformance on various document-level tasks such as document classification,\nsummarization, and question-answering. However, research on understanding their\ncapabilities on the task of self-contradictions in long documents has been very\nlimited. In this work, we introduce ContraDoc, the first human-annotated\ndataset to study self-contradictions in long documents across multiple domains,\nvarying document lengths, self-contradictions types, and scope. We then analyze\nthe current capabilities of four state-of-the-art open-source and commercially\navailable LLMs: GPT3.5, GPT4, PaLM2, and LLaMAv2 on this dataset. While GPT4\nperforms the best and can outperform humans on this task, we find that it is\nstill unreliable and struggles with self-contradictions that require more\nnuance and context. We release the dataset and all the code associated with the\nexperiments (https://github.com/ddhruvkr/CONTRADOC).\n","authors":["Jierui Li","Vipul Raheja","Dhruv Kumar"],"pdf_url":"https://arxiv.org/pdf/2311.09182v2.pdf","comment":"Accepted to NAACL 2024 main conference"},{"id":"http://arxiv.org/abs/2404.08806v1","updated":"2024-04-12T20:41:47Z","published":"2024-04-12T20:41:47Z","title":"CreativEval: Evaluating Creativity of LLM-Based Hardware Code Generation","summary":"  Large Language Models (LLMs) have proved effective and efficient in\ngenerating code, leading to their utilization within the hardware design\nprocess. Prior works evaluating LLMs' abilities for register transfer level\ncode generation solely focus on functional correctness. However, the creativity\nassociated with these LLMs, or the ability to generate novel and unique\nsolutions, is a metric not as well understood, in part due to the challenge of\nquantifying this quality.\n  To address this research gap, we present CreativeEval, a framework for\nevaluating the creativity of LLMs within the context of generating hardware\ndesigns. We quantify four creative sub-components, fluency, flexibility,\noriginality, and elaboration, through various prompting and post-processing\ntechniques. We then evaluate multiple popular LLMs (including GPT models,\nCodeLlama, and VeriGen) upon this creativity metric, with results indicating\nGPT-3.5 as the most creative model in generating hardware designs.\n","authors":["Matthew DeLorenzo","Vasudev Gohil","Jeyavijayan Rajendran"],"pdf_url":"https://arxiv.org/pdf/2404.08806v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2404.08640v1","updated":"2024-04-12T17:59:47Z","published":"2024-04-12T17:59:47Z","title":"EventEgo3D: 3D Human Motion Capture from Egocentric Event Streams","summary":"  Monocular egocentric 3D human motion capture is a challenging and actively\nresearched problem. Existing methods use synchronously operating visual sensors\n(e.g. RGB cameras) and often fail under low lighting and fast motions, which\ncan be restricting in many applications involving head-mounted devices. In\nresponse to the existing limitations, this paper 1) introduces a new problem,\ni.e., 3D human motion capture from an egocentric monocular event camera with a\nfisheye lens, and 2) proposes the first approach to it called EventEgo3D\n(EE3D). Event streams have high temporal resolution and provide reliable cues\nfor 3D human motion capture under high-speed human motions and rapidly changing\nillumination. The proposed EE3D framework is specifically tailored for learning\nwith event streams in the LNES representation, enabling high 3D reconstruction\naccuracy. We also design a prototype of a mobile head-mounted device with an\nevent camera and record a real dataset with event observations and the\nground-truth 3D human poses (in addition to the synthetic dataset). Our EE3D\ndemonstrates robustness and superior 3D accuracy compared to existing solutions\nacross various challenging experiments while supporting real-time 3D pose\nupdate rates of 140Hz.\n","authors":["Christen Millerdurai","Hiroyasu Akada","Jian Wang","Diogo Luvizon","Christian Theobalt","Vladislav Golyanik"],"pdf_url":"https://arxiv.org/pdf/2404.08640v1.pdf","comment":"14 pages, 11 figures and 6 tables; project page:\n  https://4dqv.mpi-inf.mpg.de/EventEgo3D/; Computer Vision and Pattern\n  Recognition (CVPR) 2024"},{"id":"http://arxiv.org/abs/2404.08639v1","updated":"2024-04-12T17:59:40Z","published":"2024-04-12T17:59:40Z","title":"COCONut: Modernizing COCO Segmentation","summary":"  In recent decades, the vision community has witnessed remarkable progress in\nvisual recognition, partially owing to advancements in dataset benchmarks.\nNotably, the established COCO benchmark has propelled the development of modern\ndetection and segmentation systems. However, the COCO segmentation benchmark\nhas seen comparatively slow improvement over the last decade. Originally\nequipped with coarse polygon annotations for thing instances, it gradually\nincorporated coarse superpixel annotations for stuff regions, which were\nsubsequently heuristically amalgamated to yield panoptic segmentation\nannotations. These annotations, executed by different groups of raters, have\nresulted not only in coarse segmentation masks but also in inconsistencies\nbetween segmentation types. In this study, we undertake a comprehensive\nreevaluation of the COCO segmentation annotations. By enhancing the annotation\nquality and expanding the dataset to encompass 383K images with more than 5.18M\npanoptic masks, we introduce COCONut, the COCO Next Universal segmenTation\ndataset. COCONut harmonizes segmentation annotations across semantic, instance,\nand panoptic segmentation with meticulously crafted high-quality masks, and\nestablishes a robust benchmark for all segmentation tasks. To our knowledge,\nCOCONut stands as the inaugural large-scale universal segmentation dataset,\nverified by human raters. We anticipate that the release of COCONut will\nsignificantly contribute to the community's ability to assess the progress of\nnovel neural networks.\n","authors":["Xueqing Deng","Qihang Yu","Peng Wang","Xiaohui Shen","Liang-Chieh Chen"],"pdf_url":"https://arxiv.org/pdf/2404.08639v1.pdf","comment":"Accepted at CVPR2024, data available at\n  https://xdeng7.github.io/coconut.github.io/"},{"id":"http://arxiv.org/abs/2404.08636v1","updated":"2024-04-12T17:58:04Z","published":"2024-04-12T17:58:04Z","title":"Probing the 3D Awareness of Visual Foundation Models","summary":"  Recent advances in large-scale pretraining have yielded visual foundation\nmodels with strong capabilities. Not only can recent models generalize to\narbitrary images for their training task, their intermediate representations\nare useful for other visual tasks such as detection and segmentation. Given\nthat such models can classify, delineate, and localize objects in 2D, we ask\nwhether they also represent their 3D structure? In this work, we analyze the 3D\nawareness of visual foundation models. We posit that 3D awareness implies that\nrepresentations (1) encode the 3D structure of the scene and (2) consistently\nrepresent the surface across views. We conduct a series of experiments using\ntask-specific probes and zero-shot inference procedures on frozen features. Our\nexperiments reveal several limitations of the current models. Our code and\nanalysis can be found at https://github.com/mbanani/probe3d.\n","authors":["Mohamed El Banani","Amit Raj","Kevis-Kokitsi Maninis","Abhishek Kar","Yuanzhen Li","Michael Rubinstein","Deqing Sun","Leonidas Guibas","Justin Johnson","Varun Jampani"],"pdf_url":"https://arxiv.org/pdf/2404.08636v1.pdf","comment":"Accepted to CVPR 2024. Project page:\n  https://github.com/mbanani/probe3d"},{"id":"http://arxiv.org/abs/2403.15388v4","updated":"2024-04-12T17:34:29Z","published":"2024-03-22T17:59:52Z","title":"LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal\n  Models","summary":"  Large Multimodal Models (LMMs) have shown significant reasoning capabilities\nby connecting a visual encoder and a large language model. LMMs typically use a\nfixed amount of visual tokens, such as the penultimate layer features in the\nCLIP visual encoder, as the prefix content. Recent LMMs incorporate more\ncomplex visual inputs, such as high-resolution images and videos, which\nincrease the number of visual tokens significantly. However, due to the design\nof the Transformer architecture, computational costs associated with these\nmodels tend to increase quadratically with the number of input tokens. To\ntackle this problem, we explore a token reduction mechanism and find, similar\nto prior work, that many visual tokens are spatially redundant. Based on this,\nwe propose PruMerge, a novel adaptive visual token reduction approach, which\nlargely reduces the number of visual tokens while maintaining comparable model\nperformance. We first select the unpruned visual tokens based on their\nsimilarity to class tokens and spatial tokens. We then cluster the pruned\ntokens based on key similarity and merge the clustered tokens with the unpruned\ntokens to supplement their information. Empirically, when applied to LLaVA-1.5,\nour approach can compress the visual tokens by 18 times on average, and achieve\ncomparable performance across diverse visual question-answering and reasoning\ntasks. Code and checkpoints are at https://llava-prumerge.github.io/.\n","authors":["Yuzhang Shang","Mu Cai","Bingxin Xu","Yong Jae Lee","Yan Yan"],"pdf_url":"https://arxiv.org/pdf/2403.15388v4.pdf","comment":"Project page: https://llava-prumerge.github.io/"},{"id":"http://arxiv.org/abs/2404.08611v1","updated":"2024-04-12T17:20:57Z","published":"2024-04-12T17:20:57Z","title":"Automatic Quantification of Serial PET/CT Images for Pediatric Hodgkin\n  Lymphoma Patients Using a Longitudinally-Aware Segmentation Network","summary":"  $\\textbf{Purpose}$: Automatic quantification of longitudinal changes in PET\nscans for lymphoma patients has proven challenging, as residual disease in\ninterim-therapy scans is often subtle and difficult to detect. Our goal was to\ndevelop a longitudinally-aware segmentation network (LAS-Net) that can quantify\nserial PET/CT images for pediatric Hodgkin lymphoma patients.\n$\\textbf{Materials and Methods}$: This retrospective study included baseline\n(PET1) and interim (PET2) PET/CT images from 297 patients enrolled in two\nChildren's Oncology Group clinical trials (AHOD1331 and AHOD0831). LAS-Net\nincorporates longitudinal cross-attention, allowing relevant features from PET1\nto inform the analysis of PET2. Model performance was evaluated using Dice\ncoefficients for PET1 and detection F1 scores for PET2. Additionally, we\nextracted and compared quantitative PET metrics, including metabolic tumor\nvolume (MTV) and total lesion glycolysis (TLG) in PET1, as well as qPET and\n$\\Delta$SUVmax in PET2, against physician measurements. We quantified their\nagreement using Spearman's $\\rho$ correlations and employed bootstrap\nresampling for statistical analysis. $\\textbf{Results}$: LAS-Net detected\nresidual lymphoma in PET2 with an F1 score of 0.606 (precision/recall:\n0.615/0.600), outperforming all comparator methods (P<0.01). For baseline\nsegmentation, LAS-Net achieved a mean Dice score of 0.772. In PET\nquantification, LAS-Net's measurements of qPET, $\\Delta$SUVmax, MTV and TLG\nwere strongly correlated with physician measurements, with Spearman's $\\rho$ of\n0.78, 0.80, 0.93 and 0.96, respectively. The performance remained high, with a\nslight decrease, in an external testing cohort. $\\textbf{Conclusion}$: LAS-Net\nachieved high performance in quantifying PET metrics across serial scans,\nhighlighting the value of longitudinal awareness in evaluating multi-time-point\nimaging datasets.\n","authors":["Xin Tie","Muheon Shin","Changhee Lee","Scott B. Perlman","Zachary Huemann","Amy J. Weisman","Sharon M. Castellino","Kara M. Kelly","Kathleen M. McCarten","Adina L. Alazraki","Junjie Hu","Steve Y. Cho","Tyler J. Bradshaw"],"pdf_url":"https://arxiv.org/pdf/2404.08611v1.pdf","comment":"6 figures, 4 tables in the main text"},{"id":"http://arxiv.org/abs/2310.16073v3","updated":"2024-04-12T17:04:15Z","published":"2023-10-24T14:59:51Z","title":"FloCoDe: Unbiased Dynamic Scene Graph Generation with Temporal\n  Consistency and Correlation Debiasing","summary":"  Dynamic scene graph generation (SGG) from videos requires not only a\ncomprehensive understanding of objects across scenes but also a method to\ncapture the temporal motions and interactions with different objects. Moreover,\nthe long-tailed distribution of visual relationships is a crucial bottleneck\nfor most dynamic SGG methods. This is because many of them focus on capturing\nspatio-temporal context using complex architectures, leading to the generation\nof biased scene graphs. To address these challenges, we propose FloCoDe:\nFlow-aware Temporal Consistency and Correlation Debiasing with uncertainty\nattenuation for unbiased dynamic scene graphs. FloCoDe employs feature warping\nusing flow to detect temporally consistent objects across frames. To address\nthe long-tail issue of visual relationships, we propose correlation debiasing\nand a label correlation-based loss to learn unbiased relation representations\nfor long-tailed classes. Specifically, we propose to incorporate label\ncorrelations using contrastive loss to capture commonly co-occurring relations,\nwhich aids in learning robust representations for long-tailed classes. Further,\nwe adopt the uncertainty attenuation-based classifier framework to handle noisy\nannotations in the SGG data. Extensive experimental evaluation shows a\nperformance gain as high as 4.1%, demonstrating the superiority of generating\nmore unbiased scene graphs.\n","authors":["Anant Khandelwal"],"pdf_url":"https://arxiv.org/pdf/2310.16073v3.pdf","comment":"Accepted at CVPR 2024 SG2RL, 11 pages, 5 tables, 4 figures"},{"id":"http://arxiv.org/abs/2404.08603v1","updated":"2024-04-12T17:02:56Z","published":"2024-04-12T17:02:56Z","title":"Training-free Boost for Open-Vocabulary Object Detection with Confidence\n  Aggregation","summary":"  Open-vocabulary object detection (OVOD) aims at localizing and recognizing\nvisual objects from novel classes unseen at the training time. Whereas,\nempirical studies reveal that advanced detectors generally assign lower scores\nto those novel instances, which are inadvertently suppressed during inference\nby commonly adopted greedy strategies like Non-Maximum Suppression (NMS),\nleading to sub-optimal detection performance for novel classes. This paper\nsystematically investigates this problem with the commonly-adopted two-stage\nOVOD paradigm. Specifically, in the region-proposal stage, proposals that\ncontain novel instances showcase lower objectness scores, since they are\ntreated as background proposals during the training phase. Meanwhile, in the\nobject-classification stage, novel objects share lower region-text similarities\n(i.e., classification scores) due to the biased visual-language alignment by\nseen training samples. To alleviate this problem, this paper introduces two\nadvanced measures to adjust confidence scores and conserve erroneously\ndismissed objects: (1) a class-agnostic localization quality estimate via\noverlap degree of region/object proposals, and (2) a text-guided visual\nsimilarity estimate with proxy prototypes for novel classes. Integrated with\nadjusting techniques specifically designed for the region-proposal and\nobject-classification stages, this paper derives the aggregated confidence\nestimate for the open-vocabulary object detection paradigm (AggDet). Our AggDet\nis a generic and training-free post-processing scheme, which consistently\nbolsters open-vocabulary detectors across model scales and architecture\ndesigns. For instance, AggDet receives 3.3% and 1.5% gains on OV-COCO and\nOV-LVIS benchmarks respectively, without any training cost.\n","authors":["Yanhao Zheng","Kai Liu"],"pdf_url":"https://arxiv.org/pdf/2404.08603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07520v2","updated":"2024-04-12T17:01:04Z","published":"2024-04-11T07:26:00Z","title":"PromptSync: Bridging Domain Gaps in Vision-Language Models through\n  Class-Aware Prototype Alignment and Discrimination","summary":"  The potential for zero-shot generalization in vision-language (V-L) models\nsuch as CLIP has spurred their widespread adoption in addressing numerous\ndownstream tasks. Previous methods have employed test-time prompt tuning to\nadapt the model to unseen domains, but they overlooked the issue of imbalanced\nclass distributions. In this study, we explicitly address this problem by\nemploying class-aware prototype alignment weighted by mean class probabilities\nobtained for the test sample and filtered augmented views. Additionally, we\nensure that the class probabilities are as accurate as possible by performing\nprototype discrimination using contrastive learning. The combination of\nalignment and discriminative loss serves as a geometric regularizer, preventing\nthe prompt representation from collapsing onto a single class and effectively\nbridging the distribution gap between the source and test domains. Our method,\nnamed PromptSync, synchronizes the prompts for each test sample on both the\ntext and vision branches of the V-L model. In empirical evaluations on the\ndomain generalization benchmark, our method outperforms previous best methods\nby 2.33% in overall performance, by 1% in base-to-novel generalization, and by\n2.84% in cross-dataset transfer tasks.\n","authors":["Anant Khandelwal"],"pdf_url":"https://arxiv.org/pdf/2404.07520v2.pdf","comment":"Accepted at CVPR 2024 LIMIT, 12 pages, 8 Tables, 2 Figures"},{"id":"http://arxiv.org/abs/2312.03884v2","updated":"2024-04-12T16:47:05Z","published":"2023-12-06T20:22:32Z","title":"WonderJourney: Going from Anywhere to Everywhere","summary":"  We introduce WonderJourney, a modularized framework for perpetual 3D scene\ngeneration. Unlike prior work on view generation that focuses on a single type\nof scenes, we start at any user-provided location (by a text description or an\nimage) and generate a journey through a long sequence of diverse yet coherently\nconnected 3D scenes. We leverage an LLM to generate textual descriptions of the\nscenes in this journey, a text-driven point cloud generation pipeline to make a\ncompelling and coherent sequence of 3D scenes, and a large VLM to verify the\ngenerated scenes. We show compelling, diverse visual results across various\nscene types and styles, forming imaginary \"wonderjourneys\". Project website:\nhttps://kovenyu.com/WonderJourney/\n","authors":["Hong-Xing Yu","Haoyi Duan","Junhwa Hur","Kyle Sargent","Michael Rubinstein","William T. Freeman","Forrester Cole","Deqing Sun","Noah Snavely","Jiajun Wu","Charles Herrmann"],"pdf_url":"https://arxiv.org/pdf/2312.03884v2.pdf","comment":"Project website with video results:\n  https://kovenyu.com/WonderJourney/"},{"id":"http://arxiv.org/abs/2404.08590v1","updated":"2024-04-12T16:38:48Z","published":"2024-04-12T16:38:48Z","title":"Improving Referring Image Segmentation using Vision-Aware Text Features","summary":"  Referring image segmentation is a challenging task that involves generating\npixel-wise segmentation masks based on natural language descriptions. Existing\nmethods have relied mostly on visual features to generate the segmentation\nmasks while treating text features as supporting components. This over-reliance\non visual features can lead to suboptimal results, especially in complex\nscenarios where text prompts are ambiguous or context-dependent. To overcome\nthese challenges, we present a novel framework VATEX to improve referring image\nsegmentation by enhancing object and context understanding with Vision-Aware\nText Feature. Our method involves using CLIP to derive a CLIP Prior that\nintegrates an object-centric visual heatmap with text description, which can be\nused as the initial query in DETR-based architecture for the segmentation task.\nFurthermore, by observing that there are multiple ways to describe an instance\nin an image, we enforce feature similarity between text variations referring to\nthe same visual input by two components: a novel Contextual Multimodal Decoder\nthat turns text embeddings into vision-aware text features, and a Meaning\nConsistency Constraint to ensure further the coherent and consistent\ninterpretation of language expressions with the context understanding obtained\nfrom the image. Our method achieves a significant performance improvement on\nthree benchmark datasets RefCOCO, RefCOCO+ and G-Ref. Code is available at:\nhttps://nero1342.github.io/VATEX\\_RIS.\n","authors":["Hai Nguyen-Truong","E-Ro Nguyen","Tuan-Anh Vu","Minh-Triet Tran","Binh-Son Hua","Sai-Kit Yeung"],"pdf_url":"https://arxiv.org/pdf/2404.08590v1.pdf","comment":"30 pages including supplementary"},{"id":"http://arxiv.org/abs/2401.01448v2","updated":"2024-04-12T16:37:46Z","published":"2024-01-02T22:15:20Z","title":"ProbMCL: Simple Probabilistic Contrastive Learning for Multi-label\n  Visual Classification","summary":"  Multi-label image classification presents a challenging task in many domains,\nincluding computer vision and medical imaging. Recent advancements have\nintroduced graph-based and transformer-based methods to improve performance and\ncapture label dependencies. However, these methods often include complex\nmodules that entail heavy computation and lack interpretability. In this paper,\nwe propose Probabilistic Multi-label Contrastive Learning (ProbMCL), a novel\nframework to address these challenges in multi-label image classification\ntasks. Our simple yet effective approach employs supervised contrastive\nlearning, in which samples that share enough labels with an anchor image based\non a decision threshold are introduced as a positive set. This structure\ncaptures label dependencies by pulling positive pair embeddings together and\npushing away negative samples that fall below the threshold. We enhance\nrepresentation learning by incorporating a mixture density network into\ncontrastive learning and generating Gaussian mixture distributions to explore\nthe epistemic uncertainty of the feature encoder. We validate the effectiveness\nof our framework through experimentation with datasets from the computer vision\nand medical imaging domains. Our method outperforms the existing\nstate-of-the-art methods while achieving a low computational footprint on both\ndatasets. Visualization analyses also demonstrate that ProbMCL-learned\nclassifiers maintain a meaningful semantic topology.\n","authors":["Ahmad Sajedi","Samir Khaki","Yuri A. Lawryshyn","Konstantinos N. Plataniotis"],"pdf_url":"https://arxiv.org/pdf/2401.01448v2.pdf","comment":"This paper has been accepted for the ICASSP 2024 - 2024 IEEE\n  International Conference on Acoustics, Speech and Signal Processing (ICASSP)"},{"id":"http://arxiv.org/abs/2404.08589v1","updated":"2024-04-12T16:35:23Z","published":"2024-04-12T16:35:23Z","title":"Enhancing Visual Question Answering through Question-Driven Image\n  Captions as Prompts","summary":"  Visual question answering (VQA) is known as an AI-complete task as it\nrequires understanding, reasoning, and inferring about the vision and the\nlanguage content. Over the past few years, numerous neural architectures have\nbeen suggested for the VQA problem. However, achieving success in zero-shot VQA\nremains a challenge due to its requirement for advanced generalization and\nreasoning skills. This study explores the impact of incorporating image\ncaptioning as an intermediary process within the VQA pipeline. Specifically, we\nexplore the efficacy of utilizing image captions instead of images and\nleveraging large language models (LLMs) to establish a zero-shot setting. Since\nimage captioning is the most crucial step in this process, we compare the\nimpact of state-of-the-art image captioning models on VQA performance across\nvarious question types in terms of structure and semantics. We propose a\nstraightforward and efficient question-driven image captioning approach within\nthis pipeline to transfer contextual information into the question-answering\n(QA) model. This method involves extracting keywords from the question,\ngenerating a caption for each image-question pair using the keywords, and\nincorporating the question-driven caption into the LLM prompt. We evaluate the\nefficacy of using general-purpose and question-driven image captions in the VQA\npipeline. Our study highlights the potential of employing image captions and\nharnessing the capabilities of LLMs to achieve competitive performance on GQA\nunder the zero-shot setting. Our code is available at\n\\url{https://github.com/ovguyo/captions-in-VQA}.\n","authors":["√ñvg√º √ñzdemir","Erdem Akag√ºnd√ºz"],"pdf_url":"https://arxiv.org/pdf/2404.08589v1.pdf","comment":"The paper has been accepted for presentation at CVPR 2024 Workshop on\n  Prompting in Vision"},{"id":"http://arxiv.org/abs/2404.08585v1","updated":"2024-04-12T16:30:15Z","published":"2024-04-12T16:30:15Z","title":"Advanced wood species identification based on multiple anatomical\n  sections and using deep feature transfer and fusion","summary":"  In recent years, we have seen many advancements in wood species\nidentification. Methods like DNA analysis, Near Infrared (NIR) spectroscopy,\nand Direct Analysis in Real Time (DART) mass spectrometry complement the\nlong-established wood anatomical assessment of cell and tissue morphology.\nHowever, most of these methods have some limitations such as high costs, the\nneed for skilled experts for data interpretation, and the lack of good datasets\nfor professional reference. Therefore, most of these methods, and certainly the\nwood anatomical assessment, may benefit from tools based on Artificial\nIntelligence. In this paper, we apply two transfer learning techniques with\nConvolutional Neural Networks (CNNs) to a multi-view Congolese wood species\ndataset including sections from different orientations and viewed at different\nmicroscopic magnifications. We explore two feature extraction methods in\ndetail, namely Global Average Pooling (GAP) and Random Encoding of Aggregated\nDeep Activation Maps (RADAM), for efficient and accurate wood species\nidentification. Our results indicate superior accuracy on diverse datasets and\nanatomical sections, surpassing the results of other methods. Our proposal\nrepresents a significant advancement in wood species identification, offering a\nrobust tool to support the conservation of forest ecosystems and promote\nsustainable forestry practices.\n","authors":["Kallil M. Zielinski","Leonardo Scabini","Lucas C. Ribas","N√∫bia R. da Silva","Hans Beeckman","Jan Verwaeren","Odemir M. Bruno","Bernard De Baets"],"pdf_url":"https://arxiv.org/pdf/2404.08585v1.pdf","comment":"33 pages, 7 tables, 9 figures"},{"id":"http://arxiv.org/abs/2404.08584v1","updated":"2024-04-12T16:29:49Z","published":"2024-04-12T16:29:49Z","title":"Pathological Primitive Segmentation Based on Visual Foundation Model\n  with Zero-Shot Mask Generation","summary":"  Medical image processing usually requires a model trained with carefully\ncrafted datasets due to unique image characteristics and domain-specific\nchallenges, especially in pathology. Primitive detection and segmentation in\ndigitized tissue samples are essential for objective and automated diagnosis\nand prognosis of cancer. SAM (Segment Anything Model) has recently been\ndeveloped to segment general objects from natural images with high accuracy,\nbut it requires human prompts to generate masks. In this work, we present a\nnovel approach that adapts pre-trained natural image encoders of SAM for\ndetection-based region proposals. Regions proposed by a pre-trained encoder are\nsent to cascaded feature propagation layers for projection. Then, local\nsemantic and global context is aggregated from multi-scale for bounding box\nlocalization and classification. Finally, the SAM decoder uses the identified\nbounding boxes as essential prompts to generate a comprehensive primitive\nsegmentation map. The entire base framework, SAM, requires no additional\ntraining or fine-tuning but could produce an end-to-end result for two\nfundamental segmentation tasks in pathology. Our method compares with\nstate-of-the-art models in F1 score for nuclei detection and binary/multiclass\npanoptic(bPQ/mPQ) and mask quality(dice) for segmentation quality on the\nPanNuke dataset while offering end-to-end efficiency. Our model also achieves\nremarkable Average Precision (+4.5%) on the secondary dataset (HuBMAP Kidney)\ncompared to Faster RCNN. The code is publicly available at\nhttps://github.com/learner-codec/autoprom_sam.\n","authors":["Abu Bakor Hayat Arnob","Xiangxue Wang","Yiping Jiao","Xiao Gan","Wenlong Ming","Jun Xu"],"pdf_url":"https://arxiv.org/pdf/2404.08584v1.pdf","comment":"2024 IEEE International Symposium on Biomedical Imaging"},{"id":"http://arxiv.org/abs/2404.08582v1","updated":"2024-04-12T16:28:30Z","published":"2024-04-12T16:28:30Z","title":"FashionFail: Addressing Failure Cases in Fashion Object Detection and\n  Segmentation","summary":"  In the realm of fashion object detection and segmentation for online shopping\nimages, existing state-of-the-art fashion parsing models encounter limitations,\nparticularly when exposed to non-model-worn apparel and close-up shots. To\naddress these failures, we introduce FashionFail; a new fashion dataset with\ne-commerce images for object detection and segmentation. The dataset is\nefficiently curated using our novel annotation tool that leverages recent\nfoundation models. The primary objective of FashionFail is to serve as a test\nbed for evaluating the robustness of models. Our analysis reveals the\nshortcomings of leading models, such as Attribute-Mask R-CNN and Fashionformer.\nAdditionally, we propose a baseline approach using naive data augmentation to\nmitigate common failure cases and improve model robustness. Through this work,\nwe aim to inspire and support further research in fashion item detection and\nsegmentation for industrial applications. The dataset, annotation tool, code,\nand models are available at \\url{https://rizavelioglu.github.io/fashionfail/}.\n","authors":["Riza Velioglu","Robin Chan","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2404.08582v1.pdf","comment":"to be published in 2024 International Joint Conference on Neural\n  Networks (IJCNN)"},{"id":"http://arxiv.org/abs/2404.08580v1","updated":"2024-04-12T16:23:42Z","published":"2024-04-12T16:23:42Z","title":"Lossy Image Compression with Foundation Diffusion Models","summary":"  Incorporating diffusion models in the image compression domain has the\npotential to produce realistic and detailed reconstructions, especially at\nextremely low bitrates. Previous methods focus on using diffusion models as\nexpressive decoders robust to quantization errors in the conditioning signals,\nyet achieving competitive results in this manner requires costly training of\nthe diffusion model and long inference times due to the iterative generative\nprocess. In this work we formulate the removal of quantization error as a\ndenoising task, using diffusion to recover lost information in the transmitted\nimage latent. Our approach allows us to perform less than 10\\% of the full\ndiffusion generative process and requires no architectural changes to the\ndiffusion model, enabling the use of foundation models as a strong prior\nwithout additional fine tuning of the backbone. Our proposed codec outperforms\nprevious methods in quantitative realism metrics, and we verify that our\nreconstructions are qualitatively preferred by end users, even when other\nmethods use twice the bitrate.\n","authors":["Lucas Relic","Roberto Azevedo","Markus Gross","Christopher Schroers"],"pdf_url":"https://arxiv.org/pdf/2404.08580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06994v2","updated":"2024-04-12T16:07:55Z","published":"2024-02-10T17:02:53Z","title":"A Change Detection Reality Check","summary":"  In recent years, there has been an explosion of proposed change detection\ndeep learning architectures in the remote sensing literature. These approaches\nclaim to offer state-of-the-art performance on different standard benchmark\ndatasets. However, has the field truly made significant progress? In this paper\nwe perform experiments which conclude a simple U-Net segmentation baseline\nwithout training tricks or complicated architectural changes is still a top\nperformer for the task of change detection.\n","authors":["Isaac Corley","Caleb Robinson","Anthony Ortiz"],"pdf_url":"https://arxiv.org/pdf/2402.06994v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08561v1","updated":"2024-04-12T16:00:03Z","published":"2024-04-12T16:00:03Z","title":"IDD-X: A Multi-View Dataset for Ego-relative Important Object\n  Localization and Explanation in Dense and Unstructured Traffic","summary":"  Intelligent vehicle systems require a deep understanding of the interplay\nbetween road conditions, surrounding entities, and the ego vehicle's driving\nbehavior for safe and efficient navigation. This is particularly critical in\ndeveloping countries where traffic situations are often dense and unstructured\nwith heterogeneous road occupants. Existing datasets, predominantly geared\ntowards structured and sparse traffic scenarios, fall short of capturing the\ncomplexity of driving in such environments. To fill this gap, we present IDD-X,\na large-scale dual-view driving video dataset. With 697K bounding boxes, 9K\nimportant object tracks, and 1-12 objects per video, IDD-X offers comprehensive\nego-relative annotations for multiple important road objects covering 10\ncategories and 19 explanation label categories. The dataset also incorporates\nrearview information to provide a more complete representation of the driving\nenvironment. We also introduce custom-designed deep networks aimed at multiple\nimportant object localization and per-object explanation prediction. Overall,\nour dataset and introduced prediction models form the foundation for studying\nhow road conditions and surrounding entities affect driving behavior in complex\ntraffic situations.\n","authors":["Chirag Parikh","Rohit Saluja","C. V. Jawahar","Ravi Kiran Sarvadevabhatla"],"pdf_url":"https://arxiv.org/pdf/2404.08561v1.pdf","comment":"Accepted at ICRA 2024"},{"id":"http://arxiv.org/abs/2404.08557v1","updated":"2024-04-12T15:54:48Z","published":"2024-04-12T15:54:48Z","title":"Scalability in Building Component Data Annotation: Enhancing Facade\n  Material Classification with Synthetic Data","summary":"  Computer vision models trained on Google Street View images can create\nmaterial cadastres. However, current approaches need manually annotated\ndatasets that are difficult to obtain and often have class imbalance. To\naddress these challenges, this paper fine-tuned a Swin Transformer model on a\nsynthetic dataset generated with DALL-E and compared the performance to a\nsimilar manually annotated dataset. Although manual annotation remains the gold\nstandard, the synthetic dataset performance demonstrates a reasonable\nalternative. The findings will ease annotation needed to develop material\ncadastres, offering architects insights into opportunities for material reuse,\nthus contributing to the reduction of demolition waste.\n","authors":["Josie Harrison","Alexander Hollberg","Yinan Yu"],"pdf_url":"https://arxiv.org/pdf/2404.08557v1.pdf","comment":"10 pages, 6 figures, submitted to 2024 European Conference of\n  Computing in Construction"},{"id":"http://arxiv.org/abs/2310.02557v3","updated":"2024-04-12T15:48:47Z","published":"2023-10-04T03:30:32Z","title":"Generalization in diffusion models arises from geometry-adaptive\n  harmonic representations","summary":"  Deep neural networks (DNNs) trained for image denoising are able to generate\nhigh-quality samples with score-based reverse diffusion algorithms. These\nimpressive capabilities seem to imply an escape from the curse of\ndimensionality, but recent reports of memorization of the training set raise\nthe question of whether these networks are learning the \"true\" continuous\ndensity of the data. Here, we show that two DNNs trained on non-overlapping\nsubsets of a dataset learn nearly the same score function, and thus the same\ndensity, when the number of training images is large enough. In this regime of\nstrong generalization, diffusion-generated images are distinct from the\ntraining set, and are of high visual quality, suggesting that the inductive\nbiases of the DNNs are well-aligned with the data density. We analyze the\nlearned denoising functions and show that the inductive biases give rise to a\nshrinkage operation in a basis adapted to the underlying image. Examination of\nthese bases reveals oscillating harmonic structures along contours and in\nhomogeneous regions. We demonstrate that trained denoisers are inductively\nbiased towards these geometry-adaptive harmonic bases since they arise not only\nwhen the network is trained on photographic images, but also when it is trained\non image classes supported on low-dimensional manifolds for which the harmonic\nbasis is suboptimal. Finally, we show that when trained on regular image\nclasses for which the optimal basis is known to be geometry-adaptive and\nharmonic, the denoising performance of the networks is near-optimal.\n","authors":["Zahra Kadkhodaie","Florentin Guth","Eero P. Simoncelli","St√©phane Mallat"],"pdf_url":"https://arxiv.org/pdf/2310.02557v3.pdf","comment":"Accepted for oral presentation at ICLR, Vienna, May 2024"},{"id":"http://arxiv.org/abs/2404.08549v1","updated":"2024-04-12T15:45:26Z","published":"2024-04-12T15:45:26Z","title":"Benchmarking the Cell Image Segmentation Models Robustness under the\n  Microscope Optical Aberrations","summary":"  Cell segmentation is essential in biomedical research for analyzing cellular\nmorphology and behavior. Deep learning methods, particularly convolutional\nneural networks (CNNs), have revolutionized cell segmentation by extracting\nintricate features from images. However, the robustness of these methods under\nmicroscope optical aberrations remains a critical challenge. This study\ncomprehensively evaluates the performance of cell instance segmentation models\nunder simulated aberration conditions using the DynamicNuclearNet (DNN) and\nLIVECell datasets. Aberrations, including Astigmatism, Coma, Spherical, and\nTrefoil, were simulated using Zernike polynomial equations. Various\nsegmentation models, such as Mask R-CNN with different network heads (FPN, C3)\nand backbones (ResNet, VGG19, SwinS), were trained and tested under aberrated\nconditions. Results indicate that FPN combined with SwinS demonstrates superior\nrobustness in handling simple cell images affected by minor aberrations.\nConversely, Cellpose2.0 proves effective for complex cell images under similar\nconditions. Our findings provide insights into selecting appropriate\nsegmentation models based on cell morphology and aberration severity, enhancing\nthe reliability of cell segmentation in biomedical applications. Further\nresearch is warranted to validate these methods with diverse aberration types\nand emerging segmentation models. Overall, this research aims to guide\nresearchers in effectively utilizing cell segmentation models in the presence\nof minor optical aberrations.\n","authors":["Boyuan Peng","Jiaju Chen","Qihui Ye","Minjiang Chen","Peiwu Qin","Chenggang Yan","Dongmei Yu","Zhenglin Chen"],"pdf_url":"https://arxiv.org/pdf/2404.08549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08544v1","updated":"2024-04-12T15:37:53Z","published":"2024-04-12T15:37:53Z","title":"Analyzing Decades-Long Environmental Changes in Namibia Using Archival\n  Aerial Photography and Deep Learning","summary":"  This study explores object detection in historical aerial photographs of\nNamibia to identify long-term environmental changes. Specifically, we aim to\nidentify key objects -- \\textit{Waterholes}, \\textit{Omuti homesteads}, and\n\\textit{Big trees} -- around Oshikango in Namibia using sub-meter gray-scale\naerial imagery from 1943 and 1972. In this work, we propose a workflow for\nanalyzing historical aerial imagery using a deep semantic segmentation model on\nsparse hand-labels. To this end, we employ a number of strategies including\nclass-weighting, pseudo-labeling and empirical p-value-based filtering to\nbalance skewed and sparse representations of objects in the ground truth data.\nResults demonstrate the benefits of these different training strategies\nresulting in an average $F_1=0.661$ and $F_1=0.755$ over the three objects of\ninterest for the 1943 and 1972 imagery, respectively. We also identified that\nthe average size of Waterhole and Big trees increased while the average size of\nOmutis decreased between 1943 and 1972 reflecting some of the local effects of\nthe massive post-Second World War economic, agricultural, demographic, and\nenvironmental changes. This work also highlights the untapped potential of\nhistorical aerial photographs in understanding long-term environmental changes\nbeyond Namibia (and Africa). With the lack of adequate satellite technology in\nthe past, archival aerial photography offers a great alternative to uncover\ndecades-long environmental changes.\n","authors":["Girmaw Abebe Tadesse","Caleb Robinson","Gilles Quentin Hacheme","Akram Zaytar","Rahul Dodhia","Tsering Wangyal Shawa","Juan M. Lavista Ferres","Emmanuel H. Kreike"],"pdf_url":"https://arxiv.org/pdf/2404.08544v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08540v1","updated":"2024-04-12T15:35:20Z","published":"2024-04-12T15:35:20Z","title":"On the Robustness of Language Guidance for Low-Level Vision Tasks:\n  Findings from Depth Estimation","summary":"  Recent advances in monocular depth estimation have been made by incorporating\nnatural language as additional guidance. Although yielding impressive results,\nthe impact of the language prior, particularly in terms of generalization and\nrobustness, remains unexplored. In this paper, we address this gap by\nquantifying the impact of this prior and introduce methods to benchmark its\neffectiveness across various settings. We generate \"low-level\" sentences that\nconvey object-centric, three-dimensional spatial relationships, incorporate\nthem as additional language priors and evaluate their downstream impact on\ndepth estimation. Our key finding is that current language-guided depth\nestimators perform optimally only with scene-level descriptions and\ncounter-intuitively fare worse with low level descriptions. Despite leveraging\nadditional data, these methods are not robust to directed adversarial attacks\nand decline in performance with an increase in distribution shift. Finally, to\nprovide a foundation for future research, we identify points of failures and\noffer insights to better understand these shortcomings. With an increasing\nnumber of methods using language for depth estimation, our findings highlight\nthe opportunities and pitfalls that require careful consideration for effective\ndeployment in real-world settings\n","authors":["Agneet Chatterjee","Tejas Gokhale","Chitta Baral","Yezhou Yang"],"pdf_url":"https://arxiv.org/pdf/2404.08540v1.pdf","comment":"Accepted to CVPR 2024. Project webpage:\n  https://agneetchatterjee.com/robustness_depth_lang/"},{"id":"http://arxiv.org/abs/2404.08535v1","updated":"2024-04-12T15:30:03Z","published":"2024-04-12T15:30:03Z","title":"Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking","summary":"  Contrastive learning has gained widespread adoption for retrieval tasks due\nto its minimal requirement for manual annotations. However, popular contrastive\nframeworks typically learn from binary relevance, making them ineffective at\nincorporating direct fine-grained rankings. In this paper, we curate a\nlarge-scale dataset featuring detailed relevance scores for each query-document\npair to facilitate future research and evaluation. Subsequently, we propose\nGeneralized Contrastive Learning for Multi-Modal Retrieval and Ranking (GCL),\nwhich is designed to learn from fine-grained rankings beyond binary relevance\nscores. Our results show that GCL achieves a 94.5% increase in NDCG@10 for\nin-domain and 26.3 to 48.8% increases for cold-start evaluations, all relative\nto the CLIP baseline and involving ground truth rankings.\n","authors":["Tianyu Zhu","Myong Chol Jung","Jesse Clark"],"pdf_url":"https://arxiv.org/pdf/2404.08535v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08531v1","updated":"2024-04-12T15:18:25Z","published":"2024-04-12T15:18:25Z","title":"Text Prompt with Normality Guidance for Weakly Supervised Video Anomaly\n  Detection","summary":"  Weakly supervised video anomaly detection (WSVAD) is a challenging task.\nGenerating fine-grained pseudo-labels based on weak-label and then\nself-training a classifier is currently a promising solution. However, since\nthe existing methods use only RGB visual modality and the utilization of\ncategory text information is neglected, thus limiting the generation of more\naccurate pseudo-labels and affecting the performance of self-training. Inspired\nby the manual labeling process based on the event description, in this paper,\nwe propose a novel pseudo-label generation and self-training framework based on\nText Prompt with Normality Guidance (TPWNG) for WSVAD. Our idea is to transfer\nthe rich language-visual knowledge of the contrastive language-image\npre-training (CLIP) model for aligning the video event description text and\ncorresponding video frames to generate pseudo-labels. Specifically, We first\nfine-tune the CLIP for domain adaptation by designing two ranking losses and a\ndistributional inconsistency loss. Further, we propose a learnable text prompt\nmechanism with the assist of a normality visual prompt to further improve the\nmatching accuracy of video event description text and video frames. Then, we\ndesign a pseudo-label generation module based on the normality guidance to\ninfer reliable frame-level pseudo-labels. Finally, we introduce a temporal\ncontext self-adaptive learning module to learn the temporal dependencies of\ndifferent video events more flexibly and accurately. Extensive experiments show\nthat our method achieves state-of-the-art performance on two benchmark\ndatasets, UCF-Crime and XD-Viole\n","authors":["Zhiwei Yang","Jing Liu","Peng Wu"],"pdf_url":"https://arxiv.org/pdf/2404.08531v1.pdf","comment":"Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2402.11568v2","updated":"2024-04-12T15:17:17Z","published":"2024-02-18T12:31:29Z","title":"A novel Fourier neural operator framework for classification of\n  multi-sized images: Application to three dimensional digital porous media","summary":"  Fourier neural operators (FNOs) are invariant with respect to the size of\ninput images, and thus images with any size can be fed into FNO-based\nframeworks without any modification of network architectures, in contrast to\ntraditional convolutional neural networks (CNNs). Leveraging the advantage of\nFNOs, we propose a novel deep-learning framework for classifying images with\nvarying sizes. Particularly, we simultaneously train the proposed network on\nmulti-sized images. As a practical application, we consider the problem of\npredicting the label (e.g., permeability) of three-dimensional digital porous\nmedia. To construct the framework, an intuitive approach is to connect FNO\nlayers to a classifier using adaptive max pooling. First, we show that this\napproach is only effective for porous media with fixed sizes, whereas it fails\nfor porous media of varying sizes. To overcome this limitation, we introduce\nour approach: instead of using adaptive max pooling, we use static max pooling\nwith the size of channel width of FNO layers. Since the channel width of the\nFNO layers is independent of input image size, the introduced framework can\nhandle multi-sized images during training. We show the effectiveness of the\nintroduced framework and compare its performance with the intuitive approach\nthrough the example of the classification of three-dimensional digital porous\nmedia of varying sizes.\n","authors":["Ali Kashefi","Tapan Mukerji"],"pdf_url":"https://arxiv.org/pdf/2402.11568v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08526v1","updated":"2024-04-12T15:15:39Z","published":"2024-04-12T15:15:39Z","title":"Masked Image Modeling as a Framework for Self-Supervised Learning across\n  Eye Movements","summary":"  To make sense of their surroundings, intelligent systems must transform\ncomplex sensory inputs to structured codes that are reduced to task-relevant\ninformation such as object category. Biological agents achieve this in a\nlargely autonomous manner, presumably via self-\\allowbreak super-\\allowbreak\nvised learning. Whereas previous attempts to model the underlying mechanisms\nwere largely discriminative in nature, there is ample evidence that the brain\nemploys a generative model of the world. Here, we propose that eye movements,\nin combination with the focused nature of primate vision, constitute a\ngenerative, self-supervised task of predicting and revealing visual\ninformation. We construct a proof-of-principle model starting from the\nframework of masked image modeling (MIM), a common approach in deep\nrepresentation learning. To do so, we analyze how core components of MIM such\nas masking technique and data augmentation influence the formation of\ncategory-specific representations. This allows us not only to better understand\nthe principles behind MIM, but to then reassemble a MIM more in line with the\nfocused nature of biological perception. From a theoretical angle, we find that\nMIM disentangles neurons in latent space, a property that has been suggested to\nstructure visual representations in primates, without explicit regulation.\nTogether with previous findings of invariance learning, this highlights an\ninteresting connection of MIM to latent regularization approaches for\nself-supervised learning. The source code is available under\nhttps://github.com/RobinWeiler/FocusMIM\n","authors":["Robin Weiler","Matthias Brucklacher","Cyriel M. A. Pennartz","Sander M. Boht√©"],"pdf_url":"https://arxiv.org/pdf/2404.08526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11868v3","updated":"2024-04-12T15:01:10Z","published":"2024-03-18T15:22:09Z","title":"View-Consistent 3D Editing with Gaussian Splatting","summary":"  The advent of 3D Gaussian Splatting (3DGS) has revolutionized 3D editing,\noffering efficient, high-fidelity rendering and enabling precise local\nmanipulations. Currently, diffusion-based 2D editing models are harnessed to\nmodify multi-view rendered images, which then guide the editing of 3DGS models.\nHowever, this approach faces a critical issue of multi-view inconsistency,\nwhere the guidance images exhibit significant discrepancies across views,\nleading to mode collapse and visual artifacts of 3DGS. To this end, we\nintroduce View-consistent Editing (VcEdit), a novel framework that seamlessly\nincorporates 3DGS into image editing processes, ensuring multi-view consistency\nin edited guidance images and effectively mitigating mode collapse issues.\nVcEdit employs two innovative consistency modules: the Cross-attention\nConsistency Module and the Editing Consistency Module, both designed to reduce\ninconsistencies in edited images. By incorporating these consistency modules\ninto an iterative pattern, VcEdit proficiently resolves the issue of multi-view\ninconsistency, facilitating high-quality 3DGS editing across a diverse range of\nscenes.\n","authors":["Yuxuan Wang","Xuanyu Yi","Zike Wu","Na Zhao","Long Chen","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11868v3.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2404.06710v3","updated":"2024-04-12T14:58:21Z","published":"2024-04-10T03:31:32Z","title":"SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike\n  Camera","summary":"  One of the most critical factors in achieving sharp Novel View Synthesis\n(NVS) using neural field methods like Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS) is the quality of the training images. However,\nConventional RGB cameras are susceptible to motion blur. In contrast,\nneuromorphic cameras like event and spike cameras inherently capture more\ncomprehensive temporal information, which can provide a sharp representation of\nthe scene as additional training data. Recent methods have explored the\nintegration of event cameras to improve the quality of NVS. The event-RGB\napproaches have some limitations, such as high training costs and the inability\nto work effectively in the background. Instead, our study introduces a new\nmethod that uses the spike camera to overcome these limitations. By considering\ntexture reconstruction from spike streams as ground truth, we design the\nTexture from Spike (TfS) loss. Since the spike camera relies on temporal\nintegration instead of temporal differentiation used by event cameras, our\nproposed TfS loss maintains manageable training costs. It handles foreground\nobjects with backgrounds simultaneously. We also provide a real-world dataset\ncaptured with our spike-RGB camera system to facilitate future research\nendeavors. We conduct extensive experiments using synthetic and real-world\ndatasets to demonstrate that our design can enhance novel view synthesis across\nNeRF and 3DGS. The code and dataset will be made available for public access.\n","authors":["Gaole Dai","Zhenyu Wang","Qinwen Xu","Ming Lu","Wen Chen","Boxin Shi","Shanghang Zhang","Tiejun Huang"],"pdf_url":"https://arxiv.org/pdf/2404.06710v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08515v1","updated":"2024-04-12T14:54:34Z","published":"2024-04-12T14:54:34Z","title":"ChatGPT and general-purpose AI count fruits in pictures surprisingly\n  well","summary":"  Object counting is a popular task in deep learning applications in various\ndomains, including agriculture. A conventional deep learning approach requires\na large amount of training data, often a logistic problem in a real-world\napplication. To address this issue, we examined how well ChatGPT (GPT4V) and a\ngeneral-purpose AI (foundation model for object counting, T-Rex) can count the\nnumber of fruit bodies (coffee cherries) in 100 images. The foundation model\nwith few-shot learning outperformed the trained YOLOv8 model (R2 = 0.923 and\n0.900, respectively). ChatGPT also showed some interesting potential,\nespecially when few-shot learning with human feedback was applied (R2 = 0.360\nand 0.460, respectively). Moreover, we examined the time required for\nimplementation as a practical question. Obtaining the results with the\nfoundation model and ChatGPT were much shorter than the YOLOv8 model (0.83 hrs,\n1.75 hrs, and 161 hrs). We interpret these results as two surprises for deep\nlearning users in applied domains: a foundation model with few-shot\ndomain-specific learning can drastically save time and effort compared to the\nconventional approach, and ChatGPT can reveal a relatively good performance.\nBoth approaches do not need coding skills, which can foster AI education and\ndissemination.\n","authors":["Konlavach Mengsuwan","Juan Camilo Rivera Palacio","Masahiro Ryo"],"pdf_url":"https://arxiv.org/pdf/2404.08515v1.pdf","comment":"12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2404.08514v1","updated":"2024-04-12T14:54:26Z","published":"2024-04-12T14:54:26Z","title":"NIR-Assisted Image Denoising: A Selective Fusion Approach and A\n  Real-World Benchmark Datase","summary":"  Despite the significant progress in image denoising, it is still challenging\nto restore fine-scale details while removing noise, especially in extremely\nlow-light environments. Leveraging near-infrared (NIR) images to assist visible\nRGB image denoising shows the potential to address this issue, becoming a\npromising technology. Nonetheless, existing works still struggle with taking\nadvantage of NIR information effectively for real-world image denoising, due to\nthe content inconsistency between NIR-RGB images and the scarcity of real-world\npaired datasets. To alleviate the problem, we propose an efficient Selective\nFusion Module (SFM), which can be plug-and-played into the advanced denoising\nnetworks to merge the deep NIR-RGB features. Specifically, we sequentially\nperform the global and local modulation for NIR and RGB features, and then\nintegrate the two modulated features. Furthermore, we present a Real-world\nNIR-Assisted Image Denoising (Real-NAID) dataset, which covers diverse\nscenarios as well as various noise levels. Extensive experiments on both\nsynthetic and our real-world datasets demonstrate that the proposed method\nachieves better results than state-of-the-art ones. The dataset, codes, and\npre-trained models will be publicly available at\nhttps://github.com/ronjonxu/NAID.\n","authors":["Rongjian Xu","Zhilu Zhang","Renlong Wu","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2404.08514v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2401.03785v2","updated":"2024-04-12T14:44:04Z","published":"2024-01-08T10:06:52Z","title":"Identifying Important Group of Pixels using Interactions","summary":"  To better understand the behavior of image classifiers, it is useful to\nvisualize the contribution of individual pixels to the model prediction. In\nthis study, we propose a method, MoXI ($\\textbf{Mo}$del e$\\textbf{X}$planation\nby $\\textbf{I}$nteractions), that efficiently and accurately identifies a group\nof pixels with high prediction confidence. The proposed method employs\ngame-theoretic concepts, Shapley values and interactions, taking into account\nthe effects of individual pixels and the cooperative influence of pixels on\nmodel confidence. Theoretical analysis and experiments demonstrate that our\nmethod better identifies the pixels that are highly contributing to the model\noutputs than widely-used visualization by Grad-CAM, Attention rollout, and\nShapley value. While prior studies have suffered from the exponential\ncomputational cost in the computation of Shapley value and interactions, we\nshow that this can be reduced to quadratic cost for our task. The code is\navailable at https://github.com/KosukeSumiyasu/MoXI.\n","authors":["Kosuke Sumiyasu","Kazuhiko Kawamoto","Hiroshi Kera"],"pdf_url":"https://arxiv.org/pdf/2401.03785v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2404.08506v1","updated":"2024-04-12T14:40:45Z","published":"2024-04-12T14:40:45Z","title":"LaSagnA: Language-based Segmentation Assistant for Complex Queries","summary":"  Recent advancements have empowered Large Language Models for Vision (vLLMs)\nto generate detailed perceptual outcomes, including bounding boxes and masks.\nNonetheless, there are two constraints that restrict the further application of\nthese vLLMs: the incapability of handling multiple targets per query and the\nfailure to identify the absence of query objects in the image. In this study,\nwe acknowledge that the main cause of these problems is the insufficient\ncomplexity of training queries. Consequently, we define the general sequence\nformat for complex queries. Then we incorporate a semantic segmentation task in\nthe current pipeline to fulfill the requirements of training data. Furthermore,\nwe present three novel strategies to effectively handle the challenges arising\nfrom the direct integration of the proposed format. The effectiveness of our\nmodel in processing complex queries is validated by the comparable results with\nconventional methods on both close-set and open-set semantic segmentation\ndatasets. Additionally, we outperform a series of vLLMs in reasoning and\nreferring segmentation, showcasing our model's remarkable capabilities. We\nrelease the code at https://github.com/congvvc/LaSagnA.\n","authors":["Cong Wei","Haoxian Tan","Yujie Zhong","Yujiu Yang","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2404.08506v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08504v1","updated":"2024-04-12T14:34:24Z","published":"2024-04-12T14:34:24Z","title":"3D Human Scan With A Moving Event Camera","summary":"  Capturing the 3D human body is one of the important tasks in computer vision\nwith a wide range of applications such as virtual reality and sports analysis.\nHowever, conventional frame cameras are limited by their temporal resolution\nand dynamic range, which imposes constraints in real-world application setups.\nEvent cameras have the advantages of high temporal resolution and high dynamic\nrange (HDR), but the development of event-based methods is necessary to handle\ndata with different characteristics. This paper proposes a novel event-based\nmethod for 3D pose estimation and human mesh recovery. Prior work on\nevent-based human mesh recovery require frames (images) as well as event data.\nThe proposed method solely relies on events; it carves 3D voxels by moving the\nevent camera around a stationary body, reconstructs the human pose and mesh by\nattenuated rays, and fit statistical body models, preserving high-frequency\ndetails. The experimental results show that the proposed method outperforms\nconventional frame-based methods in the estimation accuracy of both pose and\nbody mesh. We also demonstrate results in challenging situations where a\nconventional camera has motion blur. This is the first to demonstrate\nevent-only human mesh recovery, and we hope that it is the first step toward\nachieving robust and accurate 3D human body scanning from vision sensors.\n","authors":["Kai Kohyama","Shintaro Shiba","Yoshimitsu Aoki"],"pdf_url":"https://arxiv.org/pdf/2404.08504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14991v2","updated":"2024-04-12T14:21:20Z","published":"2023-12-22T11:56:22Z","title":"FoodLMM: A Versatile Food Assistant using Large Multi-modal Model","summary":"  Large Multi-modal Models (LMMs) have made impressive progress in many\nvision-language tasks. Nevertheless, the performance of general LMMs in\nspecific domains is still far from satisfactory. This paper proposes FoodLMM, a\nversatile food assistant based on LMMs with various capabilities, including\nfood recognition, ingredient recognition, recipe generation, nutrition\nestimation, food segmentation and multi-round conversation. To facilitate\nFoodLMM to deal with tasks beyond pure text output, we introduce a series of\nnovel task-specific tokens and heads, enabling the model to predict food\nnutritional values and multiple segmentation masks. We adopt a two-stage\ntraining strategy. In the first stage, we utilize multiple public food\nbenchmarks for multi-task learning by leveraging the instruct-following\nparadigm. In the second stage, we construct a multi-round conversation dataset\nand a reasoning segmentation dataset to fine-tune the model, enabling it to\nconduct professional dialogues and generate segmentation masks based on complex\nreasoning in the food domain. Our fine-tuned FoodLMM achieves state-of-the-art\nresults across several food benchmarks. We will make our code, models and\ndatasets publicly available.\n","authors":["Yuehao Yin","Huiyan Qi","Bin Zhu","Jingjing Chen","Yu-Gang Jiang","Chong-Wah Ngo"],"pdf_url":"https://arxiv.org/pdf/2312.14991v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08489v1","updated":"2024-04-12T14:12:03Z","published":"2024-04-12T14:12:03Z","title":"SpectralMamba: Efficient Mamba for Hyperspectral Image Classification","summary":"  Recurrent neural networks and Transformers have recently dominated most\napplications in hyperspectral (HS) imaging, owing to their capability to\ncapture long-range dependencies from spectrum sequences. However, despite the\nsuccess of these sequential architectures, the non-ignorable inefficiency\ncaused by either difficulty in parallelization or computationally prohibitive\nattention still hinders their practicality, especially for large-scale\nobservation in remote sensing scenarios. To address this issue, we herein\npropose SpectralMamba -- a novel state space model incorporated efficient deep\nlearning framework for HS image classification. SpectralMamba features the\nsimplified but adequate modeling of HS data dynamics at two levels. First, in\nspatial-spectral space, a dynamical mask is learned by efficient convolutions\nto simultaneously encode spatial regularity and spectral peculiarity, thus\nattenuating the spectral variability and confusion in discriminative\nrepresentation learning. Second, the merged spectrum can then be efficiently\noperated in the hidden state space with all parameters learned input-dependent,\nyielding selectively focused responses without reliance on redundant attention\nor imparallelizable recurrence. To explore the room for further computational\ndownsizing, a piece-wise scanning mechanism is employed in-between,\ntransferring approximately continuous spectrum into sequences with squeezed\nlength while maintaining short- and long-term contextual profiles among\nhundreds of bands. Through extensive experiments on four benchmark HS datasets\nacquired by satellite-, aircraft-, and UAV-borne imagers, SpectralMamba\nsurprisingly creates promising win-wins from both performance and efficiency\nperspectives.\n","authors":["Jing Yao","Danfeng Hong","Chenyu Li","Jocelyn Chanussot"],"pdf_url":"https://arxiv.org/pdf/2404.08489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00513v2","updated":"2024-04-12T13:58:33Z","published":"2024-03-31T01:20:16Z","title":"Transformer based Pluralistic Image Completion with Reduced Information\n  Loss","summary":"  Transformer based methods have achieved great success in image inpainting\nrecently. However, we find that these solutions regard each pixel as a token,\nthus suffering from an information loss issue from two aspects: 1) They\ndownsample the input image into much lower resolutions for efficiency\nconsideration. 2) They quantize $256^3$ RGB values to a small number (such as\n512) of quantized color values. The indices of quantized pixels are used as\ntokens for the inputs and prediction targets of the transformer. To mitigate\nthese issues, we propose a new transformer based framework called \"PUT\".\nSpecifically, to avoid input downsampling while maintaining computation\nefficiency, we design a patch-based auto-encoder P-VQVAE. The encoder converts\nthe masked image into non-overlapped patch tokens and the decoder recovers the\nmasked regions from the inpainted tokens while keeping the unmasked regions\nunchanged. To eliminate the information loss caused by input quantization, an\nUn-quantized Transformer is applied. It directly takes features from the\nP-VQVAE encoder as input without any quantization and only regards the\nquantized tokens as prediction targets. Furthermore, to make the inpainting\nprocess more controllable, we introduce semantic and structural conditions as\nextra guidance. Extensive experiments show that our method greatly outperforms\nexisting transformer based methods on image fidelity and achieves much higher\ndiversity and better fidelity than state-of-the-art pluralistic inpainting\nmethods on complex large-scale datasets (e.g., ImageNet). Codes are available\nat https://github.com/liuqk3/PUT.\n","authors":["Qiankun Liu","Yuqi Jiang","Zhentao Tan","Dongdong Chen","Ying Fu","Qi Chu","Gang Hua","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2404.00513v2.pdf","comment":"Accepted by TPAMI (2024). arXiv admin note: text overlap with\n  arXiv:2205.05076"},{"id":"http://arxiv.org/abs/2404.08477v1","updated":"2024-04-12T13:55:05Z","published":"2024-04-12T13:55:05Z","title":"New Efficient Visual OILU Markers","summary":"  Basic patterns are the source of a wide range of more or less complex\ngeometric structures. We will exploit such patterns to develop new efficient\nvisual markers. Besides being projective invariants, the proposed markers allow\nproducing rich panel of unique identifiers, highly required for\nresource-intensive navigation and augmented reality applications. The spiral\ntopology of our markers permits the validation of an accurate identification\nscheme, which is based on level set methods. The robustness of the markers\nagainst acquisition and geometric distortions is validated by extensive\nexperimental tests.\n","authors":["Youssef Chahir","Messaoud Mostefai","Hamza Saida"],"pdf_url":"https://arxiv.org/pdf/2404.08477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13570v2","updated":"2024-04-12T13:44:44Z","published":"2023-11-22T18:25:51Z","title":"WildFusion: Learning 3D-Aware Latent Diffusion Models in View Space","summary":"  Modern learning-based approaches to 3D-aware image synthesis achieve high\nphotorealism and 3D-consistent viewpoint changes for the generated images.\nExisting approaches represent instances in a shared canonical space. However,\nfor in-the-wild datasets a shared canonical system can be difficult to define\nor might not even exist. In this work, we instead model instances in view\nspace, alleviating the need for posed images and learned camera distributions.\nWe find that in this setting, existing GAN-based methods are prone to\ngenerating flat geometry and struggle with distribution coverage. We hence\npropose WildFusion, a new approach to 3D-aware image synthesis based on latent\ndiffusion models (LDMs). We first train an autoencoder that infers a compressed\nlatent representation, which additionally captures the images' underlying 3D\nstructure and enables not only reconstruction but also novel view synthesis. To\nlearn a faithful 3D representation, we leverage cues from monocular depth\nprediction. Then, we train a diffusion model in the 3D-aware latent space,\nthereby enabling synthesis of high-quality 3D-consistent image samples,\noutperforming recent state-of-the-art GAN-based methods. Importantly, our\n3D-aware LDM is trained without any direct supervision from multiview images or\n3D geometry and does not require posed images or learned pose or camera\ndistributions. It directly learns a 3D representation without relying on\ncanonical camera coordinates. This opens up promising research avenues for\nscalable 3D-aware image synthesis and 3D content creation from in-the-wild\nimage data. See https://katjaschwarz.github.io/wildfusion for videos of our 3D\nresults.\n","authors":["Katja Schwarz","Seung Wook Kim","Jun Gao","Sanja Fidler","Andreas Geiger","Karsten Kreis"],"pdf_url":"https://arxiv.org/pdf/2311.13570v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08452v1","updated":"2024-04-12T13:02:08Z","published":"2024-04-12T13:02:08Z","title":"MoE-FFD: Mixture of Experts for Generalized and Parameter-Efficient Face\n  Forgery Detection","summary":"  Deepfakes have recently raised significant trust issues and security concerns\namong the public. Compared to CNN face forgery detectors, ViT-based methods\ntake advantage of the expressivity of transformers, achieving superior\ndetection performance. However, these approaches still exhibit the following\nlimitations: (1). Fully fine-tuning ViT-based models from ImageNet weights\ndemands substantial computational and storage resources; (2). ViT-based methods\nstruggle to capture local forgery clues, leading to model bias and limited\ngeneralizability. To tackle these challenges, this work introduces\nMixture-of-Experts modules for Face Forgery Detection (MoE-FFD), a generalized\nyet parameter-efficient ViT-based approach. MoE-FFD only updates lightweight\nLow-Rank Adaptation (LoRA) and Adapter layers while keeping the ViT backbone\nfrozen, thereby achieving parameter-efficient training. Moreover, MoE-FFD\nleverages the expressivity of transformers and local priors of CNNs to\nsimultaneously extract global and local forgery clues. Additionally, novel MoE\nmodules are designed to scale the model's capacity and select optimal forgery\nexperts, further enhancing forgery detection performance. The proposed MoE\nlearning scheme can be seamlessly adapted to various transformer backbones in a\nplug-and-play manner. Extensive experimental results demonstrate that the\nproposed method achieves state-of-the-art face forgery detection performance\nwith reduced parameter overhead. The code will be released upon acceptance.\n","authors":["Chenqi Kong","Anwei Luo","Song Xia","Yi Yu","Haoliang Li","Alex C. Kot"],"pdf_url":"https://arxiv.org/pdf/2404.08452v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08450v1","updated":"2024-04-12T13:01:22Z","published":"2024-04-12T13:01:22Z","title":"Joint Physical-Digital Facial Attack Detection Via Simulating Spoofing\n  Clues","summary":"  Face recognition systems are frequently subjected to a variety of physical\nand digital attacks of different types. Previous methods have achieved\nsatisfactory performance in scenarios that address physical attacks and digital\nattacks, respectively. However, few methods are considered to integrate a model\nthat simultaneously addresses both physical and digital attacks, implying the\nnecessity to develop and maintain multiple models. To jointly detect physical\nand digital attacks within a single model, we propose an innovative approach\nthat can adapt to any network architecture. Our approach mainly contains two\ntypes of data augmentation, which we call Simulated Physical Spoofing Clues\naugmentation (SPSC) and Simulated Digital Spoofing Clues augmentation (SDSC).\nSPSC and SDSC augment live samples into simulated attack samples by simulating\nspoofing clues of physical and digital attacks, respectively, which\nsignificantly improve the capability of the model to detect \"unseen\" attack\ntypes. Extensive experiments show that SPSC and SDSC can achieve\nstate-of-the-art generalization in Protocols 2.1 and 2.2 of the UniAttackData\ndataset, respectively. Our method won first place in \"Unified Physical-Digital\nFace Attack Detection\" of the 5th Face Anti-spoofing Challenge@CVPR2024. Our\nfinal submission obtains 3.75% APCER, 0.93% BPCER, and 2.34% ACER,\nrespectively. Our code is available at\nhttps://github.com/Xianhua-He/cvpr2024-face-anti-spoofing-challenge.\n","authors":["Xianhua He","Dashuang Liang","Song Yang","Zhanlong Hao","Hui Ma","Binjie Mao","Xi Li","Yao Wang","Pengfei Yan","Ajian Liu"],"pdf_url":"https://arxiv.org/pdf/2404.08450v1.pdf","comment":"10 pages with 6 figures, Accepted by CVPRW 2024"},{"id":"http://arxiv.org/abs/2404.08449v1","updated":"2024-04-12T13:00:06Z","published":"2024-04-12T13:00:06Z","title":"OccGaussian: 3D Gaussian Splatting for Occluded Human Rendering","summary":"  Rendering dynamic 3D human from monocular videos is crucial for various\napplications such as virtual reality and digital entertainment. Most methods\nassume the people is in an unobstructed scene, while various objects may cause\nthe occlusion of body parts in real-life scenarios. Previous method utilizing\nNeRF for surface rendering to recover the occluded areas, but it requiring more\nthan one day to train and several seconds to render, failing to meet the\nrequirements of real-time interactive applications. To address these issues, we\npropose OccGaussian based on 3D Gaussian Splatting, which can be trained within\n6 minutes and produces high-quality human renderings up to 160 FPS with\noccluded input. OccGaussian initializes 3D Gaussian distributions in the\ncanonical space, and we perform occlusion feature query at occluded regions,\nthe aggregated pixel-align feature is extracted to compensate for the missing\ninformation. Then we use Gaussian Feature MLP to further process the feature\nalong with the occlusion-aware loss functions to better perceive the occluded\narea. Extensive experiments both in simulated and real-world occlusions,\ndemonstrate that our method achieves comparable or even superior performance\ncompared to the state-of-the-art method. And we improving training and\ninference speeds by 250x and 800x, respectively. Our code will be available for\nresearch purposes.\n","authors":["Jingrui Ye","Zongkai Zhang","Yujiao Jiang","Qingmin Liao","Wenming Yang","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2404.08449v1.pdf","comment":"12 April, 2024; originally announced April 2024"},{"id":"http://arxiv.org/abs/2404.08433v1","updated":"2024-04-12T12:30:48Z","published":"2024-04-12T12:30:48Z","title":"MSSTNet: A Multi-Scale Spatio-Temporal CNN-Transformer Network for\n  Dynamic Facial Expression Recognition","summary":"  Unlike typical video action recognition, Dynamic Facial Expression\nRecognition (DFER) does not involve distinct moving targets but relies on\nlocalized changes in facial muscles. Addressing this distinctive attribute, we\npropose a Multi-Scale Spatio-temporal CNN-Transformer network (MSSTNet). Our\napproach takes spatial features of different scales extracted by CNN and feeds\nthem into a Multi-scale Embedding Layer (MELayer). The MELayer extracts\nmulti-scale spatial information and encodes these features before sending them\ninto a Temporal Transformer (T-Former). The T-Former simultaneously extracts\ntemporal information while continually integrating multi-scale spatial\ninformation. This process culminates in the generation of multi-scale\nspatio-temporal features that are utilized for the final classification. Our\nmethod achieves state-of-the-art results on two in-the-wild datasets.\nFurthermore, a series of ablation experiments and visualizations provide\nfurther validation of our approach's proficiency in leveraging spatio-temporal\ninformation within DFER.\n","authors":["Linhuang Wang","Xin Kang","Fei Ding","Satoshi Nakagawa","Fuji Ren"],"pdf_url":"https://arxiv.org/pdf/2404.08433v1.pdf","comment":"Accepted to 2024 IEEE International Conference on Acoustics, Speech,\n  and Signal Processing (ICASSP 2024)"},{"id":"http://arxiv.org/abs/2404.08421v1","updated":"2024-04-12T12:10:53Z","published":"2024-04-12T12:10:53Z","title":"Adapting the Segment Anything Model During Usage in Novel Situations","summary":"  The interactive segmentation task consists in the creation of object\nsegmentation masks based on user interactions. The most common way to guide a\nmodel towards producing a correct segmentation consists in clicks on the object\nand background. The recently published Segment Anything Model (SAM) supports a\ngeneralized version of the interactive segmentation problem and has been\ntrained on an object segmentation dataset which contains 1.1B masks. Though\nbeing trained extensively and with the explicit purpose of serving as a\nfoundation model, we show significant limitations of SAM when being applied for\ninteractive segmentation on novel domains or object types. On the used\ndatasets, SAM displays a failure rate $\\text{FR}_{30}@90$ of up to $72.6 \\%$.\nSince we still want such foundation models to be immediately applicable, we\npresent a framework that can adapt SAM during immediate usage. For this we will\nleverage the user interactions and masks, which are constructed during the\ninteractive segmentation process. We use this information to generate\npseudo-labels, which we use to compute a loss function and optimize a part of\nthe SAM model. The presented method causes a relative reduction of up to $48.1\n\\%$ in the $\\text{FR}_{20}@85$ and $46.6 \\%$ in the $\\text{FR}_{30}@90$\nmetrics.\n","authors":["Robin Sch√∂n","Julian Lorenz","Katja Ludwig","Rainer Lienhart"],"pdf_url":"https://arxiv.org/pdf/2404.08421v1.pdf","comment":"11 pages, 2 figures, 4 tables"},{"id":"http://arxiv.org/abs/2404.08419v1","updated":"2024-04-12T12:08:06Z","published":"2024-04-12T12:08:06Z","title":"Direct May Not Be the Best: An Incremental Evolution View of Pose\n  Generation","summary":"  Pose diversity is an inherent representative characteristic of 2D images. Due\nto the 3D to 2D projection mechanism, there is evident content discrepancy\namong distinct pose images. This is the main obstacle bothering pose\ntransformation related researches. To deal with this challenge, we propose a\nfine-grained incremental evolution centered pose generation framework, rather\nthan traditional direct one-to-one in a rush. Since proposed approach actually\nbypasses the theoretical difficulty of directly modeling dramatic non-linear\nvariation, the incurred content distortion and blurring could be effectively\nconstrained, at the same time the various individual pose details, especially\nclothes texture, could be precisely maintained. In order to systematically\nguide the evolution course, both global and incremental evolution constraints\nare elaborately designed and merged into the overall frame?work. And a novel\ntriple-path knowledge fusion structure is worked out to take full advantage of\nall available valuable knowledge to conduct high-quality pose synthesis. In\naddition, our framework could generate a series of valuable byproducts, namely\nthe various intermediate poses. Extensive experiments have been conducted to\nverify the effectiveness of the proposed approach. Code is available at\nhttps://github.com/Xiaofei-CN/Incremental-Evolution-Pose-Generation.\n","authors":["Yuelong Li","Tengfei Xiao","Lei Geng","Jianming Wang"],"pdf_url":"https://arxiv.org/pdf/2404.08419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.06707v2","updated":"2024-04-12T11:56:18Z","published":"2023-04-13T17:56:08Z","title":"Toward Reliable Human Pose Forecasting with Uncertainty","summary":"  Recently, there has been an arms race of pose forecasting methods aimed at\nsolving the spatio-temporal task of predicting a sequence of future 3D poses of\na person given a sequence of past observed ones. However, the lack of unified\nbenchmarks and limited uncertainty analysis have hindered progress in the\nfield. To address this, we first develop an open-source library for human pose\nforecasting, including multiple models, supporting several datasets, and\nemploying standardized evaluation metrics, with the aim of promoting research\nand moving toward a unified and consistent evaluation. Second, we devise two\ntypes of uncertainty in the problem to increase performance and convey better\ntrust: 1) we propose a method for modeling aleatoric uncertainty by using\nuncertainty priors to inject knowledge about the pattern of uncertainty. This\nfocuses the capacity of the model in the direction of more meaningful\nsupervision while reducing the number of learned parameters and improving\nstability; 2) we introduce a novel approach for quantifying the epistemic\nuncertainty of any model through clustering and measuring the entropy of its\nassignments. Our experiments demonstrate up to $25\\%$ improvements in\nforecasting at short horizons, with no loss on longer horizons on Human3.6M,\nAMSS, and 3DPW datasets, and better performance in uncertainty estimation. The\ncode is available online at https://github.com/vita-epfl/UnPOSed.\n","authors":["Saeed Saadatnejad","Mehrshad Mirmohammadi","Matin Daghyani","Parham Saremi","Yashar Zoroofchi Benisi","Amirhossein Alimohammadi","Zahra Tehraninasab","Taylor Mordan","Alexandre Alahi"],"pdf_url":"https://arxiv.org/pdf/2304.06707v2.pdf","comment":"Published in RA-L 2024"},{"id":"http://arxiv.org/abs/2404.08406v1","updated":"2024-04-12T11:33:26Z","published":"2024-04-12T11:33:26Z","title":"MambaDFuse: A Mamba-based Dual-phase Model for Multi-modality Image\n  Fusion","summary":"  Multi-modality image fusion (MMIF) aims to integrate complementary\ninformation from different modalities into a single fused image to represent\nthe imaging scene and facilitate downstream visual tasks comprehensively. In\nrecent years, significant progress has been made in MMIF tasks due to advances\nin deep neural networks. However, existing methods cannot effectively and\nefficiently extract modality-specific and modality-fused features constrained\nby the inherent local reductive bias (CNN) or quadratic computational\ncomplexity (Transformers). To overcome this issue, we propose a Mamba-based\nDual-phase Fusion (MambaDFuse) model. Firstly, a dual-level feature extractor\nis designed to capture long-range features from single-modality images by\nextracting low and high-level features from CNN and Mamba blocks. Then, a\ndual-phase feature fusion module is proposed to obtain fusion features that\ncombine complementary information from different modalities. It uses the\nchannel exchange method for shallow fusion and the enhanced Multi-modal Mamba\n(M3) blocks for deep fusion. Finally, the fused image reconstruction module\nutilizes the inverse transformation of the feature extraction to generate the\nfused result. Through extensive experiments, our approach achieves promising\nfusion results in infrared-visible image fusion and medical image fusion.\nAdditionally, in a unified benchmark, MambaDFuse has also demonstrated improved\nperformance in downstream tasks such as object detection. Code with checkpoints\nwill be available after the peer-review process.\n","authors":["Zhe Li","Haiwei Pan","Kejia Zhang","Yuhua Wang","Fengming Yu"],"pdf_url":"https://arxiv.org/pdf/2404.08406v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08401v1","updated":"2024-04-12T11:15:15Z","published":"2024-04-12T11:15:15Z","title":"No Bells, Just Whistles: Sports Field Registration by Leveraging\n  Geometric Properties","summary":"  Broadcast sports field registration is traditionally addressed as a\nhomography estimation task, mapping the visible image area to a planar field\nmodel, predominantly focusing on the main camera shot. Addressing the\nshortcomings of previous approaches, we propose a novel calibration pipeline\nenabling camera calibration using a 3D soccer field model and extending the\nprocess to assess the multiple-view nature of broadcast videos. Our approach\nbegins with a keypoint generation pipeline derived from SoccerNet dataset\nannotations, leveraging the geometric properties of the court. Subsequently, we\nexecute classical camera calibration through DLT algorithm in a minimalist\nfashion, without further refinement. Through extensive experimentation on\nreal-world soccer broadcast datasets such as SoccerNet-Calibration, WorldCup\n2014 and TS- WorldCup, our method demonstrates superior performance in both\nmultiple- and single-view 3D camera calibration while maintaining competitive\nresults in homography estimation compared to state-of-the-art techniques.\n","authors":["Marc Guti√©rrez-P√©rez","Antonio Agudo"],"pdf_url":"https://arxiv.org/pdf/2404.08401v1.pdf","comment":"Accepted in CVPRW 2024"},{"id":"http://arxiv.org/abs/2105.03026v2","updated":"2024-04-12T11:14:04Z","published":"2021-05-07T01:32:37Z","title":"Efficient Masked Face Recognition Method during the COVID-19 Pandemic","summary":"  The coronavirus disease (COVID-19) is an unparalleled crisis leading to a\nhuge number of casualties and security problems. In order to reduce the spread\nof coronavirus, people often wear masks to protect themselves. This makes face\nrecognition a very difficult task since certain parts of the face are hidden. A\nprimary focus of researchers during the ongoing coronavirus pandemic is to come\nup with suggestions to handle this problem through rapid and efficient\nsolutions. In this paper, we propose a reliable method based on occlusion\nremoval and deep learning-based features in order to address the problem of the\nmasked face recognition process. The first step is to remove the masked face\nregion. Next, we apply three pre-trained deep Convolutional Neural Networks\n(CNN) namely, VGG-16, AlexNet, and ResNet-50, and use them to extract deep\nfeatures from the obtained regions (mostly eyes and forehead regions). The\nBag-of-features paradigm is then applied to the feature maps of the last\nconvolutional layer in order to quantize them and to get a slight\nrepresentation comparing to the fully connected layer of classical CNN.\nFinally, Multilayer Perceptron (MLP) is applied for the classification process.\nExperimental results on Real-World-Masked-Face-Dataset show high recognition\nperformance compared to other state-of-the-art methods.\n","authors":["Walid Hariri"],"pdf_url":"https://arxiv.org/pdf/2105.03026v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08399v1","updated":"2024-04-12T11:08:26Z","published":"2024-04-12T11:08:26Z","title":"Mitigating Challenges of the Space Environment for Onboard Artificial\n  Intelligence: Design Overview of the Imaging Payload on SpIRIT","summary":"  Artificial intelligence (AI) and autonomous edge computing in space are\nemerging areas of interest to augment capabilities of nanosatellites, where\nmodern sensors generate orders of magnitude more data than can typically be\ntransmitted to mission control. Here, we present the hardware and software\ndesign of an onboard AI subsystem hosted on SpIRIT. The system is optimised for\non-board computer vision experiments based on visible light and long wave\ninfrared cameras. This paper highlights the key design choices made to maximise\nthe robustness of the system in harsh space conditions, and their motivation\nrelative to key mission requirements, such as limited compute resources,\nresilience to cosmic radiation, extreme temperature variations, distribution\nshifts, and very low transmission bandwidths. The payload, called Loris,\nconsists of six visible light cameras, three infrared cameras, a camera control\nboard and a Graphics Processing Unit (GPU) system-on-module. Loris enables the\nexecution of AI models with on-orbit fine-tuning as well as a next-generation\nimage compression algorithm, including progressive coding. This innovative\napproach not only enhances the data processing capabilities of nanosatellites\nbut also lays the groundwork for broader applications to remote sensing from\nspace.\n","authors":["Miguel Ortiz del Castillo","Jonathan Morgan","Jack McRobbie","Clint Therakam","Zaher Joukhadar","Robert Mearns","Simon Barraclough","Richard Sinnott","Andrew Woods","Chris Bayliss","Kris Ehinger","Ben Rubinstein","James Bailey","Airlie Chapman","Michele Trenti"],"pdf_url":"https://arxiv.org/pdf/2404.08399v1.pdf","comment":"AI4Space 2024, 3rd Workshop on AI for Space, CVPR 2024"},{"id":"http://arxiv.org/abs/2404.08392v1","updated":"2024-04-12T10:54:11Z","published":"2024-04-12T10:54:11Z","title":"NC-TTT: A Noise Contrastive Approach for Test-Time Training","summary":"  Despite their exceptional performance in vision tasks, deep learning models\noften struggle when faced with domain shifts during testing. Test-Time Training\n(TTT) methods have recently gained popularity by their ability to enhance the\nrobustness of models through the addition of an auxiliary objective that is\njointly optimized with the main task. Being strictly unsupervised, this\nauxiliary objective is used at test time to adapt the model without any access\nto labels. In this work, we propose Noise-Contrastive Test-Time Training\n(NC-TTT), a novel unsupervised TTT technique based on the discrimination of\nnoisy feature maps. By learning to classify noisy views of projected feature\nmaps, and then adapting the model accordingly on new domains, classification\nperformance can be recovered by an important margin. Experiments on several\npopular test-time adaptation baselines demonstrate the advantages of our method\ncompared to recent approaches for this task. The code can be found\nat:https://github.com/GustavoVargasHakim/NCTTT.git\n","authors":["David Osowiechi","Gustavo A. Vargas Hakim","Mehrdad Noori","Milad Cheraghalikhani","Ali Bahri","Moslem Yazdanpanah","Ismail Ben Ayed","Christian Desrosiers"],"pdf_url":"https://arxiv.org/pdf/2404.08392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04385v2","updated":"2024-04-12T10:15:45Z","published":"2024-03-07T10:25:23Z","title":"Impacts of Color and Texture Distortions on Earth Observation Data in\n  Deep Learning","summary":"  Land cover classification and change detection are two important applications\nof remote sensing and Earth observation (EO) that have benefited greatly from\nthe advances of deep learning. Convolutional and transformer-based U-net models\nare the state-of-the-art architectures for these tasks, and their performances\nhave been boosted by an increased availability of large-scale annotated EO\ndatasets. However, the influence of different visual characteristics of the\ninput EO data on a model's predictions is not well understood. In this work we\nsystematically examine model sensitivities with respect to several color- and\ntexture-based distortions on the input EO data during inference, given models\nthat have been trained without such distortions. We conduct experiments with\nmultiple state-of-the-art segmentation networks for land cover classification\nand show that they are in general more sensitive to texture than to color\ndistortions. Beyond revealing intriguing characteristics of widely used land\ncover classification models, our results can also be used to guide the\ndevelopment of more robust models within the EO domain.\n","authors":["Martin Willbo","Aleksis Pirinen","John Martinsson","Edvin Listo Zec","Olof Mogren","Mikael Nilsson"],"pdf_url":"https://arxiv.org/pdf/2403.04385v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08363v1","updated":"2024-04-12T10:04:03Z","published":"2024-04-12T10:04:03Z","title":"Let It Flow: Simultaneous Optimization of 3D Flow and Object Clustering","summary":"  We study the problem of self-supervised 3D scene flow estimation from real\nlarge-scale raw point cloud sequences, which is crucial to various tasks like\ntrajectory prediction or instance segmentation. In the absence of ground truth\nscene flow labels, contemporary approaches concentrate on deducing optimizing\nflow across sequential pairs of point clouds by incorporating structure based\nregularization on flow and object rigidity. The rigid objects are estimated by\na variety of 3D spatial clustering methods. While state-of-the-art methods\nsuccessfully capture overall scene motion using the Neural Prior structure,\nthey encounter challenges in discerning multi-object motions. We identified the\nstructural constraints and the use of large and strict rigid clusters as the\nmain pitfall of the current approaches and we propose a novel clustering\napproach that allows for combination of overlapping soft clusters as well as\nnon-overlapping rigid clusters representation. Flow is then jointly estimated\nwith progressively growing non-overlapping rigid clusters together with fixed\nsize overlapping soft clusters. We evaluate our method on multiple datasets\nwith LiDAR point clouds, demonstrating the superior performance over the\nself-supervised baselines reaching new state of the art results. Our method\nespecially excels in resolving flow in complicated dynamic scenes with multiple\nindependently moving objects close to each other which includes pedestrians,\ncyclists and other vulnerable road users. Our codes will be publicly available.\n","authors":["Patrik Vacek","David Hurych","Tom√°≈° Svoboda","Karel Zimmermann"],"pdf_url":"https://arxiv.org/pdf/2404.08363v1.pdf","comment":"ECCV submission"},{"id":"http://arxiv.org/abs/2404.08353v1","updated":"2024-04-12T09:44:18Z","published":"2024-04-12T09:44:18Z","title":"TDANet: Target-Directed Attention Network For Object-Goal Visual\n  Navigation With Zero-Shot Ability","summary":"  The generalization of the end-to-end deep reinforcement learning (DRL) for\nobject-goal visual navigation is a long-standing challenge since object classes\nand placements vary in new test environments. Learning domain-independent\nvisual representation is critical for enabling the trained DRL agent with the\nability to generalize to unseen scenes and objects. In this letter, a\ntarget-directed attention network (TDANet) is proposed to learn the end-to-end\nobject-goal visual navigation policy with zero-shot ability. TDANet features a\nnovel target attention (TA) module that learns both the spatial and semantic\nrelationships among objects to help TDANet focus on the most relevant observed\nobjects to the target. With the Siamese architecture (SA) design, TDANet\ndistinguishes the difference between the current and target states and\ngenerates the domain-independent visual representation. To evaluate the\nnavigation performance of TDANet, extensive experiments are conducted in the\nAI2-THOR embodied AI environment. The simulation results demonstrate a strong\ngeneralization ability of TDANet to unseen scenes and target objects, with\nhigher navigation success rate (SR) and success weighted by length (SPL) than\nother state-of-the-art models.\n","authors":["Shiwei Lian","Feitian Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.08353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16588v2","updated":"2024-04-12T09:38:33Z","published":"2023-09-28T16:45:46Z","title":"Vision Transformers Need Registers","summary":"  Transformers have recently emerged as a powerful tool for learning visual\nrepresentations. In this paper, we identify and characterize artifacts in\nfeature maps of both supervised and self-supervised ViT networks. The artifacts\ncorrespond to high-norm tokens appearing during inference primarily in\nlow-informative background areas of images, that are repurposed for internal\ncomputations. We propose a simple yet effective solution based on providing\nadditional tokens to the input sequence of the Vision Transformer to fill that\nrole. We show that this solution fixes that problem entirely for both\nsupervised and self-supervised models, sets a new state of the art for\nself-supervised visual models on dense visual prediction tasks, enables object\ndiscovery methods with larger models, and most importantly leads to smoother\nfeature maps and attention maps for downstream visual processing.\n","authors":["Timoth√©e Darcet","Maxime Oquab","Julien Mairal","Piotr Bojanowski"],"pdf_url":"https://arxiv.org/pdf/2309.16588v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16254v2","updated":"2024-04-12T09:37:37Z","published":"2023-11-27T19:02:17Z","title":"Safe-CLIP: Removing NSFW Concepts from Vision-and-Language Models","summary":"  Large-scale vision-and-language models, such as CLIP, are typically trained\non web-scale data, which can introduce inappropriate content and lead to the\ndevelopment of unsafe and biased behavior. This, in turn, hampers their\napplicability in sensitive and trustworthy contexts and could raise significant\nconcerns in their adoption. Our research introduces a novel approach to\nenhancing the safety of vision-and-language models by diminishing their\nsensitivity to NSFW (not safe for work) inputs. In particular, our methodology\nseeks to sever \"toxic\" linguistic and visual concepts, unlearning the linkage\nbetween unsafe linguistic or visual items and unsafe regions of the embedding\nspace. We show how this can be done by fine-tuning a CLIP model on synthetic\ndata obtained from a large language model trained to convert between safe and\nunsafe sentences, and a text-to-image generator. We conduct extensive\nexperiments on the resulting embedding space for cross-modal retrieval,\ntext-to-image, and image-to-text generation, where we show that our model can\nbe remarkably employed with pre-trained generative models. Our source code and\ntrained models are available at: https://github.com/aimagelab/safe-clip.\n","authors":["Samuele Poppi","Tobia Poppi","Federico Cocchi","Marcella Cornia","Lorenzo Baraldi","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2311.16254v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07236v2","updated":"2024-04-12T09:34:38Z","published":"2024-04-08T08:50:09Z","title":"Lightweight Deep Learning for Resource-Constrained Environments: A\n  Survey","summary":"  Over the past decade, the dominance of deep learning has prevailed across\nvarious domains of artificial intelligence, including natural language\nprocessing, computer vision, and biomedical signal processing. While there have\nbeen remarkable improvements in model accuracy, deploying these models on\nlightweight devices, such as mobile phones and microcontrollers, is constrained\nby limited resources. In this survey, we provide comprehensive design guidance\ntailored for these devices, detailing the meticulous design of lightweight\nmodels, compression methods, and hardware acceleration strategies. The\nprincipal goal of this work is to explore methods and concepts for getting\naround hardware constraints without compromising the model's accuracy.\nAdditionally, we explore two notable paths for lightweight deep learning in the\nfuture: deployment techniques for TinyML and Large Language Models. Although\nthese paths undoubtedly have potential, they also present significant\nchallenges, encouraging research into unexplored areas.\n","authors":["Hou-I Liu","Marco Galindo","Hongxia Xie","Lai-Kuan Wong","Hong-Han Shuai","Yung-Hui Li","Wen-Huang Cheng"],"pdf_url":"https://arxiv.org/pdf/2404.07236v2.pdf","comment":"40 pages"},{"id":"http://arxiv.org/abs/2404.08351v1","updated":"2024-04-12T09:31:55Z","published":"2024-04-12T09:31:55Z","title":"OmniSat: Self-Supervised Modality Fusion for Earth Observation","summary":"  The field of Earth Observations (EO) offers a wealth of data from diverse\nsensors, presenting a great opportunity for advancing self-supervised\nmultimodal learning. However, current multimodal EO datasets and models focus\non a single data type, either mono-date images or time series, which limits\ntheir expressivity. We introduce OmniSat, a novel architecture that exploits\nthe spatial alignment between multiple EO modalities to learn expressive\nmultimodal representations without labels. To demonstrate the advantages of\ncombining modalities of different natures, we augment two existing datasets\nwith new modalities. As demonstrated on three downstream tasks: forestry, land\ncover classification, and crop mapping. OmniSat can learn rich representations\nin an unsupervised manner, leading to improved performance in the semi- and\nfully-supervised settings, even when only one modality is available for\ninference. The code and dataset are available at github.com/gastruc/OmniSat.\n","authors":["Guillaume Astruc","Nicolas Gonthier","Clement Mallet","Loic Landrieu"],"pdf_url":"https://arxiv.org/pdf/2404.08351v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08350v1","updated":"2024-04-12T09:31:11Z","published":"2024-04-12T09:31:11Z","title":"Self-Supervised k-Space Regularization for Motion-Resolved Abdominal MRI\n  Using Neural Implicit k-Space Representation","summary":"  Neural implicit k-space representations have shown promising results for\ndynamic MRI at high temporal resolutions. Yet, their exclusive training in\nk-space limits the application of common image regularization methods to\nimprove the final reconstruction. In this work, we introduce the concept of\nparallel imaging-inspired self-consistency (PISCO), which we incorporate as\nnovel self-supervised k-space regularization enforcing a consistent\nneighborhood relationship. At no additional data cost, the proposed\nregularization significantly improves neural implicit k-space reconstructions\non simulated data. Abdominal in-vivo reconstructions using PISCO result in\nenhanced spatio-temporal image quality compared to state-of-the-art methods.\nCode is available at https://github.com/vjspi/PISCO-NIK.\n","authors":["Veronika Spieker","Hannah Eichhorn","Jonathan K. Stelter","Wenqi Huang","Rickmer F. Braren","Daniel R√ºckert","Francisco Sahli Costabal","Kerstin Hammernik","Claudia Prieto","Dimitrios C. Karampinos","Julia A. Schnabel"],"pdf_url":"https://arxiv.org/pdf/2404.08350v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2404.08347v1","updated":"2024-04-12T09:22:24Z","published":"2024-04-12T09:22:24Z","title":"Learning to Rebalance Multi-Modal Optimization by Adaptively Masking\n  Subnetworks","summary":"  Multi-modal learning aims to enhance performance by unifying models from\nvarious modalities but often faces the \"modality imbalance\" problem in real\ndata, leading to a bias towards dominant modalities and neglecting others,\nthereby limiting its overall effectiveness. To address this challenge, the core\nidea is to balance the optimization of each modality to achieve a joint\noptimum. Existing approaches often employ a modal-level control mechanism for\nadjusting the update of each modal parameter. However, such a global-wise\nupdating mechanism ignores the different importance of each parameter. Inspired\nby subnetwork optimization, we explore a uniform sampling-based optimization\nstrategy and find it more effective than global-wise updating. According to the\nfindings, we further propose a novel importance sampling-based, element-wise\njoint optimization method, called Adaptively Mask Subnetworks Considering Modal\nSignificance(AMSS). Specifically, we incorporate mutual information rates to\ndetermine the modal significance and employ non-uniform adaptive sampling to\nselect foreground subnetworks from each modality for parameter updates, thereby\nrebalancing multi-modal learning. Additionally, we demonstrate the reliability\nof the AMSS strategy through convergence analysis. Building upon theoretical\ninsights, we further enhance the multi-modal mask subnetwork strategy using\nunbiased estimation, referred to as AMSS+. Extensive experiments reveal the\nsuperiority of our approach over comparison methods.\n","authors":["Yang Yang","Hongpeng Pan","Qing-Yuan Jiang","Yi Xu","Jinghui Tang"],"pdf_url":"https://arxiv.org/pdf/2404.08347v1.pdf","comment":"17 pages;6 figures"},{"id":"http://arxiv.org/abs/2308.09372v2","updated":"2024-04-12T09:21:33Z","published":"2023-08-18T08:06:49Z","title":"Which Transformer to Favor: A Comparative Analysis of Efficiency in\n  Vision Transformers","summary":"  Transformers come with a high computational cost, yet their effectiveness in\naddressing problems in language and vision has sparked extensive research aimed\nat enhancing their efficiency. However, diverse experimental conditions,\nspanning multiple input domains, prevent a fair comparison based solely on\nreported results, posing challenges for model selection. To address this gap in\ncomparability, we design a comprehensive benchmark of more than 30 models for\nimage classification, evaluating key efficiency aspects, including accuracy,\nspeed, and memory usage. This benchmark provides a standardized baseline across\nthe landscape of efficiency-oriented transformers and our framework of\nanalysis, based on Pareto optimality, reveals surprising insights. Despite\nclaims of other models being more efficient, ViT remains Pareto optimal across\nmultiple metrics. We observe that hybrid attention-CNN models exhibit\nremarkable inference memory- and parameter-efficiency. Moreover, our benchmark\nshows that using a larger model in general is more efficient than using higher\nresolution images. Thanks to our holistic evaluation, we provide a centralized\nresource for practitioners and researchers, facilitating informed decisions\nwhen selecting transformers or measuring progress of the development of\nefficient transformers.\n","authors":["Tobias Christian Nauen","Sebastian Palacio","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2308.09372v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08341v1","updated":"2024-04-12T09:13:37Z","published":"2024-04-12T09:13:37Z","title":"Counterfactual Explanations for Face Forgery Detection via Adversarial\n  Removal of Artifacts","summary":"  Highly realistic AI generated face forgeries known as deepfakes have raised\nserious social concerns. Although DNN-based face forgery detection models have\nachieved good performance, they are vulnerable to latest generative methods\nthat have less forgery traces and adversarial attacks. This limitation of\ngeneralization and robustness hinders the credibility of detection results and\nrequires more explanations. In this work, we provide counterfactual\nexplanations for face forgery detection from an artifact removal perspective.\nSpecifically, we first invert the forgery images into the StyleGAN latent\nspace, and then adversarially optimize their latent representations with the\ndiscrimination supervision from the target detection model. We verify the\neffectiveness of the proposed explanations from two aspects: (1) Counterfactual\nTrace Visualization: the enhanced forgery images are useful to reveal artifacts\nby visually contrasting the original images and two different visualization\nmethods; (2) Transferable Adversarial Attacks: the adversarial forgery images\ngenerated by attacking the detection model are able to mislead other detection\nmodels, implying the removed artifacts are general. Extensive experiments\ndemonstrate that our method achieves over 90% attack success rate and superior\nattack transferability. Compared with naive adversarial noise methods, our\nmethod adopts both generative and discriminative model priors, and optimize the\nlatent representations in a synthesis-by-analysis way, which forces the search\nof counterfactual explanations on the natural face manifold. Thus, more general\ncounterfactual traces can be found and better adversarial attack\ntransferability can be achieved.\n","authors":["Yang Li","Songlin Yang","Wei Wang","Ziwen He","Bo Peng","Jing Dong"],"pdf_url":"https://arxiv.org/pdf/2404.08341v1.pdf","comment":"Accepted to ICME2024"},{"id":"http://arxiv.org/abs/2404.07762v2","updated":"2024-04-12T09:13:29Z","published":"2024-04-11T14:03:16Z","title":"NeuroNCAP: Photorealistic Closed-loop Safety Testing for Autonomous\n  Driving","summary":"  We present a versatile NeRF-based simulator for testing autonomous driving\n(AD) software systems, designed with a focus on sensor-realistic closed-loop\nevaluation and the creation of safety-critical scenarios. The simulator learns\nfrom sequences of real-world driving sensor data and enables reconfigurations\nand renderings of new, unseen scenarios. In this work, we use our simulator to\ntest the responses of AD models to safety-critical scenarios inspired by the\nEuropean New Car Assessment Programme (Euro NCAP). Our evaluation reveals that,\nwhile state-of-the-art end-to-end planners excel in nominal driving scenarios\nin an open-loop setting, they exhibit critical flaws when navigating our\nsafety-critical scenarios in a closed-loop setting. This highlights the need\nfor advancements in the safety and real-world usability of end-to-end planners.\nBy publicly releasing our simulator and scenarios as an easy-to-run evaluation\nsuite, we invite the research community to explore, refine, and validate their\nAD models in controlled, yet highly configurable and challenging\nsensor-realistic environments. Code and instructions can be found at\nhttps://github.com/wljungbergh/NeuroNCAP\n","authors":["William Ljungbergh","Adam Tonderski","Joakim Johnander","Holger Caesar","Kalle √Östr√∂m","Michael Felsberg","Christoffer Petersson"],"pdf_url":"https://arxiv.org/pdf/2404.07762v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16794v2","updated":"2024-04-12T09:04:05Z","published":"2023-12-28T02:54:34Z","title":"ZONE: Zero-Shot Instruction-Guided Local Editing","summary":"  Recent advances in vision-language models like Stable Diffusion have shown\nremarkable power in creative image synthesis and editing.However, most existing\ntext-to-image editing methods encounter two obstacles: First, the text prompt\nneeds to be carefully crafted to achieve good results, which is not intuitive\nor user-friendly. Second, they are insensitive to local edits and can\nirreversibly affect non-edited regions, leaving obvious editing traces. To\ntackle these problems, we propose a Zero-shot instructiON-guided local image\nEditing approach, termed ZONE. We first convert the editing intent from the\nuser-provided instruction (e.g., \"make his tie blue\") into specific image\nediting regions through InstructPix2Pix. We then propose a Region-IoU scheme\nfor precise image layer extraction from an off-the-shelf segment model. We\nfurther develop an edge smoother based on FFT for seamless blending between the\nlayer and the image.Our method allows for arbitrary manipulation of a specific\nregion with a single instruction while preserving the rest. Extensive\nexperiments demonstrate that our ZONE achieves remarkable local editing results\nand user-friendliness, outperforming state-of-the-art methods. Code is\navailable at https://github.com/lsl001006/ZONE.\n","authors":["Shanglin Li","Bohan Zeng","Yutang Feng","Sicheng Gao","Xuhui Liu","Jiaming Liu","Li Lin","Xu Tang","Yao Hu","Jianzhuang Liu","Baochang Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.16794v2.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.06567v2","updated":"2024-04-12T08:52:24Z","published":"2024-03-11T10:06:45Z","title":"Leveraging Foundation Models for Content-Based Medical Image Retrieval\n  in Radiology","summary":"  Content-based image retrieval (CBIR) has the potential to significantly\nimprove diagnostic aid and medical research in radiology. Current CBIR systems\nface limitations due to their specialization to certain pathologies, limiting\ntheir utility. In response, we propose using vision foundation models as\npowerful and versatile off-the-shelf feature extractors for content-based\nmedical image retrieval. By benchmarking these models on a comprehensive\ndataset of 1.6 million 2D radiological images spanning four modalities and 161\npathologies, we identify weakly-supervised models as superior, achieving a P@1\nof up to 0.594. This performance not only competes with a specialized model but\ndoes so without the need for fine-tuning. Our analysis further explores the\nchallenges in retrieving pathological versus anatomical structures, indicating\nthat accurate retrieval of pathological features presents greater difficulty.\nDespite these challenges, our research underscores the vast potential of\nfoundation models for CBIR in radiology, proposing a shift towards versatile,\ngeneral-purpose medical image retrieval systems that do not require specific\ntuning.\n","authors":["Stefan Denner","David Zimmerer","Dimitrios Bounias","Markus Bujotzek","Shuhan Xiao","Lisa Kausch","Philipp Schader","Tobias Penzkofer","Paul F. J√§ger","Klaus Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2403.06567v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08330v1","updated":"2024-04-12T08:46:53Z","published":"2024-04-12T08:46:53Z","title":"Emerging Property of Masked Token for Effective Pre-training","summary":"  Driven by the success of Masked Language Modeling (MLM), the realm of\nself-supervised learning for computer vision has been invigorated by the\ncentral role of Masked Image Modeling (MIM) in driving recent breakthroughs.\nNotwithstanding the achievements of MIM across various downstream tasks, its\noverall efficiency is occasionally hampered by the lengthy duration of the\npre-training phase. This paper presents a perspective that the optimization of\nmasked tokens as a means of addressing the prevailing issue. Initially, we\ndelve into an exploration of the inherent properties that a masked token ought\nto possess. Within the properties, we principally dedicated to articulating and\nemphasizing the `data singularity' attribute inherent in masked tokens. Through\na comprehensive analysis of the heterogeneity between masked tokens and visible\ntokens within pre-trained models, we propose a novel approach termed masked\ntoken optimization (MTO), specifically designed to improve model efficiency\nthrough weight recalibration and the enhancement of the key property of masked\ntokens. The proposed method serves as an adaptable solution that seamlessly\nintegrates into any MIM approach that leverages masked tokens. As a result, MTO\nachieves a considerable improvement in pre-training efficiency, resulting in an\napproximately 50% reduction in pre-training epochs required to attain converged\nperformance of the recent approaches.\n","authors":["Hyesong Choi","Hunsang Lee","Seyoung Joung","Hyejin Park","Jiyeong Kim","Dongbo Min"],"pdf_url":"https://arxiv.org/pdf/2404.08330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01449v2","updated":"2024-04-12T08:40:55Z","published":"2024-03-03T09:07:16Z","title":"DUFOMap: Efficient Dynamic Awareness Mapping","summary":"  The dynamic nature of the real world is one of the main challenges in\nrobotics. The first step in dealing with it is to detect which parts of the\nworld are dynamic. A typical benchmark task is to create a map that contains\nonly the static part of the world to support, for example, localization and\nplanning. Current solutions are often applied in post-processing, where\nparameter tuning allows the user to adjust the setting for a specific dataset.\nIn this paper, we propose DUFOMap, a novel dynamic awareness mapping framework\ndesigned for efficient online processing. Despite having the same parameter\nsettings for all scenarios, it performs better or is on par with\nstate-of-the-art methods. Ray casting is utilized to identify and classify\nfully observed empty regions. Since these regions have been observed empty, it\nfollows that anything inside them at another time must be dynamic. Evaluation\nis carried out in various scenarios, including outdoor environments in KITTI\nand Argoverse 2, open areas on the KTH campus, and with different sensor types.\nDUFOMap outperforms the state of the art in terms of accuracy and computational\nefficiency. The source code, benchmarks, and links to the datasets utilized are\nprovided. See https://kth-rpl.github.io/dufomap for more details.\n","authors":["Daniel Duberg","Qingwen Zhang","MingKai Jia","Patric Jensfelt"],"pdf_url":"https://arxiv.org/pdf/2403.01449v2.pdf","comment":"The first two authors hold equal contribution. 8 pages, 7 figures,\n  project page https://kth-rpl.github.io/dufomap"},{"id":"http://arxiv.org/abs/2404.08327v1","updated":"2024-04-12T08:38:51Z","published":"2024-04-12T08:38:51Z","title":"Salience-Based Adaptive Masking: Revisiting Token Dynamics for Enhanced\n  Pre-training","summary":"  In this paper, we introduce Saliency-Based Adaptive Masking (SBAM), a novel\nand cost-effective approach that significantly enhances the pre-training\nperformance of Masked Image Modeling (MIM) approaches by prioritizing token\nsalience. Our method provides robustness against variations in masking ratios,\neffectively mitigating the performance instability issues common in existing\nmethods. This relaxes the sensitivity of MIM-based pre-training to masking\nratios, which in turn allows us to propose an adaptive strategy for `tailored'\nmasking ratios for each data sample, which no existing method can provide.\nToward this goal, we propose an Adaptive Masking Ratio (AMR) strategy that\ndynamically adjusts the proportion of masking for the unique content of each\nimage based on token salience. We show that our method significantly improves\nover the state-of-the-art in mask-based pre-training on the ImageNet-1K\ndataset.\n","authors":["Hyesong Choi","Hyejin Park","Kwang Moo Yi","Sungmin Cha","Dongbo Min"],"pdf_url":"https://arxiv.org/pdf/2404.08327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.14335v2","updated":"2024-04-12T08:37:47Z","published":"2021-09-29T10:41:41Z","title":"A Systematic Survey of Deep Learning-based Single-Image Super-Resolution","summary":"  Single-image super-resolution (SISR) is an important task in image\nprocessing, which aims to enhance the resolution of imaging systems. Recently,\nSISR has made a huge leap and has achieved promising results with the help of\ndeep learning (DL). In this survey, we give an overview of DL-based SISR\nmethods and group them according to their design targets. Specifically, we\nfirst introduce the problem definition, research background, and the\nsignificance of SISR. Secondly, we introduce some related works, including\nbenchmark datasets, upsampling methods, optimization objectives, and image\nquality assessment methods. Thirdly, we provide a detailed investigation of\nSISR and give some domain-specific applications of it. Fourthly, we present the\nreconstruction results of some classic SISR methods to intuitively know their\nperformance. Finally, we discuss some issues that still exist in SISR and\nsummarize some new trends and future directions. This is an exhaustive survey\nof SISR, which can help researchers better understand SISR and inspire more\nexciting research in this field. An investigation project for SISR is provided\nat https://github.com/CV-JunchengLi/SISR-Survey.\n","authors":["Juncheng Li","Zehua Pei","Wenjie Li","Guangwei Gao","Longguang Wang","Yingqian Wang","Tieyong Zeng"],"pdf_url":"https://arxiv.org/pdf/2109.14335v2.pdf","comment":"40 pages, 12 figures"},{"id":"http://arxiv.org/abs/2404.07537v2","updated":"2024-04-12T08:18:44Z","published":"2024-04-11T08:03:23Z","title":"How is Visual Attention Influenced by Text Guidance? Database and Model","summary":"  The analysis and prediction of visual attention have long been crucial tasks\nin the fields of computer vision and image processing. In practical\napplications, images are generally accompanied by various text descriptions,\nhowever, few studies have explored the influence of text descriptions on visual\nattention, let alone developed visual saliency prediction models considering\ntext guidance. In this paper, we conduct a comprehensive study on text-guided\nimage saliency (TIS) from both subjective and objective perspectives.\nSpecifically, we construct a TIS database named SJTU-TIS, which includes 1200\ntext-image pairs and the corresponding collected eye-tracking data. Based on\nthe established SJTU-TIS database, we analyze the influence of various text\ndescriptions on visual attention. Then, to facilitate the development of\nsaliency prediction models considering text influence, we construct a benchmark\nfor the established SJTU-TIS database using state-of-the-art saliency models.\nFinally, considering the effect of text descriptions on visual attention, while\nmost existing saliency models ignore this impact, we further propose a\ntext-guided saliency (TGSal) prediction model, which extracts and integrates\nboth image features and text features to predict the image saliency under\nvarious text-description conditions. Our proposed model significantly\noutperforms the state-of-the-art saliency models on both the SJTU-TIS database\nand the pure image saliency databases in terms of various evaluation metrics.\nThe SJTU-TIS database and the code of the proposed TGSal model will be released\nat: https://github.com/IntMeGroup/TGSal.\n","authors":["Yinan Sun","Xiongkuo Min","Huiyu Duan","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2404.07537v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08312v1","updated":"2024-04-12T08:14:17Z","published":"2024-04-12T08:14:17Z","title":"GPN: Generative Point-based NeRF","summary":"  Scanning real-life scenes with modern registration devices typically gives\nincomplete point cloud representations, primarily due to the limitations of\npartial scanning, 3D occlusions, and dynamic light conditions. Recent works on\nprocessing incomplete point clouds have always focused on point cloud\ncompletion. However, these approaches do not ensure consistency between the\ncompleted point cloud and the captured images regarding color and geometry. We\npropose using Generative Point-based NeRF (GPN) to reconstruct and repair a\npartial cloud by fully utilizing the scanning images and the corresponding\nreconstructed cloud. The repaired point cloud can achieve multi-view\nconsistency with the captured images at high spatial resolution. For the\nfinetunes of a single scene, we optimize the global latent condition by\nincorporating an Auto-Decoder architecture while retaining multi-view\nconsistency. As a result, the generated point clouds are smooth, plausible, and\ngeometrically consistent with the partial scanning images. Extensive\nexperiments on ShapeNet demonstrate that our works achieve competitive\nperformances to the other state-of-the-art point cloud-based neural scene\nrendering and editing performances.\n","authors":["Haipeng Wang"],"pdf_url":"https://arxiv.org/pdf/2404.08312v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08801v4","updated":"2024-04-12T07:48:45Z","published":"2024-02-05T12:33:37Z","title":"CoBra: Complementary Branch Fusing Class and Semantic Knowledge for\n  Robust Weakly Supervised Semantic Segmentation","summary":"  Leveraging semantically precise pseudo masks derived from image-level class\nknowledge for segmentation, namely image-level Weakly Supervised Semantic\nSegmentation (WSSS), still remains challenging. While Class Activation Maps\n(CAMs) using CNNs have steadily been contributing to the success of WSSS, the\nresulting activation maps often narrowly focus on class-specific parts (e.g.,\nonly face of human). On the other hand, recent works based on vision\ntransformers (ViT) have shown promising results based on their self-attention\nmechanism to capture the semantic parts but fail in capturing complete\nclass-specific details (e.g., entire body parts of human but also with a dog\nnearby). In this work, we propose Complementary Branch (CoBra), a novel dual\nbranch framework consisting of two distinct architectures which provide\nvaluable complementary knowledge of class (from CNN) and semantic (from ViT) to\neach branch. In particular, we learn Class-Aware Projection (CAP) for the CNN\nbranch and Semantic-Aware Projection (SAP) for the ViT branch to explicitly\nfuse their complementary knowledge and facilitate a new type of extra\npatch-level supervision. Our model, through CoBra, fuses CNN and ViT's\ncomplementary outputs to create robust pseudo masks that integrate both class\nand semantic information effectively. Extensive experiments qualitatively and\nquantitatively investigate how CNN and ViT complement each other on the PASCAL\nVOC 2012 dataset, showing a state-of-the-art WSSS result. This includes not\nonly the masks generated by our model, but also the segmentation results\nderived from utilizing these masks as pseudo labels.\n","authors":["Woojung Han","Seil Kang","Kyobin Choo","Seong Jae Hwang"],"pdf_url":"https://arxiv.org/pdf/2403.08801v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.17759v4","updated":"2024-04-12T07:44:25Z","published":"2024-01-31T11:36:12Z","title":"Rapid post-disaster infrastructure damage characterisation enabled by\n  remote sensing and deep learning technologies -- a tiered approach","summary":"  Critical infrastructure, such as transport networks and bridges, are\nsystematically targeted during wars and suffer damage during extensive natural\ndisasters because it is vital for enabling connectivity and transportation of\npeople and goods, and hence, underpins national and international economic\ngrowth. Mass destruction of transport assets, in conjunction with minimal or no\naccessibility in the wake of natural and anthropogenic disasters, prevents us\nfrom delivering rapid recovery and adaptation. As a result, systemic\noperability is drastically reduced, leading to low levels of resilience. Thus,\nthere is a need for rapid assessment of its condition to allow for informed\ndecision-making for restoration prioritisation. A solution to this challenge is\nto use technology that enables stand-off observations. Nevertheless, no methods\nexist for automated characterisation of damage at multiple scales, i.e.\nregional (e.g., network), asset (e.g., bridges), and structural (e.g., road\npavement) scales. We propose a methodology based on an integrated, multi-scale\ntiered approach to fill this capability gap. In doing so, we demonstrate how\nautomated damage characterisation can be enabled by fit-for-purpose digital\ntechnologies. Next, the methodology is applied and validated to a case study in\nUkraine that includes 17 bridges, damaged by human targeted interventions. From\nregional to component scale, we deploy technology to integrate assessments\nusing Sentinel-1 SAR images, crowdsourced information, and high-resolution\nimages for deep learning to facilitate automatic damage detection and\ncharacterisation. For the first time, the interferometric coherence difference\nand semantic segmentation of images were deployed in a tiered multi-scale\napproach to improve the reliability of damage characterisations at different\nscales.\n","authors":["Nadiia Kopiika","Andreas Karavias","Pavlos Krassakis","Zehao Ye","Jelena Ninic","Nataliya Shakhovska","Nikolaos Koukouzas","Sotirios Argyroudis","Stergios-Aristoteles Mitoulis"],"pdf_url":"https://arxiv.org/pdf/2401.17759v4.pdf","comment":"43 pages; 20 figures"},{"id":"http://arxiv.org/abs/2310.12877v4","updated":"2024-04-12T07:43:35Z","published":"2023-10-19T16:32:18Z","title":"Perceptual Assessment and Optimization of High Dynamic Range Image\n  Rendering","summary":"  High dynamic range (HDR) rendering has the ability to faithfully reproduce\nthe wide luminance ranges in natural scenes, but how to accurately assess the\nrendering quality is relatively underexplored. Existing quality models are\nmostly designed for low dynamic range (LDR) images, and do not align well with\nhuman perception of HDR image quality. To fill this gap, we propose a family of\nHDR quality metrics, in which the key step is employing a simple inverse\ndisplay model to decompose an HDR image into a stack of LDR images with varying\nexposures. Subsequently, these decomposed images are assessed through\nwell-established LDR quality metrics. Our HDR quality models present three\ndistinct benefits. First, they directly inherit the recent advancements of LDR\nquality metrics. Second, they do not rely on human perceptual data of HDR image\nquality for re-calibration. Third, they facilitate the alignment and\nprioritization of specific luminance ranges for more accurate and detailed\nquality assessment. Experimental results show that our HDR quality metrics\nconsistently outperform existing models in terms of quality assessment on four\nHDR image quality datasets and perceptual optimization of HDR novel view\nsynthesis.\n","authors":["Peibei Cao","Rafal K. Mantiuk","Kede Ma"],"pdf_url":"https://arxiv.org/pdf/2310.12877v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08298v1","updated":"2024-04-12T07:41:17Z","published":"2024-04-12T07:41:17Z","title":"Interference Motion Removal for Doppler Radar Vital Sign Detection Using\n  Variational Encoder-Decoder Neural Network","summary":"  The treatment of interfering motion contributions remains one of the key\nchallenges in the domain of radar-based vital sign monitoring. Removal of the\ninterference to extract the vital sign contributions is demanding due to\noverlapping Doppler bands, the complex structure of the interference motions\nand significant variations in the power levels of their contributions. A novel\napproach to the removal of interference through the use of a probabilistic deep\nlearning model is presented. Results show that a convolutional encoder-decoder\nneural network with a variational objective is capable of learning a meaningful\nrepresentation space of vital sign Doppler-time distribution facilitating their\nextraction from a mixture signal. The approach is tested on semi-experimental\ndata containing real vital sign signatures and simulated returns from\ninterfering body motions. The application of the proposed network enhances the\nextraction of the micro-Doppler frequency corresponding to the respiration rate\nis demonstrated.\n","authors":["Mikolaj Czerkawski","Christos Ilioudis","Carmine Clemente","Craig Michie","Ivan Andonovic","Christos Tachtatzis"],"pdf_url":"https://arxiv.org/pdf/2404.08298v1.pdf","comment":"Presented at 2021 IEEE Radar Conference (RadarConf21)"},{"id":"http://arxiv.org/abs/2404.08293v1","updated":"2024-04-12T07:30:52Z","published":"2024-04-12T07:30:52Z","title":"Overcoming Scene Context Constraints for Object Detection in wild using\n  Defilters","summary":"  This paper focuses on improving object detection performance by addressing\nthe issue of image distortions, commonly encountered in uncontrolled\nacquisition environments. High-level computer vision tasks such as object\ndetection, recognition, and segmentation are particularly sensitive to image\ndistortion. To address this issue, we propose a novel approach employing an\nimage defilter to rectify image distortion prior to object detection. This\nmethod enhances object detection accuracy, as models perform optimally when\ntrained on non-distorted images. Our experiments demonstrate that utilizing\ndefiltered images significantly improves mean average precision compared to\ntraining object detection models on distorted images. Consequently, our\nproposed method offers considerable benefits for real-world applications\nplagued by image distortion. To our knowledge, the contribution lies in\nemploying distortion-removal paradigm for object detection on images captured\nin natural settings. We achieved an improvement of 0.562 and 0.564 of mean\nAverage precision on validation and test data.\n","authors":["Vamshi Krishna Kancharla","Neelam sinha"],"pdf_url":"https://arxiv.org/pdf/2404.08293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08292v1","updated":"2024-04-12T07:30:24Z","published":"2024-04-12T07:30:24Z","title":"AdaContour: Adaptive Contour Descriptor with Hierarchical Representation","summary":"  Existing angle-based contour descriptors suffer from lossy representation for\nnon-starconvex shapes. By and large, this is the result of the shape being\nregistered with a single global inner center and a set of radii corresponding\nto a polar coordinate parameterization. In this paper, we propose AdaContour,\nan adaptive contour descriptor that uses multiple local representations to\ndesirably characterize complex shapes. After hierarchically encoding object\nshapes in a training set and constructing a contour matrix of all subdivided\nregions, we compute a robust low-rank robust subspace and approximate each\nlocal contour by linearly combining the shared basis vectors to represent an\nobject. Experiments show that AdaContour is able to represent shapes more\naccurately and robustly than other descriptors while retaining effectiveness.\nWe validate AdaContour by integrating it into off-the-shelf detectors to enable\ninstance segmentation which demonstrates faithful performance. The code is\navailable at https://github.com/tding1/AdaContour.\n","authors":["Tianyu Ding","Jinxin Zhou","Tianyi Chen","Zhihui Zhu","Ilya Zharkov","Luming Liang"],"pdf_url":"https://arxiv.org/pdf/2404.08292v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08291v1","updated":"2024-04-12T07:30:08Z","published":"2024-04-12T07:30:08Z","title":"On Input Formats for Radar Micro-Doppler Signature Processing by\n  Convolutional Neural Networks","summary":"  Convolutional neural networks have often been proposed for processing radar\nMicro-Doppler signatures, most commonly with the goal of classifying the\nsignals. The majority of works tend to disregard phase information from the\ncomplex time-frequency representation. Here, the utility of the phase\ninformation, as well as the optimal format of the Doppler-time input for a\nconvolutional neural network, is analysed. It is found that the performance\nachieved by convolutional neural network classifiers is heavily influenced by\nthe type of input representation, even across formats with equivalent\ninformation. Furthermore, it is demonstrated that the phase component of the\nDoppler-time representation contains rich information useful for classification\nand that unwrapping the phase in the temporal dimension can improve the results\ncompared to a magnitude-only solution, improving accuracy from 0.920 to 0.938\non the tested human activity dataset. Further improvement of 0.947 is achieved\nby training a linear classifier on embeddings from multiple-formats.\n","authors":["Mikolaj Czerkawski","Carmine Clemente","Craig Michie","Christos Tachtatzis"],"pdf_url":"https://arxiv.org/pdf/2404.08291v1.pdf","comment":"Presented at International Conference on Radar Systems (RADAR 2022)"},{"id":"http://arxiv.org/abs/2404.08285v1","updated":"2024-04-12T07:19:16Z","published":"2024-04-12T07:19:16Z","title":"A Survey of Neural Network Robustness Assessment in Image Recognition","summary":"  In recent years, there has been significant attention given to the robustness\nassessment of neural networks. Robustness plays a critical role in ensuring\nreliable operation of artificial intelligence (AI) systems in complex and\nuncertain environments. Deep learning's robustness problem is particularly\nsignificant, highlighted by the discovery of adversarial attacks on image\nclassification models. Researchers have dedicated efforts to evaluate\nrobustness in diverse perturbation conditions for image recognition tasks.\nRobustness assessment encompasses two main techniques: robustness verification/\ncertification for deliberate adversarial attacks and robustness testing for\nrandom data corruptions. In this survey, we present a detailed examination of\nboth adversarial robustness (AR) and corruption robustness (CR) in neural\nnetwork assessment. Analyzing current research papers and standards, we provide\nan extensive overview of robustness assessment in image recognition. Three\nessential aspects are analyzed: concepts, metrics, and assessment methods. We\ninvestigate the perturbation metrics and range representations used to measure\nthe degree of perturbations on images, as well as the robustness metrics\nspecifically for the robustness conditions of classification models. The\nstrengths and limitations of the existing methods are also discussed, and some\npotential directions for future research are provided.\n","authors":["Jie Wang","Jun Ai","Minyan Lu","Haoran Su","Dan Yu","Yutao Zhang","Junda Zhu","Jingyu Liu"],"pdf_url":"https://arxiv.org/pdf/2404.08285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08281v1","updated":"2024-04-12T07:13:32Z","published":"2024-04-12T07:13:32Z","title":"Calibration & Reconstruction: Deep Integrated Language for Referring\n  Image Segmentation","summary":"  Referring image segmentation aims to segment an object referred to by natural\nlanguage expression from an image. The primary challenge lies in the efficient\npropagation of fine-grained semantic information from textual features to\nvisual features. Many recent works utilize a Transformer to address this\nchallenge. However, conventional transformer decoders can distort linguistic\ninformation with deeper layers, leading to suboptimal results. In this paper,\nwe introduce CRFormer, a model that iteratively calibrates multi-modal features\nin the transformer decoder. We start by generating language queries using\nvision features, emphasizing different aspects of the input language. Then, we\npropose a novel Calibration Decoder (CDec) wherein the multi-modal features can\niteratively calibrated by the input language features. In the Calibration\nDecoder, we use the output of each decoder layer and the original language\nfeatures to generate new queries for continuous calibration, which gradually\nupdates the language features. Based on CDec, we introduce a Language\nReconstruction Module and a reconstruction loss. This module leverages queries\nfrom the final layer of the decoder to reconstruct the input language and\ncompute the reconstruction loss. This can further prevent the language\ninformation from being lost or distorted. Our experiments consistently show the\nsuperior performance of our approach across RefCOCO, RefCOCO+, and G-Ref\ndatasets compared to state-of-the-art methods.\n","authors":["Yichen Yan","Xingjian He","Sihan Chen","Jing Liu"],"pdf_url":"https://arxiv.org/pdf/2404.08281v1.pdf","comment":"9 pages, 8 figures ICMR2024. arXiv admin note: text overlap with\n  arXiv:2305.14969"},{"id":"http://arxiv.org/abs/2404.08279v1","updated":"2024-04-12T07:08:05Z","published":"2024-04-12T07:08:05Z","title":"Convolutional neural network classification of cancer cytopathology\n  images: taking breast cancer as an example","summary":"  Breast cancer is a relatively common cancer among gynecological cancers. Its\ndiagnosis often relies on the pathology of cells in the lesion. The\npathological diagnosis of breast cancer not only requires professionals and\ntime, but also sometimes involves subjective judgment. To address the\nchallenges of dependence on pathologists expertise and the time-consuming\nnature of achieving accurate breast pathological image classification, this\npaper introduces an approach utilizing convolutional neural networks (CNNs) for\nthe rapid categorization of pathological images, aiming to enhance the\nefficiency of breast pathological image detection. And the approach enables the\nrapid and automatic classification of pathological images into benign and\nmalignant groups. The methodology involves utilizing a convolutional neural\nnetwork (CNN) model leveraging the Inceptionv3 architecture and transfer\nlearning algorithm for extracting features from pathological images. Utilizing\na neural network with fully connected layers and employing the SoftMax function\nfor image classification. Additionally, the concept of image partitioning is\nintroduced to handle high-resolution images. To achieve the ultimate\nclassification outcome, the classification probabilities of each image block\nare aggregated using three algorithms: summation, product, and maximum.\nExperimental validation was conducted on the BreaKHis public dataset, resulting\nin accuracy rates surpassing 0.92 across all four magnification coefficients\n(40X, 100X, 200X, and 400X). It demonstrates that the proposed method\neffectively enhances the accuracy in classifying pathological images of breast\ncancer.\n","authors":["MingXuan Xiao","Yufeng Li","Xu Yan","Min Gao","Weimin Wang"],"pdf_url":"https://arxiv.org/pdf/2404.08279v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02492v3","updated":"2024-04-12T07:06:52Z","published":"2023-10-03T23:44:35Z","title":"FairVision: Equitable Deep Learning for Eye Disease Screening via Fair\n  Identity Scaling","summary":"  Equity in AI for healthcare is crucial due to its direct impact on human\nwell-being. Despite advancements in 2D medical imaging fairness, the fairness\nof 3D models remains underexplored, hindered by the small sizes of 3D fairness\ndatasets. Since 3D imaging surpasses 2D imaging in SOTA clinical care, it is\ncritical to understand the fairness of these 3D models. To address this\nresearch gap, we conduct the first comprehensive study on the fairness of 3D\nmedical imaging models across multiple protected attributes. Our investigation\nspans both 2D and 3D models and evaluates fairness across five architectures on\nthree common eye diseases, revealing significant biases across race, gender,\nand ethnicity. To alleviate these biases, we propose a novel fair identity\nscaling (FIS) method that improves both overall performance and fairness,\noutperforming various SOTA fairness methods. Moreover, we release\nHarvard-FairVision, the first large-scale medical fairness dataset with 30,000\nsubjects featuring both 2D and 3D imaging data and six demographic identity\nattributes. Harvard-FairVision provides labels for three major eye disorders\naffecting about 380 million people worldwide, serving as a valuable resource\nfor both 2D and 3D fairness learning. Our code and dataset are publicly\naccessible at\n\\url{https://ophai.hms.harvard.edu/datasets/harvard-fairvision30k}.\n","authors":["Yan Luo","Muhammad Osama Khan","Yu Tian","Min Shi","Zehao Dou","Tobias Elze","Yi Fang","Mengyu Wang"],"pdf_url":"https://arxiv.org/pdf/2310.02492v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08277v1","updated":"2024-04-12T07:04:56Z","published":"2024-04-12T07:04:56Z","title":"FaceFilterSense: A Filter-Resistant Face Recognition and Facial\n  Attribute Analysis Framework","summary":"  With the advent of social media, fun selfie filters have come into tremendous\nmainstream use affecting the functioning of facial biometric systems as well as\nimage recognition systems. These filters vary from beautification filters and\nAugmented Reality (AR)-based filters to filters that modify facial landmarks.\nHence, there is a need to assess the impact of such filters on the performance\nof existing face recognition systems. The limitation associated with existing\nsolutions is that these solutions focus more on the beautification filters.\nHowever, the current AR-based filters and filters which distort facial key\npoints are in vogue recently and make the faces highly unrecognizable even to\nthe naked eye. Also, the filters considered are mostly obsolete with limited\nvariations. To mitigate these limitations, we aim to perform a holistic impact\nanalysis of the latest filters and propose an user recognition model with the\nfiltered images. We have utilized a benchmark dataset for baseline images, and\napplied the latest filters over them to generate a beautified/filtered dataset.\nNext, we have introduced a model FaceFilterNet for beautified user recognition.\nIn this framework, we also utilize our model to comment on various attributes\nof the person including age, gender, and ethnicity. In addition, we have also\npresented a filter-wise impact analysis on face recognition, age estimation,\ngender, and ethnicity prediction. The proposed method affirms the efficacy of\nour dataset with an accuracy of 87.25% and an optimal accuracy for facial\nattribute analysis.\n","authors":["Shubham Tiwari","Yash Sethia","Ritesh Kumar","Ashwani Tanwar","Rudresh Dwivedi"],"pdf_url":"https://arxiv.org/pdf/2404.08277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08273v1","updated":"2024-04-12T06:52:40Z","published":"2024-04-12T06:52:40Z","title":"Struggle with Adversarial Defense? Try Diffusion","summary":"  Adversarial attacks induce misclassification by introducing subtle\nperturbations. Recently, diffusion models are applied to the image classifiers\nto improve adversarial robustness through adversarial training or by purifying\nadversarial noise. However, diffusion-based adversarial training often\nencounters convergence challenges and high computational expenses.\nAdditionally, diffusion-based purification inevitably causes data shift and is\ndeemed susceptible to stronger adaptive attacks. To tackle these issues, we\npropose the Truth Maximization Diffusion Classifier (TMDC), a generative\nBayesian classifier that builds upon pre-trained diffusion models and the\nBayesian theorem. Unlike data-driven classifiers, TMDC, guided by Bayesian\nprinciples, utilizes the conditional likelihood from diffusion models to\ndetermine the class probabilities of input images, thereby insulating against\nthe influences of data shift and the limitations of adversarial training.\nMoreover, to enhance TMDC's resilience against more potent adversarial attacks,\nwe propose an optimization strategy for diffusion classifiers. This strategy\ninvolves post-training the diffusion model on perturbed datasets with\nground-truth labels as conditions, guiding the diffusion model to learn the\ndata distribution and maximizing the likelihood under the ground-truth labels.\nThe proposed method achieves state-of-the-art performance on the CIFAR10\ndataset against heavy white-box attacks and strong adaptive attacks.\nSpecifically, TMDC achieves robust accuracies of 82.81% against $l_{\\infty}$\nnorm-bounded perturbations and 86.05% against $l_{2}$ norm-bounded\nperturbations, respectively, with $\\epsilon=0.05$.\n","authors":["Yujie Li","Yanbin Wang","Haitao xu","Bin Liu","Jianguo Sun","Zhenhao Guo","Wenrui Ma"],"pdf_url":"https://arxiv.org/pdf/2404.08273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.05516v2","updated":"2024-04-12T06:51:06Z","published":"2022-06-11T12:39:37Z","title":"Deep Learning-Based MR Image Re-parameterization","summary":"  Magnetic resonance (MR) image re-parameterization refers to the process of\ngenerating via simulations of an MR image with a new set of MRI scanning\nparameters. Different parameter values generate distinct contrast between\ndifferent tissues, helping identify pathologic tissue. Typically, more than one\nscan is required for diagnosis; however, acquiring repeated scans can be\ncostly, time-consuming, and difficult for patients. Thus, using MR image\nre-parameterization to predict and estimate the contrast in these imaging scans\ncan be an effective alternative. In this work, we propose a novel deep learning\n(DL) based convolutional model for MRI re-parameterization. Based on our\npreliminary results, DL-based techniques hold the potential to learn the\nnon-linearities that govern the re-parameterization.\n","authors":["Abhijeet Narang","Abhigyan Raj","Mihaela Pop","Mehran Ebrahimi"],"pdf_url":"https://arxiv.org/pdf/2206.05516v2.pdf","comment":"A. Narang, A. Raj, M. Pop and M. Ebrahimi, \"Deep Learning-Based MR\n  Image Re-parameterization,\" 2023 Congress in Computer Science, Computer\n  Engineering, & Applied Computing (CSCE), Las Vegas, NV, USA, 2023, pp.\n  536-541, doi: 10.1109/CSCE60160.2023.00094"},{"id":"http://arxiv.org/abs/2303.03761v2","updated":"2024-04-12T06:42:47Z","published":"2023-03-07T09:56:23Z","title":"Graph Neural Networks in Vision-Language Image Understanding: A Survey","summary":"  2D image understanding is a complex problem within computer vision, but it\nholds the key to providing human-level scene comprehension. It goes further\nthan identifying the objects in an image, and instead, it attempts to\nunderstand the scene. Solutions to this problem form the underpinning of a\nrange of tasks, including image captioning, visual question answering (VQA),\nand image retrieval. Graphs provide a natural way to represent the relational\narrangement between objects in an image, and thus, in recent years graph neural\nnetworks (GNNs) have become a standard component of many 2D image understanding\npipelines, becoming a core architectural component, especially in the VQA group\nof tasks. In this survey, we review this rapidly evolving field and we provide\na taxonomy of graph types used in 2D image understanding approaches, a\ncomprehensive list of the GNN models used in this domain, and a roadmap of\nfuture potential developments. To the best of our knowledge, this is the first\ncomprehensive survey that covers image captioning, visual question answering,\nand image retrieval techniques that focus on using GNNs as the main part of\ntheir architecture.\n","authors":["Henry Senior","Gregory Slabaugh","Shanxin Yuan","Luca Rossi"],"pdf_url":"https://arxiv.org/pdf/2303.03761v2.pdf","comment":"20 pages, 5 figures, 5 tables"},{"id":"http://arxiv.org/abs/2404.08264v1","updated":"2024-04-12T06:23:48Z","published":"2024-04-12T06:23:48Z","title":"Guided Masked Self-Distillation Modeling for Distributed Multimedia\n  Sensor Event Analysis","summary":"  Observations with distributed sensors are essential in analyzing a series of\nhuman and machine activities (referred to as 'events' in this paper) in complex\nand extensive real-world environments. This is because the information obtained\nfrom a single sensor is often missing or fragmented in such an environment;\nobservations from multiple locations and modalities should be integrated to\nanalyze events comprehensively. However, a learning method has yet to be\nestablished to extract joint representations that effectively combine such\ndistributed observations. Therefore, we propose Guided Masked sELf-Distillation\nmodeling (Guided-MELD) for inter-sensor relationship modeling. The basic idea\nof Guided-MELD is to learn to supplement the information from the masked sensor\nwith information from other sensors needed to detect the event. Guided-MELD is\nexpected to enable the system to effectively distill the fragmented or\nredundant target event information obtained by the sensors without being overly\ndependent on any specific sensors. To validate the effectiveness of the\nproposed method in novel tasks of distributed multimedia sensor event analysis,\nwe recorded two new datasets that fit the problem setting: MM-Store and\nMM-Office. These datasets consist of human activities in a convenience store\nand an office, recorded using distributed cameras and microphones. Experimental\nresults on these datasets show that the proposed Guided-MELD improves event\ntagging and detection performance and outperforms conventional inter-sensor\nrelationship modeling methods. Furthermore, the proposed method performed\nrobustly even when sensors were reduced.\n","authors":["Masahiro Yasuda","Noboru Harada","Yasunori Ohishi","Shoichiro Saito","Akira Nakayama","Nobutaka Ono"],"pdf_url":"https://arxiv.org/pdf/2404.08264v1.pdf","comment":"13page, 7figure, under review"},{"id":"http://arxiv.org/abs/2312.16837v3","updated":"2024-04-12T06:23:45Z","published":"2023-12-28T05:46:26Z","title":"DiffusionGAN3D: Boosting Text-guided 3D Generation and Domain Adaptation\n  by Combining 3D GANs and Diffusion Priors","summary":"  Text-guided domain adaptation and generation of 3D-aware portraits find many\napplications in various fields. However, due to the lack of training data and\nthe challenges in handling the high variety of geometry and appearance, the\nexisting methods for these tasks suffer from issues like inflexibility,\ninstability, and low fidelity. In this paper, we propose a novel framework\nDiffusionGAN3D, which boosts text-guided 3D domain adaptation and generation by\ncombining 3D GANs and diffusion priors. Specifically, we integrate the\npre-trained 3D generative models (e.g., EG3D) and text-to-image diffusion\nmodels. The former provides a strong foundation for stable and high-quality\navatar generation from text. And the diffusion models in turn offer powerful\npriors and guide the 3D generator finetuning with informative direction to\nachieve flexible and efficient text-guided domain adaptation. To enhance the\ndiversity in domain adaptation and the generation capability in text-to-avatar,\nwe introduce the relative distance loss and case-specific learnable triplane\nrespectively. Besides, we design a progressive texture refinement module to\nimprove the texture quality for both tasks above. Extensive experiments\ndemonstrate that the proposed framework achieves excellent results in both\ndomain adaptation and text-to-avatar tasks, outperforming existing methods in\nterms of generation quality and efficiency. The project homepage is at\nhttps://younglbw.github.io/DiffusionGAN3D-homepage/.\n","authors":["Biwen Lei","Kai Yu","Mengyang Feng","Miaomiao Cui","Xuansong Xie"],"pdf_url":"https://arxiv.org/pdf/2312.16837v3.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2404.05268v2","updated":"2024-04-12T06:20:49Z","published":"2024-04-08T07:59:04Z","title":"MC$^2$: Multi-concept Guidance for Customized Multi-concept Generation","summary":"  Customized text-to-image generation aims to synthesize instantiations of\nuser-specified concepts and has achieved unprecedented progress in handling\nindividual concept. However, when extending to multiple customized concepts,\nexisting methods exhibit limitations in terms of flexibility and fidelity, only\naccommodating the combination of limited types of models and potentially\nresulting in a mix of characteristics from different concepts. In this paper,\nwe introduce the Multi-concept guidance for Multi-concept customization, termed\nMC$^2$, for improved flexibility and fidelity. MC$^2$ decouples the\nrequirements for model architecture via inference time optimization, allowing\nthe integration of various heterogeneous single-concept customized models. It\nadaptively refines the attention weights between visual and textual tokens,\ndirecting image regions to focus on their associated words while diminishing\nthe impact of irrelevant ones. Extensive experiments demonstrate that MC$^2$\neven surpasses previous methods that require additional training in terms of\nconsistency with input prompt and reference images. Moreover, MC$^2$ can be\nextended to elevate the compositional capabilities of text-to-image generation,\nyielding appealing results. Code will be publicly available at\nhttps://github.com/JIANGJiaXiu/MC-2.\n","authors":["Jiaxiu Jiang","Yabo Zhang","Kailai Feng","Xiaohe Wu","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2404.05268v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08255v1","updated":"2024-04-12T06:09:24Z","published":"2024-04-12T06:09:24Z","title":"Practical Region-level Attack against Segment Anything Models","summary":"  Segment Anything Models (SAM) have made significant advancements in image\nsegmentation, allowing users to segment target portions of an image with a\nsingle click (i.e., user prompt). Given its broad applications, the robustness\nof SAM against adversarial attacks is a critical concern. While recent works\nhave explored adversarial attacks against a pre-defined prompt/click, their\nthreat model is not yet realistic: (1) they often assume the user-click\nposition is known to the attacker (point-based attack), and (2) they often\noperate under a white-box setting with limited transferability. In this paper,\nwe propose a more practical region-level attack where attackers do not need to\nknow the precise user prompt. The attack remains effective as the user clicks\non any point on the target object in the image, hiding the object from SAM.\nAlso, by adapting a spectrum transformation method, we make the attack more\ntransferable under a black-box setting. Both control experiments and testing\nagainst real-world SAM services confirm its effectiveness.\n","authors":["Yifan Shen","Zhengyuan Li","Gang Wang"],"pdf_url":"https://arxiv.org/pdf/2404.08255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08252v1","updated":"2024-04-12T05:43:10Z","published":"2024-04-12T05:43:10Z","title":"MonoPatchNeRF: Improving Neural Radiance Fields with Patch-based\n  Monocular Guidance","summary":"  The latest regularized Neural Radiance Field (NeRF) approaches produce poor\ngeometry and view extrapolation for multiview stereo (MVS) benchmarks such as\nETH3D. In this paper, we aim to create 3D models that provide accurate geometry\nand view synthesis, partially closing the large geometric performance gap\nbetween NeRF and traditional MVS methods. We propose a patch-based approach\nthat effectively leverages monocular surface normal and relative depth\npredictions. The patch-based ray sampling also enables the appearance\nregularization of normalized cross-correlation (NCC) and structural similarity\n(SSIM) between randomly sampled virtual and training views. We further show\nthat \"density restrictions\" based on sparse structure-from-motion points can\nhelp greatly improve geometric accuracy with a slight drop in novel view\nsynthesis metrics. Our experiments show 4x the performance of RegNeRF and 8x\nthat of FreeNeRF on average F1@2cm for ETH3D MVS benchmark, suggesting a\nfruitful research direction to improve the geometric accuracy of NeRF-based\nmodels, and sheds light on a potential future approach to enable NeRF-based\noptimization to eventually outperform traditional MVS.\n","authors":["Yuqun Wu","Jae Yong Lee","Chuhang Zou","Shenlong Wang","Derek Hoiem"],"pdf_url":"https://arxiv.org/pdf/2404.08252v1.pdf","comment":"26 pages, 15 figures"},{"id":"http://arxiv.org/abs/2309.08966v2","updated":"2024-04-12T05:34:02Z","published":"2023-09-16T11:42:41Z","title":"FF-LOGO: Cross-Modality Point Cloud Registration with Feature Filtering\n  and Local to Global Optimization","summary":"  Cross-modality point cloud registration is confronted with significant\nchallenges due to inherent differences in modalities between different sensors.\nWe propose a cross-modality point cloud registration framework FF-LOGO: a\ncross-modality point cloud registration method with feature filtering and\nlocal-global optimization. The cross-modality feature correlation filtering\nmodule extracts geometric transformation-invariant features from cross-modality\npoint clouds and achieves point selection by feature matching. We also\nintroduce a cross-modality optimization process, including a local adaptive key\nregion aggregation module and a global modality consistency fusion optimization\nmodule. Experimental results demonstrate that our two-stage optimization\nsignificantly improves the registration accuracy of the feature association and\nselection module. Our method achieves a substantial increase in recall rate\ncompared to the current state-of-the-art methods on the 3DCSR dataset,\nimproving from 40.59% to 75.74%. Our code will be available at\nhttps://github.com/wangmohan17/FFLOGO.\n","authors":["Nan Ma","Mohan Wang","Yiheng Han","Yong-Jin Liu"],"pdf_url":"https://arxiv.org/pdf/2309.08966v2.pdf","comment":"Accepted by 2024 IEEE International Conference on Robotics and\n  Automation (ICRA),7 pages, 2 figures"},{"id":"http://arxiv.org/abs/2308.15070v3","updated":"2024-04-12T05:26:59Z","published":"2023-08-29T07:11:52Z","title":"DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior","summary":"  We present DiffBIR, a general restoration pipeline that could handle\ndifferent blind image restoration tasks in a unified framework. DiffBIR\ndecouples blind image restoration problem into two stages: 1) degradation\nremoval: removing image-independent content; 2) information regeneration:\ngenerating the lost image content. Each stage is developed independently but\nthey work seamlessly in a cascaded manner. In the first stage, we use\nrestoration modules to remove degradations and obtain high-fidelity restored\nresults. For the second stage, we propose IRControlNet that leverages the\ngenerative ability of latent diffusion models to generate realistic details.\nSpecifically, IRControlNet is trained based on specially produced condition\nimages without distracting noisy content for stable generation performance.\nMoreover, we design a region-adaptive restoration guidance that can modify the\ndenoising process during inference without model re-training, allowing users to\nbalance realness and fidelity through a tunable guidance scale. Extensive\nexperiments have demonstrated DiffBIR's superiority over state-of-the-art\napproaches for blind image super-resolution, blind face restoration and blind\nimage denoising tasks on both synthetic and real-world datasets. The code is\navailable at https://github.com/XPixelGroup/DiffBIR.\n","authors":["Xinqi Lin","Jingwen He","Ziyan Chen","Zhaoyang Lyu","Bo Dai","Fanghua Yu","Wanli Ouyang","Yu Qiao","Chao Dong"],"pdf_url":"https://arxiv.org/pdf/2308.15070v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10356v3","updated":"2024-04-12T05:07:28Z","published":"2023-09-19T06:32:19Z","title":"RoadFormer: Duplex Transformer for RGB-Normal Semantic Road Scene\n  Parsing","summary":"  The recent advancements in deep convolutional neural networks have shown\nsignificant promise in the domain of road scene parsing. Nevertheless, the\nexisting works focus primarily on freespace detection, with little attention\ngiven to hazardous road defects that could compromise both driving safety and\ncomfort. In this paper, we introduce RoadFormer, a novel Transformer-based\ndata-fusion network developed for road scene parsing. RoadFormer utilizes a\nduplex encoder architecture to extract heterogeneous features from both RGB\nimages and surface normal information. The encoded features are subsequently\nfed into a novel heterogeneous feature synergy block for effective feature\nfusion and recalibration. The pixel decoder then learns multi-scale long-range\ndependencies from the fused and recalibrated heterogeneous features, which are\nsubsequently processed by a Transformer decoder to produce the final semantic\nprediction. Additionally, we release SYN-UDTIRI, the first large-scale road\nscene parsing dataset that contains over 10,407 RGB images, dense depth images,\nand the corresponding pixel-level annotations for both freespace and road\ndefects of different shapes and sizes. Extensive experimental evaluations\nconducted on our SYN-UDTIRI dataset, as well as on three public datasets,\nincluding KITTI road, CityScapes, and ORFD, demonstrate that RoadFormer\noutperforms all other state-of-the-art networks for road scene parsing.\nSpecifically, RoadFormer ranks first on the KITTI road benchmark. Our source\ncode, created dataset, and demo video are publicly available at\nmias.group/RoadFormer.\n","authors":["Jiahang Li","Yikang Zhang","Peng Yun","Guangliang Zhou","Qijun Chen","Rui Fan"],"pdf_url":"https://arxiv.org/pdf/2309.10356v3.pdf","comment":"9 pages 7 figures. Accepted by Transactions on Intelligent Vehicles"},{"id":"http://arxiv.org/abs/2403.14047v2","updated":"2024-04-12T05:07:27Z","published":"2024-03-21T00:09:04Z","title":"Accelerating ViT Inference on FPGA through Static and Dynamic Pruning","summary":"  Vision Transformers (ViTs) have achieved state-of-the-art accuracy on various\ncomputer vision tasks. However, their high computational complexity prevents\nthem from being applied to many real-world applications. Weight and token\npruning are two well-known methods for reducing complexity: weight pruning\nreduces the model size and associated computational demands, while token\npruning further dynamically reduces the computation based on the input.\nCombining these two techniques should significantly reduce computation\ncomplexity and model size; however, naively integrating them results in\nirregular computation patterns, leading to significant accuracy drops and\ndifficulties in hardware acceleration.\n  Addressing the above challenges, we propose a comprehensive\nalgorithm-hardware codesign for accelerating ViT on FPGA through simultaneous\npruning -combining static weight pruning and dynamic token pruning. For\nalgorithm design, we systematically combine a hardware-aware structured\nblock-pruning method for pruning model parameters and a dynamic token pruning\nmethod for removing unimportant token vectors. Moreover, we design a novel\ntraining algorithm to recover the model's accuracy. For hardware design, we\ndevelop a novel hardware accelerator for executing the pruned model. The\nproposed hardware design employs multi-level parallelism with load balancing\nstrategy to efficiently deal with the irregular computation pattern led by the\ntwo pruning approaches. Moreover, we develop an efficient hardware mechanism\nfor efficiently executing the on-the-fly token pruning.\n","authors":["Dhruv Parikh","Shouyi Li","Bingyi Zhang","Rajgopal Kannan","Carl Busart","Viktor Prasanna"],"pdf_url":"https://arxiv.org/pdf/2403.14047v2.pdf","comment":"FCCM 2024"},{"id":"http://arxiv.org/abs/2208.07463v4","updated":"2024-04-12T04:48:48Z","published":"2022-08-15T22:51:23Z","title":"Conv-Adapter: Exploring Parameter Efficient Transfer Learning for\n  ConvNets","summary":"  While parameter efficient tuning (PET) methods have shown great potential\nwith transformer architecture on Natural Language Processing (NLP) tasks, their\neffectiveness with large-scale ConvNets is still under-studied on Computer\nVision (CV) tasks. This paper proposes Conv-Adapter, a PET module designed for\nConvNets. Conv-Adapter is light-weight, domain-transferable, and\narchitecture-agnostic with generalized performance on different tasks. When\ntransferring on downstream tasks, Conv-Adapter learns tasks-specific feature\nmodulation to the intermediate representations of backbones while keeping the\npre-trained parameters frozen. By introducing only a tiny amount of learnable\nparameters, e.g., only 3.5% full fine-tuning parameters of ResNet50. It can\nalso be applied for transformer-based backbones. Conv-Adapter outperforms\nprevious PET baseline methods and achieves comparable or surpasses the\nperformance of full fine-tuning on 23 classification tasks of various domains.\nIt also presents superior performance on the few-shot classification with an\naverage margin of 3.39%. Beyond classification, Conv-Adapter can generalize to\ndetection and segmentation tasks with more than 50% reduction of parameters but\ncomparable performance to the traditional full fine-tuning.\n","authors":["Hao Chen","Ran Tao","Han Zhang","Yidong Wang","Xiang Li","Wei Ye","Jindong Wang","Guosheng Hu","Marios Savvides"],"pdf_url":"https://arxiv.org/pdf/2208.07463v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08238v1","updated":"2024-04-12T04:45:51Z","published":"2024-04-12T04:45:51Z","title":"Simulation of a Vision Correction Display System","summary":"  Eyes serve as our primary sensory organs, responsible for processing up to\n80\\% of our sensory input. However, common visual aberrations like myopia and\nhyperopia affect a significant portion of the global population. This paper\nfocuses on simulating a Vision Correction Display (VCD) to enhance the visual\nexperience of individuals with various visual impairments. Utilising Blender,\nwe digitally model the functionality of a VCD in correcting refractive errors\nsuch as myopia and hyperopia. With these simulations we can see potential\nimprovements in visual acuity and comfort. These simulations provide valuable\ninsights for the design and development of future VCD technologies, ultimately\nadvancing accessibility and usability for individuals with visual challenges.\n","authors":["Vidya Sunil","Renu M Rameshan"],"pdf_url":"https://arxiv.org/pdf/2404.08238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08237v1","updated":"2024-04-12T04:44:11Z","published":"2024-04-12T04:44:11Z","title":"IFViT: Interpretable Fixed-Length Representation for Fingerprint\n  Matching via Vision Transformer","summary":"  Determining dense feature points on fingerprints used in constructing deep\nfixed-length representations for accurate matching, particularly at the pixel\nlevel, is of significant interest. To explore the interpretability of\nfingerprint matching, we propose a multi-stage interpretable fingerprint\nmatching network, namely Interpretable Fixed-length Representation for\nFingerprint Matching via Vision Transformer (IFViT), which consists of two\nprimary modules. The first module, an interpretable dense registration module,\nestablishes a Vision Transformer (ViT)-based Siamese Network to capture\nlong-range dependencies and the global context in fingerprint pairs. It\nprovides interpretable dense pixel-wise correspondences of feature points for\nfingerprint alignment and enhances the interpretability in the subsequent\nmatching stage. The second module takes into account both local and global\nrepresentations of the aligned fingerprint pair to achieve an interpretable\nfixed-length representation extraction and matching. It employs the ViTs\ntrained in the first module with the additional fully connected layer and\nretrains them to simultaneously produce the discriminative fixed-length\nrepresentation and interpretable dense pixel-wise correspondences of feature\npoints. Extensive experimental results on diverse publicly available\nfingerprint databases demonstrate that the proposed framework not only exhibits\nsuperior performance on dense registration and matching but also significantly\npromotes the interpretability in deep fixed-length representations-based\nfingerprint matching.\n","authors":["Yuhang Qiu","Honghui Chen","Xingbo Dong","Zheng Lin","Iman Yi Liao","Massimo Tistarelli","Zhe Jin"],"pdf_url":"https://arxiv.org/pdf/2404.08237v1.pdf","comment":"ready to submit to IEEE Transactions on Information Forensics and\n  Security (TIFS)"},{"id":"http://arxiv.org/abs/2302.06874v2","updated":"2024-04-12T04:42:29Z","published":"2023-02-14T07:39:37Z","title":"Robust Representation Learning with Self-Distillation for Domain\n  Generalization","summary":"  Despite the recent success of deep neural networks, there remains a need for\neffective methods to enhance domain generalization using vision transformers.\nIn this paper, we propose a novel domain generalization technique called Robust\nRepresentation Learning with Self-Distillation (RRLD) comprising i)\nintermediate-block self-distillation and ii) augmentation-guided\nself-distillation to improve the generalization capabilities of\ntransformer-based models on unseen domains. This approach enables the network\nto learn robust and general features that are invariant to different\naugmentations and domain shifts while effectively mitigating overfitting to\nsource domains. To evaluate the effectiveness of our proposed method, we\nperform extensive experiments on PACS and OfficeHome benchmark datasets, as\nwell as an industrial wafer semiconductor defect dataset. The results\ndemonstrate that RRLD achieves robust and accurate generalization performance.\nWe observe an average accuracy improvement in the range of 1.2% to 2.3% over\nthe state-of-the-art on the three datasets.\n","authors":["Ankur Singh","Senthilnath Jayavelu"],"pdf_url":"https://arxiv.org/pdf/2302.06874v2.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2404.05960v2","updated":"2024-04-12T04:23:12Z","published":"2024-04-09T02:47:52Z","title":"EasyTrack: Efficient and Compact One-stream 3D Point Clouds Tracker","summary":"  Most of 3D single object trackers (SOT) in point clouds follow the two-stream\nmulti-stage 3D Siamese or motion tracking paradigms, which process the template\nand search area point clouds with two parallel branches, built on supervised\npoint cloud backbones. In this work, beyond typical 3D Siamese or motion\ntracking, we propose a neat and compact one-stream transformer 3D SOT paradigm\nfrom the novel perspective, termed as \\textbf{EasyTrack}, which consists of\nthree special designs: 1) A 3D point clouds tracking feature pre-training\nmodule is developed to exploit the masked autoencoding for learning 3D point\nclouds tracking representations. 2) A unified 3D tracking feature learning and\nfusion network is proposed to simultaneously learns target-aware 3D features,\nand extensively captures mutual correlation through the flexible self-attention\nmechanism. 3) A target location network in the dense bird's eye view (BEV)\nfeature space is constructed for target classification and regression.\nMoreover, we develop an enhanced version named EasyTrack++, which designs the\ncenter points interaction (CPI) strategy to reduce the ambiguous targets caused\nby the noise point cloud background information. The proposed EasyTrack and\nEasyTrack++ set a new state-of-the-art performance ($\\textbf{18\\%}$,\n$\\textbf{40\\%}$ and $\\textbf{3\\%}$ success gains) in KITTI, NuScenes, and Waymo\nwhile runing at \\textbf{52.6fps} with few parameters (\\textbf{1.3M}). The code\nwill be available at https://github.com/KnightApple427/Easytrack.\n","authors":["Baojie Fan","Wuyang Zhou","Kai Wang","Shijun Zhou","Fengyu Xu","Jiandong Tian"],"pdf_url":"https://arxiv.org/pdf/2404.05960v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08229v1","updated":"2024-04-12T04:08:21Z","published":"2024-04-12T04:08:21Z","title":"Enhancing Traffic Safety with Parallel Dense Video Captioning for\n  End-to-End Event Analysis","summary":"  This paper introduces our solution for Track 2 in AI City Challenge 2024. The\ntask aims to solve traffic safety description and analysis with the dataset of\nWoven Traffic Safety (WTS), a real-world Pedestrian-Centric Traffic Video\nDataset for Fine-grained Spatial-Temporal Understanding. Our solution mainly\nfocuses on the following points: 1) To solve dense video captioning, we\nleverage the framework of dense video captioning with parallel decoding (PDVC)\nto model visual-language sequences and generate dense caption by chapters for\nvideo. 2) Our work leverages CLIP to extract visual features to more\nefficiently perform cross-modality training between visual and textual\nrepresentations. 3) We conduct domain-specific model adaptation to mitigate\ndomain shift problem that poses recognition challenge in video understanding.\n4) Moreover, we leverage BDD-5K captioned videos to conduct knowledge transfer\nfor better understanding WTS videos and more accurate captioning. Our solution\nhas yielded on the test set, achieving 6th place in the competition. The open\nsource code will be available at https://github.com/UCF-SST-Lab/AICity2024CVPRW\n","authors":["Maged Shoman","Dongdong Wang","Armstrong Aboah","Mohamed Abdel-Aty"],"pdf_url":"https://arxiv.org/pdf/2404.08229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08226v1","updated":"2024-04-12T03:43:37Z","published":"2024-04-12T03:43:37Z","title":"Improving Continuous Sign Language Recognition with Adapted Image Models","summary":"  The increase of web-scale weakly labelled image-text pairs have greatly\nfacilitated the development of large-scale vision-language models (e.g., CLIP),\nwhich have shown impressive generalization performance over a series of\ndownstream tasks. However, the massive model size and scarcity of available\ndata limit their applications to fine-tune the whole model in downstream tasks.\nBesides, fully fine-tuning the model easily forgets the generic essential\nknowledge acquired in the pretraining stage and overfits the downstream data.\nTo enable high efficiency when adapting these large vision-language models\n(e.g., CLIP) to performing continuous sign language recognition (CSLR) while\npreserving their generalizability, we propose a novel strategy (AdaptSign).\nEspecially, CLIP is adopted as the visual backbone to extract frame-wise\nfeatures whose parameters are fixed, and a set of learnable modules are\nintroduced to model spatial sign variations or capture temporal sign movements.\nThe introduced additional modules are quite lightweight, only owning 3.2% extra\ncomputations with high efficiency. The generic knowledge acquired in the\npretraining stage is well-preserved in the frozen CLIP backbone in this\nprocess. Extensive experiments show that despite being efficient, AdaptSign is\nable to demonstrate superior performance across a series of CSLR benchmarks\nincluding PHOENIX14, PHOENIX14-T, CSL-Daily and CSL compared to existing\nmethods. Visualizations show that AdaptSign could learn to dynamically pay\nmajor attention to the informative spatial regions and cross-frame trajectories\nin sign videos.\n","authors":["Lianyu Hu","Tongkai Shi","Liqing Gao","Zekang Liu","Wei Feng"],"pdf_url":"https://arxiv.org/pdf/2404.08226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04582v2","updated":"2024-04-12T03:33:31Z","published":"2023-10-06T20:48:43Z","title":"Universal Humanoid Motion Representations for Physics-Based Control","summary":"  We present a universal motion representation that encompasses a comprehensive\nrange of motor skills for physics-based humanoid control. Due to the high\ndimensionality of humanoids and the inherent difficulties in reinforcement\nlearning, prior methods have focused on learning skill embeddings for a narrow\nrange of movement styles (e.g. locomotion, game characters) from specialized\nmotion datasets. This limited scope hampers their applicability in complex\ntasks. We close this gap by significantly increasing the coverage of our motion\nrepresentation space. To achieve this, we first learn a motion imitator that\ncan imitate all of human motion from a large, unstructured motion dataset. We\nthen create our motion representation by distilling skills directly from the\nimitator. This is achieved by using an encoder-decoder structure with a\nvariational information bottleneck. Additionally, we jointly learn a prior\nconditioned on proprioception (humanoid's own pose and velocities) to improve\nmodel expressiveness and sampling efficiency for downstream tasks. By sampling\nfrom the prior, we can generate long, stable, and diverse human motions. Using\nthis latent space for hierarchical RL, we show that our policies solve tasks\nusing human-like behavior. We demonstrate the effectiveness of our motion\nrepresentation by solving generative tasks (e.g. strike, terrain traversal) and\nmotion tracking using VR controllers.\n","authors":["Zhengyi Luo","Jinkun Cao","Josh Merel","Alexander Winkler","Jing Huang","Kris Kitani","Weipeng Xu"],"pdf_url":"https://arxiv.org/pdf/2310.04582v2.pdf","comment":"ICLR 2024 Spotlight. Project page:\n  https://zhengyiluo.github.io/PULSE/"},{"id":"http://arxiv.org/abs/2403.12416v2","updated":"2024-04-12T03:15:26Z","published":"2024-03-19T03:59:14Z","title":"Eye-gaze Guided Multi-modal Alignment Framework for Radiology","summary":"  In multi-modal frameworks, the alignment of cross-modal features presents a\nsignificant challenge. The predominant approach in multi-modal pre-training\nemphasizes either global or local alignment between modalities, utilizing\nextensive datasets. This bottom-up driven method often suffers from a lack of\ninterpretability, a critical concern in radiology. Previous studies have\nintegrated high-level labels in medical images or text, but these still rely on\nmanual annotation, a costly and labor-intensive process. Our work introduces a\nnovel approach by using eye-gaze data, collected synchronously by radiologists\nduring diagnostic evaluations. This data, indicating radiologists' focus areas,\nnaturally links chest X-rays to diagnostic texts. We propose the Eye-gaze\nGuided Multi-modal Alignment (EGMA) framework to harness eye-gaze data for\nbetter alignment of image and text features, aiming to reduce reliance on\nmanual annotations and thus cut training costs. Our model demonstrates robust\nperformance, outperforming other state-of-the-art methods in zero-shot\nclassification and retrieval tasks. The incorporation of easily-obtained\neye-gaze data during routine radiological diagnoses signifies a step towards\nminimizing manual annotation dependency. Additionally, we explore the impact of\nvarying amounts of eye-gaze data on model performance, highlighting the\nfeasibility and utility of integrating this auxiliary data into multi-modal\npre-training.\n","authors":["Chong Ma","Hanqi Jiang","Wenting Chen","Zihao Wu","Xiaowei Yu","Fang Zeng","Lei Guo","Dajiang Zhu","Tuo Zhang","Dinggang Shen","Tianming Liu","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2403.12416v2.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.15036v3","updated":"2024-04-12T03:14:34Z","published":"2023-10-23T15:34:03Z","title":"A Technique for Classifying Static Gestures Using UWB Radar","summary":"  Our paper presents a robust framework for UWB-based static gesture\nrecognition, leveraging proprietary UWB radar sensor technology. Extensive data\ncollection efforts were undertaken to compile datasets containing five commonly\nused gestures. Our approach involves a comprehensive data pre-processing\npipeline that encompasses outlier handling, aspect ratio-preserving resizing,\nand false-color image transformation. Both CNN and MobileNet models were\ntrained on the processed images. Remarkably, our best-performing model achieved\nan accuracy of 96.78%. Additionally, we developed a user-friendly GUI framework\nto assess the model's system resource usage and processing times, which\nrevealed low memory utilization and real-time task completion in under one\nsecond. This research marks a significant step towards enhancing static gesture\nrecognition using UWB technology, promising practical applications in various\ndomains.\n","authors":["Abhishek Sebastian","Pragna R"],"pdf_url":"https://arxiv.org/pdf/2310.15036v3.pdf","comment":"This is not a technical research paper, but an excerpt of what was\n  applied during a funded project for the promotion of Open Science"},{"id":"http://arxiv.org/abs/2312.17428v2","updated":"2024-04-12T03:06:07Z","published":"2023-12-29T01:42:20Z","title":"ChangeNet: Multi-Temporal Asymmetric Change Detection Dataset","summary":"  Change Detection (CD) has been attracting extensive interests with the\navailability of bi-temporal datasets. However, due to the huge cost of\nmulti-temporal images acquisition and labeling, existing change detection\ndatasets are small in quantity, short in temporal, and low in practicability.\nTherefore, a large-scale practical-oriented dataset covering wide temporal\nphases is urgently needed to facilitate the community. To this end, the\nChangeNet dataset is presented especially for multi-temporal change detection,\nalong with the new task of \"Asymmetric Change Detection\". Specifically,\nChangeNet consists of 31,000 multi-temporal images pairs, a wide range of\ncomplex scenes from 100 cities, and 6 pixel-level annotated categories, which\nis far superior to all the existing change detection datasets including\nLEVIR-CD, WHU Building CD, etc.. In addition, ChangeNet contains amounts of\nreal-world perspective distortions in different temporal phases on the same\nareas, which is able to promote the practical application of change detection\nalgorithms. The ChangeNet dataset is suitable for both binary change detection\n(BCD) and semantic change detection (SCD) tasks. Accordingly, we benchmark the\nChangeNet dataset on six BCD methods and two SCD methods, and extensive\nexperiments demonstrate its challenges and great significance. The dataset is\navailable at https://github.com/jankyee/ChangeNet.\n","authors":["Deyi Ji","Siqi Gao","Mingyuan Tao","Hongtao Lu","Feng Zhao"],"pdf_url":"https://arxiv.org/pdf/2312.17428v2.pdf","comment":"Accepted to ICASSP 2024 Oral/Lecture"},{"id":"http://arxiv.org/abs/2402.09055v2","updated":"2024-04-12T02:51:45Z","published":"2024-02-14T10:05:19Z","title":"Comment-aided Video-Language Alignment via Contrastive Pre-training for\n  Short-form Video Humor Detection","summary":"  The growing importance of multi-modal humor detection within affective\ncomputing correlates with the expanding influence of short-form video sharing\non social media platforms. In this paper, we propose a novel two-branch\nhierarchical model for short-form video humor detection (SVHD), named\nComment-aided Video-Language Alignment (CVLA) via data-augmented multi-modal\ncontrastive pre-training. Notably, our CVLA not only operates on raw signals\nacross various modal channels but also yields an appropriate multi-modal\nrepresentation by aligning the video and language components within a\nconsistent semantic space. The experimental results on two humor detection\ndatasets, including DY11k and UR-FUNNY, demonstrate that CVLA dramatically\noutperforms state-of-the-art and several competitive baseline approaches. Our\ndataset, code and model release at https://github.com/yliu-cs/CVLA.\n","authors":["Yang Liu","Tongfei Shen","Dong Zhang","Qingying Sun","Shoushan Li","Guodong Zhou"],"pdf_url":"https://arxiv.org/pdf/2402.09055v2.pdf","comment":"Accepted by ICMR 2024"},{"id":"http://arxiv.org/abs/2403.18554v2","updated":"2024-04-12T02:27:09Z","published":"2024-03-27T13:33:14Z","title":"CosalPure: Learning Concept from Group Images for Robust Co-Saliency\n  Detection","summary":"  Co-salient object detection (CoSOD) aims to identify the common and salient\n(usually in the foreground) regions across a given group of images. Although\nachieving significant progress, state-of-the-art CoSODs could be easily\naffected by some adversarial perturbations, leading to substantial accuracy\nreduction. The adversarial perturbations can mislead CoSODs but do not change\nthe high-level semantic information (e.g., concept) of the co-salient objects.\nIn this paper, we propose a novel robustness enhancement framework by first\nlearning the concept of the co-salient objects based on the input group images\nand then leveraging this concept to purify adversarial perturbations, which are\nsubsequently fed to CoSODs for robustness enhancement. Specifically, we propose\nCosalPure containing two modules, i.e., group-image concept learning and\nconcept-guided diffusion purification. For the first module, we adopt a\npre-trained text-to-image diffusion model to learn the concept of co-salient\nobjects within group images where the learned concept is robust to adversarial\nexamples. For the second module, we map the adversarial image to the latent\nspace and then perform diffusion generation by embedding the learned concept\ninto the noise prediction function as an extra condition. Our method can\neffectively alleviate the influence of the SOTA adversarial attack containing\ndifferent adversarial patterns, including exposure and noise. The extensive\nresults demonstrate that our method could enhance the robustness of CoSODs\nsignificantly.\n","authors":["Jiayi Zhu","Qing Guo","Felix Juefei-Xu","Yihao Huang","Yang Liu","Geguang Pu"],"pdf_url":"https://arxiv.org/pdf/2403.18554v2.pdf","comment":"This paper is accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2404.08201v1","updated":"2024-04-12T02:14:35Z","published":"2024-04-12T02:14:35Z","title":"A Mutual Inclusion Mechanism for Precise Boundary Segmentation in\n  Medical Images","summary":"  In medical imaging, accurate image segmentation is crucial for quantifying\ndiseases, assessing prognosis, and evaluating treatment outcomes. However,\nexisting methods lack an in-depth integration of global and local features,\nfailing to pay special attention to abnormal regions and boundary details in\nmedical images. To this end, we present a novel deep learning-based approach,\nMIPC-Net, for precise boundary segmentation in medical images. Our approach,\ninspired by radiologists' working patterns, features two distinct modules: (i)\n\\textbf{Mutual Inclusion of Position and Channel Attention (MIPC) module}: To\nenhance the precision of boundary segmentation in medical images, we introduce\nthe MIPC module, which enhances the focus on channel information when\nextracting position features and vice versa; (ii) \\textbf{GL-MIPC-Residue}: To\nimprove the restoration of medical images, we propose the GL-MIPC-Residue, a\nglobal residual connection that enhances the integration of the encoder and\ndecoder by filtering out invalid information and restoring the most effective\ninformation lost during the feature extraction process. We evaluate the\nperformance of the proposed model using metrics such as Dice coefficient (DSC)\nand Hausdorff Distance (HD) on three publicly accessible datasets: Synapse,\nISIC2018-Task, and Segpc. Our ablation study shows that each module contributes\nto improving the quality of segmentation results. Furthermore, with the\nassistance of both modules, our approach outperforms state-of-the-art methods\nacross all metrics on the benchmark datasets, notably achieving a 2.23mm\nreduction in HD on the Synapse dataset, strongly evidencing our model's\nenhanced capability for precise image boundary segmentation. Codes will be\navailable at https://github.com/SUN-1024/MIPC-Net.\n","authors":["Yizhi Pan","Junyi Xin","Tianhua Yang","Teeradaj Racharak","Le-Minh Nguyen","Guanqun Sun"],"pdf_url":"https://arxiv.org/pdf/2404.08201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08197v1","updated":"2024-04-12T02:04:34Z","published":"2024-04-12T02:04:34Z","title":"Scaling (Down) CLIP: A Comprehensive Analysis of Data, Architecture, and\n  Training Strategies","summary":"  This paper investigates the performance of the Contrastive Language-Image\nPre-training (CLIP) when scaled down to limited computation budgets. We explore\nCLIP along three dimensions: data, architecture, and training strategies. With\nregards to data, we demonstrate the significance of high-quality training data\nand show that a smaller dataset of high-quality data can outperform a larger\ndataset with lower quality. We also examine how model performance varies with\ndifferent dataset sizes, suggesting that smaller ViT models are better suited\nfor smaller datasets, while larger models perform better on larger datasets\nwith fixed compute. Additionally, we provide guidance on when to choose a\nCNN-based architecture or a ViT-based architecture for CLIP training. We\ncompare four CLIP training strategies - SLIP, FLIP, CLIP, and CLIP+Data\nAugmentation - and show that the choice of training strategy depends on the\navailable compute resource. Our analysis reveals that CLIP+Data Augmentation\ncan achieve comparable performance to CLIP using only half of the training\ndata. This work provides practical insights into how to effectively train and\ndeploy CLIP models, making them more accessible and affordable for practical\nuse in various applications.\n","authors":["Zichao Li","Cihang Xie","Ekin Dogus Cubuk"],"pdf_url":"https://arxiv.org/pdf/2404.08197v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08195v1","updated":"2024-04-12T01:54:59Z","published":"2024-04-12T01:54:59Z","title":"Tackling Ambiguity from Perspective of Uncertainty Inference and\n  Affinity Diversification for Weakly Supervised Semantic Segmentation","summary":"  Weakly supervised semantic segmentation (WSSS) with image-level labels\nintends to achieve dense tasks without laborious annotations. However, due to\nthe ambiguous contexts and fuzzy regions, the performance of WSSS, especially\nthe stages of generating Class Activation Maps (CAMs) and refining pseudo\nmasks, widely suffers from ambiguity while being barely noticed by previous\nliterature. In this work, we propose UniA, a unified single-staged WSSS\nframework, to efficiently tackle this issue from the perspective of uncertainty\ninference and affinity diversification, respectively. When activating class\nobjects, we argue that the false activation stems from the bias to the\nambiguous regions during the feature extraction. Therefore, we design a more\nrobust feature representation with a probabilistic Gaussian distribution and\nintroduce the uncertainty estimation to avoid the bias. A distribution loss is\nparticularly proposed to supervise the process, which effectively captures the\nambiguity and models the complex dependencies among features. When refining\npseudo labels, we observe that the affinity from the prevailing refinement\nmethods intends to be similar among ambiguities. To this end, an affinity\ndiversification module is proposed to promote diversity among semantics. A\nmutual complementing refinement is proposed to initially rectify the ambiguous\naffinity with multiple inferred pseudo labels. More importantly, a contrastive\naffinity loss is further designed to diversify the relations among unrelated\nsemantics, which reliably propagates the diversity into the whole feature\nrepresentations and helps generate better pseudo masks. Extensive experiments\nare conducted on PASCAL VOC, MS COCO, and medical ACDC datasets, which validate\nthe efficiency of UniA tackling ambiguity and the superiority over recent\nsingle-staged or even most multi-staged competitors.\n","authors":["Zhiwei Yang","Yucong Meng","Kexue Fu","Shuo Wang","Zhijian Song"],"pdf_url":"https://arxiv.org/pdf/2404.08195v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08187v1","updated":"2024-04-12T01:36:00Z","published":"2024-04-12T01:36:00Z","title":"Adapting CNNs for Fisheye Cameras without Retraining","summary":"  The majority of image processing approaches assume images are in or can be\nrectified to a perspective projection. However, in many applications it is\nbeneficial to use non conventional cameras, such as fisheye cameras, that have\na larger field of view (FOV). The issue arises that these large-FOV images\ncan't be rectified to a perspective projection without significant cropping of\nthe original image. To address this issue we propose Rectified Convolutions\n(RectConv); a new approach for adapting pre-trained convolutional networks to\noperate with new non-perspective images, without any retraining. Replacing the\nconvolutional layers of the network with RectConv layers allows the network to\nsee both rectified patches and the entire FOV. We demonstrate RectConv adapting\nmultiple pre-trained networks to perform segmentation and detection on fisheye\nimagery from two publicly available datasets. Our approach requires no\nadditional data or training, and operates directly on the native image as\ncaptured from the camera. We believe this work is a step toward adapting the\nvast resources available for perspective images to operate across a broad range\nof camera geometries.\n","authors":["Ryan Griffiths","Donald G. Dansereau"],"pdf_url":"https://arxiv.org/pdf/2404.08187v1.pdf","comment":"Project page: https://roboticimaging.org/Projects/RectConv/"},{"id":"http://arxiv.org/abs/2404.08184v1","updated":"2024-04-12T01:13:23Z","published":"2024-04-12T01:13:23Z","title":"Measuring Domain Shifts using Deep Learning Remote Photoplethysmography\n  Model Similarity","summary":"  Domain shift differences between training data for deep learning models and\nthe deployment context can result in severe performance issues for models which\nfail to generalize. We study the domain shift problem under the context of\nremote photoplethysmography (rPPG), a technique for video-based heart rate\ninference. We propose metrics based on model similarity which may be used as a\nmeasure of domain shift, and we demonstrate high correlation between these\nmetrics and empirical performance. One of the proposed metrics with viable\ncorrelations, DS-diff, does not assume access to the ground truth of the target\ndomain, i.e. it may be applied to in-the-wild data. To that end, we investigate\na model selection problem in which ground truth results for the evaluation\ndomain is not known, demonstrating a 13.9% performance improvement over the\naverage case baseline.\n","authors":["Nathan Vance","Patrick Flynn"],"pdf_url":"https://arxiv.org/pdf/2404.08184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08181v1","updated":"2024-04-12T01:08:04Z","published":"2024-04-12T01:08:04Z","title":"Pay Attention to Your Neighbours: Training-Free Open-Vocabulary Semantic\n  Segmentation","summary":"  Despite the significant progress in deep learning for dense visual\nrecognition problems, such as semantic segmentation, traditional methods are\nconstrained by fixed class sets. Meanwhile, vision-language foundation models,\nsuch as CLIP, have showcased remarkable effectiveness in numerous zero-shot\nimage-level tasks, owing to their robust generalizability. Recently, a body of\nwork has investigated utilizing these models in open-vocabulary semantic\nsegmentation (OVSS). However, existing approaches often rely on impractical\nsupervised pre-training or access to additional pre-trained networks. In this\nwork, we propose a strong baseline for training-free OVSS, termed\nNeighbour-Aware CLIP (NACLIP), representing a straightforward adaptation of\nCLIP tailored for this scenario. Our method enforces localization of patches in\nthe self-attention of CLIP's vision transformer which, despite being crucial\nfor dense prediction tasks, has been overlooked in the OVSS literature. By\nincorporating design choices favouring segmentation, our approach significantly\nimproves performance without requiring additional data, auxiliary pre-trained\nnetworks, or extensive hyperparameter tuning, making it highly practical for\nreal-world applications. Experiments are performed on 8 popular semantic\nsegmentation benchmarks, yielding state-of-the-art performance on most\nscenarios. Our code is publicly available at https://github.com/sinahmr/NACLIP .\n","authors":["Sina Hajimiri","Ismail Ben Ayed","Jose Dolz"],"pdf_url":"https://arxiv.org/pdf/2404.08181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.03584v2","updated":"2024-04-12T00:52:35Z","published":"2023-06-06T11:03:05Z","title":"RDFC-GAN: RGB-Depth Fusion CycleGAN for Indoor Depth Completion","summary":"  Raw depth images captured in indoor scenarios frequently exhibit extensive\nmissing values due to the inherent limitations of the sensors and environments.\nFor example, transparent materials frequently elude detection by depth sensors;\nsurfaces may introduce measurement inaccuracies due to their polished textures,\nextended distances, and oblique incidence angles from the sensor. The presence\nof incomplete depth maps imposes significant challenges for subsequent vision\napplications, prompting the development of numerous depth completion techniques\nto mitigate this problem. Numerous methods excel at reconstructing dense depth\nmaps from sparse samples, but they often falter when faced with extensive\ncontiguous regions of missing depth values, a prevalent and critical challenge\nin indoor environments. To overcome these challenges, we design a novel\ntwo-branch end-to-end fusion network named RDFC-GAN, which takes a pair of RGB\nand incomplete depth images as input to predict a dense and completed depth\nmap. The first branch employs an encoder-decoder structure, by adhering to the\nManhattan world assumption and utilizing normal maps from RGB-D information as\nguidance, to regress the local dense depth values from the raw depth map. The\nother branch applies an RGB-depth fusion CycleGAN, adept at translating RGB\nimagery into detailed, textured depth maps while ensuring high fidelity through\ncycle consistency. We fuse the two branches via adaptive fusion modules named\nW-AdaIN and train the model with the help of pseudo depth maps. Comprehensive\nevaluations on NYU-Depth V2 and SUN RGB-D datasets show that our method\nsignificantly enhances depth completion performance particularly in realistic\nindoor settings.\n","authors":["Haowen Wang","Zhengping Che","Yufan Yang","Mingyuan Wang","Zhiyuan Xu","Xiuquan Qiao","Mengshi Qi","Feifei Feng","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2306.03584v2.pdf","comment":"Haowen Wang and Zhengping Che are with equal contributions. Paper\n  accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (TPAMI). An earlier version has been accepted by CVPR 2022\n  (arXiv:2203.10856). arXiv admin note: text overlap with arXiv:2203.10856"},{"id":"http://arxiv.org/abs/2305.09948v5","updated":"2024-04-12T00:46:26Z","published":"2023-05-17T05:03:46Z","title":"HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic\n  Generalization Performance of Human-Object Interaction Detection Models","summary":"  Human-Object Interaction (HOI) detection is a task to localize humans and\nobjects in an image and predict the interactions in human-object pairs. In\nreal-world scenarios, HOI detection models need systematic generalization,\ni.e., generalization to novel combinations of objects and interactions, because\nthe train data are expected to cover a limited portion of all possible\ncombinations. To evaluate the systematic generalization performance of HOI\ndetection models, we created two new sets of HOI detection data splits named\nHICO-DET-SG and V-COCO-SG based on the HICO-DET and V-COCO datasets,\nrespectively. When evaluated on the new data splits, HOI detection models with\nvarious characteristics performed much more poorly than when evaluated on the\noriginal splits. This shows that systematic generalization is a challenging\ngoal in HOI detection. By analyzing the evaluation results, we also gain\ninsights for improving the systematic generalization performance and identify\nfour possible future research directions. We hope that our new data splits and\npresented analysis will encourage further research on systematic generalization\nin HOI detection.\n","authors":["Kentaro Takemoto","Moyuru Yamada","Tomotake Sasaki","Hisanao Akima"],"pdf_url":"https://arxiv.org/pdf/2305.09948v5.pdf","comment":"19 pages, 3 figures, 4 tables"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2404.08630v1","updated":"2024-04-12T17:48:18Z","published":"2024-04-12T17:48:18Z","title":"A Conceptual Framework for Conversational Search and Recommendation:\n  Conceptualizing Agent-Human Interactions During the Conversational Search\n  Process","summary":"  The conversational search task aims to enable a user to resolve information\nneeds via natural language dialogue with an agent. In this paper, we aim to\ndevelop a conceptual framework of the actions and intents of users and agents\nexplaining how these actions enable the user to explore the search space and\nresolve their information need. We outline the different actions and intents,\nbefore discussing key decision points in the conversation where the agent needs\nto decide how to steer the conversational search process to a successful and/or\nsatisfactory conclusion. Essentially, this paper provides a conceptualization\nof the conversational search process between an agent and user, which provides\na framework and a starting point for research, development and evaluation of\nconversational search agents.\n","authors":["Leif Azzopardi","Mateusz Dubiel","Martin Halvey","Jeffery Dalton"],"pdf_url":"https://arxiv.org/pdf/2404.08630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08628v1","updated":"2024-04-12T17:46:13Z","published":"2024-04-12T17:46:13Z","title":"Accessibility in Information Retrieval","summary":"  This paper introduces the concept of accessibility from the field of\ntransportation planning and adopts it within the context of Information\nRetrieval (IR). An analogy is drawn between the fields, which motivates the\ndevelopment of document accessibility measures for IR systems. Considering the\naccessibility of documents within a collection given an IR System provides a\ndifferent perspective on the analysis and evaluation of such systems which\ncould be used to inform the design, tuning and management of current and future\nIR systems.\n","authors":["Leif Azzopardi","Vishwa Vinay"],"pdf_url":"https://arxiv.org/pdf/2404.08628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08535v1","updated":"2024-04-12T15:30:03Z","published":"2024-04-12T15:30:03Z","title":"Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking","summary":"  Contrastive learning has gained widespread adoption for retrieval tasks due\nto its minimal requirement for manual annotations. However, popular contrastive\nframeworks typically learn from binary relevance, making them ineffective at\nincorporating direct fine-grained rankings. In this paper, we curate a\nlarge-scale dataset featuring detailed relevance scores for each query-document\npair to facilitate future research and evaluation. Subsequently, we propose\nGeneralized Contrastive Learning for Multi-Modal Retrieval and Ranking (GCL),\nwhich is designed to learn from fine-grained rankings beyond binary relevance\nscores. Our results show that GCL achieves a 94.5% increase in NDCG@10 for\nin-domain and 26.3 to 48.8% increases for cold-start evaluations, all relative\nto the CLIP baseline and involving ground truth rankings.\n","authors":["Tianyu Zhu","Myong Chol Jung","Jesse Clark"],"pdf_url":"https://arxiv.org/pdf/2404.08535v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08443v1","updated":"2024-04-12T12:52:31Z","published":"2024-04-12T12:52:31Z","title":"Toward FAIR Semantic Publishing of Research Dataset Metadata in the Open\n  Research Knowledge Graph","summary":"  Search engines these days can serve datasets as search results. Datasets get\npicked up by search technologies based on structured descriptions on their\nofficial web pages, informed by metadata ontologies such as the Dataset content\ntype of schema.org. Despite this promotion of the content type dataset as a\nfirst-class citizen of search results, a vast proportion of datasets,\nparticularly research datasets, still need to be made discoverable and,\ntherefore, largely remain unused. This is due to the sheer volume of datasets\nreleased every day and the inability of metadata to reflect a dataset's content\nand context accurately. This work seeks to improve this situation for a\nspecific class of datasets, namely research datasets, which are the result of\nresearch endeavors and are accompanied by a scholarly publication. We propose\nthe ORKG-Dataset content type, a specialized branch of the Open Research\nKnowledge Graoh (ORKG) platform, which provides descriptive information and a\nsemantic model for research datasets, integrating them with their accompanying\nscholarly publications. This work aims to establish a standardized framework\nfor recording and reporting research datasets within the ORKG-Dataset content\ntype. This, in turn, increases research dataset transparency on the web for\ntheir improved discoverability and applied use. In this paper, we present a\nproposal -- the minimum FAIR, comparable, semantic description of research\ndatasets in terms of salient properties of their supporting publication. We\ndesign a specific application of the ORKG-Dataset semantic model based on 40\ndiverse research datasets on scientific information extraction.\n","authors":["Raia Abu Ahmad","Jennifer D'Souza","Matth√§us Zloch","Wolfgang Otto","Georg Rehm","Allard Oelen","Stefan Dietze","S√∂ren Auer"],"pdf_url":"https://arxiv.org/pdf/2404.08443v1.pdf","comment":"8 pages, 1 figure, published in the Joint Proceedings of the\n  Onto4FAIR 2023 Workshops"},{"id":"http://arxiv.org/abs/2404.08361v1","updated":"2024-04-12T09:57:17Z","published":"2024-04-12T09:57:17Z","title":"Large-Scale Multi-Domain Recommendation: an Automatic Domain Feature\n  Extraction and Personalized Integration Framework","summary":"  Feed recommendation is currently the mainstream mode for many real-world\napplications (e.g., TikTok, Dianping), it is usually necessary to model and\npredict user interests in multiple scenarios (domains) within and even outside\nthe application. Multi-domain learning is a typical solution in this regard.\nWhile considerable efforts have been made in this regard, there are still two\nlong-standing challenges: (1) Accurately depicting the differences among\ndomains using domain features is crucial for enhancing the performance of each\ndomain. However, manually designing domain features and models for numerous\ndomains can be a laborious task. (2) Users typically have limited impressions\nin only a few domains. Extracting features automatically from other domains and\nleveraging them to improve the predictive capabilities of each domain has\nconsistently posed a challenging problem. In this paper, we propose an\nAutomatic Domain Feature Extraction and Personalized Integration (DFEI)\nframework for the large-scale multi-domain recommendation. The framework\nautomatically transforms the behavior of each individual user into an\naggregation of all user behaviors within the domain, which serves as the domain\nfeatures. Unlike offline feature engineering methods, the extracted domain\nfeatures are higher-order representations and directly related to the target\nlabel. Besides, by personalized integration of domain features from other\ndomains for each user and the innovation in the training mode, the DFEI\nframework can yield more accurate conversion identification. Experimental\nresults on both public and industrial datasets, consisting of over 20 domains,\nclearly demonstrate that the proposed framework achieves significantly better\nperformance compared with SOTA baselines. Furthermore, we have released the\nsource code of the proposed framework at https://github.com/xidongbo/DFEI.\n","authors":["Dongbo Xi","Zhen Chen","Yuexian Wang","He Cui","Chong Peng","Fuzhen Zhuang","Peng Yan"],"pdf_url":"https://arxiv.org/pdf/2404.08361v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2404.08359v1","updated":"2024-04-12T09:56:12Z","published":"2024-04-12T09:56:12Z","title":"Improving Health Question Answering with Reliable and Time-Aware\n  Evidence Retrieval","summary":"  In today's digital world, seeking answers to health questions on the Internet\nis a common practice. However, existing question answering (QA) systems often\nrely on using pre-selected and annotated evidence documents, thus making them\ninadequate for addressing novel questions. Our study focuses on the open-domain\nQA setting, where the key challenge is to first uncover relevant evidence in\nlarge knowledge bases. By utilizing the common retrieve-then-read QA pipeline\nand PubMed as a trustworthy collection of medical research documents, we answer\nhealth questions from three diverse datasets. We modify different retrieval\nsettings to observe their influence on the QA pipeline's performance, including\nthe number of retrieved documents, sentence selection process, the publication\nyear of articles, and their number of citations. Our results reveal that\ncutting down on the amount of retrieved documents and favoring more recent and\nhighly cited documents can improve the final macro F1 score up to 10%. We\ndiscuss the results, highlight interesting examples, and outline challenges for\nfuture research, like managing evidence disagreement and crafting user-friendly\nexplanations.\n","authors":["Juraj Vladika","Florian Matthes"],"pdf_url":"https://arxiv.org/pdf/2404.08359v1.pdf","comment":"Accepted to NAACL 2024 (Findings)"},{"id":"http://arxiv.org/abs/2403.06567v2","updated":"2024-04-12T08:52:24Z","published":"2024-03-11T10:06:45Z","title":"Leveraging Foundation Models for Content-Based Medical Image Retrieval\n  in Radiology","summary":"  Content-based image retrieval (CBIR) has the potential to significantly\nimprove diagnostic aid and medical research in radiology. Current CBIR systems\nface limitations due to their specialization to certain pathologies, limiting\ntheir utility. In response, we propose using vision foundation models as\npowerful and versatile off-the-shelf feature extractors for content-based\nmedical image retrieval. By benchmarking these models on a comprehensive\ndataset of 1.6 million 2D radiological images spanning four modalities and 161\npathologies, we identify weakly-supervised models as superior, achieving a P@1\nof up to 0.594. This performance not only competes with a specialized model but\ndoes so without the need for fine-tuning. Our analysis further explores the\nchallenges in retrieving pathological versus anatomical structures, indicating\nthat accurate retrieval of pathological features presents greater difficulty.\nDespite these challenges, our research underscores the vast potential of\nfoundation models for CBIR in radiology, proposing a shift towards versatile,\ngeneral-purpose medical image retrieval systems that do not require specific\ntuning.\n","authors":["Stefan Denner","David Zimmerer","Dimitrios Bounias","Markus Bujotzek","Shuhan Xiao","Lisa Kausch","Philipp Schader","Tobias Penzkofer","Paul F. J√§ger","Klaus Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2403.06567v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08301v1","updated":"2024-04-12T07:47:02Z","published":"2024-04-12T07:47:02Z","title":"Collaborative-Enhanced Prediction of Spending on Newly Downloaded Mobile\n  Games under Consumption Uncertainty","summary":"  With the surge in mobile gaming, accurately predicting user spending on newly\ndownloaded games has become paramount for maximizing revenue. However, the\ninherently unpredictable nature of user behavior poses significant challenges\nin this endeavor. To address this, we propose a robust model training and\nevaluation framework aimed at standardizing spending data to mitigate label\nvariance and extremes, ensuring stability in the modeling process. Within this\nframework, we introduce a collaborative-enhanced model designed to predict user\ngame spending without relying on user IDs, thus ensuring user privacy and\nenabling seamless online training. Our model adopts a unique approach by\nseparately representing user preferences and game features before merging them\nas input to the spending prediction module. Through rigorous experimentation,\nour approach demonstrates notable improvements over production models,\nachieving a remarkable \\textbf{17.11}\\% enhancement on offline data and an\nimpressive \\textbf{50.65}\\% boost in an online A/B test. In summary, our\ncontributions underscore the importance of stable model training frameworks and\nthe efficacy of collaborative-enhanced models in predicting user spending\nbehavior in mobile gaming.\n","authors":["Peijie Sun","Yifan Wang","Min Zhang","Chuhan Wu","Yan Fang","Hong Zhu","Yuan Fang","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2404.08301v1.pdf","comment":"10 pages,6 figures, WWW 2024 Industry Track, with three accept, two\n  weak accept scores"},{"id":"http://arxiv.org/abs/2311.04590v6","updated":"2024-04-12T05:56:47Z","published":"2023-11-08T10:44:20Z","title":"Rethinking Cross-Domain Sequential Recommendation under Open-World\n  Assumptions","summary":"  Cross-Domain Sequential Recommendation (CDSR) methods aim to tackle the data\nsparsity and cold-start problems present in Single-Domain Sequential\nRecommendation (SDSR). Existing CDSR works design their elaborate structures\nrelying on overlapping users to propagate the cross-domain information.\nHowever, current CDSR methods make closed-world assumptions, assuming fully\noverlapping users across multiple domains and that the data distribution\nremains unchanged from the training environment to the test environment. As a\nresult, these methods typically result in lower performance on online\nreal-world platforms due to the data distribution shifts. To address these\nchallenges under open-world assumptions, we design an \\textbf{A}daptive\n\\textbf{M}ulti-\\textbf{I}nterest \\textbf{D}ebiasing framework for cross-domain\nsequential recommendation (\\textbf{AMID}), which consists of a multi-interest\ninformation module (\\textbf{MIM}) and a doubly robust estimator (\\textbf{DRE}).\nOur framework is adaptive for open-world environments and can improve the model\nof most off-the-shelf single-domain sequential backbone models for CDSR. Our\nMIM establishes interest groups that consider both overlapping and\nnon-overlapping users, allowing us to effectively explore user intent and\nexplicit interest. To alleviate biases across multiple domains, we developed\nthe DRE for the CDSR methods. We also provide a theoretical analysis that\ndemonstrates the superiority of our proposed estimator in terms of bias and\ntail bound, compared to the IPS estimator used in previous work.\n","authors":["Wujiang Xu","Qitian Wu","Runzhong Wang","Mingming Ha","Qiongxu Ma","Linxun Chen","Bing Han","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2311.04590v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08189v1","updated":"2024-04-12T01:42:09Z","published":"2024-04-12T01:42:09Z","title":"Reducing hallucination in structured outputs via Retrieval-Augmented\n  Generation","summary":"  A common and fundamental limitation of Generative AI (GenAI) is its\npropensity to hallucinate. While large language models (LLM) have taken the\nworld by storm, without eliminating or at least reducing hallucinations,\nreal-world GenAI systems may face challenges in user adoption. In the process\nof deploying an enterprise application that produces workflows based on natural\nlanguage requirements, we devised a system leveraging Retrieval Augmented\nGeneration (RAG) to greatly improve the quality of the structured output that\nrepresents such workflows. Thanks to our implementation of RAG, our proposed\nsystem significantly reduces hallucinations in the output and improves the\ngeneralization of our LLM in out-of-domain settings. In addition, we show that\nusing a small, well-trained retriever encoder can reduce the size of the\naccompanying LLM, thereby making deployments of LLM-based systems less\nresource-intensive.\n","authors":["Patrice B√©chard","Orlando Marquez Ayala"],"pdf_url":"https://arxiv.org/pdf/2404.08189v1.pdf","comment":"To be presented at NAACL 2024. 11 pages and 4 figures"},{"id":"http://arxiv.org/abs/2404.04522v2","updated":"2024-04-12T00:18:06Z","published":"2024-04-06T06:44:41Z","title":"Q-PEFT: Query-dependent Parameter Efficient Fine-tuning for Text\n  Reranking with Large Language Models","summary":"  Parameter Efficient Fine-Tuning (PEFT) methods have been extensively utilized\nin Large Language Models (LLMs) to improve the down-streaming tasks without the\ncost of fine-tuing the whole LLMs. Recent studies have shown how to effectively\nuse PEFT for fine-tuning LLMs in ranking tasks with convincing performance;\nthere are some limitations, including the learned prompt being fixed for\ndifferent documents, overfitting to specific tasks, and low adaptation ability.\nIn this paper, we introduce a query-dependent parameter efficient fine-tuning\n(Q-PEFT) approach for text reranking to leak the information of the true\nqueries to LLMs and then make the generation of true queries from input\ndocuments much easier. Specifically, we utilize the query to extract the\ntop-$k$ tokens from concatenated documents, serving as contextual clues. We\nfurther augment Q-PEFT by substituting the retrieval mechanism with a\nmulti-head attention layer to achieve end-to-end training and cover all the\ntokens in the documents, guiding the LLMs to generate more document-specific\nsynthetic queries, thereby further improving the reranking performance.\nExtensive experiments are conducted on four public datasets, demonstrating the\neffectiveness of our proposed approach.\n","authors":["Zhiyuan Peng","Xuyang Wu","Qifan Wang","Sravanthi Rajanala","Yi Fang"],"pdf_url":"https://arxiv.org/pdf/2404.04522v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08847v1","updated":"2024-04-12T23:32:06Z","published":"2024-04-12T23:32:06Z","title":"LazyDP: Co-Designing Algorithm-Software for Scalable Training of\n  Differentially Private Recommendation Models","summary":"  Differential privacy (DP) is widely being employed in the industry as a\npractical standard for privacy protection. While private training of computer\nvision or natural language processing applications has been studied\nextensively, the computational challenges of training of recommender systems\n(RecSys) with DP have not been explored. In this work, we first present our\ndetailed characterization of private RecSys training using DP-SGD, root-causing\nits several performance bottlenecks. Specifically, we identify DP-SGD's noise\nsampling and noisy gradient update stage to suffer from a severe compute and\nmemory bandwidth limitation, respectively, causing significant performance\noverhead in training private RecSys. Based on these findings, we propose\nLazyDP, an algorithm-software co-design that addresses the compute and memory\nchallenges of training RecSys with DP-SGD. Compared to a state-of-the-art\nDP-SGD training system, we demonstrate that LazyDP provides an average 119x\ntraining throughput improvement while also ensuring mathematically equivalent,\ndifferentially private RecSys models to be trained.\n","authors":["Juntaek Lim","Youngeun Kwon","Ranggi Hwang","Kiwan Maeng","G. Edward Suh","Minsoo Rhu"],"pdf_url":"https://arxiv.org/pdf/2404.08847v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08829v1","updated":"2024-04-12T22:00:27Z","published":"2024-04-12T22:00:27Z","title":"Measuring the Predictability of Recommender Systems using Structural\n  Complexity Metrics","summary":"  Recommender systems (RS) are central to the filtering and curation of online\ncontent. These algorithms predict user ratings for unseen items based on past\npreferences. Despite their importance, the innate predictability of RS has\nreceived limited attention. This study introduces data-driven metrics to\nmeasure the predictability of RS based on the structural complexity of the\nuser-item rating matrix. A low predictability score indicates complex and\nunpredictable user-item interactions, while a high predictability score reveals\nless complex patterns with predictive potential. We propose two strategies that\nuse singular value decomposition (SVD) and matrix factorization (MF) to measure\nstructural complexity. By perturbing the data and evaluating the prediction of\nthe perturbed version, we explore the structural consistency indicated by the\nSVD singular vectors. The assumption is that a random perturbation of highly\nstructured data does not change its structure. Empirical results show a high\ncorrelation between our metrics and the accuracy of the best-performing\nprediction algorithms on real data sets.\n","authors":["Alfonso Valderrama","Andr√©s Abeliuk"],"pdf_url":"https://arxiv.org/pdf/2404.08829v1.pdf","comment":"Accepted at WWW-24 Workshop: DCAI Data-centric Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2404.08796v1","updated":"2024-04-12T20:03:06Z","published":"2024-04-12T20:03:06Z","title":"The Elephant in the Room: Rethinking the Usage of Pre-trained Language\n  Model in Sequential Recommendation","summary":"  Sequential recommendation (SR) has seen significant advancements with the\nhelp of Pre-trained Language Models (PLMs). Some PLM-based SR models directly\nuse PLM to encode user historical behavior's text sequences to learn user\nrepresentations, while there is seldom an in-depth exploration of the\ncapability and suitability of PLM in behavior sequence modeling. In this work,\nwe first conduct extensive model analyses between PLMs and PLM-based SR models,\ndiscovering great underutilization and parameter redundancy of PLMs in behavior\nsequence modeling. Inspired by this, we explore different lightweight usages of\nPLMs in SR, aiming to maximally stimulate the ability of PLMs for SR while\nsatisfying the efficiency and usability demands of practical systems. We\ndiscover that adopting behavior-tuned PLMs for item initializations of\nconventional ID-based SR models is the most economical framework of PLM-based\nSR, which would not bring in any additional inference cost but could achieve a\ndramatic performance boost compared with the original version. Extensive\nexperiments on five datasets show that our simple and universal framework leads\nto significant improvement compared to classical SR and SOTA PLM-based SR\nmodels without additional inference costs.\n","authors":["Zekai Qu","Ruobing Xie","Chaojun Xiao","Xingwu Sun","Zhanhui Kang"],"pdf_url":"https://arxiv.org/pdf/2404.08796v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2404.08720v1","updated":"2024-04-12T11:12:16Z","published":"2024-04-12T11:12:16Z","title":"Exploring Contrastive Learning for Long-Tailed Multi-Label Text\n  Classification","summary":"  Learning an effective representation in multi-label text classification\n(MLTC) is a significant challenge in NLP. This challenge arises from the\ninherent complexity of the task, which is shaped by two key factors: the\nintricate connections between labels and the widespread long-tailed\ndistribution of the data. To overcome this issue, one potential approach\ninvolves integrating supervised contrastive learning with classical supervised\nloss functions. Although contrastive learning has shown remarkable performance\nin multi-class classification, its impact in the multi-label framework has not\nbeen thoroughly investigated. In this paper, we conduct an in-depth study of\nsupervised contrastive learning and its influence on representation in MLTC\ncontext. We emphasize the importance of considering long-tailed data\ndistributions to build a robust representation space, which effectively\naddresses two critical challenges associated with contrastive learning that we\nidentify: the \"lack of positives\" and the \"attraction-repulsion imbalance\".\nBuilding on this insight, we introduce a novel contrastive loss function for\nMLTC. It attains Micro-F1 scores that either match or surpass those obtained\nwith other frequently employed loss functions, and demonstrates a significant\nimprovement in Macro-F1 scores across three multi-label datasets.\n","authors":["Alexandre Audibert","Aur√©lien Gauffre","Massih-Reza Amini"],"pdf_url":"https://arxiv.org/pdf/2404.08720v1.pdf","comment":"14 pages, 2 figures"},{"id":"http://arxiv.org/abs/2307.06985v7","updated":"2024-04-12T05:36:03Z","published":"2023-07-13T17:25:28Z","title":"Retrieval Augmented Generation using Engineering Design Knowledge","summary":"  Large-language Models (LLMs) need to adopt Retrieval-Augmented Generation\n(RAG) to generate factual responses that are better suited to knowledge-based\napplications in the design process. We present a data-driven method to identify\nexplicit facts of the form - head entity :: relationship :: tail entity from\npatented artefact descriptions. We train roBERTa Transformer-based sequence\nclassification models using our proprietary dataset of 44,227 sentences. Upon\nclassifying tokens in a sentence as entities or relationships, our method uses\nanother classifier to identify specific relationship tokens for a given pair of\nentities. We compare the performances against linear classifiers and Graph\nNeural Networks (GNNs) that both incorporate BERT Transformer-based token\nembeddings to predict associations among the entities and relationships. We\napply our method to 4,870 fan system related patents and populate a knowledge\nbase that constitutes around 3 million facts. Using the knowledge base, we\ndemonstrate retrieving generalisable and specific domain knowledge for\ncontextualising LLMs.\n","authors":["L Siddharth","Jianxi Luo"],"pdf_url":"https://arxiv.org/pdf/2307.06985v7.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2404.08634v1","updated":"2024-04-12T17:53:34Z","published":"2024-04-12T17:53:34Z","title":"Pre-training Small Base LMs with Fewer Tokens","summary":"  We study the effectiveness of a simple approach to develop a small base\nlanguage model (LM) starting from an existing large base LM: first inherit a\nfew transformer blocks from the larger LM, and then train this smaller model on\na very small subset (0.1\\%) of the raw pretraining data of the larger model. We\ncall our simple recipe Inheritune and first demonstrate it for building a small\nbase LM with 1.5B parameters using 1B tokens (and a starting few layers of\nlarger LM of 3B parameters); we do this using a single A6000 GPU for less than\nhalf a day. Across 9 diverse evaluation datasets as well as the MMLU benchmark,\nthe resulting model compares favorably to publicly available base models of\n1B-2B size, some of which have been trained using 50-1000 times more tokens.\n  We investigate Inheritune in a slightly different setting where we train\nsmall LMs utilizing larger LMs and their full pre-training dataset. Here we\nshow that smaller LMs trained utilizing some of the layers of GPT2-medium\n(355M) and GPT-2-large (770M) can effectively match the val loss of their\nbigger counterparts when trained from scratch for the same number of training\nsteps on OpenWebText dataset with 9B tokens. We analyze our recipe with\nextensive experiments and demonstrate it efficacy on diverse settings. Our code\nis available at https://github.com/sanyalsunny111/LLM-Inheritune.\n","authors":["Sunny Sanyal","Sujay Sanghavi","Alexandros G. Dimakis"],"pdf_url":"https://arxiv.org/pdf/2404.08634v1.pdf","comment":"15 pages, 6 figures, 10 tables"},{"id":"http://arxiv.org/abs/2404.08627v1","updated":"2024-04-12T17:41:05Z","published":"2024-04-12T17:41:05Z","title":"Is ChatGPT Transforming Academics' Writing Style?","summary":"  Based on one million arXiv papers submitted from May 2018 to January 2024, we\nassess the textual density of ChatGPT's writing style in their abstracts by\nmeans of a statistical analysis of word frequency changes. Our model is\ncalibrated and validated on a mixture of real abstracts and ChatGPT-modified\nabstracts (simulated data) after a careful noise analysis. We find that ChatGPT\nis having an increasing impact on arXiv abstracts, especially in the field of\ncomputer science, where the fraction of ChatGPT-revised abstracts is estimated\nto be approximately 35%, if we take the output of one of the simplest prompts,\n\"revise the following sentences\", as a baseline. We conclude with an analysis\nof both positive and negative aspects of the penetration of ChatGPT into\nacademics' writing style.\n","authors":["Mingmeng Geng","Roberto Trotta"],"pdf_url":"https://arxiv.org/pdf/2404.08627v1.pdf","comment":"15 pages, 19 figures"},{"id":"http://arxiv.org/abs/2404.08624v1","updated":"2024-04-12T17:37:42Z","published":"2024-04-12T17:37:42Z","title":"Regularized Gradient Clipping Provably Trains Wide and Deep Neural\n  Networks","summary":"  In this work, we instantiate a regularized form of the gradient clipping\nalgorithm and prove that it can converge to the global minima of deep neural\nnetwork loss functions provided that the net is of sufficient width. We present\nempirical evidence that our theoretically founded regularized gradient clipping\nalgorithm is also competitive with the state-of-the-art deep-learning\nheuristics. Hence the algorithm presented here constitutes a new approach to\nrigorous deep learning.\n  The modification we do to standard gradient clipping is designed to leverage\nthe PL* condition, a variant of the Polyak-Lojasiewicz inequality which was\nrecently proven to be true for various neural networks for any depth within a\nneighborhood of the initialisation.\n","authors":["Matteo Tucat","Anirbit Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2404.08624v1.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2402.01092v2","updated":"2024-04-12T17:16:09Z","published":"2024-02-02T01:41:38Z","title":"A Dynamical Model of Neural Scaling Laws","summary":"  On a variety of tasks, the performance of neural networks predictably\nimproves with training time, dataset size and model size across many orders of\nmagnitude. This phenomenon is known as a neural scaling law. Of fundamental\nimportance is the compute-optimal scaling law, which reports the performance as\na function of units of compute when choosing model sizes optimally. We analyze\na random feature model trained with gradient descent as a solvable model of\nnetwork training and generalization. This reproduces many observations about\nneural scaling laws. First, our model makes a prediction about why the scaling\nof performance with training time and with model size have different power law\nexponents. Consequently, the theory predicts an asymmetric compute-optimal\nscaling rule where the number of training steps are increased faster than model\nparameters, consistent with recent empirical observations. Second, it has been\nobserved that early in training, networks converge to their infinite-width\ndynamics at a rate $1/\\textit{width}$ but at late time exhibit a rate\n$\\textit{width}^{-c}$, where $c$ depends on the structure of the architecture\nand task. We show that our model exhibits this behavior. Lastly, our theory\nshows how the gap between training and test loss can gradually build up over\ntime due to repeated reuse of data.\n","authors":["Blake Bordelon","Alexander Atanasov","Cengiz Pehlevan"],"pdf_url":"https://arxiv.org/pdf/2402.01092v2.pdf","comment":"Updated Appendix with new SGD section, more ensembling verification,\n  and connection to timescale/eigenvalue densities"},{"id":"http://arxiv.org/abs/2404.08608v1","updated":"2024-04-12T17:14:58Z","published":"2024-04-12T17:14:58Z","title":"Hyperbolic Delaunay Geometric Alignment","summary":"  Hyperbolic machine learning is an emerging field aimed at representing data\nwith a hierarchical structure. However, there is a lack of tools for evaluation\nand analysis of the resulting hyperbolic data representations. To this end, we\npropose Hyperbolic Delaunay Geometric Alignment (HyperDGA) -- a similarity\nscore for comparing datasets in a hyperbolic space. The core idea is counting\nthe edges of the hyperbolic Delaunay graph connecting datapoints across the\ngiven sets. We provide an empirical investigation on synthetic and real-life\nbiological data and demonstrate that HyperDGA outperforms the hyperbolic\nversion of classical distances between sets. Furthermore, we showcase the\npotential of HyperDGA for evaluating latent representations inferred by a\nHyperbolic Variational Auto-Encoder.\n","authors":["Aniss Aiman Medbouhi","Giovanni Luca Marchetti","Vladislav Polianskii","Alexander Kravberg","Petra Poklukar","Anastasia Varava","Danica Kragic"],"pdf_url":"https://arxiv.org/pdf/2404.08608v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08602v1","updated":"2024-04-12T17:01:25Z","published":"2024-04-12T17:01:25Z","title":"Sliding down the stairs: how correlated latent variables accelerate\n  learning with neural networks","summary":"  Neural networks extract features from data using stochastic gradient descent\n(SGD). In particular, higher-order input cumulants (HOCs) are crucial for their\nperformance. However, extracting information from the $p$th cumulant of\n$d$-dimensional inputs is computationally hard: the number of samples required\nto recover a single direction from an order-$p$ tensor (tensor PCA) using\nonline SGD grows as $d^{p-1}$, which is prohibitive for high-dimensional\ninputs. This result raises the question of how neural networks extract relevant\ndirections from the HOCs of their inputs efficiently. Here, we show that\ncorrelations between latent variables along the directions encoded in different\ninput cumulants speed up learning from higher-order correlations. We show this\neffect analytically by deriving nearly sharp thresholds for the number of\nsamples required by a single neuron to weakly-recover these directions using\nonline SGD from a random start in high dimensions. Our analytical results are\nconfirmed in simulations of two-layer neural networks and unveil a new\nmechanism for hierarchical learning in neural networks.\n","authors":["Lorenzo Bardone","Sebastian Goldt"],"pdf_url":"https://arxiv.org/pdf/2404.08602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08601v1","updated":"2024-04-12T16:55:08Z","published":"2024-04-12T16:55:08Z","title":"Generating Synthetic Time Series Data for Cyber-Physical Systems","summary":"  Data augmentation is an important facilitator of deep learning applications\nin the time series domain. A gap is identified in the literature, demonstrating\nsparse exploration of the transformer, the dominant sequence model, for data\naugmentation in time series. A architecture hybridizing several successful\npriors is put forth and tested using a powerful time domain similarity metric.\nResults suggest the challenge of this domain, and several valuable directions\nfor future work.\n","authors":["Alexander Sommers","Somayeh Bakhtiari Ramezani","Logan Cummins","Sudip Mittal","Shahram Rahimi","Maria Seale","Joseph Jaboure"],"pdf_url":"https://arxiv.org/pdf/2404.08601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01448v2","updated":"2024-04-12T16:37:46Z","published":"2024-01-02T22:15:20Z","title":"ProbMCL: Simple Probabilistic Contrastive Learning for Multi-label\n  Visual Classification","summary":"  Multi-label image classification presents a challenging task in many domains,\nincluding computer vision and medical imaging. Recent advancements have\nintroduced graph-based and transformer-based methods to improve performance and\ncapture label dependencies. However, these methods often include complex\nmodules that entail heavy computation and lack interpretability. In this paper,\nwe propose Probabilistic Multi-label Contrastive Learning (ProbMCL), a novel\nframework to address these challenges in multi-label image classification\ntasks. Our simple yet effective approach employs supervised contrastive\nlearning, in which samples that share enough labels with an anchor image based\non a decision threshold are introduced as a positive set. This structure\ncaptures label dependencies by pulling positive pair embeddings together and\npushing away negative samples that fall below the threshold. We enhance\nrepresentation learning by incorporating a mixture density network into\ncontrastive learning and generating Gaussian mixture distributions to explore\nthe epistemic uncertainty of the feature encoder. We validate the effectiveness\nof our framework through experimentation with datasets from the computer vision\nand medical imaging domains. Our method outperforms the existing\nstate-of-the-art methods while achieving a low computational footprint on both\ndatasets. Visualization analyses also demonstrate that ProbMCL-learned\nclassifiers maintain a meaningful semantic topology.\n","authors":["Ahmad Sajedi","Samir Khaki","Yuri A. Lawryshyn","Konstantinos N. Plataniotis"],"pdf_url":"https://arxiv.org/pdf/2401.01448v2.pdf","comment":"This paper has been accepted for the ICASSP 2024 - 2024 IEEE\n  International Conference on Acoustics, Speech and Signal Processing (ICASSP)"},{"id":"http://arxiv.org/abs/2403.13005v2","updated":"2024-04-12T16:26:04Z","published":"2024-03-14T11:53:35Z","title":"Leap: molecular synthesisability scoring with intermediates","summary":"  Assessing whether a molecule can be synthesised is a primary task in drug\ndiscovery. It enables computational chemists to filter for viable compounds or\nbias molecular generative models. The notion of synthesisability is dynamic as\nit evolves depending on the availability of key compounds. A common approach in\ndrug discovery involves exploring the chemical space surrounding\nsynthetically-accessible intermediates. This strategy improves the\nsynthesisability of the derived molecules due to the availability of key\nintermediates. Existing synthesisability scoring methods such as SAScore,\nSCScore and RAScore, cannot condition on intermediates dynamically. Our\napproach, Leap, is a GPT-2 model trained on the depth, or longest linear path,\nof predicted synthesis routes that allows information on the availability of\nkey intermediates to be included at inference time. We show that Leap surpasses\nall other scoring methods by at least 5% on AUC score when identifying\nsynthesisable molecules, and can successfully adapt predicted scores when\npresented with a relevant intermediate compound.\n","authors":["Antonia Calvi","Th√©ophile Gaudin","Dominik Miketa","Dominique Sydow","Liam Wilbraham"],"pdf_url":"https://arxiv.org/pdf/2403.13005v2.pdf","comment":"New Frontiers of AI for Drug Discovery and Development workshop paper"},{"id":"http://arxiv.org/abs/2404.08579v1","updated":"2024-04-12T16:23:41Z","published":"2024-04-12T16:23:41Z","title":"Small Models Are (Still) Effective Cross-Domain Argument Extractors","summary":"  Effective ontology transfer has been a major goal of recent work on event\nargument extraction (EAE). Two methods in particular -- question answering (QA)\nand template infilling (TI) -- have emerged as promising approaches to this\nproblem. However, detailed explorations of these techniques' ability to\nactually enable this transfer are lacking. In this work, we provide such a\nstudy, exploring zero-shot transfer using both techniques on six major EAE\ndatasets at both the sentence and document levels. Further, we challenge the\ngrowing reliance on LLMs for zero-shot extraction, showing that vastly smaller\nmodels trained on an appropriate source ontology can yield zero-shot\nperformance superior to that of GPT-3.5 or GPT-4.\n","authors":["William Gantt","Aaron Steven White"],"pdf_url":"https://arxiv.org/pdf/2404.08579v1.pdf","comment":"ACL Rolling Review Short Paper"},{"id":"http://arxiv.org/abs/2404.08570v1","updated":"2024-04-12T16:13:10Z","published":"2024-04-12T16:13:10Z","title":"Enhancing Autonomous Vehicle Training with Language Model Integration\n  and Critical Scenario Generation","summary":"  This paper introduces CRITICAL, a novel closed-loop framework for autonomous\nvehicle (AV) training and testing. CRITICAL stands out for its ability to\ngenerate diverse scenarios, focusing on critical driving situations that target\nspecific learning and performance gaps identified in the Reinforcement Learning\n(RL) agent. The framework achieves this by integrating real-world traffic\ndynamics, driving behavior analysis, surrogate safety measures, and an optional\nLarge Language Model (LLM) component. It is proven that the establishment of a\nclosed feedback loop between the data generation pipeline and the training\nprocess can enhance the learning rate during training, elevate overall system\nperformance, and augment safety resilience. Our evaluations, conducted using\nthe Proximal Policy Optimization (PPO) and the HighwayEnv simulation\nenvironment, demonstrate noticeable performance improvements with the\nintegration of critical case generation and LLM analysis, indicating CRITICAL's\npotential to improve the robustness of AV systems and streamline the generation\nof critical scenarios. This ultimately serves to hasten the development of AV\nagents, expand the general scope of RL training, and ameliorate validation\nefforts for AV safety.\n","authors":["Hanlin Tian","Kethan Reddy","Yuxiang Feng","Mohammed Quddus","Yiannis Demiris","Panagiotis Angeloudis"],"pdf_url":"https://arxiv.org/pdf/2404.08570v1.pdf","comment":"7 pages, 5 figures"},{"id":"http://arxiv.org/abs/2401.08047v2","updated":"2024-04-12T16:13:06Z","published":"2024-01-16T02:00:17Z","title":"Incremental Extractive Opinion Summarization Using Cover Trees","summary":"  Extractive opinion summarization involves automatically producing a summary\nof text about an entity (e.g., a product's reviews) by extracting\nrepresentative sentences that capture prevalent opinions in the review set.\nTypically, in online marketplaces user reviews accumulate over time, and\nopinion summaries need to be updated periodically to provide customers with\nup-to-date information. In this work, we study the task of extractive opinion\nsummarization in an incremental setting, where the underlying review set\nevolves over time. Many of the state-of-the-art extractive opinion\nsummarization approaches are centrality-based, such as CentroidRank (Radev et\nal., 2004; Chowdhury et al., 2022). CentroidRank performs extractive\nsummarization by selecting a subset of review sentences closest to the centroid\nin the representation space as the summary. However, these methods are not\ncapable of operating efficiently in an incremental setting, where reviews\narrive one at a time. In this paper, we present an efficient algorithm for\naccurately computing the CentroidRank summaries in an incremental setting. Our\napproach, CoverSumm, relies on indexing review representations in a cover tree\nand maintaining a reservoir of candidate summary review sentences. CoverSumm's\nefficacy is supported by a theoretical and empirical analysis of running time.\nEmpirically, on a diverse collection of data (both real and synthetically\ncreated to illustrate scaling considerations), we demonstrate that CoverSumm is\nup to 36x faster than baseline methods, and capable of adapting to nuanced\nchanges in data distribution. We also conduct human evaluations of the\ngenerated summaries and find that CoverSumm is capable of producing informative\nsummaries consistent with the underlying review set.\n","authors":["Somnath Basu Roy Chowdhury","Nicholas Monath","Avinava Dubey","Manzil Zaheer","Andrew McCallum","Amr Ahmed","Snigdha Chaturvedi"],"pdf_url":"https://arxiv.org/pdf/2401.08047v2.pdf","comment":"Accepted at TMLR"},{"id":"http://arxiv.org/abs/2404.08566v1","updated":"2024-04-12T16:08:32Z","published":"2024-04-12T16:08:32Z","title":"Mitigating Receiver Impact on Radio Frequency Fingerprint Identification\n  via Domain Adaptation","summary":"  Radio Frequency Fingerprint Identification (RFFI), which exploits non-ideal\nhardware-induced unique distortion resident in the transmit signals to identify\nan emitter, is emerging as a means to enhance the security of communication\nsystems. Recently, machine learning has achieved great success in developing\nstate-of-the-art RFFI models. However, few works consider cross-receiver RFFI\nproblems, where the RFFI model is trained and deployed on different receivers.\nDue to altered receiver characteristics, direct deployment of RFFI model on a\nnew receiver leads to significant performance degradation. To address this\nissue, we formulate the cross-receiver RFFI as a model adaptation problem,\nwhich adapts the trained model to unlabeled signals from a new receiver. We\nfirst develop a theoretical generalization error bound for the adaptation\nmodel. Motivated by the bound, we propose a novel method to solve the\ncross-receiver RFFI problem, which includes domain alignment and adaptive\npseudo-labeling. The former aims at finding a feature space where both domains\nexhibit similar distributions, effectively reducing the domain discrepancy.\nMeanwhile, the latter employs a dynamic pseudo-labeling scheme to implicitly\ntransfer the label information from the labeled receiver to the new receiver.\nExperimental results indicate that the proposed method can effectively mitigate\nthe receiver impact and improve the cross-receiver RFFI performance.\n","authors":["Liu Yang","Qiang Li","Xiaoyang Ren","Yi Fang","Shafei Wang"],"pdf_url":"https://arxiv.org/pdf/2404.08566v1.pdf","comment":"Accepted by IEEE Internet of Things Journal"},{"id":"http://arxiv.org/abs/2402.06994v2","updated":"2024-04-12T16:07:55Z","published":"2024-02-10T17:02:53Z","title":"A Change Detection Reality Check","summary":"  In recent years, there has been an explosion of proposed change detection\ndeep learning architectures in the remote sensing literature. These approaches\nclaim to offer state-of-the-art performance on different standard benchmark\ndatasets. However, has the field truly made significant progress? In this paper\nwe perform experiments which conclude a simple U-Net segmentation baseline\nwithout training tricks or complicated architectural changes is still a top\nperformer for the task of change detection.\n","authors":["Isaac Corley","Caleb Robinson","Anthony Ortiz"],"pdf_url":"https://arxiv.org/pdf/2402.06994v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.08952v2","updated":"2024-04-12T16:05:03Z","published":"2022-06-17T18:09:34Z","title":"The Impact of Variable Ordering on Bayesian Network Structure Learning","summary":"  Causal Bayesian Networks provide an important tool for reasoning under\nuncertainty with potential application to many complex causal systems.\nStructure learning algorithms that can tell us something about the causal\nstructure of these systems are becoming increasingly important. In the\nliterature, the validity of these algorithms is often tested for sensitivity\nover varying sample sizes, hyper-parameters, and occasionally objective\nfunctions. In this paper, we show that the order in which the variables are\nread from data can have much greater impact on the accuracy of the algorithm\nthan these factors. Because the variable ordering is arbitrary, any significant\neffect it has on learnt graph accuracy is concerning, and this raises questions\nabout the validity of the results produced by algorithms that are sensitive to,\nbut have not been assessed against, different variable orderings.\n","authors":["Neville K Kitson","Anthony C Constantinou"],"pdf_url":"https://arxiv.org/pdf/2206.08952v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08557v1","updated":"2024-04-12T15:54:48Z","published":"2024-04-12T15:54:48Z","title":"Scalability in Building Component Data Annotation: Enhancing Facade\n  Material Classification with Synthetic Data","summary":"  Computer vision models trained on Google Street View images can create\nmaterial cadastres. However, current approaches need manually annotated\ndatasets that are difficult to obtain and often have class imbalance. To\naddress these challenges, this paper fine-tuned a Swin Transformer model on a\nsynthetic dataset generated with DALL-E and compared the performance to a\nsimilar manually annotated dataset. Although manual annotation remains the gold\nstandard, the synthetic dataset performance demonstrates a reasonable\nalternative. The findings will ease annotation needed to develop material\ncadastres, offering architects insights into opportunities for material reuse,\nthus contributing to the reduction of demolition waste.\n","authors":["Josie Harrison","Alexander Hollberg","Yinan Yu"],"pdf_url":"https://arxiv.org/pdf/2404.08557v1.pdf","comment":"10 pages, 6 figures, submitted to 2024 European Conference of\n  Computing in Construction"},{"id":"http://arxiv.org/abs/2404.08555v1","updated":"2024-04-12T15:54:15Z","published":"2024-04-12T15:54:15Z","title":"RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\n  Human Feedback for LLMs","summary":"  State-of-the-art large language models (LLMs) have become indispensable tools\nfor various tasks. However, training LLMs to serve as effective assistants for\nhumans requires careful consideration. A promising approach is reinforcement\nlearning from human feedback (RLHF), which leverages human feedback to update\nthe model in accordance with human preferences and mitigate issues like\ntoxicity and hallucinations. Yet, an understanding of RLHF for LLMs is largely\nentangled with initial design choices that popularized the method and current\nresearch focuses on augmenting those choices rather than fundamentally\nimproving the framework. In this paper, we analyze RLHF through the lens of\nreinforcement learning principles to develop an understanding of its\nfundamentals, dedicating substantial focus to the core component of RLHF -- the\nreward model. Our study investigates modeling choices, caveats of function\napproximation, and their implications on RLHF training algorithms, highlighting\nthe underlying assumptions made about the expressivity of reward. Our analysis\nimproves the understanding of the role of reward models and methods for their\ntraining, concurrently revealing limitations of the current methodology. We\ncharacterize these limitations, including incorrect generalization, model\nmisspecification, and the sparsity of feedback, along with their impact on the\nperformance of a language model. The discussion and analysis are substantiated\nby a categorical review of current literature, serving as a reference for\nresearchers and practitioners to understand the challenges of RLHF and build\nupon existing efforts.\n","authors":["Shreyas Chaudhari","Pranjal Aggarwal","Vishvak Murahari","Tanmay Rajpurohit","Ashwin Kalyan","Karthik Narasimhan","Ameet Deshpande","Bruno Castro da Silva"],"pdf_url":"https://arxiv.org/pdf/2404.08555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.03226v3","updated":"2024-04-12T15:52:37Z","published":"2022-11-06T22:05:27Z","title":"Rotation-equivariant Graph Neural Networks for Learning Glassy Liquids\n  Representations","summary":"  The difficult problem of relating the static structure of glassy liquids and\ntheir dynamics is a good target for Machine Learning, an approach which excels\nat finding complex patterns hidden in data. Indeed, this approach is currently\na hot topic in the glassy liquids community, where the state of the art\nconsists in Graph Neural Networks (GNNs), which have great expressive power but\nare heavy models and lack interpretability. Inspired by recent advances in the\nfield of Machine Learning group-equivariant representations, we build a GNN\nthat learns a robust representation of the glass' static structure by\nconstraining it to preserve the roto-translation (SE(3)) equivariance. We show\nthat this constraint significantly improves the predictive power at comparable\nor reduced number of parameters but most importantly, improves the ability to\ngeneralize to unseen temperatures. While remaining a Deep network, our model\nhas improved interpretability compared to other GNNs, as the action of our\nbasic convolution layer relates directly to well-known rotation-invariant\nexpert features. Through transfer-learning experiments displaying unprecedented\nperformance, we demonstrate that our network learns a robust representation,\nwhich allows us to push forward the idea of a learned structural order\nparameter for glasses.\n","authors":["Francesco Saverio Pezzicoli","Guillaume Charpiat","Fran√ßois P. Landes"],"pdf_url":"https://arxiv.org/pdf/2211.03226v3.pdf","comment":"Submitted to SciPost. 15 pages, 9 figures plus references and 4 pages\n  of appendix"},{"id":"http://arxiv.org/abs/2310.02557v3","updated":"2024-04-12T15:48:47Z","published":"2023-10-04T03:30:32Z","title":"Generalization in diffusion models arises from geometry-adaptive\n  harmonic representations","summary":"  Deep neural networks (DNNs) trained for image denoising are able to generate\nhigh-quality samples with score-based reverse diffusion algorithms. These\nimpressive capabilities seem to imply an escape from the curse of\ndimensionality, but recent reports of memorization of the training set raise\nthe question of whether these networks are learning the \"true\" continuous\ndensity of the data. Here, we show that two DNNs trained on non-overlapping\nsubsets of a dataset learn nearly the same score function, and thus the same\ndensity, when the number of training images is large enough. In this regime of\nstrong generalization, diffusion-generated images are distinct from the\ntraining set, and are of high visual quality, suggesting that the inductive\nbiases of the DNNs are well-aligned with the data density. We analyze the\nlearned denoising functions and show that the inductive biases give rise to a\nshrinkage operation in a basis adapted to the underlying image. Examination of\nthese bases reveals oscillating harmonic structures along contours and in\nhomogeneous regions. We demonstrate that trained denoisers are inductively\nbiased towards these geometry-adaptive harmonic bases since they arise not only\nwhen the network is trained on photographic images, but also when it is trained\non image classes supported on low-dimensional manifolds for which the harmonic\nbasis is suboptimal. Finally, we show that when trained on regular image\nclasses for which the optimal basis is known to be geometry-adaptive and\nharmonic, the denoising performance of the networks is near-optimal.\n","authors":["Zahra Kadkhodaie","Florentin Guth","Eero P. Simoncelli","St√©phane Mallat"],"pdf_url":"https://arxiv.org/pdf/2310.02557v3.pdf","comment":"Accepted for oral presentation at ICLR, Vienna, May 2024"},{"id":"http://arxiv.org/abs/2402.16891v2","updated":"2024-04-12T15:34:18Z","published":"2024-02-23T13:25:23Z","title":"Multi-Task Learning for Routing Problem with Cross-Problem Zero-Shot\n  Generalization","summary":"  Vehicle routing problems (VRPs), which can be found in numerous real-world\napplications, have been an important research topic for several decades.\nRecently, the neural combinatorial optimization (NCO) approach that leverages a\nlearning-based model to solve VRPs without manual algorithm design has gained\nsubstantial attention. However, current NCO methods typically require building\none model for each routing problem, which significantly hinders their practical\napplication for real-world industry problems with diverse attributes. In this\nwork, we make the first attempt to tackle the crucial challenge of\ncross-problem generalization. In particular, we formulate VRPs as different\ncombinations of a set of shared underlying attributes and solve them\nsimultaneously via a single model through attribute composition. In this way,\nour proposed model can successfully solve VRPs with unseen attribute\ncombinations in a zero-shot generalization manner. Extensive experiments are\nconducted on eleven VRP variants, benchmark datasets, and industry logistic\nscenarios. The results show that the unified model demonstrates superior\nperformance in the eleven VRPs, reducing the average gap to around 5% from over\n20% in the existing approach and achieving a significant performance boost on\nbenchmark datasets as well as a real-world logistics application. The source\ncode is included in https://github.com/FeiLiu36/MTNCO.\n","authors":["Fei Liu","Xi Lin","Zhenkun Wang","Qingfu Zhang","Xialiang Tong","Mingxuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2402.16891v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08535v1","updated":"2024-04-12T15:30:03Z","published":"2024-04-12T15:30:03Z","title":"Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking","summary":"  Contrastive learning has gained widespread adoption for retrieval tasks due\nto its minimal requirement for manual annotations. However, popular contrastive\nframeworks typically learn from binary relevance, making them ineffective at\nincorporating direct fine-grained rankings. In this paper, we curate a\nlarge-scale dataset featuring detailed relevance scores for each query-document\npair to facilitate future research and evaluation. Subsequently, we propose\nGeneralized Contrastive Learning for Multi-Modal Retrieval and Ranking (GCL),\nwhich is designed to learn from fine-grained rankings beyond binary relevance\nscores. Our results show that GCL achieves a 94.5% increase in NDCG@10 for\nin-domain and 26.3 to 48.8% increases for cold-start evaluations, all relative\nto the CLIP baseline and involving ground truth rankings.\n","authors":["Tianyu Zhu","Myong Chol Jung","Jesse Clark"],"pdf_url":"https://arxiv.org/pdf/2404.08535v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08523v1","updated":"2024-04-12T15:10:57Z","published":"2024-04-12T15:10:57Z","title":"Advancing Forest Fire Prevention: Deep Reinforcement Learning for\n  Effective Firebreak Placement","summary":"  Over the past decades, the increase in both frequency and intensity of\nlarge-scale wildfires due to climate change has emerged as a significant\nnatural threat. The pressing need to design resilient landscapes capable of\nwithstanding such disasters has become paramount, requiring the development of\nadvanced decision-support tools. Existing methodologies, including Mixed\nInteger Programming, Stochastic Optimization, and Network Theory, have proven\neffective but are hindered by computational demands, limiting their\napplicability.\n  In response to this challenge, we propose using artificial intelligence\ntechniques, specifically Deep Reinforcement Learning, to address the complex\nproblem of firebreak placement in the landscape. We employ value-function based\napproaches like Deep Q-Learning, Double Deep Q-Learning, and Dueling Double\nDeep Q-Learning. Utilizing the Cell2Fire fire spread simulator combined with\nConvolutional Neural Networks, we have successfully implemented a computational\nagent capable of learning firebreak locations within a forest environment,\nachieving good results.\n  Furthermore, we incorporate a pre-training loop, initially teaching our agent\nto mimic a heuristic-based algorithm and observe that it consistently exceeds\nthe performance of these solutions. Our findings underscore the immense\npotential of Deep Reinforcement Learning for operational research challenges,\nespecially in fire prevention. Our approach demonstrates convergence with\nhighly favorable results in problem instances as large as 40 x 40 cells,\nmarking a significant milestone in applying Reinforcement Learning to this\ncritical issue.\n  To the best of our knowledge, this study represents a pioneering effort in\nusing Reinforcement Learning to address the aforementioned problem, offering\npromising perspectives in fire prevention and landscape management\n","authors":["Lucas Murray","Tatiana Castillo","Jaime Carrasco","Andr√©s Weintraub","Richard Weber","Isaac Mart√≠n de Diego","Jos√© Ram√≥n Gonz√°lez","Jordi Garc√≠a-Gonzalo"],"pdf_url":"https://arxiv.org/pdf/2404.08523v1.pdf","comment":"20 pages, 15 figures"},{"id":"http://arxiv.org/abs/2404.06407v2","updated":"2024-04-12T15:02:15Z","published":"2024-04-09T15:54:16Z","title":"Rethinking How to Evaluate Language Model Jailbreak","summary":"  Large language models (LLMs) have become increasingly integrated with various\napplications. To ensure that LLMs do not generate unsafe responses, they are\naligned with safeguards that specify what content is restricted. However, such\nalignment can be bypassed to produce prohibited content using a technique\ncommonly referred to as jailbreak. Different systems have been proposed to\nperform the jailbreak automatically. These systems rely on evaluation methods\nto determine whether a jailbreak attempt is successful. However, our analysis\nreveals that current jailbreak evaluation methods have two limitations. (1)\nTheir objectives lack clarity and do not align with the goal of identifying\nunsafe responses. (2) They oversimplify the jailbreak result as a binary\noutcome, successful or not. In this paper, we propose three metrics, safeguard\nviolation, informativeness, and relative truthfulness, to evaluate language\nmodel jailbreak. Additionally, we demonstrate how these metrics correlate with\nthe goal of different malicious actors. To compute these metrics, we introduce\na multifaceted approach that extends the natural language generation evaluation\nmethod after preprocessing the response. We evaluate our metrics on a benchmark\ndataset produced from three malicious intent datasets and three jailbreak\nsystems. The benchmark dataset is labeled by three annotators. We compare our\nmultifaceted approach with three existing jailbreak evaluation methods.\nExperiments demonstrate that our multifaceted evaluation outperforms existing\nmethods, with F1 scores improving on average by 17% compared to existing\nbaselines. Our findings motivate the need to move away from the binary view of\nthe jailbreak problem and incorporate a more comprehensive evaluation to ensure\nthe safety of the language model.\n","authors":["Hongyu Cai","Arjun Arunasalam","Leo Y. Lin","Antonio Bianchi","Z. Berkay Celik"],"pdf_url":"https://arxiv.org/pdf/2404.06407v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08522v1","updated":"2024-04-12T15:02:14Z","published":"2024-04-12T15:02:14Z","title":"Fuxi-DA: A Generalized Deep Learning Data Assimilation Framework for\n  Assimilating Satellite Observations","summary":"  Data assimilation (DA), as an indispensable component within contemporary\nNumerical Weather Prediction (NWP) systems, plays a crucial role in generating\nthe analysis that significantly impacts forecast performance. Nevertheless, the\ndevelopment of an efficient DA system poses significant challenges,\nparticularly in establishing intricate relationships between the background\ndata and the vast amount of multi-source observation data within limited time\nwindows in operational settings. To address these challenges, researchers\ndesign complex pre-processing methods for each observation type, leveraging\napproximate modeling and the power of super-computing clusters to expedite\nsolutions. The emergence of deep learning (DL) models has been a game-changer,\noffering unified multi-modal modeling, enhanced nonlinear representation\ncapabilities, and superior parallelization. These advantages have spurred\nefforts to integrate DL models into various domains of weather modeling.\nRemarkably, DL models have shown promise in matching, even surpassing, the\nforecast accuracy of leading operational NWP models worldwide. This success\nmotivates the exploration of DL-based DA frameworks tailored for weather\nforecasting models. In this study, we introduces FuxiDA, a generalized DL-based\nDA framework for assimilating satellite observations. By assimilating data from\nAdvanced Geosynchronous Radiation Imager (AGRI) aboard Fengyun-4B, FuXi-DA\nconsistently mitigates analysis errors and significantly improves forecast\nperformance. Furthermore, through a series of single-observation experiments,\nFuxi-DA has been validated against established atmospheric physics,\ndemonstrating its consistency and reliability.\n","authors":["Xiaoze Xu","Xiuyu Sun","Wei Han","Xiaohui Zhong","Lei Chen","Hao Li"],"pdf_url":"https://arxiv.org/pdf/2404.08522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08517v1","updated":"2024-04-12T14:55:16Z","published":"2024-04-12T14:55:16Z","title":"Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path\n  Forward","summary":"  While Large Language Models (LLMs) have seen widespread applications across\nnumerous fields, their limited interpretability poses concerns regarding their\nsafe operations from multiple aspects, e.g., truthfulness, robustness, and\nfairness. Recent research has started developing quality assurance methods for\nLLMs, introducing techniques such as offline detector-based or uncertainty\nestimation methods. However, these approaches predominantly concentrate on\npost-generation analysis, leaving the online safety analysis for LLMs during\nthe generation phase an unexplored area. To bridge this gap, we conduct in this\nwork a comprehensive evaluation of the effectiveness of existing online safety\nanalysis methods on LLMs. We begin with a pilot study that validates the\nfeasibility of detecting unsafe outputs in the early generation process.\nFollowing this, we establish the first publicly available benchmark of online\nsafety analysis for LLMs, including a broad spectrum of methods, models, tasks,\ndatasets, and evaluation metrics. Utilizing this benchmark, we extensively\nanalyze the performance of state-of-the-art online safety analysis methods on\nboth open-source and closed-source LLMs. This analysis reveals the strengths\nand weaknesses of individual methods and offers valuable insights into\nselecting the most appropriate method based on specific application scenarios\nand task requirements. Furthermore, we also explore the potential of using\nhybridization methods, i.e., combining multiple methods to derive a collective\nsafety conclusion, to enhance the efficacy of online safety analysis for LLMs.\nOur findings indicate a promising direction for the development of innovative\nand trustworthy quality assurance methodologies for LLMs, facilitating their\nreliable deployments across diverse domains.\n","authors":["Xuan Xie","Jiayang Song","Zhehua Zhou","Yuheng Huang","Da Song","Lei Ma"],"pdf_url":"https://arxiv.org/pdf/2404.08517v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08513v1","updated":"2024-04-12T14:53:36Z","published":"2024-04-12T14:53:36Z","title":"Adversarial Imitation Learning via Boosting","summary":"  Adversarial imitation learning (AIL) has stood out as a dominant framework\nacross various imitation learning (IL) applications, with Discriminator Actor\nCritic (DAC) (Kostrikov et al.,, 2019) demonstrating the effectiveness of\noff-policy learning algorithms in improving sample efficiency and scalability\nto higher-dimensional observations. Despite DAC's empirical success, the\noriginal AIL objective is on-policy and DAC's ad-hoc application of off-policy\ntraining does not guarantee successful imitation (Kostrikov et al., 2019;\n2020). Follow-up work such as ValueDICE (Kostrikov et al., 2020) tackles this\nissue by deriving a fully off-policy AIL objective. Instead in this work, we\ndevelop a novel and principled AIL algorithm via the framework of boosting.\nLike boosting, our new algorithm, AILBoost, maintains an ensemble of properly\nweighted weak learners (i.e., policies) and trains a discriminator that\nwitnesses the maximum discrepancy between the distributions of the ensemble and\nthe expert policy. We maintain a weighted replay buffer to represent the\nstate-action distribution induced by the ensemble, allowing us to train\ndiscriminators using the entire data collected so far. In the weighted replay\nbuffer, the contribution of the data from older policies are properly\ndiscounted with the weight computed based on the boosting framework.\nEmpirically, we evaluate our algorithm on both controller state-based and\npixel-based environments from the DeepMind Control Suite. AILBoost outperforms\nDAC on both types of environments, demonstrating the benefit of properly\nweighting replay buffer data for off-policy training. On state-based\nenvironments, DAC outperforms ValueDICE and IQ-Learn (Gary et al., 2021),\nachieving competitive performance with as little as one expert trajectory.\n","authors":["Jonathan D. Chang","Dhruv Sreenivas","Yingbing Huang","Kiant√© Brantley","Wen Sun"],"pdf_url":"https://arxiv.org/pdf/2404.08513v1.pdf","comment":"19 pages, 7 figures, 4 tables, 3 algorithms, ICLR 2024"},{"id":"http://arxiv.org/abs/2212.10564v3","updated":"2024-04-12T14:53:30Z","published":"2022-12-20T18:59:50Z","title":"Re-evaluating the Need for Multimodal Signals in Unsupervised Grammar\n  Induction","summary":"  Are multimodal inputs necessary for grammar induction? Recent work has shown\nthat multimodal training inputs can improve grammar induction. However, these\nimprovements are based on comparisons to weak text-only baselines that were\ntrained on relatively little textual data. To determine whether multimodal\ninputs are needed in regimes with large amounts of textual training data, we\ndesign a stronger text-only baseline, which we refer to as LC-PCFG. LC-PCFG is\na C-PFCG that incorporates em-beddings from text-only large language models\n(LLMs). We use a fixed grammar family to directly compare LC-PCFG to various\nmulti-modal grammar induction methods. We compare performance on four benchmark\ndatasets. LC-PCFG provides an up to 17% relative improvement in Corpus-F1\ncompared to state-of-the-art multimodal grammar induction methods. LC-PCFG is\nalso more computationally efficient, providing an up to 85% reduction in\nparameter count and 8.8x reduction in training time compared to multimodal\napproaches. These results suggest that multimodal inputs may not be necessary\nfor grammar induction, and emphasize the importance of strong vision-free\nbaselines for evaluating the benefit of multimodal approaches.\n","authors":["Boyi Li","Rodolfo Corona","Karttikeya Mangalam","Catherine Chen","Daniel Flaherty","Serge Belongie","Kilian Q. Weinberger","Jitendra Malik","Trevor Darrell","Dan Klein"],"pdf_url":"https://arxiv.org/pdf/2212.10564v3.pdf","comment":"NAACL Findings 2024"},{"id":"http://arxiv.org/abs/2211.06410v2","updated":"2024-04-12T14:51:32Z","published":"2022-11-11T18:50:34Z","title":"RFFNet: Large-Scale Interpretable Kernel Methods via Random Fourier\n  Features","summary":"  Kernel methods provide a flexible and theoretically grounded approach to\nnonlinear and nonparametric learning. While memory and run-time requirements\nhinder their applicability to large datasets, many low-rank kernel\napproximations, such as random Fourier features, were recently developed to\nscale up such kernel methods. However, these scalable approaches are based on\napproximations of isotropic kernels, which cannot remove the influence of\nirrelevant features. In this work, we design random Fourier features for a\nfamily of automatic relevance determination (ARD) kernels, and introduce\nRFFNet, a new large-scale kernel method that learns the kernel relevances' on\nthe fly via first-order stochastic optimization. We present an effective\ninitialization scheme for the method's non-convex objective function, evaluate\nif hard-thresholding RFFNet's learned relevances yield a sensible rule for\nvariable selection, and perform an extensive ablation study of RFFNet's\ncomponents. Numerical validation on simulated and real-world data shows that\nour approach has a small memory footprint and run-time, achieves low prediction\nerror, and effectively identifies relevant features, thus leading to more\ninterpretable solutions. We supply users with an efficient, PyTorch-based\nlibrary, that adheres to the scikit-learn standard API and code for fully\nreproducing our results.\n","authors":["Mateus P. Otto","Rafael Izbicki"],"pdf_url":"https://arxiv.org/pdf/2211.06410v2.pdf","comment":"New datasets, ablation studies, and discussion of method's\n  components. 45 pages, 11 figures"},{"id":"http://arxiv.org/abs/2404.08509v1","updated":"2024-04-12T14:46:15Z","published":"2024-04-12T14:46:15Z","title":"Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\n  Prediction","summary":"  Large language models (LLMs) have been driving a new wave of interactive AI\napplications across numerous domains. However, efficiently serving LLM\ninference requests is challenging due to their unpredictable execution times\noriginating from the autoregressive nature of generative models. Existing LLM\nserving systems exploit first-come-first-serve (FCFS) scheduling, suffering\nfrom head-of-line blocking issues. To address the non-deterministic nature of\nLLMs and enable efficient interactive LLM serving, we present a speculative\nshortest-job-first (SSJF) scheduler that uses a light proxy model to predict\nLLM output sequence lengths. Our open-source SSJF implementation does not\nrequire changes to memory management or batching strategies. Evaluations on\nreal-world datasets and production workload traces show that SSJF reduces\naverage job completion times by 30.5-39.6% and increases throughput by 2.2-3.6x\ncompared to FCFS schedulers, across no batching, dynamic batching, and\ncontinuous batching settings.\n","authors":["Haoran Qiu","Weichao Mao","Archit Patke","Shengkun Cui","Saurabh Jha","Chen Wang","Hubertus Franke","Zbigniew T. Kalbarczyk","Tamer Ba≈üar","Ravishankar K. Iyer"],"pdf_url":"https://arxiv.org/pdf/2404.08509v1.pdf","comment":"Accepted at AIOps'24"},{"id":"http://arxiv.org/abs/2306.00602v2","updated":"2024-04-12T14:45:07Z","published":"2023-06-01T12:18:58Z","title":"Approximate Stein Classes for Truncated Density Estimation","summary":"  Estimating truncated density models is difficult, as these models have\nintractable normalising constants and hard to satisfy boundary conditions.\nScore matching can be adapted to solve the truncated density estimation\nproblem, but requires a continuous weighting function which takes zero at the\nboundary and is positive elsewhere. Evaluation of such a weighting function\n(and its gradient) often requires a closed-form expression of the truncation\nboundary and finding a solution to a complicated optimisation problem. In this\npaper, we propose approximate Stein classes, which in turn leads to a relaxed\nStein identity for truncated density estimation. We develop a novel discrepancy\nmeasure, truncated kernelised Stein discrepancy (TKSD), which does not require\nfixing a weighting function in advance, and can be evaluated using only samples\non the boundary. We estimate a truncated density model by minimising the\nLagrangian dual of TKSD. Finally, experiments show the accuracy of our method\nto be an improvement over previous works even without the explicit functional\nform of the boundary.\n","authors":["Daniel J. Williams","Song Liu"],"pdf_url":"https://arxiv.org/pdf/2306.00602v2.pdf","comment":"Accepted to ICML 2023"},{"id":"http://arxiv.org/abs/2401.03785v2","updated":"2024-04-12T14:44:04Z","published":"2024-01-08T10:06:52Z","title":"Identifying Important Group of Pixels using Interactions","summary":"  To better understand the behavior of image classifiers, it is useful to\nvisualize the contribution of individual pixels to the model prediction. In\nthis study, we propose a method, MoXI ($\\textbf{Mo}$del e$\\textbf{X}$planation\nby $\\textbf{I}$nteractions), that efficiently and accurately identifies a group\nof pixels with high prediction confidence. The proposed method employs\ngame-theoretic concepts, Shapley values and interactions, taking into account\nthe effects of individual pixels and the cooperative influence of pixels on\nmodel confidence. Theoretical analysis and experiments demonstrate that our\nmethod better identifies the pixels that are highly contributing to the model\noutputs than widely-used visualization by Grad-CAM, Attention rollout, and\nShapley value. While prior studies have suffered from the exponential\ncomputational cost in the computation of Shapley value and interactions, we\nshow that this can be reduced to quadratic cost for our task. The code is\navailable at https://github.com/KosukeSumiyasu/MoXI.\n","authors":["Kosuke Sumiyasu","Kazuhiko Kawamoto","Hiroshi Kera"],"pdf_url":"https://arxiv.org/pdf/2401.03785v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2310.14888v2","updated":"2024-04-12T14:36:18Z","published":"2023-10-23T12:57:03Z","title":"Beyond Bayesian Model Averaging over Paths in Probabilistic Programs\n  with Stochastic Support","summary":"  The posterior in probabilistic programs with stochastic support decomposes as\na weighted sum of the local posterior distributions associated with each\npossible program path. We show that making predictions with this full posterior\nimplicitly performs a Bayesian model averaging (BMA) over paths. This is\npotentially problematic, as BMA weights can be unstable due to model\nmisspecification or inference approximations, leading to sub-optimal\npredictions in turn. To remedy this issue, we propose alternative mechanisms\nfor path weighting: one based on stacking and one based on ideas from\nPAC-Bayes. We show how both can be implemented as a cheap post-processing step\non top of existing inference engines. In our experiments, we find them to be\nmore robust and lead to better predictions compared to the default BMA weights.\n","authors":["Tim Reichelt","Luke Ong","Tom Rainforth"],"pdf_url":"https://arxiv.org/pdf/2310.14888v2.pdf","comment":"Accepted at the 27th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2024"},{"id":"http://arxiv.org/abs/2308.14142v2","updated":"2024-04-12T14:31:51Z","published":"2023-08-27T15:44:28Z","title":"Integrated Variational Fourier Features for Fast Spatial Modelling with\n  Gaussian Processes","summary":"  Sparse variational approximations are popular methods for scaling up\ninference and learning in Gaussian processes to larger datasets. For $N$\ntraining points, exact inference has $O(N^3)$ cost; with $M \\ll N$ features,\nstate of the art sparse variational methods have $O(NM^2)$ cost. Recently,\nmethods have been proposed using more sophisticated features; these promise\n$O(M^3)$ cost, with good performance in low dimensional tasks such as spatial\nmodelling, but they only work with a very limited class of kernels, excluding\nsome of the most commonly used. In this work, we propose integrated Fourier\nfeatures, which extends these performance benefits to a very broad class of\nstationary covariance functions. We motivate the method and choice of\nparameters from a convergence analysis and empirical exploration, and show\npractical speedup in synthetic and real world spatial regression tasks.\n","authors":["Talay M Cheema","Carl Edward Rasmussen"],"pdf_url":"https://arxiv.org/pdf/2308.14142v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00589v2","updated":"2024-04-12T14:30:10Z","published":"2024-03-31T07:38:39Z","title":"Harnessing the Power of Large Language Model for Uncertainty Aware Graph\n  Processing","summary":"  Handling graph data is one of the most difficult tasks. Traditional\ntechniques, such as those based on geometry and matrix factorization, rely on\nassumptions about the data relations that become inadequate when handling large\nand complex graph data. On the other hand, deep learning approaches demonstrate\npromising results in handling large graph data, but they often fall short of\nproviding interpretable explanations. To equip the graph processing with both\nhigh accuracy and explainability, we introduce a novel approach that harnesses\nthe power of a large language model (LLM), enhanced by an uncertainty-aware\nmodule to provide a confidence score on the generated answer. We experiment\nwith our approach on two graph processing tasks: few-shot knowledge graph\ncompletion and graph classification. Our results demonstrate that through\nparameter efficient fine-tuning, the LLM surpasses state-of-the-art algorithms\nby a substantial margin across ten diverse benchmark datasets. Moreover, to\naddress the challenge of explainability, we propose an uncertainty estimation\nbased on perturbation, along with a calibration scheme to quantify the\nconfidence scores of the generated answers. Our confidence measure achieves an\nAUC of 0.8 or higher on seven out of the ten datasets in predicting the\ncorrectness of the answer generated by LLM.\n","authors":["Zhenyu Qian","Yiming Qian","Yuting Song","Fei Gao","Hai Jin","Chen Yu","Xia Xie"],"pdf_url":"https://arxiv.org/pdf/2404.00589v2.pdf","comment":"Because my organization does not allow members to privately upload\n  papers to arXiv, I am requesting a withdrawal of my submission"},{"id":"http://arxiv.org/abs/2309.02425v2","updated":"2024-04-12T14:28:39Z","published":"2023-09-05T17:53:10Z","title":"On the Minimax Regret in Online Ranking with Top-k Feedback","summary":"  In online ranking, a learning algorithm sequentially ranks a set of items and\nreceives feedback on its ranking in the form of relevance scores. Since\nobtaining relevance scores typically involves human annotation, it is of great\ninterest to consider a partial feedback setting where feedback is restricted to\nthe top-$k$ items in the rankings. Chaudhuri and Tewari [2017] developed a\nframework to analyze online ranking algorithms with top $k$ feedback. A key\nelement in their work was the use of techniques from partial monitoring. In\nthis paper, we further investigate online ranking with top $k$ feedback and\nsolve some open problems posed by Chaudhuri and Tewari [2017]. We provide a\nfull characterization of minimax regret rates with the top $k$ feedback model\nfor all $k$ and for the following ranking performance measures: Pairwise Loss,\nDiscounted Cumulative Gain, and Precision@n. In addition, we give an efficient\nalgorithm that achieves the minimax regret rate for Precision@n.\n","authors":["Mingyuan Zhang","Ambuj Tewari"],"pdf_url":"https://arxiv.org/pdf/2309.02425v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08495v1","updated":"2024-04-12T14:25:49Z","published":"2024-04-12T14:25:49Z","title":"Dataset Reset Policy Optimization for RLHF","summary":"  Reinforcement Learning (RL) from Human Preference-based feedback is a popular\nparadigm for fine-tuning generative models, which has produced impressive\nmodels such as GPT-4 and Claude3 Opus. This framework often consists of two\nsteps: learning a reward model from an offline preference dataset followed by\nrunning online RL to optimize the learned reward model. In this work,\nleveraging the idea of reset, we propose a new RLHF algorithm with provable\nguarantees. Motivated by the fact that offline preference dataset provides\ninformative states (i.e., data that is preferred by the labelers), our new\nalgorithm, Dataset Reset Policy Optimization (DR-PO), integrates the existing\noffline preference dataset into the online policy training procedure via\ndataset reset: it directly resets the policy optimizer to the states in the\noffline dataset, instead of always starting from the initial state\ndistribution. In theory, we show that DR-PO learns to perform at least as good\nas any policy that is covered by the offline dataset under general function\napproximation with finite sample complexity. In experiments, we demonstrate\nthat on both the TL;DR summarization and the Anthropic Helpful Harmful (HH)\ndataset, the generation from DR-PO is better than that from Proximal Policy\nOptimization (PPO) and Direction Preference Optimization (DPO), under the\nmetric of GPT4 win-rate. Code for this work can be found at\nhttps://github.com/Cornell-RL/drpo.\n","authors":["Jonathan D. Chang","Wenhao Shan","Owen Oertell","Kiant√© Brantley","Dipendra Misra","Jason D. Lee","Wen Sun"],"pdf_url":"https://arxiv.org/pdf/2404.08495v1.pdf","comment":"28 pages, 6 tables, 3 Figures, 3 Algorithms"},{"id":"http://arxiv.org/abs/2312.00111v3","updated":"2024-04-12T14:17:34Z","published":"2023-11-30T18:35:29Z","title":"Multimodal Learning for Materials","summary":"  Artificial intelligence is transforming computational materials science,\nimproving the prediction of material properties, and accelerating the discovery\nof novel materials. Recently, publicly available material data repositories\nhave grown rapidly. This growth encompasses not only more materials, but also a\ngreater variety and quantity of their associated properties. Existing machine\nlearning efforts in materials science focus primarily on single-modality tasks,\ni.e., relationships between materials and a single physical property, thus not\ntaking advantage of the rich and multimodal set of material properties. Here,\nwe introduce Multimodal Learning for Materials (MultiMat), which enables\nself-supervised multi-modality training of foundation models for materials. We\ndemonstrate our framework's potential using data from the Materials Project\ndatabase on multiple axes: (i) MultiMat achieves state-of-the-art performance\nfor challenging material property prediction tasks; (ii) MultiMat enables novel\nand accurate material discovery via latent space similarity, enabling screening\nfor stable materials with desired properties; and (iii) MultiMat encodes\ninterpretable emergent features that may provide novel scientific insights.\n","authors":["Viggo Moro","Charlotte Loh","Rumen Dangovski","Ali Ghorashi","Andrew Ma","Zhuo Chen","Samuel Kim","Peter Y. Lu","Thomas Christensen","Marin Soljaƒçiƒá"],"pdf_url":"https://arxiv.org/pdf/2312.00111v3.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2404.08483v1","updated":"2024-04-12T14:03:41Z","published":"2024-04-12T14:03:41Z","title":"Semantic Communication for Cooperative Multi-Task Processing over\n  Wireless Networks","summary":"  In this paper, we have expanded the current status of semantic communication\nlimited to processing one task to a more general system that can handle\nmultiple tasks concurrently. In pursuit of this, we first introduced our\ndefinition of the \"semantic source\", enabling the interpretation of multiple\nsemantics based on a single observation. A semantic encoder design is then\nintroduced, featuring the division of the encoder into a common unit and\nmultiple specific units enabling cooperative multi-task processing. Simulation\nresults demonstrate the effectiveness of the proposed semantic source and the\nsystem design. Our approach employs information maximization (infomax) and\nend-to-end design principles.\n","authors":["Ahmad Halimi Razlighi","Carsten Bockelmann","Armin Dekorsy"],"pdf_url":"https://arxiv.org/pdf/2404.08483v1.pdf","comment":"This work has been submitted to the IEEE Wireless Communications\n  Letters"},{"id":"http://arxiv.org/abs/2310.14423v2","updated":"2024-04-12T13:59:01Z","published":"2023-10-22T21:38:57Z","title":"A Quadratic Synchronization Rule for Distributed Deep Learning","summary":"  In distributed deep learning with data parallelism, synchronizing gradients\nat each training step can cause a huge communication overhead, especially when\nmany nodes work together to train large models. Local gradient methods, such as\nLocal SGD, address this issue by allowing workers to compute locally for $H$\nsteps without synchronizing with others, hence reducing communication\nfrequency. While $H$ has been viewed as a hyperparameter to trade optimization\nefficiency for communication cost, recent research indicates that setting a\nproper $H$ value can lead to generalization improvement. Yet, selecting a\nproper $H$ is elusive. This work proposes a theory-grounded method for\ndetermining $H$, named the Quadratic Synchronization Rule (QSR), which\nrecommends dynamically setting $H$ in proportion to $\\frac{1}{\\eta^2}$ as the\nlearning rate $\\eta$ decays over time. Extensive ImageNet experiments on ResNet\nand ViT show that local gradient methods with QSR consistently improve the test\naccuracy over other synchronization strategies. Compared with the standard data\nparallel training, QSR enables Local AdamW on ViT-B to cut the training time on\n16 or 64 GPUs down from 26.7 to 20.2 hours or from 8.6 to 5.5 hours and, at the\nsame time, achieves $1.16\\%$ or $0.84\\%$ higher top-1 validation accuracy.\n","authors":["Xinran Gu","Kaifeng Lyu","Sanjeev Arora","Jingzhao Zhang","Longbo Huang"],"pdf_url":"https://arxiv.org/pdf/2310.14423v2.pdf","comment":"camera-ready version for ICLR'24"},{"id":"http://arxiv.org/abs/2404.08480v1","updated":"2024-04-12T13:57:30Z","published":"2024-04-12T13:57:30Z","title":"Decoding AI: The inside story of data analysis in ChatGPT","summary":"  As a result of recent advancements in generative AI, the field of Data\nScience is prone to various changes. This review critically examines the Data\nAnalysis (DA) capabilities of ChatGPT assessing its performance across a wide\nrange of tasks. While DA provides researchers and practitioners with\nunprecedented analytical capabilities, it is far from being perfect, and it is\nimportant to recognize and address its limitations.\n","authors":["Ozan Evkaya","Miguel de Carvalho"],"pdf_url":"https://arxiv.org/pdf/2404.08480v1.pdf","comment":"15 pages with figures and appendix"},{"id":"http://arxiv.org/abs/2404.08476v1","updated":"2024-04-12T13:54:21Z","published":"2024-04-12T13:54:21Z","title":"Combining Statistical Depth and Fermat Distance for Uncertainty\n  Quantification","summary":"  We measure the Out-of-domain uncertainty in the prediction of Neural Networks\nusing a statistical notion called ``Lens Depth'' (LD) combined with Fermat\nDistance, which is able to capture precisely the ``depth'' of a point with\nrespect to a distribution in feature space, without any assumption about the\nform of distribution. Our method has no trainable parameter. The method is\napplicable to any classification model as it is applied directly in feature\nspace at test time and does not intervene in training process. As such, it does\nnot impact the performance of the original model. The proposed method gives\nexcellent qualitative result on toy datasets and can give competitive or better\nuncertainty estimation on standard deep learning datasets compared to strong\nbaseline methods.\n","authors":["Hai-Vy Nguyen","Fabrice Gamboa","Reda Chhaibi","Sixin Zhang","Serge Gratton","Thierry Giaccone"],"pdf_url":"https://arxiv.org/pdf/2404.08476v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2404.06834v2","updated":"2024-04-12T13:47:07Z","published":"2024-04-10T08:52:12Z","title":"Solving Parametric PDEs with Radial Basis Functions and Deep Neural\n  Networks","summary":"  We propose the POD-DNN, a novel algorithm leveraging deep neural networks\n(DNNs) along with radial basis functions (RBFs) in the context of the proper\northogonal decomposition (POD) reduced basis method (RBM), aimed at\napproximating the parametric mapping of parametric partial differential\nequations on irregular domains. The POD-DNN algorithm capitalizes on the\nlow-dimensional characteristics of the solution manifold for parametric\nequations, alongside the inherent offline-online computational strategy of RBM\nand DNNs. In numerical experiments, POD-DNN demonstrates significantly\naccelerated computation speeds during the online phase. Compared to other\nalgorithms that utilize RBF without integrating DNNs, POD-DNN substantially\nimproves the computational speed in the online inference process. Furthermore,\nunder reasonable assumptions, we have rigorously derived upper bounds on the\ncomplexity of approximating parametric mappings with POD-DNN, thereby providing\na theoretical analysis of the algorithm's empirical performance.\n","authors":["Guanhang Lei","Zhen Lei","Lei Shi","Chenyu Zeng"],"pdf_url":"https://arxiv.org/pdf/2404.06834v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08472v1","updated":"2024-04-12T13:41:29Z","published":"2024-04-12T13:41:29Z","title":"TSLANet: Rethinking Transformers for Time Series Representation Learning","summary":"  Time series data, characterized by its intrinsic long and short-range\ndependencies, poses a unique challenge across analytical applications. While\nTransformer-based models excel at capturing long-range dependencies, they face\nlimitations in noise sensitivity, computational efficiency, and overfitting\nwith smaller datasets. In response, we introduce a novel Time Series\nLightweight Adaptive Network (TSLANet), as a universal convolutional model for\ndiverse time series tasks. Specifically, we propose an Adaptive Spectral Block,\nharnessing Fourier analysis to enhance feature representation and to capture\nboth long-term and short-term interactions while mitigating noise via adaptive\nthresholding. Additionally, we introduce an Interactive Convolution Block and\nleverage self-supervised learning to refine the capacity of TSLANet for\ndecoding complex temporal patterns and improve its robustness on different\ndatasets. Our comprehensive experiments demonstrate that TSLANet outperforms\nstate-of-the-art models in various tasks spanning classification, forecasting,\nand anomaly detection, showcasing its resilience and adaptability across a\nspectrum of noise levels and data sizes. The code is available at\n\\url{https://github.com/emadeldeen24/TSLANet}\n","authors":["Emadeldeen Eldele","Mohamed Ragab","Zhenghua Chen","Min Wu","Xiaoli Li"],"pdf_url":"https://arxiv.org/pdf/2404.08472v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08461v1","updated":"2024-04-12T13:18:47Z","published":"2024-04-12T13:18:47Z","title":"OTTER: Improving Zero-Shot Classification via Optimal Transport","summary":"  Popular zero-shot models suffer due to artifacts inherited from pretraining.\nA particularly detrimental artifact, caused by unbalanced web-scale pretraining\ndata, is mismatched label distribution. Existing approaches that seek to repair\nthe label distribution are not suitable in zero-shot settings, as they have\nincompatible requirements such as access to labeled downstream task data or\nknowledge of the true label balance in the pretraining distribution. We\nsidestep these challenges and introduce a simple and lightweight approach to\nadjust pretrained model predictions via optimal transport. Our technique\nrequires only an estimate of the label distribution of a downstream task.\nTheoretically, we characterize the improvement produced by our procedure under\ncertain mild conditions and provide bounds on the error caused by\nmisspecification. Empirically, we validate our method in a wide array of\nzero-shot image and text classification tasks, improving accuracy by 4.8% and\n15.9% on average, and beating baselines like Prior Matching -- often by\nsignificant margins -- in 17 out of 21 datasets.\n","authors":["Changho Shin","Jitian Zhao","Sonia Cromp","Harit Vishwakarma","Frederic Sala"],"pdf_url":"https://arxiv.org/pdf/2404.08461v1.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2202.07559v3","updated":"2024-04-12T13:16:54Z","published":"2022-02-15T16:44:21Z","title":"Unsupervised Learning of Group Invariant and Equivariant Representations","summary":"  Equivariant neural networks, whose hidden features transform according to\nrepresentations of a group G acting on the data, exhibit training efficiency\nand an improved generalisation performance. In this work, we extend group\ninvariant and equivariant representation learning to the field of unsupervised\ndeep learning. We propose a general learning strategy based on an\nencoder-decoder framework in which the latent representation is separated in an\ninvariant term and an equivariant group action component. The key idea is that\nthe network learns to encode and decode data to and from a group-invariant\nrepresentation by additionally learning to predict the appropriate group action\nto align input and output pose to solve the reconstruction task. We derive the\nnecessary conditions on the equivariant encoder, and we present a construction\nvalid for any G, both discrete and continuous. We describe explicitly our\nconstruction for rotations, translations and permutations. We test the validity\nand the robustness of our approach in a variety of experiments with diverse\ndata types employing different network architectures.\n","authors":["Robin Winter","Marco Bertolini","Tuan Le","Frank No√©","Djork-Arn√© Clevert"],"pdf_url":"https://arxiv.org/pdf/2202.07559v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08458v1","updated":"2024-04-12T13:09:48Z","published":"2024-04-12T13:09:48Z","title":"On the Independence Assumption in Neurosymbolic Learning","summary":"  State-of-the-art neurosymbolic learning systems use probabilistic reasoning\nto guide neural networks towards predictions that conform to logical\nconstraints over symbols. Many such systems assume that the probabilities of\nthe considered symbols are conditionally independent given the input to\nsimplify learning and reasoning. We study and criticise this assumption,\nhighlighting how it can hinder optimisation and prevent uncertainty\nquantification. We prove that loss functions bias conditionally independent\nneural networks to become overconfident in their predictions. As a result, they\nare unable to represent uncertainty over multiple valid options. Furthermore,\nwe prove that these loss functions are difficult to optimise: they are\nnon-convex, and their minima are usually highly disconnected. Our theoretical\nanalysis gives the foundation for replacing the conditional independence\nassumption and designing more expressive neurosymbolic probabilistic models.\n","authors":["Emile van Krieken","Pasquale Minervini","Edoardo M. Ponti","Antonio Vergari"],"pdf_url":"https://arxiv.org/pdf/2404.08458v1.pdf","comment":"11 pages, 8 appendix pages, 9 figures"},{"id":"http://arxiv.org/abs/2404.08456v1","updated":"2024-04-12T13:05:35Z","published":"2024-04-12T13:05:35Z","title":"A backward differential deep learning-based algorithm for solving\n  high-dimensional nonlinear backward stochastic differential equations","summary":"  In this work, we propose a novel backward differential deep learning-based\nalgorithm for solving high-dimensional nonlinear backward stochastic\ndifferential equations (BSDEs), where the deep neural network (DNN) models are\ntrained not only on the inputs and labels but also the differentials of the\ncorresponding labels. This is motivated by the fact that differential deep\nlearning can provide an efficient approximation of the labels and their\nderivatives with respect to inputs. The BSDEs are reformulated as differential\ndeep learning problems by using Malliavin calculus. The Malliavin derivatives\nof solution to a BSDE satisfy themselves another BSDE, resulting thus in a\nsystem of BSDEs. Such formulation requires the estimation of the solution, its\ngradient, and the Hessian matrix, represented by the triple of processes\n$\\left(Y, Z, \\Gamma\\right).$ All the integrals within this system are\ndiscretized by using the Euler-Maruyama method. Subsequently, DNNs are employed\nto approximate the triple of these unknown processes. The DNN parameters are\nbackwardly optimized at each time step by minimizing a differential learning\ntype loss function, which is defined as a weighted sum of the dynamics of the\ndiscretized BSDE system, with the first term providing the dynamics of the\nprocess $Y$ and the other the process $Z$. An error analysis is carried out to\nshow the convergence of the proposed algorithm. Various numerical experiments\nup to $50$ dimensions are provided to demonstrate the high efficiency. Both\ntheoretically and numerically, it is demonstrated that our proposed scheme is\nmore efficient compared to other contemporary deep learning-based\nmethodologies, especially in the computation of the process $\\Gamma$.\n","authors":["Lorenc Kapllani","Long Teng"],"pdf_url":"https://arxiv.org/pdf/2404.08456v1.pdf","comment":"40 pages, 5 figures, 5 tables"},{"id":"http://arxiv.org/abs/2404.08453v1","updated":"2024-04-12T13:02:33Z","published":"2024-04-12T13:02:33Z","title":"Lightweight Multi-System Multivariate Interconnection and Divergence\n  Discovery","summary":"  Identifying outlier behavior among sensors and subsystems is essential for\ndiscovering faults and facilitating diagnostics in large systems. At the same\ntime, exploring large systems with numerous multivariate data sets is\nchallenging. This study presents a lightweight interconnection and divergence\ndiscovery mechanism (LIDD) to identify abnormal behavior in multi-system\nenvironments. The approach employs a multivariate analysis technique that first\nestimates the similarity heatmaps among the sensors for each system and then\napplies information retrieval algorithms to provide relevant multi-level\ninterconnection and discrepancy details. Our experiment on the readout systems\nof the Hadron Calorimeter of the Compact Muon Solenoid (CMS) experiment at CERN\ndemonstrates the effectiveness of the proposed method. Our approach clusters\nreadout systems and their sensors consistent with the expected calorimeter\ninterconnection configurations, while capturing unusual behavior in divergent\nclusters and estimating their root causes.\n","authors":["Mulugeta Weldezgina Asres","Christian Walter Omlin","Jay Dittmann","Pavel Parygin","Joshua Hiltbrand","Seth I. Cooper","Grace Cummings","David Yu"],"pdf_url":"https://arxiv.org/pdf/2404.08453v1.pdf","comment":"8 pages, 12 figures"},{"id":"http://arxiv.org/abs/2404.08447v1","updated":"2024-04-12T12:57:43Z","published":"2024-04-12T12:57:43Z","title":"Federated Optimization with Doubly Regularized Drift Correction","summary":"  Federated learning is a distributed optimization paradigm that allows\ntraining machine learning models across decentralized devices while keeping the\ndata localized. The standard method, FedAvg, suffers from client drift which\ncan hamper performance and increase communication costs over centralized\nmethods. Previous works proposed various strategies to mitigate drift, yet none\nhave shown uniformly improved communication-computation trade-offs over vanilla\ngradient descent.\n  In this work, we revisit DANE, an established method in distributed\noptimization. We show that (i) DANE can achieve the desired communication\nreduction under Hessian similarity constraints. Furthermore, (ii) we present an\nextension, DANE+, which supports arbitrary inexact local solvers and has more\nfreedom to choose how to aggregate the local updates. We propose (iii) a novel\nmethod, FedRed, which has improved local computational complexity and retains\nthe same communication complexity compared to DANE/DANE+. This is achieved by\nusing doubly regularized drift correction.\n","authors":["Xiaowen Jiang","Anton Rodomanov","Sebastian U. Stich"],"pdf_url":"https://arxiv.org/pdf/2404.08447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11321v3","updated":"2024-04-12T12:57:40Z","published":"2023-11-19T13:31:30Z","title":"Bounds on Representation-Induced Confounding Bias for Treatment Effect\n  Estimation","summary":"  State-of-the-art methods for conditional average treatment effect (CATE)\nestimation make widespread use of representation learning. Here, the idea is to\nreduce the variance of the low-sample CATE estimation by a (potentially\nconstrained) low-dimensional representation. However, low-dimensional\nrepresentations can lose information about the observed confounders and thus\nlead to bias, because of which the validity of representation learning for CATE\nestimation is typically violated. In this paper, we propose a new,\nrepresentation-agnostic refutation framework for estimating bounds on the\nrepresentation-induced confounding bias that comes from dimensionality\nreduction (or other constraints on the representations) in CATE estimation.\nFirst, we establish theoretically under which conditions CATE is\nnon-identifiable given low-dimensional (constrained) representations. Second,\nas our remedy, we propose a neural refutation framework which performs partial\nidentification of CATE or, equivalently, aims at estimating lower and upper\nbounds of the representation-induced confounding bias. We demonstrate the\neffectiveness of our bounds in a series of experiments. In sum, our refutation\nframework is of direct relevance in practice where the validity of CATE\nestimation is of importance.\n","authors":["Valentyn Melnychuk","Dennis Frauen","Stefan Feuerriegel"],"pdf_url":"https://arxiv.org/pdf/2311.11321v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08444v1","updated":"2024-04-12T12:56:16Z","published":"2024-04-12T12:56:16Z","title":"Anti-Byzantine Attacks Enabled Vehicle Selection for Asynchronous\n  Federated Learning in Vehicular Edge Computing","summary":"  In vehicle edge computing (VEC), asynchronous federated learning (AFL) is\nused, where the edge receives a local model and updates the global model,\neffectively reducing the global aggregation latency.Due to different amounts of\nlocal data,computing capabilities and locations of the vehicles, renewing the\nglobal model with same weight is inappropriate.The above factors will affect\nthe local calculation time and upload time of the local model, and the vehicle\nmay also be affected by Byzantine attacks, leading to the deterioration of the\nvehicle data. However, based on deep reinforcement learning (DRL), we can\nconsider these factors comprehensively to eliminate vehicles with poor\nperformance as much as possible and exclude vehicles that have suffered\nByzantine attacks before AFL. At the same time, when aggregating AFL, we can\nfocus on those vehicles with better performance to improve the accuracy and\nsafety of the system. In this paper, we proposed a vehicle selection scheme\nbased on DRL in VEC. In this scheme, vehicle s mobility, channel conditions\nwith temporal variations, computational resources with temporal variations,\ndifferent data amount, transmission channel status of vehicles as well as\nByzantine attacks were taken into account.Simulation results show that the\nproposed scheme effectively improves the safety and accuracy of the global\nmodel.\n","authors":["Cui Zhang","Xiao Xu","Qiong Wu","Pingyi Fan","Qiang Fan","Huiling Zhu","Jiangzhou Wang"],"pdf_url":"https://arxiv.org/pdf/2404.08444v1.pdf","comment":"This paper has been accepted by China Communications.The source code\n  has been released at:https://github.com/giongwu86/By-AFLDDPG"},{"id":"http://arxiv.org/abs/2404.07817v2","updated":"2024-04-12T12:33:26Z","published":"2024-04-11T14:59:49Z","title":"Calibration of Continual Learning Models","summary":"  Continual Learning (CL) focuses on maximizing the predictive performance of a\nmodel across a non-stationary stream of data. Unfortunately, CL models tend to\nforget previous knowledge, thus often underperforming when compared with an\noffline model trained jointly on the entire data stream. Given that any CL\nmodel will eventually make mistakes, it is of crucial importance to build\ncalibrated CL models: models that can reliably tell their confidence when\nmaking a prediction. Model calibration is an active research topic in machine\nlearning, yet to be properly investigated in CL. We provide the first empirical\nstudy of the behavior of calibration approaches in CL, showing that CL\nstrategies do not inherently learn calibrated models. To mitigate this issue,\nwe design a continual calibration approach that improves the performance of\npost-processing calibration methods over a wide range of different benchmarks\nand CL strategies. CL does not necessarily need perfect predictive models, but\nrather it can benefit from reliable predictive models. We believe our study on\ncontinual calibration represents a first step towards this direction.\n","authors":["Lanpei Li","Elia Piccoli","Andrea Cossu","Davide Bacciu","Vincenzo Lomonaco"],"pdf_url":"https://arxiv.org/pdf/2404.07817v2.pdf","comment":"Accepted at CLVISION workshop, CVPR 2024"},{"id":"http://arxiv.org/abs/2404.08434v1","updated":"2024-04-12T12:31:06Z","published":"2024-04-12T12:31:06Z","title":"An improved tabular data generator with VAE-GMM integration","summary":"  The rising use of machine learning in various fields requires robust methods\nto create synthetic tabular data. Data should preserve key characteristics\nwhile addressing data scarcity challenges. Current approaches based on\nGenerative Adversarial Networks, such as the state-of-the-art CTGAN model,\nstruggle with the complex structures inherent in tabular data. These data often\ncontain both continuous and discrete features with non-Gaussian distributions.\nTherefore, we propose a novel Variational Autoencoder (VAE)-based model that\naddresses these limitations. Inspired by the TVAE model, our approach\nincorporates a Bayesian Gaussian Mixture model (BGM) within the VAE\narchitecture. This avoids the limitations imposed by assuming a strictly\nGaussian latent space, allowing for a more accurate representation of the\nunderlying data distribution during data generation. Furthermore, our model\noffers enhanced flexibility by allowing the use of various differentiable\ndistributions for individual features, making it possible to handle both\ncontinuous and discrete data types. We thoroughly validate our model on three\nreal-world datasets with mixed data types, including two medically relevant\nones, based on their resemblance and utility. This evaluation demonstrates\nsignificant outperformance against CTGAN and TVAE, establishing its potential\nas a valuable tool for generating synthetic tabular data in various domains,\nparticularly in healthcare.\n","authors":["Patricia A. Apell√°niz","Juan Parras","Santiago Zazo"],"pdf_url":"https://arxiv.org/pdf/2404.08434v1.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2311.09266v2","updated":"2024-04-12T12:18:19Z","published":"2023-11-15T08:33:46Z","title":"Adversarially Robust Spiking Neural Networks Through Conversion","summary":"  Spiking neural networks (SNNs) provide an energy-efficient alternative to a\nvariety of artificial neural network (ANN) based AI applications. As the\nprogress in neuromorphic computing with SNNs expands their use in applications,\nthe problem of adversarial robustness of SNNs becomes more pronounced. To the\ncontrary of the widely explored end-to-end adversarial training based\nsolutions, we address the limited progress in scalable robust SNN training\nmethods by proposing an adversarially robust ANN-to-SNN conversion algorithm.\nOur method provides an efficient approach to embrace various computationally\ndemanding robust learning objectives that have been proposed for ANNs. During a\npost-conversion robust finetuning phase, our method adversarially optimizes\nboth layer-wise firing thresholds and synaptic connectivity weights of the SNN\nto maintain transferred robustness gains from the pre-trained ANN. We perform\nexperimental evaluations in a novel setting proposed to rigorously assess the\nrobustness of SNNs, where numerous adaptive adversarial attacks that account\nfor the spike-based operation dynamics are considered. Results show that our\napproach yields a scalable state-of-the-art solution for adversarially robust\ndeep SNNs with low-latency.\n","authors":["Ozan √ñzdenizci","Robert Legenstein"],"pdf_url":"https://arxiv.org/pdf/2311.09266v2.pdf","comment":"Transactions on Machine Learning Research (TMLR), 2024"},{"id":"http://arxiv.org/abs/2404.08423v1","updated":"2024-04-12T12:11:51Z","published":"2024-04-12T12:11:51Z","title":"SIR-RL: Reinforcement Learning for Optimized Policy Control during\n  Epidemiological Outbreaks in Emerging Market and Developing Economies","summary":"  The outbreak of COVID-19 has highlighted the intricate interplay between\npublic health and economic stability on a global scale. This study proposes a\nnovel reinforcement learning framework designed to optimize health and economic\noutcomes during pandemics. The framework leverages the SIR model, integrating\nboth lockdown measures (via a stringency index) and vaccination strategies to\nsimulate disease dynamics. The stringency index, indicative of the severity of\nlockdown measures, influences both the spread of the disease and the economic\nhealth of a country. Developing nations, which bear a disproportionate economic\nburden under stringent lockdowns, are the primary focus of our study. By\nimplementing reinforcement learning, we aim to optimize governmental responses\nand strike a balance between the competing costs associated with public health\nand economic stability. This approach also enhances transparency in\ngovernmental decision-making by establishing a well-defined reward function for\nthe reinforcement learning agent. In essence, this study introduces an\ninnovative and ethical strategy to navigate the challenge of balancing public\nhealth and economic stability amidst infectious disease outbreaks.\n","authors":["Maeghal Jain","Ziya Uddin","Wubshet Ibrahim"],"pdf_url":"https://arxiv.org/pdf/2404.08423v1.pdf","comment":"27 pages, 12 figures"},{"id":"http://arxiv.org/abs/2404.08417v1","updated":"2024-04-12T12:06:02Z","published":"2024-04-12T12:06:02Z","title":"AdapterSwap: Continuous Training of LLMs with Data Removal and\n  Access-Control Guarantees","summary":"  Large language models (LLMs) are increasingly capable of completing knowledge\nintensive tasks by recalling information from a static pretraining corpus. Here\nwe are concerned with LLMs in the context of evolving data requirements. For\ninstance: batches of new data that are introduced periodically; subsets of data\nwith user-based access controls; or requirements on dynamic removal of\ndocuments with guarantees that associated knowledge cannot be recalled. We wish\nto satisfy these requirements while at the same time ensuring a model does not\nforget old information when new data becomes available. To address these\nissues, we introduce AdapterSwap, a training and inference scheme that\norganizes knowledge from a data collection into a set of low-rank adapters,\nwhich are dynamically composed during inference. Our experiments demonstrate\nAdapterSwap's ability to support efficient continual learning, while also\nenabling organizations to have fine-grained control over data access and\ndeletion.\n","authors":["William Fleshman","Aleem Khan","Marc Marone","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2404.08417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.11133v2","updated":"2024-04-12T12:05:57Z","published":"2023-07-07T11:49:55Z","title":"Contrastive Graph Pooling for Explainable Classification of Brain\n  Networks","summary":"  Functional magnetic resonance imaging (fMRI) is a commonly used technique to\nmeasure neural activation. Its application has been particularly important in\nidentifying underlying neurodegenerative conditions such as Parkinson's,\nAlzheimer's, and Autism. Recent analysis of fMRI data models the brain as a\ngraph and extracts features by graph neural networks (GNNs). However, the\nunique characteristics of fMRI data require a special design of GNN. Tailoring\nGNN to generate effective and domain-explainable features remains challenging.\nIn this paper, we propose a contrastive dual-attention block and a\ndifferentiable graph pooling method called ContrastPool to better utilize GNN\nfor brain networks, meeting fMRI-specific requirements. We apply our method to\n5 resting-state fMRI brain network datasets of 3 diseases and demonstrate its\nsuperiority over state-of-the-art baselines. Our case study confirms that the\npatterns extracted by our method match the domain knowledge in neuroscience\nliterature, and disclose direct and interesting insights. Our contributions\nunderscore the potential of ContrastPool for advancing the understanding of\nbrain networks and neurodegenerative conditions. The source code is available\nat https://github.com/AngusMonroe/ContrastPool.\n","authors":["Jiaxing Xu","Qingtian Bian","Xinhang Li","Aihu Zhang","Yiping Ke","Miao Qiao","Wei Zhang","Wei Khang Jeremy Sim","Bal√°zs Guly√°s"],"pdf_url":"https://arxiv.org/pdf/2307.11133v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06137v2","updated":"2024-04-12T11:51:29Z","published":"2023-03-10T18:55:02Z","title":"Enhancing MAP-Elites with Multiple Parallel Evolution Strategies","summary":"  With the development of fast and massively parallel evaluations in many\ndomains, Quality-Diversity (QD) algorithms, that already proved promising in a\nlarge range of applications, have seen their potential multiplied. However, we\nhave yet to understand how to best use a large number of evaluations as using\nthem for random variations alone is not always effective. High-dimensional\nsearch spaces are a typical situation where random variations struggle to\neffectively search. Another situation is uncertain settings where solutions can\nappear better than they truly are and naively evaluating more solutions might\nmislead QD algorithms. In this work, we propose MAP-Elites-Multi-ES (MEMES), a\nnovel QD algorithm based on Evolution Strategies (ES) designed to exploit fast\nparallel evaluations more effectively. MEMES maintains multiple (up to 100)\nsimultaneous ES processes, each with its own independent objective and reset\nmechanism designed for QD optimisation, all on just a single GPU. We show that\nMEMES outperforms both gradient-based and mutation-based QD algorithms on\nblack-box optimisation and QD-Reinforcement-Learning tasks, demonstrating its\nbenefit across domains. Additionally, our approach outperforms sampling-based\nQD methods in uncertain domains when given the same evaluation budget. Overall,\nMEMES generates reproducible solutions that are high-performing and diverse\nthrough large-scale ES optimisation on easily accessible hardware.\n","authors":["Manon Flageat","Bryan Lim","Antoine Cully"],"pdf_url":"https://arxiv.org/pdf/2303.06137v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.02090v4","updated":"2024-04-12T11:50:26Z","published":"2023-06-03T11:45:16Z","title":"Deep Classifier Mimicry without Data Access","summary":"  Access to pre-trained models has recently emerged as a standard across\nnumerous machine learning domains. Unfortunately, access to the original data\nthe models were trained on may not equally be granted. This makes it\ntremendously challenging to fine-tune, compress models, adapt continually, or\nto do any other type of data-driven update. We posit that original data access\nmay however not be required. Specifically, we propose Contrastive Abductive\nKnowledge Extraction (CAKE), a model-agnostic knowledge distillation procedure\nthat mimics deep classifiers without access to the original data. To this end,\nCAKE generates pairs of noisy synthetic samples and diffuses them contrastively\ntoward a model's decision boundary. We empirically corroborate CAKE's\neffectiveness using several benchmark datasets and various architectural\nchoices, paving the way for broad application.\n","authors":["Steven Braun","Martin Mundt","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2306.02090v4.pdf","comment":"11 pages main, 4 figures, 2 tables, 4 pages appendix"},{"id":"http://arxiv.org/abs/2307.08509v3","updated":"2024-04-12T11:48:03Z","published":"2023-07-17T14:10:01Z","title":"Kernel-Based Testing for Single-Cell Differential Analysis","summary":"  Single-cell technologies offer insights into molecular feature distributions,\nbut comparing them poses challenges. We propose a kernel-testing framework for\nnon-linear cell-wise distribution comparison, analyzing gene expression and\nepigenomic modifications. Our method allows feature-wise and global\ntranscriptome/epigenome comparisons, revealing cell population heterogeneities.\nUsing a classifier based on embedding variability, we identify transitions in\ncell states, overcoming limitations of traditional single-cell analysis.\nApplied to single-cell ChIP-Seq data, our approach identifies untreated breast\ncancer cells with an epigenomic profile resembling persister cells. This\ndemonstrates the effectiveness of kernel testing in uncovering subtle\npopulation variations that might be missed by other methods.\n","authors":["Anthony Ozier-Lafontaine","Camille Fourneaux","Ghislain Durif","Polina Arsenteva","C√©line Vallot","Olivier Gandrillon","Sandrine Giraud","Bertrand Michel","Franck Picard"],"pdf_url":"https://arxiv.org/pdf/2307.08509v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02572v2","updated":"2024-04-12T11:43:07Z","published":"2024-04-03T08:47:32Z","title":"Incremental Learning with Concept Drift Detection and Prototype-based\n  Embeddings for Graph Stream Classification","summary":"  Data stream mining aims at extracting meaningful knowledge from continually\nevolving data streams, addressing the challenges posed by nonstationary\nenvironments, particularly, concept drift which refers to a change in the\nunderlying data distribution over time. Graph structures offer a powerful\nmodelling tool to represent complex systems, such as, critical infrastructure\nsystems and social networks. Learning from graph streams becomes a necessity to\nunderstand the dynamics of graph structures and to facilitate informed\ndecision-making. This work introduces a novel method for graph stream\nclassification which operates under the general setting where a data generating\nprocess produces graphs with varying nodes and edges over time. The method uses\nincremental learning for continual model adaptation, selecting representative\ngraphs (prototypes) for each class, and creating graph embeddings.\nAdditionally, it incorporates a loss-based concept drift detection mechanism to\nrecalculate graph prototypes when drift is detected.\n","authors":["Kleanthis Malialis","Jin Li","Christos G. Panayiotou","Marios M. Polycarpou"],"pdf_url":"https://arxiv.org/pdf/2404.02572v2.pdf","comment":"IEEE World Congress on Computational Intelligence (WCCI) 2024;\n  Keywords: graph streams, concept drift, incremental learning, graph\n  prototypes, nonstationary environments"},{"id":"http://arxiv.org/abs/2402.16814v3","updated":"2024-04-12T11:38:20Z","published":"2024-02-26T18:37:16Z","title":"Box Facets and Cut Facets of Lifted Multicut Polytopes","summary":"  The lifted multicut problem is a combinatorial optimization problem whose\nfeasible solutions relate one-to-one to the decompositions of a graph $G = (V,\nE)$. Given an augmentation $\\widehat{G} = (V, E \\cup F)$ of $G$ and given costs\n$c \\in \\mathbb{R}^{E \\cup F}$, the objective is to minimize the sum of those\n$c_{uw}$ with $uw \\in E \\cup F$ for which $u$ and $w$ are in distinct\ncomponents. For $F = \\emptyset$, the problem specializes to the multicut\nproblem, and for $E = \\tbinom{V}{2}$ to the clique partitioning problem. We\nstudy a binary linear program formulation of the lifted multicut problem. More\nspecifically, we contribute to the analysis of the associated lifted multicut\npolytopes: Firstly, we establish a necessary, sufficient and efficiently\ndecidable condition for a lower box inequality to define a facet. Secondly, we\nshow that deciding whether a cut inequality of the binary linear program\ndefines a facet is NP-hard.\n","authors":["Lucas Fabian Naumann","Jannik Irmai","Shengxian Zhao","Bjoern Andres"],"pdf_url":"https://arxiv.org/pdf/2402.16814v3.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2404.08408v1","updated":"2024-04-12T11:36:24Z","published":"2024-04-12T11:36:24Z","title":"Seismic First Break Picking in a Higher Dimension Using Deep Graph\n  Learning","summary":"  Contemporary automatic first break (FB) picking methods typically analyze 1D\nsignals, 2D source gathers, or 3D source-receiver gathers. Utilizing\nhigher-dimensional data, such as 2D or 3D, incorporates global features,\nimproving the stability of local picking. Despite the benefits,\nhigh-dimensional data requires structured input and increases computational\ndemands. Addressing this, we propose a novel approach using deep graph learning\ncalled DGL-FB, constructing a large graph to efficiently extract information.\nIn this graph, each seismic trace is represented as a node, connected by edges\nthat reflect similarities. To manage the size of the graph, we develop a\nsubgraph sampling technique to streamline model training and inference. Our\nproposed framework, DGL-FB, leverages deep graph learning for FB picking. It\nencodes subgraphs into global features using a deep graph encoder.\nSubsequently, the encoded global features are combined with local node signals\nand fed into a ResUNet-based 1D segmentation network for FB detection. Field\nsurvey evaluations of DGL-FB show superior accuracy and stability compared to a\n2D U-Net-based benchmark method.\n","authors":["Hongtao Wang","Li Long","Jiangshe Zhang","Xiaoli Wei","Chunxia Zhang","Zhenbo Guo"],"pdf_url":"https://arxiv.org/pdf/2404.08408v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08404v1","updated":"2024-04-12T11:31:37Z","published":"2024-04-12T11:31:37Z","title":"Complexity of Probabilistic Reasoning for Neurosymbolic Classification\n  Techniques","summary":"  Neurosymbolic artificial intelligence is a growing field of research aiming\nto combine neural network learning capabilities with the reasoning abilities of\nsymbolic systems. Informed multi-label classification is a sub-field of\nneurosymbolic AI which studies how to leverage prior knowledge to improve\nneural classification systems. A well known family of neurosymbolic techniques\nfor informed classification use probabilistic reasoning to integrate this\nknowledge during learning, inference or both. Therefore, the asymptotic\ncomplexity of probabilistic reasoning is of cardinal importance to assess the\nscalability of such techniques. However, this topic is rarely tackled in the\nneurosymbolic literature, which can lead to a poor understanding of the limits\nof probabilistic neurosymbolic techniques. In this paper, we introduce a\nformalism for informed supervised classification tasks and techniques. We then\nbuild upon this formalism to define three abstract neurosymbolic techniques\nbased on probabilistic reasoning. Finally, we show computational complexity\nresults on several representation languages for prior knowledge commonly found\nin the neurosymbolic literature.\n","authors":["Arthur Ledaguenel","C√©line Hudelot","Mostepha Khouadjia"],"pdf_url":"https://arxiv.org/pdf/2404.08404v1.pdf","comment":"21 pages, 5 figures"},{"id":"http://arxiv.org/abs/2404.08403v1","updated":"2024-04-12T11:30:16Z","published":"2024-04-12T11:30:16Z","title":"Learning representations of learning representations","summary":"  The ICLR conference is unique among the top machine learning conferences in\nthat all submitted papers are openly available. Here we present the ICLR\ndataset consisting of abstracts of all 24 thousand ICLR submissions from\n2017-2024 with meta-data, decision scores, and custom keyword-based labels. We\nfind that on this dataset, bag-of-words representation outperforms most\ndedicated sentence transformer models in terms of $k$NN classification\naccuracy, and the top performing language models barely outperform TF-IDF. We\nsee this as a challenge for the NLP community. Furthermore, we use the ICLR\ndataset to study how the field of machine learning has changed over the last\nseven years, finding some improvement in gender balance. Using a 2D embedding\nof the abstracts' texts, we describe a shift in research topics from 2017 to\n2024 and identify hedgehogs and foxes among the authors with the highest number\nof ICLR submissions.\n","authors":["Rita Gonz√°lez-M√°rquez","Dmitry Kobak"],"pdf_url":"https://arxiv.org/pdf/2404.08403v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.07504v2","updated":"2024-04-12T11:30:04Z","published":"2023-05-12T14:19:15Z","title":"Calibration-Aware Bayesian Learning","summary":"  Deep learning models, including modern systems like large language models,\nare well known to offer unreliable estimates of the uncertainty of their\ndecisions. In order to improve the quality of the confidence levels, also known\nas calibration, of a model, common approaches entail the addition of either\ndata-dependent or data-independent regularization terms to the training loss.\nData-dependent regularizers have been recently introduced in the context of\nconventional frequentist learning to penalize deviations between confidence and\naccuracy. In contrast, data-independent regularizers are at the core of\nBayesian learning, enforcing adherence of the variational distribution in the\nmodel parameter space to a prior density. The former approach is unable to\nquantify epistemic uncertainty, while the latter is severely affected by model\nmisspecification. In light of the limitations of both methods, this paper\nproposes an integrated framework, referred to as calibration-aware Bayesian\nneural networks (CA-BNNs), that applies both regularizers while optimizing over\na variational distribution as in Bayesian learning. Numerical results validate\nthe advantages of the proposed approach in terms of expected calibration error\n(ECE) and reliability diagrams.\n","authors":["Jiayi Huang","Sangwoo Park","Osvaldo Simeone"],"pdf_url":"https://arxiv.org/pdf/2305.07504v2.pdf","comment":"submitted for conference publication"},{"id":"http://arxiv.org/abs/2404.08397v1","updated":"2024-04-12T11:06:22Z","published":"2024-04-12T11:06:22Z","title":"Data-Driven Preference Sampling for Pareto Front Learning","summary":"  Pareto front learning is a technique that introduces preference vectors in a\nneural network to approximate the Pareto front. Previous Pareto front learning\nmethods have demonstrated high performance in approximating simple Pareto\nfronts. These methods often sample preference vectors from a fixed Dirichlet\ndistribution. However, no fixed sampling distribution can be adapted to diverse\nPareto fronts. Efficiently sampling preference vectors and accurately\nestimating the Pareto front is a challenge. To address this challenge, we\npropose a data-driven preference vector sampling framework for Pareto front\nlearning. We utilize the posterior information of the objective functions to\nadjust the parameters of the sampling distribution flexibly. In this manner,\nthe proposed method can sample preference vectors from the location of the\nPareto front with a high probability. Moreover, we design the distribution of\nthe preference vector as a mixture of Dirichlet distributions to improve the\nperformance of the model in disconnected Pareto fronts. Extensive experiments\nvalidate the superiority of the proposed method compared with state-of-the-art\nalgorithms.\n","authors":["Rongguang Ye","Lei Chen","Weiduo Liao","Jinyuan Zhang","Hisao Ishibuchi"],"pdf_url":"https://arxiv.org/pdf/2404.08397v1.pdf","comment":"International Joint Conference on Neural Network (IJCNN'24)"},{"id":"http://arxiv.org/abs/2404.08392v1","updated":"2024-04-12T10:54:11Z","published":"2024-04-12T10:54:11Z","title":"NC-TTT: A Noise Contrastive Approach for Test-Time Training","summary":"  Despite their exceptional performance in vision tasks, deep learning models\noften struggle when faced with domain shifts during testing. Test-Time Training\n(TTT) methods have recently gained popularity by their ability to enhance the\nrobustness of models through the addition of an auxiliary objective that is\njointly optimized with the main task. Being strictly unsupervised, this\nauxiliary objective is used at test time to adapt the model without any access\nto labels. In this work, we propose Noise-Contrastive Test-Time Training\n(NC-TTT), a novel unsupervised TTT technique based on the discrimination of\nnoisy feature maps. By learning to classify noisy views of projected feature\nmaps, and then adapting the model accordingly on new domains, classification\nperformance can be recovered by an important margin. Experiments on several\npopular test-time adaptation baselines demonstrate the advantages of our method\ncompared to recent approaches for this task. The code can be found\nat:https://github.com/GustavoVargasHakim/NCTTT.git\n","authors":["David Osowiechi","Gustavo A. Vargas Hakim","Mehrdad Noori","Milad Cheraghalikhani","Ali Bahri","Moslem Yazdanpanah","Ismail Ben Ayed","Christian Desrosiers"],"pdf_url":"https://arxiv.org/pdf/2404.08392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11701v2","updated":"2024-04-12T10:36:28Z","published":"2024-02-18T20:47:33Z","title":"Explaining the Machine Learning Solution of the Ising Model","summary":"  As powerful as machine learning (ML) techniques are in solving problems\ninvolving data with large dimensionality, explaining the results from the\nfitted parameters remains a challenging task of utmost importance, especially\nin physics applications. This work shows how this can be accomplished for the\nferromagnetic Ising model, the main target of several ML studies in statistical\nphysics. Here it is demonstrated that the successful unsupervised\nidentification of the phases and order parameter by principal component\nanalysis, a common method in those studies, detects that the magnetization per\nspin has its greatest variation with the temperature, the actual control\nparameter of the phase transition. Then, by using a neural network (NN) without\nhidden layers (the simplest possible) and informed by the symmetry of the\nHamiltonian, an explanation is provided for the strategy used in finding the\nsupervised learning solution for the critical temperature of the model's\ncontinuous phase transition. This allows the prediction of the minimal\nextension of the NN to solve the problem when the symmetry is not known, which\nbecomes also explainable. These results pave the way to a physics-informed\nexplainable generalized framework, enabling the extraction of physical laws and\nprinciples from the parameters of the models.\n","authors":["Roberto C. Alamino"],"pdf_url":"https://arxiv.org/pdf/2402.11701v2.pdf","comment":"10 pages, 3 figures, updated to include the unsupervised learning of\n  phases with PCA"},{"id":"http://arxiv.org/abs/2404.08376v1","updated":"2024-04-12T10:22:55Z","published":"2024-04-12T10:22:55Z","title":"Graph data augmentation with Gromow-Wasserstein Barycenters","summary":"  Graphs are ubiquitous in various fields, and deep learning methods have been\nsuccessful applied in graph classification tasks. However, building large and\ndiverse graph datasets for training can be expensive. While augmentation\ntechniques exist for structured data like images or numerical data, the\naugmentation of graph data remains challenging. This is primarily due to the\ncomplex and non-Euclidean nature of graph data. In this paper, it has been\nproposed a novel augmentation strategy for graphs that operates in a\nnon-Euclidean space. This approach leverages graphon estimation, which models\nthe generative mechanism of networks sequences. Computational results\ndemonstrate the effectiveness of the proposed augmentation framework in\nimproving the performance of graph classification models. Additionally, using a\nnon-Euclidean distance, specifically the Gromow-Wasserstein distance, results\nin better approximations of the graphon. This framework also provides a means\nto validate different graphon estimation approaches, particularly in real-world\nscenarios where the true graphon is unknown.\n","authors":["Andrea Ponti"],"pdf_url":"https://arxiv.org/pdf/2404.08376v1.pdf","comment":"6 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.04385v2","updated":"2024-04-12T10:15:45Z","published":"2024-03-07T10:25:23Z","title":"Impacts of Color and Texture Distortions on Earth Observation Data in\n  Deep Learning","summary":"  Land cover classification and change detection are two important applications\nof remote sensing and Earth observation (EO) that have benefited greatly from\nthe advances of deep learning. Convolutional and transformer-based U-net models\nare the state-of-the-art architectures for these tasks, and their performances\nhave been boosted by an increased availability of large-scale annotated EO\ndatasets. However, the influence of different visual characteristics of the\ninput EO data on a model's predictions is not well understood. In this work we\nsystematically examine model sensitivities with respect to several color- and\ntexture-based distortions on the input EO data during inference, given models\nthat have been trained without such distortions. We conduct experiments with\nmultiple state-of-the-art segmentation networks for land cover classification\nand show that they are in general more sensitive to texture than to color\ndistortions. Beyond revealing intriguing characteristics of widely used land\ncover classification models, our results can also be used to guide the\ndevelopment of more robust models within the EO domain.\n","authors":["Martin Willbo","Aleksis Pirinen","John Martinsson","Edvin Listo Zec","Olof Mogren","Mikael Nilsson"],"pdf_url":"https://arxiv.org/pdf/2403.04385v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18998v3","updated":"2024-04-12T10:09:16Z","published":"2024-03-27T20:38:04Z","title":"Few-Shot Cross-System Anomaly Trace Classification for\n  Microservice-based systems","summary":"  Microservice-based systems (MSS) may experience failures in various fault\ncategories due to their complex and dynamic nature. To effectively handle\nfailures, AIOps tools utilize trace-based anomaly detection and root cause\nanalysis. In this paper, we propose a novel framework for few-shot abnormal\ntrace classification for MSS. Our framework comprises two main components: (1)\nMulti-Head Attention Autoencoder for constructing system-specific trace\nrepresentations, which enables (2) Transformer Encoder-based Model-Agnostic\nMeta-Learning to perform effective and efficient few-shot learning for abnormal\ntrace classification. The proposed framework is evaluated on two representative\nMSS, Trainticket and OnlineBoutique, with open datasets. The results show that\nour framework can adapt the learned knowledge to classify new, unseen abnormal\ntraces of novel fault categories both within the same system it was initially\ntrained on and even in the different MSS. Within the same MSS, our framework\nachieves an average accuracy of 93.26\\% and 85.2\\% across 50 meta-testing tasks\nfor Trainticket and OnlineBoutique, respectively, when provided with 10\ninstances for each task. In a cross-system context, our framework gets an\naverage accuracy of 92.19\\% and 84.77\\% for the same meta-testing tasks of the\nrespective system, also with 10 instances provided for each task. Our work\ndemonstrates the applicability of achieving few-shot abnormal trace\nclassification for MSS and shows how it can enable cross-system adaptability.\nThis opens an avenue for building more generalized AIOps tools that require\nless system-specific data labeling for anomaly detection and root cause\nanalysis.\n","authors":["Yuqing Wang","Mika V. M√§ntyl√§","Serge Demeyer","Mutlu Beyazit","Joanna Kisaakye","Jesse Nyyss√∂l√§"],"pdf_url":"https://arxiv.org/pdf/2403.18998v3.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2404.07970v2","updated":"2024-04-12T09:58:58Z","published":"2024-04-11T17:55:05Z","title":"Differentiable All-pole Filters for Time-varying Audio Systems","summary":"  Infinite impulse response filters are an essential building block of many\ntime-varying audio systems, such as audio effects and synthesisers. However,\ntheir recursive structure impedes end-to-end training of these systems using\nautomatic differentiation. Although non-recursive filter approximations like\nfrequency sampling and frame-based processing have been proposed and widely\nused in previous works, they cannot accurately reflect the gradient of the\noriginal system. We alleviate this difficulty by re-expressing a time-varying\nall-pole filter to backpropagate the gradients through itself, so the filter\nimplementation is not bound to the technical limitations of automatic\ndifferentiation frameworks. This implementation can be employed within any\naudio system containing filters with poles for efficient gradient evaluation.\nWe demonstrate its training efficiency and expressive capabilities for\nmodelling real-world dynamic audio systems on a phaser, time-varying\nsubtractive synthesiser, and feed-forward compressor. We make our code\navailable and provide the trained audio effect and synth models in a VST plugin\nat https://christhetree.github.io/all_pole_filters/.\n","authors":["Chin-Yun Yu","Christopher Mitcheltree","Alistair Carson","Stefan Bilbao","Joshua D. Reiss","Gy√∂rgy Fazekas"],"pdf_url":"https://arxiv.org/pdf/2404.07970v2.pdf","comment":"Submitted to DAFx 2024"},{"id":"http://arxiv.org/abs/2404.07236v2","updated":"2024-04-12T09:34:38Z","published":"2024-04-08T08:50:09Z","title":"Lightweight Deep Learning for Resource-Constrained Environments: A\n  Survey","summary":"  Over the past decade, the dominance of deep learning has prevailed across\nvarious domains of artificial intelligence, including natural language\nprocessing, computer vision, and biomedical signal processing. While there have\nbeen remarkable improvements in model accuracy, deploying these models on\nlightweight devices, such as mobile phones and microcontrollers, is constrained\nby limited resources. In this survey, we provide comprehensive design guidance\ntailored for these devices, detailing the meticulous design of lightweight\nmodels, compression methods, and hardware acceleration strategies. The\nprincipal goal of this work is to explore methods and concepts for getting\naround hardware constraints without compromising the model's accuracy.\nAdditionally, we explore two notable paths for lightweight deep learning in the\nfuture: deployment techniques for TinyML and Large Language Models. Although\nthese paths undoubtedly have potential, they also present significant\nchallenges, encouraging research into unexplored areas.\n","authors":["Hou-I Liu","Marco Galindo","Hongxia Xie","Lai-Kuan Wong","Hong-Han Shuai","Yung-Hui Li","Wen-Huang Cheng"],"pdf_url":"https://arxiv.org/pdf/2404.07236v2.pdf","comment":"40 pages"},{"id":"http://arxiv.org/abs/2404.08350v1","updated":"2024-04-12T09:31:11Z","published":"2024-04-12T09:31:11Z","title":"Self-Supervised k-Space Regularization for Motion-Resolved Abdominal MRI\n  Using Neural Implicit k-Space Representation","summary":"  Neural implicit k-space representations have shown promising results for\ndynamic MRI at high temporal resolutions. Yet, their exclusive training in\nk-space limits the application of common image regularization methods to\nimprove the final reconstruction. In this work, we introduce the concept of\nparallel imaging-inspired self-consistency (PISCO), which we incorporate as\nnovel self-supervised k-space regularization enforcing a consistent\nneighborhood relationship. At no additional data cost, the proposed\nregularization significantly improves neural implicit k-space reconstructions\non simulated data. Abdominal in-vivo reconstructions using PISCO result in\nenhanced spatio-temporal image quality compared to state-of-the-art methods.\nCode is available at https://github.com/vjspi/PISCO-NIK.\n","authors":["Veronika Spieker","Hannah Eichhorn","Jonathan K. Stelter","Wenqi Huang","Rickmer F. Braren","Daniel R√ºckert","Francisco Sahli Costabal","Kerstin Hammernik","Claudia Prieto","Dimitrios C. Karampinos","Julia A. Schnabel"],"pdf_url":"https://arxiv.org/pdf/2404.08350v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2402.00071v2","updated":"2024-04-12T09:28:47Z","published":"2024-01-30T20:08:15Z","title":"Unraveling the Impact of Initial Choices and In-Loop Interventions on\n  Learning Dynamics in Autonomous Scanning Probe Microscopy","summary":"  The current focus in Autonomous Experimentation (AE) is on developing robust\nworkflows to conduct the AE effectively. This entails the need for well-defined\napproaches to guide the AE process, including strategies for hyperparameter\ntuning and high-level human interventions within the workflow loop. This paper\npresents a comprehensive analysis of the influence of initial experimental\nconditions and in-loop interventions on the learning dynamics of Deep Kernel\nLearning (DKL) within the realm of AE in Scanning Probe Microscopy. We explore\nthe concept of 'seed effect', where the initial experiment setup has a\nsubstantial impact on the subsequent learning trajectory. Additionally, we\nintroduce an approach of the seed point interventions in AE allowing the\noperator to influence the exploration process. Using a dataset from\nPiezoresponse Force Microscopy (PFM) on PbTiO3 thin films, we illustrate the\nimpact of the 'seed effect' and in-loop seed interventions on the effectiveness\nof DKL in predicting material properties. The study highlights the importance\nof initial choices and adaptive interventions in optimizing learning rates and\nenhancing the efficiency of automated material characterization. This work\noffers valuable insights into designing more robust and effective AE workflows\nin microscopy with potential applications across various characterization\ntechniques. The analysis code that supports the funding is publicly available\nat https://github.com/Slautin/2024_Seed_effect_DKL_BO.\n","authors":["Boris N. Slautin","Yongtao Liu","Hiroshi Funakubo","Sergei V. Kalinin"],"pdf_url":"https://arxiv.org/pdf/2402.00071v2.pdf","comment":"24 pages, 11 figures"},{"id":"http://arxiv.org/abs/2404.08347v1","updated":"2024-04-12T09:22:24Z","published":"2024-04-12T09:22:24Z","title":"Learning to Rebalance Multi-Modal Optimization by Adaptively Masking\n  Subnetworks","summary":"  Multi-modal learning aims to enhance performance by unifying models from\nvarious modalities but often faces the \"modality imbalance\" problem in real\ndata, leading to a bias towards dominant modalities and neglecting others,\nthereby limiting its overall effectiveness. To address this challenge, the core\nidea is to balance the optimization of each modality to achieve a joint\noptimum. Existing approaches often employ a modal-level control mechanism for\nadjusting the update of each modal parameter. However, such a global-wise\nupdating mechanism ignores the different importance of each parameter. Inspired\nby subnetwork optimization, we explore a uniform sampling-based optimization\nstrategy and find it more effective than global-wise updating. According to the\nfindings, we further propose a novel importance sampling-based, element-wise\njoint optimization method, called Adaptively Mask Subnetworks Considering Modal\nSignificance(AMSS). Specifically, we incorporate mutual information rates to\ndetermine the modal significance and employ non-uniform adaptive sampling to\nselect foreground subnetworks from each modality for parameter updates, thereby\nrebalancing multi-modal learning. Additionally, we demonstrate the reliability\nof the AMSS strategy through convergence analysis. Building upon theoretical\ninsights, we further enhance the multi-modal mask subnetwork strategy using\nunbiased estimation, referred to as AMSS+. Extensive experiments reveal the\nsuperiority of our approach over comparison methods.\n","authors":["Yang Yang","Hongpeng Pan","Qing-Yuan Jiang","Yi Xu","Jinghui Tang"],"pdf_url":"https://arxiv.org/pdf/2404.08347v1.pdf","comment":"17 pages;6 figures"},{"id":"http://arxiv.org/abs/2308.09372v2","updated":"2024-04-12T09:21:33Z","published":"2023-08-18T08:06:49Z","title":"Which Transformer to Favor: A Comparative Analysis of Efficiency in\n  Vision Transformers","summary":"  Transformers come with a high computational cost, yet their effectiveness in\naddressing problems in language and vision has sparked extensive research aimed\nat enhancing their efficiency. However, diverse experimental conditions,\nspanning multiple input domains, prevent a fair comparison based solely on\nreported results, posing challenges for model selection. To address this gap in\ncomparability, we design a comprehensive benchmark of more than 30 models for\nimage classification, evaluating key efficiency aspects, including accuracy,\nspeed, and memory usage. This benchmark provides a standardized baseline across\nthe landscape of efficiency-oriented transformers and our framework of\nanalysis, based on Pareto optimality, reveals surprising insights. Despite\nclaims of other models being more efficient, ViT remains Pareto optimal across\nmultiple metrics. We observe that hybrid attention-CNN models exhibit\nremarkable inference memory- and parameter-efficiency. Moreover, our benchmark\nshows that using a larger model in general is more efficient than using higher\nresolution images. Thanks to our holistic evaluation, we provide a centralized\nresource for practitioners and researchers, facilitating informed decisions\nwhen selecting transformers or measuring progress of the development of\nefficient transformers.\n","authors":["Tobias Christian Nauen","Sebastian Palacio","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2308.09372v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08335v1","updated":"2024-04-12T09:01:14Z","published":"2024-04-12T09:01:14Z","title":"Toward a Theory of Tokenization in LLMs","summary":"  While there has been a large body of research attempting to circumvent\ntokenization for language modeling (Clark et al., 2022; Xue et al., 2022), the\ncurrent consensus is that it is a necessary initial step for designing\nstate-of-the-art performant language models. In this paper, we investigate\ntokenization from a theoretical point of view by studying the behavior of\ntransformers on simple data generating processes. When trained on data drawn\nfrom certain simple $k^{\\text{th}}$-order Markov processes for $k > 1$,\ntransformers exhibit a surprising phenomenon - in the absence of tokenization,\nthey empirically fail to learn the right distribution and predict characters\naccording to a unigram model (Makkuva et al., 2024). With the addition of\ntokenization, however, we empirically observe that transformers break through\nthis barrier and are able to model the probabilities of sequences drawn from\nthe source near-optimally, achieving small cross-entropy loss. With this\nobservation as starting point, we study the end-to-end cross-entropy loss\nachieved by transformers with and without tokenization. With the appropriate\ntokenization, we show that even the simplest unigram models (over tokens)\nlearnt by transformers are able to model the probability of sequences drawn\nfrom $k^{\\text{th}}$-order Markov sources near optimally. Our analysis provides\na justification for the use of tokenization in practice through studying the\nbehavior of transformers on Markovian data.\n","authors":["Nived Rajaraman","Jiantao Jiao","Kannan Ramchandran"],"pdf_url":"https://arxiv.org/pdf/2404.08335v1.pdf","comment":"58 pages, 10 figures"},{"id":"http://arxiv.org/abs/2307.10352v4","updated":"2024-04-12T08:51:55Z","published":"2023-07-19T21:21:18Z","title":"Properties of Discrete Sliced Wasserstein Losses","summary":"  The Sliced Wasserstein (SW) distance has become a popular alternative to the\nWasserstein distance for comparing probability measures. Widespread\napplications include image processing, domain adaptation and generative\nmodelling, where it is common to optimise some parameters in order to minimise\nSW, which serves as a loss function between discrete probability measures\n(since measures admitting densities are numerically unattainable). All these\noptimisation problems bear the same sub-problem, which is minimising the Sliced\nWasserstein energy. In this paper we study the properties of $\\mathcal{E}: Y\n\\longmapsto \\mathrm{SW}_2^2(\\gamma_Y, \\gamma_Z)$, i.e. the SW distance between\ntwo uniform discrete measures with the same amount of points as a function of\nthe support $Y \\in \\mathbb{R}^{n \\times d}$ of one of the measures. We\ninvestigate the regularity and optimisation properties of this energy, as well\nas its Monte-Carlo approximation $\\mathcal{E}_p$ (estimating the expectation in\nSW using only $p$ samples) and show convergence results on the critical points\nof $\\mathcal{E}_p$ to those of $\\mathcal{E}$, as well as an almost-sure uniform\nconvergence and a uniform Central Limit result on the process\n$\\mathcal{E}_p(Y)$. Finally, we show that in a certain sense, Stochastic\nGradient Descent methods minimising $\\mathcal{E}$ and $\\mathcal{E}_p$ converge\ntowards (Clarke) critical points of these energies.\n","authors":["Eloi Tanguy","R√©mi Flamary","Julie Delon"],"pdf_url":"https://arxiv.org/pdf/2307.10352v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.13027v2","updated":"2024-04-12T08:37:18Z","published":"2023-10-19T07:28:39Z","title":"Be Bayesian by Attachments to Catch More Uncertainty","summary":"  Bayesian Neural Networks (BNNs) have become one of the promising approaches\nfor uncertainty estimation due to the solid theorical foundations. However, the\nperformance of BNNs is affected by the ability of catching uncertainty. Instead\nof only seeking the distribution of neural network weights by in-distribution\n(ID) data, in this paper, we propose a new Bayesian Neural Network with an\nAttached structure (ABNN) to catch more uncertainty from out-of-distribution\n(OOD) data. We first construct a mathematical description for the uncertainty\nof OOD data according to the prior distribution, and then develop an attached\nBayesian structure to integrate the uncertainty of OOD data into the backbone\nnetwork. ABNN is composed of an expectation module and several distribution\nmodules. The expectation module is a backbone deep network which focuses on the\noriginal task, and the distribution modules are mini Bayesian structures which\nserve as attachments of the backbone. In particular, the distribution modules\naim at extracting the uncertainty from both ID and OOD data. We further provide\ntheoretical analysis for the convergence of ABNN, and experimentally validate\nits superiority by comparing with some state-of-the-art uncertainty estimation\nmethods Code will be made available.\n","authors":["Shiyu Shen","Bin Pan","Tianyang Shi","Tao Li","Zhenwei Shi"],"pdf_url":"https://arxiv.org/pdf/2310.13027v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08325v1","updated":"2024-04-12T08:35:38Z","published":"2024-04-12T08:35:38Z","title":"Uncertainty Aware Tropical Cyclone Wind Speed Estimation from Satellite\n  Data","summary":"  Deep neural networks (DNNs) have been successfully applied to earth\nobservation (EO) data and opened new research avenues. Despite the theoretical\nand practical advances of these techniques, DNNs are still considered black box\ntools and by default are designed to give point predictions. However, the\nmajority of EO applications demand reliable uncertainty estimates that can\nsupport practitioners in critical decision making tasks. This work provides a\ntheoretical and quantitative comparison of existing uncertainty quantification\nmethods for DNNs applied to the task of wind speed estimation in satellite\nimagery of tropical cyclones. We provide a detailed evaluation of predictive\nuncertainty estimates from state-of-the-art uncertainty quantification (UQ)\nmethods for DNNs. We find that predictive uncertainties can be utilized to\nfurther improve accuracy and analyze the predictive uncertainties of different\nmethods across storm categories.\n","authors":["Nils Lehmann","Nina Maria Gottschling","Stefan Depeweg","Eric Nalisnick"],"pdf_url":"https://arxiv.org/pdf/2404.08325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09852v5","updated":"2024-04-12T08:32:58Z","published":"2023-11-16T12:28:31Z","title":"Short vs. Long-term Coordination of Drones: When Distributed\n  Optimization Meets Deep Reinforcement Learning","summary":"  Swarms of autonomous interactive drones, with the support of recharging\ntechnology, can provide compelling sensing capabilities in Smart Cities, such\nas traffic monitoring and disaster response. This paper aims to deliver a novel\ncoordination solution for the cost-effective navigation, sensing, and\nrecharging of drones. Existing approaches, such as deep reinforcement learning\n(DRL), offer long-term adaptability, but lack energy efficiency, resilience,\nand flexibility in dynamic environments. Therefore, this paper proposes a novel\napproach where each drone independently determines its flying direction and\nrecharging place using DRL, while adapting navigation and sensing through\ndistributed optimization, which improves energy-efficiency during sensing\ntasks. Furthermore, drones efficiently exchange information while retaining\ndecision-making autonomy via a structured tree communication model. Extensive\nexperimentation with datasets generated from realistic urban mobility\nunderscores an outstanding performance of the proposed solution compared to\nstate-of-the-art methods. Significant new insights show that long-term methods\noptimize scarce drone resource for traffic management, while the integration of\nshort-term methods is crucial for advising on charging policies and maintaining\nbattery safety.\n","authors":["Chuhao Qin","Evangelos Pournaras"],"pdf_url":"https://arxiv.org/pdf/2311.09852v5.pdf","comment":"This work has been submitted to the IEEE Transactions on Systems, Man\n  and Cybernetics: Systems for possible publication. Copyright may be\n  transferred without notice, after which this version may no longer be\n  accessible"},{"id":"http://arxiv.org/abs/2404.08314v1","updated":"2024-04-12T08:20:01Z","published":"2024-04-12T08:20:01Z","title":"Multi-Step Traffic Prediction for Multi-Period Planning in Optical\n  Networks","summary":"  A multi-period planning framework is proposed that exploits multi-step ahead\ntraffic predictions to address service overprovisioning and improve\nadaptability to traffic changes, while ensuring the necessary\nquality-of-service (QoS) levels. An encoder-decoder deep learning model is\ninitially leveraged for multi-step ahead prediction by analyzing real-traffic\ntraces. This information is then exploited by multi-period planning heuristics\nto efficiently utilize available network resources while minimizing undesired\nservice disruptions (caused due to lightpath re-allocations), with these\nheuristics outperforming a single-step ahead prediction approach.\n","authors":["Hafsa Maryam","Tania Panayiotou","Georgios Ellinas"],"pdf_url":"https://arxiv.org/pdf/2404.08314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.16589v2","updated":"2024-04-12T08:09:33Z","published":"2023-05-26T02:32:03Z","title":"The Curious Price of Distributional Robustness in Reinforcement Learning\n  with a Generative Model","summary":"  This paper investigates model robustness in reinforcement learning (RL) to\nreduce the sim-to-real gap in practice. We adopt the framework of\ndistributionally robust Markov decision processes (RMDPs), aimed at learning a\npolicy that optimizes the worst-case performance when the deployed environment\nfalls within a prescribed uncertainty set around the nominal MDP. Despite\nrecent efforts, the sample complexity of RMDPs remained mostly unsettled\nregardless of the uncertainty set in use. It was unclear if distributional\nrobustness bears any statistical consequences when benchmarked against standard\nRL. Assuming access to a generative model that draws samples based on the\nnominal MDP, we characterize the sample complexity of RMDPs when the\nuncertainty set is specified via either the total variation (TV) distance or\n$\\chi^2$ divergence. The algorithm studied here is a model-based method called\n{\\em distributionally robust value iteration}, which is shown to be\nnear-optimal for the full range of uncertainty levels. Somewhat surprisingly,\nour results uncover that RMDPs are not necessarily easier or harder to learn\nthan standard MDPs. The statistical consequence incurred by the robustness\nrequirement depends heavily on the size and shape of the uncertainty set: in\nthe case w.r.t.~the TV distance, the minimax sample complexity of RMDPs is\nalways smaller than that of standard MDPs; in the case w.r.t.~the $\\chi^2$\ndivergence, the sample complexity of RMDPs can often far exceed the standard\nMDP counterpart.\n","authors":["Laixi Shi","Gen Li","Yuting Wei","Yuxin Chen","Matthieu Geist","Yuejie Chi"],"pdf_url":"https://arxiv.org/pdf/2305.16589v2.pdf","comment":"Neural Information Processing Systems (2023)"},{"id":"http://arxiv.org/abs/2404.08303v1","updated":"2024-04-12T07:51:21Z","published":"2024-04-12T07:51:21Z","title":"A Large Scale Survey of Motivation in Software Development and Analysis\n  of its Validity","summary":"  Context: Motivation is known to improve performance. In software development\nin particular, there has been considerable interest in the motivation of\ncontributors to open source. Objective: We identify 11 motivators from the\nliterature (enjoying programming, ownership of code, learning, self use, etc.),\nand evaluate their relative effect on motivation. Since motivation is an\ninternal subjective feeling, we also analyze the validity of the answers.\nMethod: We conducted a survey with 66 questions on motivation which was\ncompleted by 521 developers. Most of the questions used an 11 point scale. We\nevaluated the validity of the answers validity by comparing related questions,\ncomparing to actual behavior on GitHub, and comparison with the same developer\nin a follow up survey. Results: Validity problems include moderate correlations\nbetween answers to related questions, as well as self promotion and mistakes in\nthe answers. Despite these problems, predictive analysis, investigating how\ndiverse motivators influence the probability of high motivation, provided\nvaluable insights. The correlations between the different motivators are low,\nimplying their independence. High values in all 11 motivators predict increased\nprobability of high motivation. In addition, improvement analysis shows that an\nincrease in most motivators predicts an increase in general motivation.\n","authors":["Idan Amit","Dror G. Feitelson"],"pdf_url":"https://arxiv.org/pdf/2404.08303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.04284v4","updated":"2024-04-12T07:49:57Z","published":"2023-09-08T12:06:48Z","title":"Viewing the process of generating counterfactuals as a source of\n  knowledge: a new approach for explaining classifiers","summary":"  There are now many explainable AI methods for understanding the decisions of\na machine learning model. Among these are those based on counterfactual\nreasoning, which involve simulating features changes and observing the impact\non the prediction. This article proposes to view this simulation process as a\nsource of creating a certain amount of knowledge that can be stored to be used,\nlater, in different ways. This process is illustrated in the additive model\nand, more specifically, in the case of the naive Bayes classifier, whose\ninteresting properties for this purpose are shown.\n","authors":["Vincent Lemaire","Nathan Le Boudec","Victor Guyomard","Fran√ßoise Fessant"],"pdf_url":"https://arxiv.org/pdf/2309.04284v4.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2404.08301v1","updated":"2024-04-12T07:47:02Z","published":"2024-04-12T07:47:02Z","title":"Collaborative-Enhanced Prediction of Spending on Newly Downloaded Mobile\n  Games under Consumption Uncertainty","summary":"  With the surge in mobile gaming, accurately predicting user spending on newly\ndownloaded games has become paramount for maximizing revenue. However, the\ninherently unpredictable nature of user behavior poses significant challenges\nin this endeavor. To address this, we propose a robust model training and\nevaluation framework aimed at standardizing spending data to mitigate label\nvariance and extremes, ensuring stability in the modeling process. Within this\nframework, we introduce a collaborative-enhanced model designed to predict user\ngame spending without relying on user IDs, thus ensuring user privacy and\nenabling seamless online training. Our model adopts a unique approach by\nseparately representing user preferences and game features before merging them\nas input to the spending prediction module. Through rigorous experimentation,\nour approach demonstrates notable improvements over production models,\nachieving a remarkable \\textbf{17.11}\\% enhancement on offline data and an\nimpressive \\textbf{50.65}\\% boost in an online A/B test. In summary, our\ncontributions underscore the importance of stable model training frameworks and\nthe efficacy of collaborative-enhanced models in predicting user spending\nbehavior in mobile gaming.\n","authors":["Peijie Sun","Yifan Wang","Min Zhang","Chuhan Wu","Yan Fang","Hong Zhu","Yuan Fang","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2404.08301v1.pdf","comment":"10 pages,6 figures, WWW 2024 Industry Track, with three accept, two\n  weak accept scores"},{"id":"http://arxiv.org/abs/2310.12544v2","updated":"2024-04-12T07:45:49Z","published":"2023-10-19T07:51:39Z","title":"Neural Likelihood Approximation for Integer Valued Time Series Data","summary":"  Stochastic processes defined on integer valued state spaces are popular\nwithin the physical and biological sciences. These models are necessary for\ncapturing the dynamics of small systems where the individual nature of the\npopulations cannot be ignored and stochastic effects are important. The\ninference of the parameters of such models, from time series data, is\nchallenging due to intractability of the likelihood. To work at all, current\nsimulation based inference methods require the generation of realisations of\nthe model conditional on the data, which can be both tricky to implement and\ncomputationally expensive. In this paper we instead construct a neural\nlikelihood approximation that can be trained using unconditional simulation of\nthe underlying model, which is much simpler. We demonstrate our method by\nperforming inference on a number of ecological and epidemiological models,\nshowing that we can accurately approximate the true posterior while achieving\nsignificant computational speed ups compared to current best methods.\n","authors":["Luke O'Loughlin","John Maclean","Andrew Black"],"pdf_url":"https://arxiv.org/pdf/2310.12544v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08295v1","updated":"2024-04-12T07:34:46Z","published":"2024-04-12T07:34:46Z","title":"Study of Emotion Concept Formation by Integrating Vision, Physiology,\n  and Word Information using Multilayered Multimodal Latent Dirichlet\n  Allocation","summary":"  How are emotions formed? Through extensive debate and the promulgation of\ndiverse theories , the theory of constructed emotion has become prevalent in\nrecent research on emotions. According to this theory, an emotion concept\nrefers to a category formed by interoceptive and exteroceptive information\nassociated with a specific emotion. An emotion concept stores past experiences\nas knowledge and can predict unobserved information from acquired information.\nTherefore, in this study, we attempted to model the formation of emotion\nconcepts using a constructionist approach from the perspective of the\nconstructed emotion theory. Particularly, we constructed a model using\nmultilayered multimodal latent Dirichlet allocation , which is a probabilistic\ngenerative model. We then trained the model for each subject using vision,\nphysiology, and word information obtained from multiple people who experienced\ndifferent visual emotion-evoking stimuli. To evaluate the model, we verified\nwhether the formed categories matched human subjectivity and determined whether\nunobserved information could be predicted via categories. The verification\nresults exceeded chance level, suggesting that emotion concept formation can be\nexplained by the proposed model.\n","authors":["Kazuki Tsurumaki","Chie Hieida","Kazuki Miyazawa"],"pdf_url":"https://arxiv.org/pdf/2404.08295v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible. We would like to thank Professor Takayuki Nagai for\n  useful discussions"},{"id":"http://arxiv.org/abs/2404.08279v1","updated":"2024-04-12T07:08:05Z","published":"2024-04-12T07:08:05Z","title":"Convolutional neural network classification of cancer cytopathology\n  images: taking breast cancer as an example","summary":"  Breast cancer is a relatively common cancer among gynecological cancers. Its\ndiagnosis often relies on the pathology of cells in the lesion. The\npathological diagnosis of breast cancer not only requires professionals and\ntime, but also sometimes involves subjective judgment. To address the\nchallenges of dependence on pathologists expertise and the time-consuming\nnature of achieving accurate breast pathological image classification, this\npaper introduces an approach utilizing convolutional neural networks (CNNs) for\nthe rapid categorization of pathological images, aiming to enhance the\nefficiency of breast pathological image detection. And the approach enables the\nrapid and automatic classification of pathological images into benign and\nmalignant groups. The methodology involves utilizing a convolutional neural\nnetwork (CNN) model leveraging the Inceptionv3 architecture and transfer\nlearning algorithm for extracting features from pathological images. Utilizing\na neural network with fully connected layers and employing the SoftMax function\nfor image classification. Additionally, the concept of image partitioning is\nintroduced to handle high-resolution images. To achieve the ultimate\nclassification outcome, the classification probabilities of each image block\nare aggregated using three algorithms: summation, product, and maximum.\nExperimental validation was conducted on the BreaKHis public dataset, resulting\nin accuracy rates surpassing 0.92 across all four magnification coefficients\n(40X, 100X, 200X, and 400X). It demonstrates that the proposed method\neffectively enhances the accuracy in classifying pathological images of breast\ncancer.\n","authors":["MingXuan Xiao","Yufeng Li","Xu Yan","Min Gao","Weimin Wang"],"pdf_url":"https://arxiv.org/pdf/2404.08279v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08271v1","updated":"2024-04-12T06:50:32Z","published":"2024-04-12T06:50:32Z","title":"Transfer Learning Study of Motion Transformer-based Trajectory\n  Predictions","summary":"  Trajectory planning in autonomous driving is highly dependent on predicting\nthe emergent behavior of other road users. Learning-based methods are currently\nshowing impressive results in simulation-based challenges, with\ntransformer-based architectures technologically leading the way. Ultimately,\nhowever, predictions are needed in the real world. In addition to the shifts\nfrom simulation to the real world, many vehicle- and country-specific shifts,\ni.e. differences in sensor systems, fusion and perception algorithms as well as\ntraffic rules and laws, are on the agenda. Since models that can cover all\nsystem setups and design domains at once are not yet foreseeable, model\nadaptation plays a central role. Therefore, a simulation-based study on\ntransfer learning techniques is conducted on basis of a transformer-based\nmodel. Furthermore, the study aims to provide insights into possible trade-offs\nbetween computational time and performance to support effective transfers into\nthe real world.\n","authors":["Lars Ullrich","Alex McMaster","Knut Graichen"],"pdf_url":"https://arxiv.org/pdf/2404.08271v1.pdf","comment":"Accepted to be published as part of the 2024 IEEE Intelligent\n  Vehicles Symposium (IV), Jeju Shinhwa World, Jeju Island, Korea, June 2-5,\n  2024"},{"id":"http://arxiv.org/abs/2303.03761v2","updated":"2024-04-12T06:42:47Z","published":"2023-03-07T09:56:23Z","title":"Graph Neural Networks in Vision-Language Image Understanding: A Survey","summary":"  2D image understanding is a complex problem within computer vision, but it\nholds the key to providing human-level scene comprehension. It goes further\nthan identifying the objects in an image, and instead, it attempts to\nunderstand the scene. Solutions to this problem form the underpinning of a\nrange of tasks, including image captioning, visual question answering (VQA),\nand image retrieval. Graphs provide a natural way to represent the relational\narrangement between objects in an image, and thus, in recent years graph neural\nnetworks (GNNs) have become a standard component of many 2D image understanding\npipelines, becoming a core architectural component, especially in the VQA group\nof tasks. In this survey, we review this rapidly evolving field and we provide\na taxonomy of graph types used in 2D image understanding approaches, a\ncomprehensive list of the GNN models used in this domain, and a roadmap of\nfuture potential developments. To the best of our knowledge, this is the first\ncomprehensive survey that covers image captioning, visual question answering,\nand image retrieval techniques that focus on using GNNs as the main part of\ntheir architecture.\n","authors":["Henry Senior","Gregory Slabaugh","Shanxin Yuan","Luca Rossi"],"pdf_url":"https://arxiv.org/pdf/2303.03761v2.pdf","comment":"20 pages, 5 figures, 5 tables"},{"id":"http://arxiv.org/abs/2302.06434v3","updated":"2024-04-12T06:38:32Z","published":"2023-02-13T15:13:22Z","title":"Efficient Graph Laplacian Estimation by Proximal Newton","summary":"  The Laplacian-constrained Gaussian Markov Random Field (LGMRF) is a common\nmultivariate statistical model for learning a weighted sparse dependency graph\nfrom given data. This graph learning problem can be formulated as a maximum\nlikelihood estimation (MLE) of the precision matrix, subject to Laplacian\nstructural constraints, with a sparsity-inducing penalty term. This paper aims\nto solve this learning problem accurately and efficiently. First, since the\ncommonly used $\\ell_1$-norm penalty is inappropriate in this setting and may\nlead to a complete graph, we employ the nonconvex minimax concave penalty\n(MCP), which promotes sparse solutions with lower estimation bias. Second, as\nopposed to existing first-order methods for this problem, we develop a\nsecond-order proximal Newton approach to obtain an efficient solver, utilizing\nseveral algorithmic features, such as using Conjugate Gradients,\npreconditioning, and splitting to active/free sets. Numerical experiments\ndemonstrate the advantages of the proposed method in terms of both\ncomputational complexity and graph learning accuracy compared to existing\nmethods.\n","authors":["Yakov Medvedovsky","Eran Treister","Tirza Routtenberg"],"pdf_url":"https://arxiv.org/pdf/2302.06434v3.pdf","comment":"Proceedings of Artificial Intelligence and Statistics (AISTATS), 2024"},{"id":"http://arxiv.org/abs/2303.15799v4","updated":"2024-04-12T06:26:04Z","published":"2023-03-28T08:07:28Z","title":"FedAgg: Adaptive Federated Learning with Aggregated Gradients","summary":"  Federated Learning (FL) has emerged as a pivotal paradigm within distributed\nmodel training, facilitating collaboration among multiple devices to refine a\nshared model, harnessing their respective datasets as orchestrated by a central\nserver, while ensuring the localization of private data. Nonetheless, the\nnon-independent-and-identically-distributed (Non-IID) data generated on\nheterogeneous clients and the incessant information exchange among participants\nmay markedly impede training efficacy and retard the convergence rate. In this\npaper, we refine the conventional stochastic gradient descent (SGD) methodology\nby introducing aggregated gradients at each local training epoch and propose an\nadaptive learning rate iterative algorithm that concerns the divergence between\nlocal and average parameters. To surmount the obstacle that acquiring other\nclients' local information, we introduce the mean-field approach by leveraging\ntwo mean-field terms to approximately estimate the average local parameters and\ngradients over time in a manner that precludes the need for local information\nexchange among clients and design the decentralized adaptive learning rate for\neach client. Through meticulous theoretical analysis, we provide a robust\nconvergence guarantee for our proposed algorithm and ensure its wide\napplicability. Our numerical experiments substantiate the superiority of our\nframework in comparison with existing state-of-the-art FL strategies for\nenhancing model performance and accelerating convergence rate under IID and\nNon-IID data distributions.\n","authors":["Wenhao Yuan","Xuehe Wang"],"pdf_url":"https://arxiv.org/pdf/2303.15799v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.15301v3","updated":"2024-04-12T06:25:43Z","published":"2023-10-23T19:07:33Z","title":"ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital\n  Biomarkers of Alzheimer's Disease","summary":"  Alzheimer's Disease (AD) and related dementia are a growing global health\nchallenge due to the aging population. In this paper, we present ADMarker, the\nfirst end-to-end system that integrates multi-modal sensors and new federated\nlearning algorithms for detecting multidimensional AD digital biomarkers in\nnatural living environments. ADMarker features a novel three-stage multi-modal\nfederated learning architecture that can accurately detect digital biomarkers\nin a privacy-preserving manner. Our approach collectively addresses several\nmajor real-world challenges, such as limited data labels, data heterogeneity,\nand limited computing resources. We built a compact multi-modality hardware\nsystem and deployed it in a four-week clinical trial involving 91 elderly\nparticipants. The results indicate that ADMarker can accurately detect a\ncomprehensive set of digital biomarkers with up to 93.8% accuracy and identify\nearly AD with an average of 88.9% accuracy. ADMarker offers a new platform that\ncan allow AD clinicians to characterize and track the complex correlation\nbetween multidimensional interpretable digital biomarkers, demographic factors\nof patients, and AD diagnosis in a longitudinal manner.\n","authors":["Xiaomin Ouyang","Xian Shuai","Yang Li","Li Pan","Xifan Zhang","Heming Fu","Sitong Cheng","Xinyan Wang","Shihua Cao","Jiang Xin","Hazel Mok","Zhenyu Yan","Doris Sau Fung Yu","Timothy Kwok","Guoliang Xing"],"pdf_url":"https://arxiv.org/pdf/2310.15301v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08263v1","updated":"2024-04-12T06:23:07Z","published":"2024-04-12T06:23:07Z","title":"Relational Prompt-based Pre-trained Language Models for Social Event\n  Detection","summary":"  Social Event Detection (SED) aims to identify significant events from social\nstreams, and has a wide application ranging from public opinion analysis to\nrisk management. In recent years, Graph Neural Network (GNN) based solutions\nhave achieved state-of-the-art performance. However, GNN-based methods often\nstruggle with noisy and missing edges between messages, affecting the quality\nof learned message embedding. Moreover, these methods statically initialize\nnode embedding before training, which, in turn, limits the ability to learn\nfrom message texts and relations simultaneously. In this paper, we approach\nsocial event detection from a new perspective based on Pre-trained Language\nModels (PLMs), and present RPLM_SED (Relational prompt-based Pre-trained\nLanguage Models for Social Event Detection). We first propose a new pairwise\nmessage modeling strategy to construct social messages into message pairs with\nmulti-relational sequences. Secondly, a new multi-relational prompt-based\npairwise message learning mechanism is proposed to learn more comprehensive\nmessage representation from message pairs with multi-relational prompts using\nPLMs. Thirdly, we design a new clustering constraint to optimize the encoding\nprocess by enhancing intra-cluster compactness and inter-cluster dispersion,\nmaking the message representation more distinguishable. We evaluate the\nRPLM_SED on three real-world datasets, demonstrating that the RPLM_SED model\nachieves state-of-the-art performance in offline, online, low-resource, and\nlong-tail distribution scenarios for social event detection tasks.\n","authors":["Pu Li","Xiaoyan Yu","Hao Peng","Yantuan Xian","Linqin Wang","Li Sun","Jingyun Zhang","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2404.08263v1.pdf","comment":"ACM TOIS Under Review"},{"id":"http://arxiv.org/abs/2404.08254v1","updated":"2024-04-12T06:08:43Z","published":"2024-04-12T06:08:43Z","title":"Balanced Mixed-Type Tabular Data Synthesis with Diffusion Models","summary":"  Diffusion models have emerged as a robust framework for various generative\ntasks, such as image and audio synthesis, and have also demonstrated a\nremarkable ability to generate mixed-type tabular data comprising both\ncontinuous and discrete variables. However, current approaches to training\ndiffusion models on mixed-type tabular data tend to inherit the imbalanced\ndistributions of features present in the training dataset, which can result in\nbiased sampling. In this research, we introduce a fair diffusion model designed\nto generate balanced data on sensitive attributes. We present empirical\nevidence demonstrating that our method effectively mitigates the class\nimbalance in training data while maintaining the quality of the generated\nsamples. Furthermore, we provide evidence that our approach outperforms\nexisting methods for synthesizing tabular data in terms of performance and\nfairness.\n","authors":["Zeyu Yang","Peikun Guo","Khadija Zanna","Akane Sano"],"pdf_url":"https://arxiv.org/pdf/2404.08254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14966v3","updated":"2024-04-12T06:04:55Z","published":"2023-03-27T07:57:04Z","title":"Adaptive Federated Learning via New Entropy Approach","summary":"  Federated Learning (FL) has emerged as a prominent distributed machine\nlearning framework that enables geographically discrete clients to train a\nglobal model collaboratively while preserving their privacy-sensitive data.\nHowever, due to the non-independent-and-identically-distributed (Non-IID) data\ngenerated by heterogeneous clients, the performances of the conventional\nfederated optimization schemes such as FedAvg and its variants deteriorate,\nrequiring the design to adaptively adjust specific model parameters to\nalleviate the negative influence of heterogeneity. In this paper, by leveraging\nentropy as a new metric for assessing the degree of system disorder, we propose\nan adaptive FEDerated learning algorithm based on ENTropy theory (FedEnt) to\nalleviate the parameter deviation among heterogeneous clients and achieve fast\nconvergence. Nevertheless, given the data disparity and parameter deviation of\nheterogeneous clients, determining the optimal dynamic learning rate for each\nclient becomes a challenging task as there is no communication among\nparticipating clients during the local training epochs. To enable a\ndecentralized learning rate for each participating client, we first introduce\nthe mean-field terms to estimate the components associated with other clients'\nlocal parameters. Furthermore, we provide rigorous theoretical analysis on the\nexistence and determination of the mean-field estimators. Based on the\nmean-field estimators, the closed-form adaptive learning rate for each client\nis derived by constructing the Hamilton equation. Moreover, the convergence\nrate of our proposed FedEnt is proved. The extensive experimental results on\nthe real-world datasets (i.e., MNIST, EMNIST-L, CIFAR10, and CIFAR100) show\nthat our FedEnt algorithm surpasses FedAvg and its variants (i.e., FedAdam,\nFedProx, and FedDyn) under Non-IID settings and achieves a faster convergence\nrate.\n","authors":["Shensheng Zheng","Wenhao Yuan","Xuehe Wang","Lingjie Duan"],"pdf_url":"https://arxiv.org/pdf/2303.14966v3.pdf","comment":"16 pages, 13 figures"},{"id":"http://arxiv.org/abs/2404.08246v1","updated":"2024-04-12T05:25:03Z","published":"2024-04-12T05:25:03Z","title":"Agile and versatile bipedal robot tracking control through reinforcement\n  learning","summary":"  The remarkable athletic intelligence displayed by humans in complex dynamic\nmovements such as dancing and gymnastics suggests that the balance mechanism in\nbiological beings is decoupled from specific movement patterns. This decoupling\nallows for the execution of both learned and unlearned movements under certain\nconstraints while maintaining balance through minor whole-body coordination. To\nreplicate this balance ability and body agility, this paper proposes a\nversatile controller for bipedal robots. This controller achieves ankle and\nbody trajectory tracking across a wide range of gaits using a single\nsmall-scale neural network, which is based on a model-based IK solver and\nreinforcement learning. We consider a single step as the smallest control unit\nand design a universally applicable control input form suitable for any\nsingle-step variation. Highly flexible gait control can be achieved by\ncombining these minimal control units with high-level policy through our\nextensible control interface. To enhance the trajectory-tracking capability of\nour controller, we utilize a three-stage training curriculum. After training,\nthe robot can move freely between target footholds at varying distances and\nheights. The robot can also maintain static balance without repeated stepping\nto adjust posture. Finally, we evaluate the tracking accuracy of our controller\non various bipedal tasks, and the effectiveness of our control framework is\nverified in the simulation environment.\n","authors":["Jiayi Li","Linqi Ye","Yi Cheng","Houde Liu","Bin Liang"],"pdf_url":"https://arxiv.org/pdf/2404.08246v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2311.16254v2","updated":"2024-04-12T09:37:37Z","published":"2023-11-27T19:02:17Z","title":"Safe-CLIP: Removing NSFW Concepts from Vision-and-Language Models","summary":"  Large-scale vision-and-language models, such as CLIP, are typically trained\non web-scale data, which can introduce inappropriate content and lead to the\ndevelopment of unsafe and biased behavior. This, in turn, hampers their\napplicability in sensitive and trustworthy contexts and could raise significant\nconcerns in their adoption. Our research introduces a novel approach to\nenhancing the safety of vision-and-language models by diminishing their\nsensitivity to NSFW (not safe for work) inputs. In particular, our methodology\nseeks to sever \"toxic\" linguistic and visual concepts, unlearning the linkage\nbetween unsafe linguistic or visual items and unsafe regions of the embedding\nspace. We show how this can be done by fine-tuning a CLIP model on synthetic\ndata obtained from a large language model trained to convert between safe and\nunsafe sentences, and a text-to-image generator. We conduct extensive\nexperiments on the resulting embedding space for cross-modal retrieval,\ntext-to-image, and image-to-text generation, where we show that our model can\nbe remarkably employed with pre-trained generative models. Our source code and\ntrained models are available at: https://github.com/aimagelab/safe-clip.\n","authors":["Samuele Poppi","Tobia Poppi","Federico Cocchi","Marcella Cornia","Lorenzo Baraldi","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2311.16254v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08281v1","updated":"2024-04-12T07:13:32Z","published":"2024-04-12T07:13:32Z","title":"Calibration & Reconstruction: Deep Integrated Language for Referring\n  Image Segmentation","summary":"  Referring image segmentation aims to segment an object referred to by natural\nlanguage expression from an image. The primary challenge lies in the efficient\npropagation of fine-grained semantic information from textual features to\nvisual features. Many recent works utilize a Transformer to address this\nchallenge. However, conventional transformer decoders can distort linguistic\ninformation with deeper layers, leading to suboptimal results. In this paper,\nwe introduce CRFormer, a model that iteratively calibrates multi-modal features\nin the transformer decoder. We start by generating language queries using\nvision features, emphasizing different aspects of the input language. Then, we\npropose a novel Calibration Decoder (CDec) wherein the multi-modal features can\niteratively calibrated by the input language features. In the Calibration\nDecoder, we use the output of each decoder layer and the original language\nfeatures to generate new queries for continuous calibration, which gradually\nupdates the language features. Based on CDec, we introduce a Language\nReconstruction Module and a reconstruction loss. This module leverages queries\nfrom the final layer of the decoder to reconstruct the input language and\ncompute the reconstruction loss. This can further prevent the language\ninformation from being lost or distorted. Our experiments consistently show the\nsuperior performance of our approach across RefCOCO, RefCOCO+, and G-Ref\ndatasets compared to state-of-the-art methods.\n","authors":["Yichen Yan","Xingjian He","Sihan Chen","Jing Liu"],"pdf_url":"https://arxiv.org/pdf/2404.08281v1.pdf","comment":"9 pages, 8 figures ICMR2024. arXiv admin note: text overlap with\n  arXiv:2305.14969"},{"id":"http://arxiv.org/abs/2404.08264v1","updated":"2024-04-12T06:23:48Z","published":"2024-04-12T06:23:48Z","title":"Guided Masked Self-Distillation Modeling for Distributed Multimedia\n  Sensor Event Analysis","summary":"  Observations with distributed sensors are essential in analyzing a series of\nhuman and machine activities (referred to as 'events' in this paper) in complex\nand extensive real-world environments. This is because the information obtained\nfrom a single sensor is often missing or fragmented in such an environment;\nobservations from multiple locations and modalities should be integrated to\nanalyze events comprehensively. However, a learning method has yet to be\nestablished to extract joint representations that effectively combine such\ndistributed observations. Therefore, we propose Guided Masked sELf-Distillation\nmodeling (Guided-MELD) for inter-sensor relationship modeling. The basic idea\nof Guided-MELD is to learn to supplement the information from the masked sensor\nwith information from other sensors needed to detect the event. Guided-MELD is\nexpected to enable the system to effectively distill the fragmented or\nredundant target event information obtained by the sensors without being overly\ndependent on any specific sensors. To validate the effectiveness of the\nproposed method in novel tasks of distributed multimedia sensor event analysis,\nwe recorded two new datasets that fit the problem setting: MM-Store and\nMM-Office. These datasets consist of human activities in a convenience store\nand an office, recorded using distributed cameras and microphones. Experimental\nresults on these datasets show that the proposed Guided-MELD improves event\ntagging and detection performance and outperforms conventional inter-sensor\nrelationship modeling methods. Furthermore, the proposed method performed\nrobustly even when sensors were reduced.\n","authors":["Masahiro Yasuda","Noboru Harada","Yasunori Ohishi","Shoichiro Saito","Akira Nakayama","Nobutaka Ono"],"pdf_url":"https://arxiv.org/pdf/2404.08264v1.pdf","comment":"13page, 7figure, under review"},{"id":"http://arxiv.org/abs/2312.04552v2","updated":"2024-04-12T18:34:31Z","published":"2023-12-07T18:59:20Z","title":"Generating Illustrated Instructions","summary":"  We introduce the new task of generating Illustrated Instructions, i.e.,\nvisual instructions customized to a user's needs. We identify desiderata unique\nto this task, and formalize it through a suite of automatic and human\nevaluation metrics, designed to measure the validity, consistency, and efficacy\nof the generations. We combine the power of large language models (LLMs)\ntogether with strong text-to-image generation diffusion models to propose a\nsimple approach called StackedDiffusion, which generates such illustrated\ninstructions given text as input. The resulting model strongly outperforms\nbaseline approaches and state-of-the-art multimodal LLMs; and in 30% of cases,\nusers even prefer it to human-generated articles. Most notably, it enables\nvarious new and exciting applications far beyond what static articles on the\nweb can provide, such as personalized instructions complete with intermediate\nsteps and pictures in response to a user's individual situation.\n","authors":["Sachit Menon","Ishan Misra","Rohit Girdhar"],"pdf_url":"https://arxiv.org/pdf/2312.04552v2.pdf","comment":"Accepted to CVPR 2024. Project website:\n  http://facebookresearch.github.io/IllustratedInstructions. Code reproduction:\n  https://github.com/sachit-menon/generating-illustrated-instructions-reproduction"}]},"2024-04-15T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2404.09753v1","updated":"2024-04-15T12:54:31Z","published":"2024-04-15T12:54:31Z","title":"Personalized Collaborative Fine-Tuning for On-Device Large Language\n  Models","summary":"  We explore on-device self-supervised collaborative fine-tuning of large\nlanguage models with limited local data availability. Taking inspiration from\nthe collaborative learning community, we introduce three distinct\ntrust-weighted gradient aggregation schemes: weight similarity-based,\nprediction similarity-based and validation performance-based. To minimize\ncommunication overhead, we integrate Low-Rank Adaptation (LoRA) and only\nexchange LoRA weight updates. Our protocols, driven by prediction and\nperformance metrics, surpass both FedAvg and local fine-tuning methods, which\nis particularly evident in realistic scenarios with more diverse local data\ndistributions. The results underscore the effectiveness of our approach in\naddressing heterogeneity and scarcity within local datasets.\n","authors":["Nicolas Wagner","Dongyang Fan","Martin Jaggi"],"pdf_url":"https://arxiv.org/pdf/2404.09753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09737v1","updated":"2024-04-15T12:38:46Z","published":"2024-04-15T12:38:46Z","title":"Quantization of Large Language Models with an Overdetermined Basis","summary":"  In this paper, we introduce an algorithm for data quantization based on the\nprinciples of Kashin representation. This approach hinges on decomposing any\ngiven vector, matrix, or tensor into two factors. The first factor maintains a\nsmall infinity norm, while the second exhibits a similarly constrained norm\nwhen multiplied by an orthogonal matrix. Surprisingly, the entries of factors\nafter decomposition are well-concentrated around several peaks, which allows us\nto efficiently replace them with corresponding centroids for quantization\npurposes. We study the theoretical properties of the proposed approach and\nrigorously evaluate our compression algorithm in the context of next-word\nprediction tasks and on a set of downstream tasks for text classification. Our\nfindings demonstrate that Kashin Quantization achieves competitive or superior\nquality in model performance while ensuring data compression, marking a\nsignificant advancement in the field of data quantization.\n","authors":["Daniil Merkulov","Daria Cherniuk","Alexander Rudikov","Ivan Oseledets","Ekaterina Muravleva","Aleksandr Mikhalev","Boris Kashin"],"pdf_url":"https://arxiv.org/pdf/2404.09737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05406v2","updated":"2024-04-15T12:38:33Z","published":"2024-04-08T11:14:58Z","title":"PerkwE_COQA: Enhanced Persian Conversational Question Answering by\n  combining contextual keyword extraction with Large Language Models","summary":"  Smart cities need the involvement of their residents to enhance quality of\nlife. Conversational query-answering is an emerging approach for user\nengagement. There is an increasing demand of an advanced conversational\nquestion-answering that goes beyond classic systems. Existing approaches have\nshown that LLMs offer promising capabilities for CQA, but may struggle to\ncapture the nuances of conversational contexts. The new approach involves\nunderstanding the content and engaging in a multi-step conversation with the\nuser to fulfill their needs. This paper presents a novel method to elevate the\nperformance of Persian Conversational question-answering (CQA) systems. It\ncombines the strengths of Large Language Models (LLMs) with contextual keyword\nextraction. Our method extracts keywords specific to the conversational flow,\nproviding the LLM with additional context to understand the user's intent and\ngenerate more relevant and coherent responses. We evaluated the effectiveness\nof this combined approach through various metrics, demonstrating significant\nimprovements in CQA performance compared to an LLM-only baseline. The proposed\nmethod effectively handles implicit questions, delivers contextually relevant\nanswers, and tackles complex questions that rely heavily on conversational\ncontext. The findings indicate that our method outperformed the evaluation\nbenchmarks up to 8% higher than existing methods and the LLM-only baseline.\n","authors":["Pardis Moradbeiki","Nasser Ghadiri"],"pdf_url":"https://arxiv.org/pdf/2404.05406v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09717v1","updated":"2024-04-15T12:20:09Z","published":"2024-04-15T12:20:09Z","title":"Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\n  Large Language Model","summary":"  Many recent studies endeavor to improve open-source language models through\nimitation learning, and re-training on the synthetic instruction data from\nstate-of-the-art proprietary models like ChatGPT and GPT-4. However, the innate\nnature of synthetic data inherently contains noisy data, giving rise to a\nsubstantial presence of low-quality data replete with erroneous responses, and\nflawed reasoning. Although we intuitively grasp the potential harm of noisy\ndata, we lack a quantitative understanding of its impact. To this end, this\npaper explores the correlation between the degree of noise and its impact on\nlanguage models through instruction tuning. We first introduce the\nFalsity-Controllable (FACO) dataset, which comprises pairs of true answers with\ncorresponding reasoning, as well as false pairs to manually control the falsity\nratio of the dataset.Through our extensive experiments, we found multiple\nintriguing findings of the correlation between the factuality of the dataset\nand instruction tuning: Specifically, we verified falsity of the instruction is\nhighly relevant to various benchmark scores. Moreover, when LLMs are trained\nwith false instructions, they learn to lie and generate fake unfaithful\nanswers, even though they know the correct answer for the user request.\nAdditionally, we noted that once the language model is trained with a dataset\ncontaminated by noise, restoring its original performance is possible, but it\nfailed to reach full performance.\n","authors":["Hyunsoo Cho"],"pdf_url":"https://arxiv.org/pdf/2404.09717v1.pdf","comment":"Under review @ *ACL"},{"id":"http://arxiv.org/abs/2404.09696v1","updated":"2024-04-15T11:54:27Z","published":"2024-04-15T11:54:27Z","title":"Are Large Language Models Reliable Argument Quality Annotators?","summary":"  Evaluating the quality of arguments is a crucial aspect of any system\nleveraging argument mining. However, it is a challenge to obtain reliable and\nconsistent annotations regarding argument quality, as this usually requires\ndomain-specific expertise of the annotators. Even among experts, the assessment\nof argument quality is often inconsistent due to the inherent subjectivity of\nthis task. In this paper, we study the potential of using state-of-the-art\nlarge language models (LLMs) as proxies for argument quality annotators. To\nassess the capability of LLMs in this regard, we analyze the agreement between\nmodel, human expert, and human novice annotators based on an established\ntaxonomy of argument quality dimensions. Our findings highlight that LLMs can\nproduce consistent annotations, with a moderately high agreement with human\nexperts across most of the quality dimensions. Moreover, we show that using\nLLMs as additional annotators can significantly improve the agreement between\nannotators. These results suggest that LLMs can serve as a valuable tool for\nautomated argument quality assessment, thus streamlining and accelerating the\nevaluation of large argument datasets.\n","authors":["Nailia Mirzakhmedova","Marcel Gohsen","Chia Hao Chang","Benno Stein"],"pdf_url":"https://arxiv.org/pdf/2404.09696v1.pdf","comment":"18 pages, 5 figures, 5 tables"},{"id":"http://arxiv.org/abs/2404.09695v1","updated":"2024-04-15T11:53:22Z","published":"2024-04-15T11:53:22Z","title":"LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\n  Compression for Large Language Models","summary":"  Large language models (LLMs) show excellent performance in difficult tasks,\nbut they often require massive memories and computational resources. How to\nreduce the parameter scale of LLMs has become research hotspots. In this study,\nwe make an important observation that the multi-head self-attention (MHA)\nsub-layer of Transformer exhibits noticeable low-rank structure, while the\nfeed-forward network (FFN) sub-layer does not. With this regard, we design a\nmixed compression model, which organically combines Low-Rank matrix\napproximation And structured Pruning (LoRAP). For the MHA sub-layer, we propose\nan input activation weighted singular value decomposition method to strengthen\nthe low-rank characteristic. Furthermore, we discover that the weight matrices\nin MHA sub-layer have different low-rank degrees. Thus, a novel parameter\nallocation scheme according to the discrepancy of low-rank degrees is devised.\nFor the FFN sub-layer, we propose a gradient-free structured channel pruning\nmethod. During the pruning, we get an interesting finding that the least\nimportant 1% of parameter actually play a vital role in model performance.\nExtensive evaluations on zero-shot perplexity and zero-shot task classification\nindicate that our proposal is superior to previous structured compression\nrivals under multiple compression ratios.\n","authors":["Guangyan Li","Yongqiang Tang","Wensheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.09695v1.pdf","comment":"8 pages,4 figures"},{"id":"http://arxiv.org/abs/2404.09690v1","updated":"2024-04-15T11:45:30Z","published":"2024-04-15T11:45:30Z","title":"Harnessing GPT-4V(ision) for Insurance: A Preliminary Exploration","summary":"  The emergence of Large Multimodal Models (LMMs) marks a significant milestone\nin the development of artificial intelligence. Insurance, as a vast and complex\ndiscipline, involves a wide variety of data forms in its operational processes,\nincluding text, images, and videos, thereby giving rise to diverse multimodal\ntasks. Despite this, there has been limited systematic exploration of\nmultimodal tasks specific to insurance, nor a thorough investigation into how\nLMMs can address these challenges. In this paper, we explore GPT-4V's\ncapabilities in the insurance domain. We categorize multimodal tasks by\nfocusing primarily on visual aspects based on types of insurance (e.g., auto,\nhousehold/commercial property, health, and agricultural insurance) and\ninsurance stages (e.g., risk assessment, risk monitoring, and claims\nprocessing). Our experiment reveals that GPT-4V exhibits remarkable abilities\nin insurance-related tasks, demonstrating not only a robust understanding of\nmultimodal content in the insurance domain but also a comprehensive knowledge\nof insurance scenarios. However, there are notable shortcomings: GPT-4V\nstruggles with detailed risk rating and loss assessment, suffers from\nhallucination in image understanding, and shows variable support for different\nlanguages. Through this work, we aim to bridge the insurance domain with\ncutting-edge LMM technology, facilitate interdisciplinary exchange and\ndevelopment, and provide a foundation for the continued advancement and\nevolution of future research endeavors.\n","authors":["Chenwei Lin","Hanjia Lyu","Jiebo Luo","Xian Xu"],"pdf_url":"https://arxiv.org/pdf/2404.09690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09682v1","updated":"2024-04-15T11:36:10Z","published":"2024-04-15T11:36:10Z","title":"Multi-News+: Cost-efficient Dataset Cleansing via LLM-based Data\n  Annotation","summary":"  The quality of the dataset is crucial for ensuring optimal performance and\nreliability of downstream task models. However, datasets often contain noisy\ndata inadvertently included during the construction process. Numerous attempts\nhave been made to correct this issue through human annotators. However, hiring\nand managing human annotators is expensive and time-consuming. As an\nalternative, recent studies are exploring the use of large language models\n(LLMs) for data annotation.\n  In this study, we present a case study that extends the application of\nLLM-based data annotation to enhance the quality of existing datasets through a\ncleansing strategy. Specifically, we leverage approaches such as\nchain-of-thought (CoT) and majority voting to imitate human annotation and\nclassify unrelated documents from the Multi-News dataset, which is widely used\nfor the multi-document summarization task. Through our proposed cleansing\nmethod, we introduce an enhanced Multi-News+. By employing LLMs for data\ncleansing, we demonstrate an efficient and effective approach to improving\ndataset quality without relying on expensive human annotation efforts.\n","authors":["Juhwan Choi","Jungmin Yun","Kyohoon Jin","YoungBin Kim"],"pdf_url":"https://arxiv.org/pdf/2404.09682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07950v4","updated":"2024-04-15T10:57:16Z","published":"2023-12-13T07:56:27Z","title":"CBQ: Cross-Block Quantization for Large Language Models","summary":"  Post-training quantization (PTQ) has played a key role in compressing large\nlanguage models (LLMs) with ultra-low costs. However, existing PTQ methods only\nfocus on handling the outliers within one layer or one block, which ignores the\ndependency of blocks and leads to severe performance degradation in low-bit\nsettings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ\nmethod for LLMs. CBQ employs a cross-block dependency using a homologous\nreconstruction scheme, establishing long-range dependencies across multiple\nblocks to minimize error accumulation. Furthermore, CBQ incorporates a\ncoarse-to-fine preprocessing (CFP) strategy for suppressing weight and\nactivation outliers, coupled with an adaptive LoRA-Rounding technique for\nprecise weight quantization. These innovations enable CBQ to not only handle\nextreme outliers effectively but also improve overall quantization accuracy.\nExtensive experiments show that CBQ achieves superior low-bit quantization\n(W4A4, W4A8, W2A16) and outperforms existing state-of-the-art methods across\nvarious LLMs and datasets. Notably, CBQ quantizes the 4-bit LLAMA1-65B model\nwithin only 4.3 hours on a single GPU, achieving a commendable tradeoff between\nperformance and quantization efficiency.\n","authors":["Xin Ding","Xiaoyu Liu","Zhijun Tu","Yun Zhang","Wei Li","Jie Hu","Hanting Chen","Yehui Tang","Zhiwei Xiong","Baoqun Yin","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2312.07950v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09656v1","updated":"2024-04-15T10:44:31Z","published":"2024-04-15T10:44:31Z","title":"Learn Your Reference Model for Real Good Alignment","summary":"  The complexity of the alignment problem stems from the fact that existing\nmethods are unstable. Researchers continuously invent various tricks to address\nthis shortcoming. For instance, in the fundamental Reinforcement Learning From\nHuman Feedback (RLHF) technique of Language Model alignment, in addition to\nreward maximization, the Kullback-Leibler divergence between the trainable\npolicy and the SFT policy is minimized. This addition prevents the model from\nbeing overfitted to the Reward Model (RM) and generating texts that are\nout-of-domain for the RM. The Direct Preference Optimization (DPO) method\nreformulates the optimization task of RLHF and eliminates the Reward Model\nwhile tacitly maintaining the requirement for the policy to be close to the SFT\npolicy. In our paper, we argue that this implicit limitation in the DPO method\nleads to sub-optimal results. We propose a new method called Trust Region DPO\n(TR-DPO), which updates the reference policy during training. With such a\nstraightforward update, we demonstrate the effectiveness of TR-DPO against DPO\non the Anthropic HH and TLDR datasets. We show that TR-DPO outperforms DPO by\nup to 19%, measured by automatic evaluation with GPT-4. The new alignment\napproach that we propose allows us to improve the quality of models across\nseveral parameters at once, such as coherence, correctness, level of detail,\nhelpfulness, and harmlessness.\n","authors":["Alexey Gorbatovski","Boris Shaposhnikov","Alexey Malakhov","Nikita Surnachev","Yaroslav Aksenov","Ian Maksimov","Nikita Balagansky","Daniil Gavrilov"],"pdf_url":"https://arxiv.org/pdf/2404.09656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09645v1","updated":"2024-04-15T10:24:32Z","published":"2024-04-15T10:24:32Z","title":"Real-world Instance-specific Image Goal Navigation for Service Robots:\n  Bridging the Domain Gap with Contrastive Learning","summary":"  Improving instance-specific image goal navigation (InstanceImageNav), which\nlocates the identical object in a real-world environment from a query image, is\nessential for robotic systems to assist users in finding desired objects. The\nchallenge lies in the domain gap between low-quality images observed by the\nmoving robot, characterized by motion blur and low-resolution, and high-quality\nquery images provided by the user. Such domain gaps could significantly reduce\nthe task success rate but have not been the focus of previous work. To address\nthis, we propose a novel method called Few-shot Cross-quality Instance-aware\nAdaptation (CrossIA), which employs contrastive learning with an instance\nclassifier to align features between massive low- and few high-quality images.\nThis approach effectively reduces the domain gap by bringing the latent\nrepresentations of cross-quality images closer on an instance basis.\nAdditionally, the system integrates an object image collection with a\npre-trained deblurring model to enhance the observed image quality. Our method\nfine-tunes the SimSiam model, pre-trained on ImageNet, using CrossIA. We\nevaluated our method's effectiveness through an InstanceImageNav task with 20\ndifferent types of instances, where the robot identifies the same instance in a\nreal-world environment as a high-quality query image. Our experiments showed\nthat our method improves the task success rate by up to three times compared to\nthe baseline, a conventional approach based on SuperGlue. These findings\nhighlight the potential of leveraging contrastive learning and image\nenhancement techniques to bridge the domain gap and improve object localization\nin robotic applications. The project website is\nhttps://emergentsystemlabstudent.github.io/DomainBridgingNav/.\n","authors":["Taichi Sakaguchi","Akira Taniguchi","Yoshinobu Hagiwara","Lotfi El Hafi","Shoichi Hasegawa","Tadahiro Taniguchi"],"pdf_url":"https://arxiv.org/pdf/2404.09645v1.pdf","comment":"See website at\n  https://emergentsystemlabstudent.github.io/DomainBridgingNav/. Submitted to\n  IROS2024"},{"id":"http://arxiv.org/abs/2404.09615v1","updated":"2024-04-15T09:37:52Z","published":"2024-04-15T09:37:52Z","title":"If there's a Trigger Warning, then where's the Trigger? Investigating\n  Trigger Warnings at the Passage Level","summary":"  Trigger warnings are labels that preface documents with sensitive content if\nthis content could be perceived as harmful by certain groups of readers. Since\nwarnings about a document intuitively need to be shown before reading it,\nauthors usually assign trigger warnings at the document level. What parts of\ntheir writing prompted them to assign a warning, however, remains unclear. We\ninvestigate for the first time the feasibility of identifying the triggering\npassages of a document, both manually and computationally. We create a dataset\nof 4,135 English passages, each annotated with one of eight common trigger\nwarnings. In a large-scale evaluation, we then systematically evaluate the\neffectiveness of fine-tuned and few-shot classifiers, and their\ngeneralizability. We find that trigger annotation belongs to the group of\nsubjective annotation tasks in NLP, and that automatic trigger classification\nremains challenging but feasible.\n","authors":["Matti Wiegmann","Jennifer Rakete","Magdalena Wolska","Benno Stein","Martin Potthast"],"pdf_url":"https://arxiv.org/pdf/2404.09615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09593v1","updated":"2024-04-15T09:03:05Z","published":"2024-04-15T09:03:05Z","title":"Improving Recall of Large Language Models: A Model Collaboration\n  Approach for Relational Triple Extraction","summary":"  Relation triple extraction, which outputs a set of triples from long\nsentences, plays a vital role in knowledge acquisition. Large language models\ncan accurately extract triples from simple sentences through few-shot learning\nor fine-tuning when given appropriate instructions. However, they often miss\nout when extracting from complex sentences. In this paper, we design an\nevaluation-filtering framework that integrates large language models with small\nmodels for relational triple extraction tasks. The framework includes an\nevaluation model that can extract related entity pairs with high precision. We\npropose a simple labeling principle and a deep neural network to build the\nmodel, embedding the outputs as prompts into the extraction process of the\nlarge model. We conduct extensive experiments to demonstrate that the proposed\nmethod can assist large language models in obtaining more accurate extraction\nresults, especially from complex sentences containing multiple relational\ntriples. Our evaluation model can also be embedded into traditional extraction\nmodels to enhance their extraction precision from complex sentences.\n","authors":["Zepeng Ding","Wenhao Huang","Jiaqing Liang","Deqing Yang","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2404.09593v1.pdf","comment":"Accepted at LREC-COLING 2024 main conference"},{"id":"http://arxiv.org/abs/2307.12973v2","updated":"2024-04-15T09:00:26Z","published":"2023-07-24T17:49:31Z","title":"Wisdom of Instruction-Tuned Language Model Crowds. Exploring Model Label\n  Variation","summary":"  Large Language Models (LLMs) exhibit remarkable text classification\ncapabilities, excelling in zero- and few-shot learning (ZSL and FSL) scenarios.\nHowever, since they are trained on different datasets, performance varies\nwidely across tasks between those models. Recent studies emphasize the\nimportance of considering human label variation in data annotation. However,\nhow this human label variation also applies to LLMs remains unexplored. Given\nthis likely model specialization, we ask: Do aggregate LLM labels improve over\nindividual models (as for human annotators)? We evaluate four recent\ninstruction-tuned LLMs as annotators on five subjective tasks across four\nlanguages. We use ZSL and FSL setups and label aggregation from human\nannotation. Aggregations are indeed substantially better than any individual\nmodel, benefiting from specialization in diverse tasks or languages.\nSurprisingly, FSL does not surpass ZSL, as it depends on the quality of the\nselected examples. However, there seems to be no good information-theoretical\nstrategy to select those. We find that no LLM method rivals even simple\nsupervised models. We also discuss the tradeoffs in accuracy, cost, and\nmoral/ethical considerations between LLM and human annotation.\n","authors":["Flor Miriam Plaza-del-Arco","Debora Nozza","Dirk Hovy"],"pdf_url":"https://arxiv.org/pdf/2307.12973v2.pdf","comment":"Accepted to the 3rd Workshop on Perspectivist Approaches to NLP at\n  LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2404.09579v1","updated":"2024-04-15T08:40:01Z","published":"2024-04-15T08:40:01Z","title":"Modelling Language","summary":"  This paper argues that large language models have a valuable scientific role\nto play in serving as scientific models of a language. Linguistic study should\nnot only be concerned with the cognitive processes behind linguistic\ncompetence, but also with language understood as an external, social entity.\nOnce this is recognized, the value of large language models as scientific\nmodels becomes clear. This paper defends this position against a number of\narguments to the effect that language models provide no linguistic insight. It\nalso draws upon recent work in philosophy of science to show how large language\nmodels could serve as scientific models.\n","authors":["Jumbly Grindrod"],"pdf_url":"https://arxiv.org/pdf/2404.09579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09577v1","updated":"2024-04-15T08:38:43Z","published":"2024-04-15T08:38:43Z","title":"Transformers, Contextualism, and Polysemy","summary":"  The transformer architecture, introduced by Vaswani et al. (2017), is at the\nheart of the remarkable recent progress in the development of language models,\nincluding famous chatbots such as Chat-gpt and Bard. In this paper, I argue\nthat we an extract from the way the transformer architecture works a picture of\nthe relationship between context and meaning. I call this the transformer\npicture, and I argue that it is a novel with regard to two related\nphilosophical debates: the contextualism debate regarding the extent of\ncontext-sensitivity across natural language, and the polysemy debate regarding\nhow polysemy should be captured within an account of word meaning. Although\nmuch of the paper merely tries to position the transformer picture with respect\nto these two debates, I will also begin to make the case for the transformer\npicture.\n","authors":["Jumbly Grindrod"],"pdf_url":"https://arxiv.org/pdf/2404.09577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09576v1","updated":"2024-04-15T08:37:26Z","published":"2024-04-15T08:37:26Z","title":"Large language models and linguistic intentionality","summary":"  Do large language models like Chat-GPT or LLaMa meaningfully use the words\nthey produce? Or are they merely clever prediction machines, simulating\nlanguage use by producing statistically plausible text? There have already been\nsome initial attempts to answer this question by showing that these models meet\nthe criteria for entering meaningful states according to metasemantic theories\nof mental content. In this paper, I will argue for a different approach - that\nwe should instead consider whether language models meet the criteria given by\nour best metasemantic theories of linguistic content. In that vein, I will\nillustrate how this can be done by applying two such theories to the case of\nlanguage models: Gareth Evans' (1982) account of naming practices and Ruth\nMillikan's (1984, 2004, 2005) teleosemantics. In doing so, I will argue that it\nis a mistake to think that the failure of LLMs to meet plausible conditions for\nmental intentionality thereby renders their outputs meaningless, and that a\ndistinguishing feature of linguistic intentionality - dependency on a\npre-existing linguistic system - allows for the plausible result LLM outputs\nare meaningful.\n","authors":["Jumbly Grindrod"],"pdf_url":"https://arxiv.org/pdf/2404.09576v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09565v1","updated":"2024-04-15T08:27:47Z","published":"2024-04-15T08:27:47Z","title":"Reliability Estimation of News Media Sources: Birds of a Feather Flock\n  Together","summary":"  Evaluating the reliability of news sources is a routine task for journalists\nand organizations committed to acquiring and disseminating accurate\ninformation. Recent research has shown that predicting sources' reliability\nrepresents an important first-prior step in addressing additional challenges\nsuch as fake news detection and fact-checking. In this paper, we introduce a\nnovel approach for source reliability estimation that leverages reinforcement\nlearning strategies for estimating the reliability degree of news sources.\nContrary to previous research, our proposed approach models the problem as the\nestimation of a reliability degree, and not a reliability label, based on how\nall the news media sources interact with each other on the Web. We validated\nthe effectiveness of our method on a news media reliability dataset that is an\norder of magnitude larger than comparable existing datasets. Results show that\nthe estimated reliability degrees strongly correlates with journalists-provided\nscores (Spearman=0.80) and can effectively predict reliability labels\n(macro-avg. F$_1$ score=81.05). We release our implementation and dataset,\naiming to provide a valuable resource for the NLP community working on\ninformation verification.\n","authors":["Sergio Burdisso","Dairazalia S√°nchez-Cort√©s","Esa√∫ Villatoro-Tello","Petr Motlicek"],"pdf_url":"https://arxiv.org/pdf/2404.09565v1.pdf","comment":"Accepted to NAACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2302.02568v4","updated":"2024-04-15T08:11:18Z","published":"2023-02-06T05:11:27Z","title":"Less is More: Understanding Word-level Textual Adversarial Attack via\n  n-gram Frequency Descend","summary":"  Word-level textual adversarial attacks have demonstrated notable efficacy in\nmisleading Natural Language Processing (NLP) models. Despite their success, the\nunderlying reasons for their effectiveness and the fundamental characteristics\nof adversarial examples (AEs) remain obscure. This work aims to interpret\nword-level attacks by examining their $n$-gram frequency patterns. Our\ncomprehensive experiments reveal that in approximately 90\\% of cases,\nword-level attacks lead to the generation of examples where the frequency of\n$n$-grams decreases, a tendency we term as the $n$-gram Frequency Descend\n($n$-FD). This finding suggests a straightforward strategy to enhance model\nrobustness: training models using examples with $n$-FD. To examine the\nfeasibility of this strategy, we employed the $n$-gram frequency information,\nas an alternative to conventional loss gradients, to generate perturbed\nexamples in adversarial training. The experiment results indicate that the\nfrequency-based approach performs comparably with the gradient-based approach\nin improving model robustness. Our research offers a novel and more intuitive\nperspective for understanding word-level textual adversarial attacks and\nproposes a new direction to improve model robustness.\n","authors":["Ning Lu","Shengcai Liu","Zhirui Zhang","Qi Wang","Haifeng Liu","Ke Tang"],"pdf_url":"https://arxiv.org/pdf/2302.02568v4.pdf","comment":"To be published in: 2024 IEEE Conference on Artificial Intelligence\n  (CAI 2024)"},{"id":"http://arxiv.org/abs/2309.03409v3","updated":"2024-04-15T07:50:32Z","published":"2023-09-07T00:07:15Z","title":"Large Language Models as Optimizers","summary":"  Optimization is ubiquitous. While derivative-based algorithms have been\npowerful tools for various problems, the absence of gradient imposes challenges\non many real-world applications. In this work, we propose Optimization by\nPROmpting (OPRO), a simple and effective approach to leverage large language\nmodels (LLMs) as optimizers, where the optimization task is described in\nnatural language. In each optimization step, the LLM generates new solutions\nfrom the prompt that contains previously generated solutions with their values,\nthen the new solutions are evaluated and added to the prompt for the next\noptimization step. We first showcase OPRO on linear regression and traveling\nsalesman problems, then move on to our main application in prompt optimization,\nwhere the goal is to find instructions that maximize the task accuracy. With a\nvariety of LLMs, we demonstrate that the best prompts optimized by OPRO\noutperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on\nBig-Bench Hard tasks. Code at https://github.com/google-deepmind/opro.\n","authors":["Chengrun Yang","Xuezhi Wang","Yifeng Lu","Hanxiao Liu","Quoc V. Le","Denny Zhou","Xinyun Chen"],"pdf_url":"https://arxiv.org/pdf/2309.03409v3.pdf","comment":"ICLR 2024; 42 pages, 26 figures, 15 tables. Code at\n  https://github.com/google-deepmind/opro"},{"id":"http://arxiv.org/abs/2404.09529v1","updated":"2024-04-15T07:49:10Z","published":"2024-04-15T07:49:10Z","title":"Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\n  in Large Language Models","summary":"  During inference for transformer-based large language models (LLM),\nprefilling is the computation of the key-value (KV) cache for input tokens in\nthe prompt prior to autoregressive generation. For longer input prompt lengths,\nprefilling will incur a significant overhead on decoding time. In this work, we\nhighlight the following pitfall of prefilling: for batches containing\nhigh-varying prompt lengths, significant computation is wasted by the standard\npractice of padding sequences to the maximum length. As LLMs increasingly\nsupport longer context lengths, potentially up to 10 million tokens, variations\nin prompt lengths within a batch become more pronounced. To address this, we\npropose Prepacking, a simple yet effective method to optimize prefilling\ncomputation. To avoid redundant computation on pad tokens, prepacking combines\nprompts of varying lengths into a sequence and packs multiple sequences into a\ncompact batch using a bin-packing algorithm. It then modifies the attention\nmask and positional encoding to compute multiple prefilled KV-caches for\nmultiple prompts within a single sequence. On standard curated dataset\ncontaining prompts with varying lengths, we obtain a significant speed and\nmemory efficiency improvements as compared to the default padding-based\nprefilling computation within Huggingface across a range of base model\nconfigurations and inference serving scenarios.\n","authors":["Siyan Zhao","Daniel Israel","Guy Van den Broeck","Aditya Grover"],"pdf_url":"https://arxiv.org/pdf/2404.09529v1.pdf","comment":"18 pages, code in https://github.com/siyan-zhao/prepacking"},{"id":"http://arxiv.org/abs/2312.05356v3","updated":"2024-04-15T07:31:00Z","published":"2023-12-08T20:28:08Z","title":"Neuron-level LLM Patching for Code Generation","summary":"  Large Language Models (LLMs) have found widespread adoption in software\nengineering, particularly in code generation tasks. However, updating these\nmodels with new knowledge can be prohibitively expensive, yet it is essential\nfor maximizing their utility. In this paper, we propose a novel and effective\nmodel editing approach, \\textsc{MENT}, to patch LLMs in coding tasks.\n\\textsc{MENT} is effective, efficient, and reliable. It can correct a neural\nmodel by patching 1 or 2 neurons. As the pioneer work on neuron-level model\nediting of generative models, we formalize the editing process and introduce\nthe involved concepts. Besides, we also introduce new measures to evaluate its\ngeneralization ability, and build a benchmark for further study. Our approach\nis evaluated on three coding tasks, including API-seq recommendation,\nline-level code generation, and pseudocode-to-code transaction. The\nexperimental results show that the proposed approach outperforms the state of\nthe arts by a significant margin in both effectiveness and efficiency measures.\nIn addition, we demonstrate the usages of \\textsc{MENT} for LLM reasoning in\nsoftware engineering. By editing LLM knowledge, the directly or indirectly\ndependent behaviors of API invocation in the chain-of-thought will change\naccordingly. It explained the significance of repairing LLMs.\n","authors":["Jian Gu","Aldeida Aleti","Chunyang Chen","Hongyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.05356v3.pdf","comment":"12 pages, 6 figures, 6 tables, under peer-review"},{"id":"http://arxiv.org/abs/2402.14320v4","updated":"2024-04-15T07:28:10Z","published":"2024-02-22T06:23:37Z","title":"Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve\n  Knowledge Base Question Answering","summary":"  Recent progress with LLM-based agents has shown promising results across\nvarious tasks. However, their use in answering questions from knowledge bases\nremains largely unexplored. Implementing a KBQA system using traditional\nmethods is challenging due to the shortage of task-specific training data and\nthe complexity of creating task-focused model structures. In this paper, we\npresent Triad, a unified framework that utilizes an LLM-based agent with three\nroles for KBQA tasks. The agent is assigned three roles to tackle different\nKBQA subtasks: agent as a generalist for mastering various subtasks, as a\ndecision maker for the selection of candidates, and as an advisor for answering\nquestions with knowledge. Our KBQA framework is executed in four phases,\ninvolving the collaboration of the agent's multiple roles. We evaluated the\nperformance of our framework using three benchmark datasets, and the results\nshow that our framework outperforms state-of-the-art systems on the LC-QuAD and\nYAGO-QA benchmarks, yielding F1 scores of 11.8% and 20.7%, respectively.\n","authors":["Chang Zong","Yuchen Yan","Weiming Lu","Eliot Huang","Jian Shao","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2402.14320v4.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2308.08043v4","updated":"2024-04-15T07:25:40Z","published":"2023-08-15T21:14:09Z","title":"DiagGPT: An LLM-based and Multi-agent Dialogue System with Automatic\n  Topic Management for Flexible Task-Oriented Dialogue","summary":"  A significant application of Large Language Models (LLMs), like ChatGPT, is\ntheir deployment as chat agents, which respond to human inquiries across a\nvariety of domains. While current LLMs proficiently answer general questions,\nthey often fall short in complex diagnostic scenarios such as legal, medical,\nor other specialized consultations. These scenarios typically require\nTask-Oriented Dialogue (TOD), where an AI chat agent must proactively pose\nquestions and guide users toward specific goals or task completion. Previous\nfine-tuning models have underperformed in TOD and the full potential of\nconversational capability in current LLMs has not yet been fully explored. In\nthis paper, we introduce DiagGPT (Dialogue in Diagnosis GPT), an innovative\napproach that extends LLMs to more TOD scenarios. In addition to guiding users\nto complete tasks, DiagGPT can effectively manage the status of all topics\nthroughout the dialogue development. This feature enhances user experience and\noffers a more flexible interaction in TOD. Our experiments demonstrate that\nDiagGPT exhibits outstanding performance in conducting TOD with users, showing\nits potential for practical applications in various fields.\n","authors":["Lang Cao"],"pdf_url":"https://arxiv.org/pdf/2308.08043v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09516v1","updated":"2024-04-15T07:24:45Z","published":"2024-04-15T07:24:45Z","title":"State Space Model for New-Generation Network Alternative to\n  Transformers: A Survey","summary":"  In the post-deep learning era, the Transformer architecture has demonstrated\nits powerful performance across pre-trained big models and various downstream\ntasks. However, the enormous computational demands of this architecture have\ndeterred many researchers. To further reduce the complexity of attention\nmodels, numerous efforts have been made to design more efficient methods. Among\nthem, the State Space Model (SSM), as a possible replacement for the\nself-attention based Transformer model, has drawn more and more attention in\nrecent years. In this paper, we give the first comprehensive review of these\nworks and also provide experimental comparisons and analysis to better\ndemonstrate the features and advantages of SSM. Specifically, we first give a\ndetailed description of principles to help the readers quickly capture the key\nideas of SSM. After that, we dive into the reviews of existing SSMs and their\nvarious applications, including natural language processing, computer vision,\ngraph, multi-modal and multi-media, point cloud/event stream, time series data,\nand other domains. In addition, we give statistical comparisons and analysis of\nthese models and hope it helps the readers to understand the effectiveness of\ndifferent structures on various tasks. Then, we propose possible research\npoints in this direction to better promote the development of the theoretical\nmodel and application of SSM. More related works will be continuously updated\non the following GitHub:\nhttps://github.com/Event-AHU/Mamba_State_Space_Model_Paper_List.\n","authors":["Xiao Wang","Shiao Wang","Yuhe Ding","Yuehang Li","Wentao Wu","Yao Rong","Weizhe Kong","Ju Huang","Shihao Li","Haoxiang Yang","Ziwen Wang","Bo Jiang","Chenglong Li","Yaowei Wang","Yonghong Tian","Jin Tang"],"pdf_url":"https://arxiv.org/pdf/2404.09516v1.pdf","comment":"The First review of State Space Model (SSM)/Mamba and their\n  applications in artificial intelligence, 33 pages"},{"id":"http://arxiv.org/abs/2402.00658v2","updated":"2024-04-15T06:36:24Z","published":"2024-02-01T15:18:33Z","title":"Learning Planning-based Reasoning by Trajectories Collection and Process\n  Reward Synthesizing","summary":"  Large Language Models (LLMs) have demonstrated significant potential in\nhandling complex reasoning tasks through step-by-step rationale generation.\nHowever, recent studies have raised concerns regarding the hallucination and\nflaws in their reasoning process. Substantial efforts are being made to improve\nthe reliability and faithfulness of the generated rationales. Some approaches\nmodel reasoning as planning, while others focus on annotating for process\nsupervision. Nevertheless, the planning-based search process often results in\nhigh latency due to the frequent assessment of intermediate reasoning states\nand the extensive exploration space. Additionally, supervising the reasoning\nprocess with human annotation is costly and challenging to scale for LLM\ntraining. To address these issues, in this paper, we propose a framework to\nlearn planning-based reasoning through Direct Preference Optimization (DPO) on\ncollected trajectories, which are ranked according to synthesized process\nrewards. Our results on challenging logical reasoning benchmarks demonstrate\nthe effectiveness of our learning framework, showing that our 7B model can\nsurpass the strong counterparts like GPT-3.5-Turbo.\n","authors":["Fangkai Jiao","Chengwei Qin","Zhengyuan Liu","Nancy F. Chen","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2402.00658v2.pdf","comment":"17 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.11430v2","updated":"2024-04-15T06:34:04Z","published":"2024-03-18T02:53:49Z","title":"A Novel Paradigm Boosting Translation Capabilities of Large Language\n  Models","summary":"  This paper presents a study on strategies to enhance the translation\ncapabilities of large language models (LLMs) in the context of machine\ntranslation (MT) tasks. The paper proposes a novel paradigm consisting of three\nstages: Secondary Pre-training using Extensive Monolingual Data, Continual\nPre-training with Interlinear Text Format Documents, and Leveraging\nSource-Language Consistent Instruction for Supervised Fine-Tuning. Previous\nresearch on LLMs focused on various strategies for supervised fine-tuning\n(SFT), but their effectiveness has been limited. While traditional machine\ntranslation approaches rely on vast amounts of parallel bilingual data, our\nparadigm highlights the importance of using smaller sets of high-quality\nbilingual data. We argue that the focus should be on augmenting LLMs'\ncross-lingual alignment abilities during pre-training rather than solely\nrelying on extensive bilingual data during SFT. Experimental results conducted\nusing the Llama2 model, particularly on Chinese-Llama2 after monolingual\naugmentation, demonstrate the improved translation capabilities of LLMs. A\nsignificant contribution of our approach lies in Stage2: Continual Pre-training\nwith Interlinear Text Format Documents, which requires less than 1B training\ndata, making our method highly efficient. Additionally, in Stage3, we observed\nthat setting instructions consistent with the source language benefits the\nsupervised fine-tuning process. Experimental results demonstrate that our\napproach surpasses previous work and achieves superior performance compared to\nmodels such as NLLB-54B and GPT3.5-text-davinci-003, despite having a\nsignificantly smaller parameter count of only 7B or 13B. This achievement\nestablishes our method as a pioneering strategy in the field of machine\ntranslation.\n","authors":["Jiaxin Guo","Hao Yang","Zongyao Li","Daimeng Wei","Hengchao Shang","Xiaoyu Chen"],"pdf_url":"https://arxiv.org/pdf/2403.11430v2.pdf","comment":"Accepted in NAACL 2024"},{"id":"http://arxiv.org/abs/2404.09492v1","updated":"2024-04-15T06:28:20Z","published":"2024-04-15T06:28:20Z","title":"Bridging the Gap between Different Vocabularies for LLM Ensemble","summary":"  Ensembling different large language models (LLMs) to unleash their\ncomplementary potential and harness their individual strengths is highly\nvaluable. Nevertheless, vocabulary discrepancies among various LLMs have\nconstrained previous studies to either selecting or blending completely\ngenerated outputs. This limitation hinders the dynamic correction and\nenhancement of outputs during the generation process, resulting in a limited\ncapacity for effective ensemble. To address this issue, we propose a novel\nmethod to Ensemble LLMs via Vocabulary Alignment (EVA). EVA bridges the lexical\ngap among various LLMs, enabling meticulous ensemble at each generation step.\nSpecifically, we first learn mappings between the vocabularies of different\nLLMs with the assistance of overlapping tokens. Subsequently, these mappings\nare employed to project output distributions of LLMs into a unified space,\nfacilitating a fine-grained ensemble. Finally, we design a filtering strategy\nto exclude models that generate unfaithful tokens. Experimental results on\ncommonsense reasoning, arithmetic reasoning, machine translation, and\ndata-to-text generation tasks demonstrate the superiority of our approach\ncompared with individual LLMs and previous ensemble methods conducted on\ncomplete outputs. Further analyses confirm that our approach can leverage\nknowledge from different language models and yield consistent improvement.\n","authors":["Yangyifan Xu","Jinliang Lu","Jiajun Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.09492v1.pdf","comment":"Accepted to the main conference of NAACL 2024"},{"id":"http://arxiv.org/abs/2404.09486v1","updated":"2024-04-15T06:15:46Z","published":"2024-04-15T06:15:46Z","title":"MMCode: Evaluating Multi-Modal Code Large Language Models with Visually\n  Rich Programming Problems","summary":"  Programming often involves converting detailed and complex specifications\ninto code, a process during which developers typically utilize visual aids to\nmore effectively convey concepts. While recent developments in Large Multimodal\nModels have demonstrated remarkable abilities in visual reasoning and\nmathematical tasks, there is little work on investigating whether these models\ncan effectively interpret visual elements for code generation. To this end, we\npresent MMCode, the first multi-modal coding dataset for evaluating algorithmic\nproblem-solving skills in visually rich contexts. MMCode contains 3,548\nquestions and 6,620 images collected from real-world programming challenges\nharvested from 10 code competition websites, presenting significant challenges\ndue to the extreme demand for reasoning abilities. Our experiment results show\nthat current state-of-the-art models struggle to solve these problems. The\nresults highlight the lack of powerful vision-code models, and we hope MMCode\ncan serve as an inspiration for future works in this domain. The data and code\nare publicly available at https://github.com/happylkx/MMCode.\n","authors":["Kaixin Li","Yuchen Tian","Qisheng Hu","Ziyang Luo","Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2404.09486v1.pdf","comment":"46 pages, 21 figures and 6 tables"},{"id":"http://arxiv.org/abs/2404.01744v4","updated":"2024-04-15T06:09:24Z","published":"2024-04-02T09:01:32Z","title":"Octopus v2: On-device language model for super agent","summary":"  Language models have shown effectiveness in a variety of software\napplications, particularly in tasks related to automatic workflow. These models\npossess the crucial ability to call functions, which is essential in creating\nAI agents. Despite the high performance of large-scale language models in cloud\nenvironments, they are often associated with concerns over privacy and cost.\nCurrent on-device models for function calling face issues with latency and\naccuracy. Our research presents a new method that empowers an on-device model\nwith 2 billion parameters to surpass the performance of GPT-4 in both accuracy\nand latency, and decrease the context length by 95\\%. When compared to Llama-7B\nwith a RAG-based function calling mechanism, our method enhances latency by\n35-fold. This method reduces the latency to levels deemed suitable for\ndeployment across a variety of edge devices in production environments,\naligning with the performance requisites for real-world applications.\n","authors":["Wei Chen","Zhiyuan Li"],"pdf_url":"https://arxiv.org/pdf/2404.01744v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09480v1","updated":"2024-04-15T06:06:43Z","published":"2024-04-15T06:06:43Z","title":"Mitigating Hallucination in Abstractive Summarization with\n  Domain-Conditional Mutual Information","summary":"  A primary challenge in abstractive summarization is hallucination -- the\nphenomenon where a model generates plausible text that is absent in the source\ntext. We hypothesize that the domain (or topic) of the source text triggers the\nmodel to generate text that is highly probable in the domain, neglecting the\ndetails of the source text. To alleviate this model bias, we introduce a\ndecoding strategy based on domain-conditional pointwise mutual information.\nThis strategy adjusts the generation probability of each token by comparing it\nwith the token's marginal probability within the domain of the source text.\nAccording to evaluation on the XSUM dataset, our method demonstrates\nimprovement in terms of faithfulness and source relevance. The code is publicly\navailable at \\url{https://github.com/qqplot/dcpmi}.\n","authors":["Kyubyung Chae","Jaepill Choi","Yohan Jo","Taesup Kim"],"pdf_url":"https://arxiv.org/pdf/2404.09480v1.pdf","comment":"Accepted by Findings of NAACL 2024"},{"id":"http://arxiv.org/abs/2404.01343v2","updated":"2024-04-15T06:03:56Z","published":"2024-03-31T07:11:48Z","title":"CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs","summary":"  Businesses and software platforms are increasingly turning to Large Language\nModels (LLMs) such as GPT-3.5, GPT-4, GLM-3, and LLaMa-2 for chat assistance\nwith file access or as reasoning agents for customer service. However, current\nLLM-based customer service models have limited integration with customer\nprofiles and lack the operational capabilities necessary for effective service.\nMoreover, existing API integrations emphasize diversity over the precision and\nerror avoidance essential in real-world customer service scenarios. To address\nthese issues, we propose an LLM agent named CHOPS (CHat with custOmer Profile\nin existing System), designed to: (1) efficiently utilize existing databases or\nsystems for accessing user information or interacting with these systems\nfollowing existing guidelines; (2) provide accurate and reasonable responses or\ncarry out required operations in the system while avoiding harmful operations;\nand (3) leverage a combination of small and large LLMs to achieve satisfying\nperformance at a reasonable inference cost. We introduce a practical dataset,\nthe CPHOS-dataset, which includes a database, guiding files, and QA pairs\ncollected from CPHOS, an online platform that facilitates the organization of\nsimulated Physics Olympiads for high school teachers and students. We have\nconducted extensive experiments to validate the performance of our proposed\nCHOPS architecture using the CPHOS-dataset, with the aim of demonstrating how\nLLMs can enhance or serve as alternatives to human customer service. Code for\nour proposed architecture and dataset can be found at\n{https://github.com/JingzheShi/CHOPS}.\n","authors":["Jingzhe Shi","Jialuo Li","Qinwei Ma","Zaiwen Yang","Huan Ma","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2404.01343v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2305.15083v4","updated":"2024-04-15T06:02:59Z","published":"2023-05-24T12:00:24Z","title":"Eliciting the Translation Ability of Large Language Models via\n  Multilingual Finetuning with Translation Instructions","summary":"  Large-scale Pretrained Language Models (LLMs), such as ChatGPT and GPT4, have\nshown strong abilities in multilingual translations, without being explicitly\ntrained on parallel corpora. It is interesting how the LLMs obtain their\nability to carry out translation instructions for different languages. In this\npaper, we present a detailed analysis by finetuning a multilingual pretrained\nlanguage model, XGLM-7B, to perform multilingual translation following given\ninstructions. Firstly, we show that multilingual LLMs have stronger translation\nabilities than previously demonstrated. For a certain language, the performance\ndepends on its similarity to English and the amount of data used in the\npretraining phase. Secondly, we find that LLMs' ability to carry out\ntranslation instructions relies on the understanding of translation\ninstructions and the alignment among different languages. With multilingual\nfinetuning, LLMs could learn to perform the translation task well even for\nthose language pairs unseen during the instruction tuning phase.\n","authors":["Jiahuan Li","Hao Zhou","Shujian Huang","Shanbo Cheng","Jiajun Chen"],"pdf_url":"https://arxiv.org/pdf/2305.15083v4.pdf","comment":"accepted by Transaction of ACL, pre-MIT version"},{"id":"http://arxiv.org/abs/2403.09488v3","updated":"2024-04-15T04:29:33Z","published":"2024-03-14T15:30:14Z","title":"Rectifying Demonstration Shortcut in In-Context Learning","summary":"  Large language models (LLMs) are able to solve various tasks with only a few\ndemonstrations utilizing their in-context learning (ICL) abilities. However,\nLLMs often rely on their pre-trained semantic priors of demonstrations rather\nthan on the input-label relationships to proceed with ICL prediction. In this\nwork, we term this phenomenon as the 'Demonstration Shortcut'. While previous\nworks have primarily focused on improving ICL prediction results for predefined\ntasks, we aim to rectify the Demonstration Shortcut, thereby enabling the LLM\nto effectively learn new input-label relationships from demonstrations. To\nachieve this, we introduce In-Context Calibration, a demonstration-aware\ncalibration method. We evaluate the effectiveness of the proposed method in two\nsettings: (1) the Original ICL Task using the standard label space and (2) the\nTask Learning setting, where the label space is replaced with semantically\nunrelated tokens. In both settings, In-Context Calibration demonstrates\nsubstantial improvements, with results generalized across three LLM families\n(OPT, GPT, and Llama2) under various configurations.\n","authors":["Joonwon Jang","Sanghwan Jang","Wonbin Kweon","Minjin Jeon","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2403.09488v3.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2311.06899v4","updated":"2024-04-15T04:18:59Z","published":"2023-11-12T17:18:21Z","title":"Flames: Benchmarking Value Alignment of LLMs in Chinese","summary":"  The widespread adoption of large language models (LLMs) across various\nregions underscores the urgent need to evaluate their alignment with human\nvalues. Current benchmarks, however, fall short of effectively uncovering\nsafety vulnerabilities in LLMs. Despite numerous models achieving high scores\nand 'topping the chart' in these evaluations, there is still a significant gap\nin LLMs' deeper alignment with human values and achieving genuine harmlessness.\nTo this end, this paper proposes a value alignment benchmark named Flames,\nwhich encompasses both common harmlessness principles and a unique morality\ndimension that integrates specific Chinese values such as harmony. Accordingly,\nwe carefully design adversarial prompts that incorporate complex scenarios and\njailbreaking methods, mostly with implicit malice. By prompting 17 mainstream\nLLMs, we obtain model responses and rigorously annotate them for detailed\nevaluation. Our findings indicate that all the evaluated LLMs demonstrate\nrelatively poor performance on Flames, particularly in the safety and fairness\ndimensions. We also develop a lightweight specified scorer capable of scoring\nLLMs across multiple dimensions to efficiently evaluate new models on the\nbenchmark. The complexity of Flames has far exceeded existing benchmarks,\nsetting a new challenge for contemporary LLMs and highlighting the need for\nfurther alignment of LLMs. Our benchmark is publicly available at\nhttps://github.com/AIFlames/Flames.\n","authors":["Kexin Huang","Xiangyang Liu","Qianyu Guo","Tianxiang Sun","Jiawei Sun","Yaru Wang","Zeyang Zhou","Yixu Wang","Yan Teng","Xipeng Qiu","Yingchun Wang","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2311.06899v4.pdf","comment":"Accepted to the NAACL 2024"},{"id":"http://arxiv.org/abs/2307.02046v3","updated":"2024-04-15T04:18:34Z","published":"2023-07-05T06:03:40Z","title":"Recommender Systems in the Era of Large Language Models (LLMs)","summary":"  With the prosperity of e-commerce and web applications, Recommender Systems\n(RecSys) have become an important component of our daily life, providing\npersonalized suggestions that cater to user preferences. While Deep Neural\nNetworks (DNNs) have made significant advancements in enhancing recommender\nsystems by modeling user-item interactions and incorporating textual side\ninformation, DNN-based methods still face limitations, such as difficulties in\nunderstanding users' interests and capturing textual side information,\ninabilities in generalizing to various recommendation scenarios and reasoning\non their predictions, etc. Meanwhile, the emergence of Large Language Models\n(LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural\nLanguage Processing (NLP) and Artificial Intelligence (AI), due to their\nremarkable abilities in fundamental responsibilities of language understanding\nand generation, as well as impressive generalization and reasoning\ncapabilities. As a result, recent studies have attempted to harness the power\nof LLMs to enhance recommender systems. Given the rapid evolution of this\nresearch direction in recommender systems, there is a pressing need for a\nsystematic overview that summarizes existing LLM-empowered recommender systems,\nto provide researchers in relevant fields with an in-depth understanding.\nTherefore, in this paper, we conduct a comprehensive review of LLM-empowered\nrecommender systems from various aspects including Pre-training, Fine-tuning,\nand Prompting. More specifically, we first introduce representative methods to\nharness the power of LLMs (as a feature encoder) for learning representations\nof users and items. Then, we review recent techniques of LLMs for enhancing\nrecommender systems from three paradigms, namely pre-training, fine-tuning, and\nprompting. Finally, we comprehensively discuss future directions in this\nemerging field.\n","authors":["Wenqi Fan","Zihuai Zhao","Jiatong Li","Yunqing Liu","Xiaowei Mei","Yiqi Wang","Zhen Wen","Fei Wang","Xiangyu Zhao","Jiliang Tang","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2307.02046v3.pdf","comment":"Accepted by IEEE TKDE"},{"id":"http://arxiv.org/abs/2403.02181v2","updated":"2024-04-15T03:57:44Z","published":"2024-03-04T16:23:58Z","title":"Not all Layers of LLMs are Necessary during Inference","summary":"  The inference phase of Large Language Models (LLMs) is very expensive. An\nideal inference stage of LLMs could utilize fewer computational resources while\nstill maintaining its capabilities (e.g., generalization and in-context\nlearning ability). In this paper, we try to answer the question, \"During LLM\ninference, can we use shallow layers for easy instances; and deep layers for\nhard ones?\" To answer this question, we first indicate that Not all Layers are\nNecessary during Inference by statistically analyzing the activated layers\nacross tasks. Then, we propose a simple algorithm named AdaInfer to determine\nthe inference termination moment based on the input instance adaptively. More\nimportantly, AdaInfer does not alter LLM parameters and maintains\ngeneralizability across tasks. Experiments on well-known LLMs (i.e., Llama2\nseries and OPT) show that AdaInfer saves an average of 14.8% of computational\nresources, even up to 50% on sentiment tasks, while maintaining comparable\nperformance. Additionally, this method is orthogonal to other model\nacceleration techniques, potentially boosting inference efficiency further.\n","authors":["Siqi Fan","Xin Jiang","Xiang Li","Xuying Meng","Peng Han","Shuo Shang","Aixin Sun","Yequan Wang","Zhongyuan Wang"],"pdf_url":"https://arxiv.org/pdf/2403.02181v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.13788v4","updated":"2024-04-15T03:01:09Z","published":"2023-09-25T00:45:07Z","title":"Can LLM-Generated Misinformation Be Detected?","summary":"  The advent of Large Language Models (LLMs) has made a transformative impact.\nHowever, the potential that LLMs such as ChatGPT can be exploited to generate\nmisinformation has posed a serious concern to online safety and public trust. A\nfundamental research question is: will LLM-generated misinformation cause more\nharm than human-written misinformation? We propose to tackle this question from\nthe perspective of detection difficulty. We first build a taxonomy of\nLLM-generated misinformation. Then we categorize and validate the potential\nreal-world methods for generating misinformation with LLMs. Then, through\nextensive empirical investigation, we discover that LLM-generated\nmisinformation can be harder to detect for humans and detectors compared to\nhuman-written misinformation with the same semantics, which suggests it can\nhave more deceptive styles and potentially cause more harm. We also discuss the\nimplications of our discovery on combating misinformation in the age of LLMs\nand the countermeasures.\n","authors":["Canyu Chen","Kai Shu"],"pdf_url":"https://arxiv.org/pdf/2309.13788v4.pdf","comment":"Accepted to Proceedings of ICLR 2024. 9 pages for main paper, 38\n  pages including appendix. The code, results, dataset for this paper and more\n  resources on \"LLMs Meet Misinformation\" have been released on the project\n  website: https://llm-misinformation.github.io/"},{"id":"http://arxiv.org/abs/2403.13269v2","updated":"2024-04-15T02:55:19Z","published":"2024-03-20T03:07:50Z","title":"AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient\n  Fine-Tuning of Large Models","summary":"  We present a novel Parameter-Efficient Fine-Tuning (PEFT) method, dubbed as\nAdaptive Freezing of Low Rank Adaptation (AFLoRA). Specifically, for each\npre-trained frozen weight tensor, we add a parallel path of trainable low-rank\nmatrices, namely a down-projection and an up-projection matrix, each of which\nis followed by a feature transformation vector. Based on a novel freezing\nscore, we the incrementally freeze these projection matrices during fine-tuning\nto reduce the computation and alleviate over-fitting. Our experimental results\ndemonstrate that we can achieve state-of-the-art performance with an average\nimprovement of up to $0.85\\%$ as evaluated on GLUE benchmark while yeilding up\nto $9.5\\times$ fewer average trainable parameters. While compared in terms of\nruntime, AFLoRA can yield up to $1.86\\times$ improvement as opposed to similar\nPEFT alternatives. Besides the practical utility of our approach, we provide\ninsights on the trainability requirements of LoRA paths at different modules\nand the freezing schedule for the different projection matrices. Code will be\nreleased.\n","authors":["Zeyu Liu","Souvik Kundu","Anni Li","Junrui Wan","Lianghao Jiang","Peter Anthony Beerel"],"pdf_url":"https://arxiv.org/pdf/2403.13269v2.pdf","comment":"5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2304.11872v2","updated":"2024-04-15T02:40:54Z","published":"2023-04-24T07:35:38Z","title":"Generation-driven Contrastive Self-training for Zero-shot Text\n  Classification with Instruction-following LLM","summary":"  The remarkable performance of large language models (LLMs) in zero-shot\nlanguage understanding has garnered significant attention. However, employing\nLLMs for large-scale inference or domain-specific fine-tuning requires immense\ncomputational resources due to their substantial model size. To overcome these\nlimitations, we introduce a novel method, namely GenCo, which leverages the\nstrong generative power of LLMs to assist in training a smaller and more\nadaptable language model. In our method, an LLM plays an important role in the\nself-training loop of a smaller model in two important ways. Firstly, the LLM\nis used to augment each input instance with a variety of possible\ncontinuations, enriching its semantic context for better understanding.\nSecondly, it helps crafting additional high-quality training pairs, by\nrewriting input texts conditioned on predicted labels. This ensures the\ngenerated texts are highly relevant to the predicted labels, alleviating the\nprediction error during pseudo-labeling, while reducing the dependency on large\nvolumes of unlabeled text. In our experiments, GenCo outperforms previous\nstate-of-the-art methods when only limited ($<5\\%$ of original) in-domain text\ndata is available. Notably, our approach surpasses the performance of Alpaca-7B\nwith human prompts, highlighting the potential of leveraging LLM for\nself-training.\n","authors":["Ruohong Zhang","Yau-Shian Wang","Yiming Yang"],"pdf_url":"https://arxiv.org/pdf/2304.11872v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18376v3","updated":"2024-04-15T02:26:43Z","published":"2023-10-27T00:13:59Z","title":"SQLformer: Deep Auto-Regressive Query Graph Generation for Text-to-SQL\n  Translation","summary":"  In recent years, there has been growing interest in text-to-SQL translation,\nwhich is the task of converting natural language questions into executable SQL\nqueries. This technology is important for its potential to democratize data\nextraction from databases. However, some of its key hurdles include domain\ngeneralisation, which is the ability to adapt to previously unseen databases,\nand alignment of natural language questions with the corresponding SQL queries.\nTo overcome these challenges, we introduce SQLformer, a novel Transformer\narchitecture specifically crafted to perform text-to-SQL translation tasks. Our\nmodel predicts SQL queries as abstract syntax trees (ASTs) in an autoregressive\nway, incorporating structural inductive bias in the encoder and decoder layers.\nThis bias, guided by database table and column selection, aids the decoder in\ngenerating SQL query ASTs represented as graphs in a Breadth-First Search\ncanonical order. Comprehensive experiments show the state-of-the-art\nperformance of SQLformer across five widely used text-to-SQL benchmarks. Our\nimplementation is available at https://github.com/AdrianBZG/SQLformer.\n","authors":["Adri√°n Bazaga","Pietro Li√≤","Gos Micklem"],"pdf_url":"https://arxiv.org/pdf/2310.18376v3.pdf","comment":"13 pages, 4 figures, 8 tables"},{"id":"http://arxiv.org/abs/2404.09416v1","updated":"2024-04-15T02:08:28Z","published":"2024-04-15T02:08:28Z","title":"Automatic Knowledge Graph Construction for Judicial Cases","summary":"  In this paper, we explore the application of cognitive intelligence in legal\nknowledge, focusing on the development of judicial artificial intelligence.\nUtilizing natural language processing (NLP) as the core technology, we propose\na method for the automatic construction of case knowledge graphs for judicial\ncases. Our approach centers on two fundamental NLP tasks: entity recognition\nand relationship extraction. We compare two pre-trained models for entity\nrecognition to establish their efficacy. Additionally, we introduce a\nmulti-task semantic relationship extraction model that incorporates\ntranslational embedding, leading to a nuanced contextualized case knowledge\nrepresentation. Specifically, in a case study involving a \"Motor Vehicle\nTraffic Accident Liability Dispute,\" our approach significantly outperforms the\nbaseline model. The entity recognition F1 score improved by 0.36, while the\nrelationship extraction F1 score increased by 2.37. Building on these results,\nwe detail the automatic construction process of case knowledge graphs for\njudicial cases, enabling the assembly of knowledge graphs for hundreds of\nthousands of judgments. This framework provides robust semantic support for\napplications of judicial AI, including the precise categorization and\nrecommendation of related cases.\n","authors":["Jie Zhou","Xin Chen","Hang Zhang","Zhe Li"],"pdf_url":"https://arxiv.org/pdf/2404.09416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08495v2","updated":"2024-04-15T01:56:27Z","published":"2024-04-12T14:25:49Z","title":"Dataset Reset Policy Optimization for RLHF","summary":"  Reinforcement Learning (RL) from Human Preference-based feedback is a popular\nparadigm for fine-tuning generative models, which has produced impressive\nmodels such as GPT-4 and Claude3 Opus. This framework often consists of two\nsteps: learning a reward model from an offline preference dataset followed by\nrunning online RL to optimize the learned reward model. In this work,\nleveraging the idea of reset, we propose a new RLHF algorithm with provable\nguarantees. Motivated by the fact that offline preference dataset provides\ninformative states (i.e., data that is preferred by the labelers), our new\nalgorithm, Dataset Reset Policy Optimization (DR-PO), integrates the existing\noffline preference dataset into the online policy training procedure via\ndataset reset: it directly resets the policy optimizer to the states in the\noffline dataset, instead of always starting from the initial state\ndistribution. In theory, we show that DR-PO learns to perform at least as good\nas any policy that is covered by the offline dataset under general function\napproximation with finite sample complexity. In experiments, we demonstrate\nthat on both the TL;DR summarization and the Anthropic Helpful Harmful (HH)\ndataset, the generation from DR-PO is better than that from Proximal Policy\nOptimization (PPO) and Direction Preference Optimization (DPO), under the\nmetric of GPT4 win-rate. Code for this work can be found at\nhttps://github.com/Cornell-RL/drpo.\n","authors":["Jonathan D. Chang","Wenhao Zhan","Owen Oertell","Kiant√© Brantley","Dipendra Misra","Jason D. Lee","Wen Sun"],"pdf_url":"https://arxiv.org/pdf/2404.08495v2.pdf","comment":"28 pages, 6 tables, 3 Figures, 3 Algorithms"},{"id":"http://arxiv.org/abs/2404.09405v1","updated":"2024-04-15T01:43:14Z","published":"2024-04-15T01:43:14Z","title":"Few-shot Name Entity Recognition on StackOverflow","summary":"  StackOverflow, with its vast question repository and limited labeled\nexamples, raise an annotation challenge for us. We address this gap by\nproposing RoBERTa+MAML, a few-shot named entity recognition (NER) method\nleveraging meta-learning. Our approach, evaluated on the StackOverflow NER\ncorpus (27 entity types), achieves a 5% F1 score improvement over the baseline.\nWe improved the results further domain-specific phrase processing enhance\nresults.\n","authors":["Xinwei Chen","Kun Li","Tianyou Song","Jiangjian Guo"],"pdf_url":"https://arxiv.org/pdf/2404.09405v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2404.05415v2","updated":"2024-04-15T00:45:32Z","published":"2024-04-08T11:33:00Z","title":"Relation Extraction Using Large Language Models: A Case Study on\n  Acupuncture Point Locations","summary":"  In acupuncture therapy, the accurate location of acupoints is essential for\nits effectiveness. The advanced language understanding capabilities of large\nlanguage models (LLMs) like Generative Pre-trained Transformers (GPT) present a\nsignificant opportunity for extracting relations related to acupoint locations\nfrom textual knowledge sources. This study aims to compare the performance of\nGPT with traditional deep learning models (Long Short-Term Memory (LSTM) and\nBidirectional Encoder Representations from Transformers for Biomedical Text\nMining (BioBERT)) in extracting acupoint-related location relations and assess\nthe impact of pretraining and fine-tuning on GPT's performance. We utilized the\nWorld Health Organization Standard Acupuncture Point Locations in the Western\nPacific Region (WHO Standard) as our corpus, which consists of descriptions of\n361 acupoints. Five types of relations ('direction_of,' 'distance_of,'\n'part_of,' 'near_acupoint,' and 'located_near') (n= 3,174) between acupoints\nwere annotated. Five models were compared: BioBERT, LSTM, pre-trained GPT-3.5,\nfine-tuned GPT-3.5, as well as pre-trained GPT-4. Performance metrics included\nmicro-average exact match precision, recall, and F1 scores. Our results\ndemonstrate that fine-tuned GPT-3.5 consistently outperformed other models in\nF1 scores across all relation types. Overall, it achieved the highest\nmicro-average F1 score of 0.92. This study underscores the effectiveness of\nLLMs like GPT in extracting relations related to acupoint locations, with\nimplications for accurately modeling acupuncture knowledge and promoting\nstandard implementation in acupuncture training and practice. The findings also\ncontribute to advancing informatics applications in traditional and\ncomplementary medicine, showcasing the potential of LLMs in natural language\nprocessing.\n","authors":["Yiming Li","Xueqing Peng","Jianfu Li","Xu Zuo","Suyuan Peng","Donghong Pei","Cui Tao","Hua Xu","Na Hong"],"pdf_url":"https://arxiv.org/pdf/2404.05415v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09385v1","updated":"2024-04-15T00:03:16Z","published":"2024-04-15T00:03:16Z","title":"A Large-Scale Evaluation of Speech Foundation Models","summary":"  The foundation model paradigm leverages a shared foundation model to achieve\nstate-of-the-art (SOTA) performance for various tasks, requiring minimal\ndownstream-specific modeling and data annotation. This approach has proven\ncrucial in the field of Natural Language Processing (NLP). However, the speech\nprocessing community lacks a similar setup to explore the paradigm\nsystematically. In this work, we establish the Speech processing Universal\nPERformance Benchmark (SUPERB) to study the effectiveness of the paradigm for\nspeech. We propose a unified multi-tasking framework to address speech\nprocessing tasks in SUPERB using a frozen foundation model followed by\ntask-specialized, lightweight prediction heads. Combining our results with\ncommunity submissions, we verify that the foundation model paradigm is\npromising for speech, and our multi-tasking framework is simple yet effective,\nas the best-performing foundation model shows competitive generalizability\nacross most SUPERB tasks. For reproducibility and extensibility, we have\ndeveloped a long-term maintained platform that enables deterministic\nbenchmarking, allows for result sharing via an online leaderboard, and promotes\ncollaboration through a community-driven benchmark database to support new\ndevelopment cycles. Finally, we conduct a series of analyses to offer an\nin-depth understanding of SUPERB and speech foundation models, including\ninformation flows across tasks inside the models, the correctness of the\nweighted-sum benchmarking protocol and the statistical significance and\nrobustness of the benchmark.\n","authors":["Shu-wen Yang","Heng-Jui Chang","Zili Huang","Andy T. Liu","Cheng-I Lai","Haibin Wu","Jiatong Shi","Xuankai Chang","Hsiang-Sheng Tsai","Wen-Chin Huang","Tzu-hsun Feng","Po-Han Chi","Yist Y. Lin","Yung-Sung Chuang","Tzu-Hsien Huang","Wei-Cheng Tseng","Kushal Lakhotia","Shang-Wen Li","Abdelrahman Mohamed","Shinji Watanabe","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2404.09385v1.pdf","comment":"The extended journal version for SUPERB and SUPERB-SG. Accepted to\n  TASLP. The arxiv version is further refined"},{"id":"http://arxiv.org/abs/2404.10180v1","updated":"2024-04-15T23:28:13Z","published":"2024-04-15T23:28:13Z","title":"Deferred NAM: Low-latency Top-K Context Injection via DeferredContext\n  Encoding for Non-Streaming ASR","summary":"  Contextual biasing enables speech recognizers to transcribe important phrases\nin the speaker's context, such as contact names, even if they are rare in, or\nabsent from, the training data. Attention-based biasing is a leading approach\nwhich allows for full end-to-end cotraining of the recognizer and biasing\nsystem and requires no separate inference-time components. Such biasers\ntypically consist of a context encoder; followed by a context filter which\nnarrows down the context to apply, improving per-step inference time; and,\nfinally, context application via cross attention. Though much work has gone\ninto optimizing per-frame performance, the context encoder is at least as\nimportant: recognition cannot begin before context encoding ends. Here, we show\nthe lightweight phrase selection pass can be moved before context encoding,\nresulting in a speedup of up to 16.1 times and enabling biasing to scale to 20K\nphrases with a maximum pre-decoding delay under 33ms. With the addition of\nphrase- and wordpiece-level cross-entropy losses, our technique also achieves\nup to a 37.5% relative WER reduction over the baseline without the losses and\nlightweight phrase selection pass.\n","authors":["Zelin Wu","Gan Song","Christopher Li","Pat Rondon","Zhong Meng","Xavier Velez","Weiran Wang","Diamantino Caseiro","Golan Pundak","Tsendsuren Munkhdalai","Angad Chandorkar","Rohit Prabhavalkar"],"pdf_url":"https://arxiv.org/pdf/2404.10180v1.pdf","comment":"9 pages, 3 figures, accepted by NAACL 2024 - Industry Track"},{"id":"http://arxiv.org/abs/2404.10174v1","updated":"2024-04-15T23:05:57Z","published":"2024-04-15T23:05:57Z","title":"On the Effects of Fine-tuning Language Models for Text-Based\n  Reinforcement Learning","summary":"  Text-based reinforcement learning involves an agent interacting with a\nfictional environment using observed text and admissible actions in natural\nlanguage to complete a task. Previous works have shown that agents can succeed\nin text-based interactive environments even in the complete absence of semantic\nunderstanding or other linguistic capabilities. The success of these agents in\nplaying such games suggests that semantic understanding may not be important\nfor the task. This raises an important question about the benefits of LMs in\nguiding the agents through the game states. In this work, we show that rich\nsemantic understanding leads to efficient training of text-based RL agents.\nMoreover, we describe the occurrence of semantic degeneration as a consequence\nof inappropriate fine-tuning of language models in text-based reinforcement\nlearning (TBRL). Specifically, we describe the shift in the semantic\nrepresentation of words in the LM, as well as how it affects the performance of\nthe agent in tasks that are semantically similar to the training games. We\nbelieve these results may help develop better strategies to fine-tune agents in\ntext-based RL scenarios.\n","authors":["Mauricio Gruppi","Soham Dan","Keerthiram Murugesan","Subhajit Chaudhury"],"pdf_url":"https://arxiv.org/pdf/2404.10174v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18139v2","updated":"2024-04-15T22:42:11Z","published":"2024-02-28T08:02:14Z","title":"Cause and Effect: Can Large Language Models Truly Understand Causality?","summary":"  With the rise of Large Language Models(LLMs), it has become crucial to\nunderstand their capabilities and limitations in deciphering and explaining the\ncomplex web of causal relationships that language entails. Current methods use\neither explicit or implicit causal reasoning, yet there is a strong need for a\nunified approach combining both to tackle a wide array of causal relationships\nmore effectively. This research proposes a novel architecture called Context\nAware Reasoning Enhancement with Counterfactual Analysis(CARE CA) framework to\nenhance causal reasoning and explainability. The proposed framework\nincorporates an explicit causal detection module with ConceptNet and\ncounterfactual statements, as well as implicit causal detection through LLMs.\nOur framework goes one step further with a layer of counterfactual explanations\nto accentuate LLMs understanding of causality. The knowledge from ConceptNet\nenhances the performance of multiple causal reasoning tasks such as causal\ndiscovery, causal identification and counterfactual reasoning. The\ncounterfactual sentences add explicit knowledge of the not caused by scenarios.\nBy combining these powerful modules, our model aims to provide a deeper\nunderstanding of causal relationships, enabling enhanced interpretability.\nEvaluation of benchmark datasets shows improved performance across all metrics,\nsuch as accuracy, precision, recall, and F1 scores. We also introduce\nCausalNet, a new dataset accompanied by our code, to facilitate further\nresearch in this domain.\n","authors":["Swagata Ashwani","Kshiteesh Hegde","Nishith Reddy Mannuru","Mayank Jindal","Dushyant Singh Sengar","Krishna Chaitanya Rao Kathala","Dishant Banga","Vinija Jain","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2402.18139v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.16382v3","updated":"2024-04-15T22:34:22Z","published":"2023-07-31T03:17:51Z","title":"Does fine-tuning GPT-3 with the OpenAI API leak personally-identifiable\n  information?","summary":"  Machine learning practitioners often fine-tune generative pre-trained models\nlike GPT-3 to improve model performance at specific tasks. Previous works,\nhowever, suggest that fine-tuned machine learning models memorize and emit\nsensitive information from the original fine-tuning dataset. Companies such as\nOpenAI offer fine-tuning services for their models, but no prior work has\nconducted a memorization attack on any closed-source models. In this work, we\nsimulate a privacy attack on GPT-3 using OpenAI's fine-tuning API. Our\nobjective is to determine if personally identifiable information (PII) can be\nextracted from this model. We (1) explore the use of naive prompting methods on\na GPT-3 fine-tuned classification model, and (2) we design a practical word\ngeneration task called Autocomplete to investigate the extent of PII\nmemorization in fine-tuned GPT-3 within a real-world context. Our findings\nreveal that fine-tuning GPT3 for both tasks led to the model memorizing and\ndisclosing critical personally identifiable information (PII) obtained from the\nunderlying fine-tuning dataset. To encourage further research, we have made our\ncodes and datasets publicly available on GitHub at:\nhttps://github.com/albertsun1/gpt3-pii-attacks\n","authors":["Albert Yu Sun","Eliott Zemour","Arushi Saxena","Udith Vaidyanathan","Eric Lin","Christian Lau","Vaikkunth Mugunthan"],"pdf_url":"https://arxiv.org/pdf/2307.16382v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.05800v2","updated":"2024-04-15T22:11:33Z","published":"2023-11-10T00:17:10Z","title":"Leveraging LLMs for Synthesizing Training Data Across Many Languages in\n  Multilingual Dense Retrieval","summary":"  There has been limited success for dense retrieval models in multilingual\nretrieval, due to uneven and scarce training data available across multiple\nlanguages. Synthetic training data generation is promising (e.g., InPars or\nPromptagator), but has been investigated only for English. Therefore, to study\nmodel capabilities across both cross-lingual and monolingual retrieval tasks,\nwe develop SWIM-IR, a synthetic retrieval training dataset containing 33 (high\nto very-low resource) languages for fine-tuning multilingual dense retrievers\nwithout requiring any human supervision. To construct SWIM-IR, we propose SAP\n(summarize-then-ask prompting), where the large language model (LLM) generates\na textual summary prior to the query generation step. SAP assists the LLM in\ngenerating informative queries in the target language. Using SWIM-IR, we\nexplore synthetic fine-tuning of multilingual dense retrieval models and\nevaluate them robustly on three retrieval benchmarks: XOR-Retrieve\n(cross-lingual), MIRACL (monolingual) and XTREME-UP (cross-lingual). Our\nmodels, called SWIM-X, are competitive with human-supervised dense retrieval\nmodels, e.g., mContriever-X, finding that SWIM-IR can cheaply substitute for\nexpensive human-labeled retrieval training data. SWIM-IR dataset and SWIM-X\nmodels are available at https://github.com/google-research-datasets/SWIM-IR.\n","authors":["Nandan Thakur","Jianmo Ni","Gustavo Hern√°ndez √Åbrego","John Wieting","Jimmy Lin","Daniel Cer"],"pdf_url":"https://arxiv.org/pdf/2311.05800v2.pdf","comment":"Accepted at NAACL 2024. Data released at\n  https://github.com/google-research-datasets/swim-ir"},{"id":"http://arxiv.org/abs/2404.02933v2","updated":"2024-04-15T22:10:17Z","published":"2024-04-03T01:09:41Z","title":"NL2KQL: From Natural Language to Kusto Query","summary":"  Data is growing rapidly in volume and complexity. Proficiency in database\nquery languages is pivotal for crafting effective queries. As coding assistants\nbecome more prevalent, there is significant opportunity to enhance database\nquery languages. The Kusto Query Language (KQL) is a widely used query language\nfor large semi-structured data such as logs, telemetries, and time-series for\nbig data analytics platforms. This paper introduces NL2KQL an innovative\nframework that uses large language models (LLMs) to convert natural language\nqueries (NLQs) to KQL queries. The proposed NL2KQL framework includes several\nkey components: Schema Refiner which narrows down the schema to its most\npertinent elements; the Few-shot Selector which dynamically selects relevant\nexamples from a few-shot dataset; and the Query Refiner which repairs syntactic\nand semantic errors in KQL queries. Additionally, this study outlines a method\nfor generating large datasets of synthetic NLQ-KQL pairs which are valid within\na specific database contexts. To validate NL2KQL's performance, we utilize an\narray of online (based on query execution) and offline (based on query parsing)\nmetrics. Through ablation studies, the significance of each framework component\nis examined, and the datasets used for benchmarking are made publicly\navailable. This work is the first of its kind and is compared with available\nbaselines to demonstrate its effectiveness.\n","authors":["Amir H. Abdi","Xinye Tang","Jeremias Eichelbaum","Mahan Das","Alex Klein","Nihal Irmak Pakis","William Blum","Daniel L Mace","Tanvi Raja","Namrata Padmanabhan","Ye Xing"],"pdf_url":"https://arxiv.org/pdf/2404.02933v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.14050v2","updated":"2024-04-15T21:58:27Z","published":"2023-06-24T20:15:07Z","title":"Symbolic Chain-of-Thought Distillation: Small Models Can Also \"Think\"\n  Step-by-Step","summary":"  Chain-of-thought prompting (e.g., \"Let's think step-by-step\") primes large\nlanguage models to verbalize rationalization for their predictions. While\nchain-of-thought can lead to dramatic performance gains, benefits appear to\nemerge only for sufficiently large models (beyond 50B parameters). We show that\norders-of-magnitude smaller models (125M -- 1.3B parameters) can still benefit\nfrom chain-of-thought prompting. To achieve this, we introduce Symbolic\nChain-of-Thought Distillation (SCoTD), a method to train a smaller student\nmodel on rationalizations sampled from a significantly larger teacher model.\nExperiments across several commonsense benchmarks show that: 1) SCoTD enhances\nthe performance of the student model in both supervised and few-shot settings,\nand especially for challenge sets; 2) sampling many reasoning chains per\ninstance from the teacher is paramount; and 3) after distillation, student\nchain-of-thoughts are judged by humans as comparable to the teacher, despite\norders of magnitude fewer parameters. We test several hypotheses regarding what\nproperties of chain-of-thought samples are important, e.g., diversity vs.\nteacher likelihood vs. open-endedness. We release our corpus of\nchain-of-thought samples and code.\n","authors":["Liunian Harold Li","Jack Hessel","Youngjae Yu","Xiang Ren","Kai-Wei Chang","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2306.14050v2.pdf","comment":"ACL 2023"},{"id":"http://arxiv.org/abs/2404.10150v1","updated":"2024-04-15T21:42:20Z","published":"2024-04-15T21:42:20Z","title":"TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table\n  Decomposition","summary":"  Table reasoning is a challenging task that requires understanding both\nnatural language questions and structured tabular data. Large language models\n(LLMs) have shown impressive capabilities in natural language understanding and\ngeneration, but they often struggle with large tables due to their limited\ninput length. In this paper, we propose TabSQLify, a novel method that\nleverages text-to-SQL generation to decompose tables into smaller and relevant\nsub-tables, containing only essential information for answering questions or\nverifying statements, before performing the reasoning task. In our\ncomprehensive evaluation on four challenging datasets, our approach\ndemonstrates comparable or superior performance compared to prevailing methods\nreliant on full tables as input. Moreover, our method can reduce the input\ncontext length significantly, making it more scalable and efficient for\nlarge-scale table reasoning applications. Our method performs remarkably well\non the WikiTQ benchmark, achieving an accuracy of 64.7%. Additionally, on the\nTabFact benchmark, it achieves a high accuracy of 79.5%. These results surpass\nother LLM-based baseline models on gpt-3.5-turbo (chatgpt). TabSQLify can\nreduce the table size significantly alleviating the computational load on LLMs\nwhen handling large tables without compromising performance.\n","authors":["Md Mahadi Hasan Nahid","Davood Rafiei"],"pdf_url":"https://arxiv.org/pdf/2404.10150v1.pdf","comment":"Accepted to NAACL 2024 (long, main)"},{"id":"http://arxiv.org/abs/2402.01293v2","updated":"2024-04-15T21:30:10Z","published":"2024-02-02T10:30:05Z","title":"Can MLLMs Perform Text-to-Image In-Context Learning?","summary":"  The evolution from Large Language Models (LLMs) to Multimodal Large Language\nModels (MLLMs) has spurred research into extending In-Context Learning (ICL) to\nits multimodal counterpart. Existing such studies have primarily concentrated\non image-to-text ICL. However, the Text-to-Image ICL (T2I-ICL), with its unique\ncharacteristics and potential applications, remains underexplored. To address\nthis gap, we formally define the task of T2I-ICL and present CoBSAT, the first\nT2I-ICL benchmark dataset, encompassing ten tasks. Utilizing our dataset to\nbenchmark six state-of-the-art MLLMs, we uncover considerable difficulties\nMLLMs encounter in solving T2I-ICL. We identify the primary challenges as the\ninherent complexity of multimodality and image generation, and show that\nstrategies such as fine-tuning and Chain-of-Thought prompting help to mitigate\nthese difficulties, leading to notable improvements in performance. Our code\nand dataset are available at https://github.com/UW-Madison-Lee-Lab/CoBSAT.\n","authors":["Yuchen Zeng","Wonjun Kang","Yicong Chen","Hyung Il Koo","Kangwook Lee"],"pdf_url":"https://arxiv.org/pdf/2402.01293v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10141v1","updated":"2024-04-15T21:19:10Z","published":"2024-04-15T21:19:10Z","title":"ANCHOR: LLM-driven News Subject Conditioning for Text-to-Image Synthesis","summary":"  Text-to-Image (T2I) Synthesis has made tremendous strides in enhancing\nsynthesized image quality, but current datasets evaluate model performance only\non descriptive, instruction-based prompts. Real-world news image captions take\na more pragmatic approach, providing high-level situational and Named-Entity\n(NE) information and limited physical object descriptions, making them\nabstractive. To evaluate the ability of T2I models to capture intended subjects\nfrom news captions, we introduce the Abstractive News Captions with High-level\ncOntext Representation (ANCHOR) dataset, containing 70K+ samples sourced from 5\ndifferent news media organizations. With Large Language Models (LLM) achieving\nsuccess in language and commonsense reasoning tasks, we explore the ability of\ndifferent LLMs to identify and understand key subjects from abstractive\ncaptions. Our proposed method Subject-Aware Finetuning (SAFE), selects and\nenhances the representation of key subjects in synthesized images by leveraging\nLLM-generated subject weights. It also adapts to the domain distribution of\nnews images and captions through custom Domain Fine-tuning, outperforming\ncurrent T2I baselines on ANCHOR. By launching the ANCHOR dataset, we hope to\nmotivate research in furthering the Natural Language Understanding (NLU)\ncapabilities of T2I models.\n","authors":["Aashish Anantha Ramakrishnan","Sharon X. Huang","Dongwon Lee"],"pdf_url":"https://arxiv.org/pdf/2404.10141v1.pdf","comment":"23 pages, 9 figures"},{"id":"http://arxiv.org/abs/2404.10136v1","updated":"2024-04-15T21:02:48Z","published":"2024-04-15T21:02:48Z","title":"Language Model Cascades: Token-level uncertainty and beyond","summary":"  Recent advances in language models (LMs) have led to significant improvements\nin quality on complex NLP tasks, but at the expense of increased inference\ncosts. Cascading offers a simple strategy to achieve more favorable\ncost-quality tradeoffs: here, a small model is invoked for most \"easy\"\ninstances, while a few \"hard\" instances are deferred to the large model. While\nthe principles underpinning cascading are well-studied for classification tasks\n- with deferral based on predicted class uncertainty favored theoretically and\npractically - a similar understanding is lacking for generative LM tasks. In\nthis work, we initiate a systematic study of deferral rules for LM cascades. We\nbegin by examining the natural extension of predicted class uncertainty to\ngenerative LM tasks, namely, the predicted sequence uncertainty. We show that\nthis measure suffers from the length bias problem, either over- or\nunder-emphasizing outputs based on their lengths. This is because LMs produce a\nsequence of uncertainty values, one for each output token; and moreover, the\nnumber of output tokens is variable across examples. To mitigate this issue, we\npropose to exploit the richer token-level uncertainty information implicit in\ngenerative LMs. We argue that naive predicted sequence uncertainty corresponds\nto a simple aggregation of these uncertainties. By contrast, we show that\nincorporating token-level uncertainty through learned post-hoc deferral rules\ncan significantly outperform such simple aggregation strategies, via\nexperiments on a range of natural language benchmarks with FLAN-T5 models. We\nfurther show that incorporating embeddings from the smaller model and\nintermediate layers of the larger model can give an additional boost in the\noverall cost-quality tradeoff.\n","authors":["Neha Gupta","Harikrishna Narasimhan","Wittawat Jitkrittum","Ankit Singh Rawat","Aditya Krishna Menon","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2404.10136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10112v1","updated":"2024-04-15T20:03:58Z","published":"2024-04-15T20:03:58Z","title":"PRODIS - a speech database and a phoneme-based language model for the\n  study of predictability effects in Polish","summary":"  We present a speech database and a phoneme-level language model of Polish.\nThe database and model are designed for the analysis of prosodic and discourse\nfactors and their impact on acoustic parameters in interaction with\npredictability effects. The database is also the first large, publicly\navailable Polish speech corpus of excellent acoustic quality that can be used\nfor phonetic analysis and training of multi-speaker speech technology systems.\nThe speech in the database is processed in a pipeline that achieves a 90%\ndegree of automation. It incorporates state-of-the-art, freely available tools\nenabling database expansion or adaptation to additional languages.\n","authors":["Zofia Malisz","Jan Foremski","Ma≈Çgorzata Kul"],"pdf_url":"https://arxiv.org/pdf/2404.10112v1.pdf","comment":"To appear in the proceedings of LREC2024: Language Resources and\n  Evaluation Conference 2024, Turin, Italy"},{"id":"http://arxiv.org/abs/2403.15822v2","updated":"2024-04-15T19:24:12Z","published":"2024-03-23T12:19:49Z","title":"Computational Sentence-level Metrics Predicting Human Sentence\n  Comprehension","summary":"  The majority of research in computational psycholinguistics has concentrated\non the processing of words. This study introduces innovative methods for\ncomputing sentence-level metrics using multilingual large language models. The\nmetrics developed sentence surprisal and sentence relevance and then are tested\nand compared to validate whether they can predict how humans comprehend\nsentences as a whole across languages. These metrics offer significant\ninterpretability and achieve high accuracy in predicting human sentence reading\nspeeds. Our results indicate that these computational sentence-level metrics\nare exceptionally effective at predicting and elucidating the processing\ndifficulties encountered by readers in comprehending sentences as a whole\nacross a variety of languages. Their impressive performance and generalization\ncapabilities provide a promising avenue for future research in integrating LLMs\nand cognitive science.\n","authors":["Kun Sun","Rong Wang"],"pdf_url":"https://arxiv.org/pdf/2403.15822v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10102v1","updated":"2024-04-15T19:19:56Z","published":"2024-04-15T19:19:56Z","title":"Chinchilla Scaling: A replication attempt","summary":"  Hoffmann et al. (2022) propose three methods for estimating a compute-optimal\nscaling law. We attempt to replicate their third estimation procedure, which\ninvolves fitting a parametric loss function to a reconstruction of data from\ntheir plots. We find that the reported estimates are inconsistent with their\nfirst two estimation methods, fail at fitting the extracted data, and report\nimplausibly narrow confidence intervals--intervals this narrow would require\nover 600,000 experiments, while they likely only ran fewer than 500. In\ncontrast, our rederivation of the scaling law using the third approach yields\nresults that are compatible with the findings from the first two estimation\nprocedures described by Hoffmann et al.\n","authors":["Tamay Besiroglu","Ege Erdil","Matthew Barnett","Josh You"],"pdf_url":"https://arxiv.org/pdf/2404.10102v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2004.05704v3","updated":"2024-04-15T19:09:39Z","published":"2020-04-12T21:45:23Z","title":"Visual Grounding Methods for VQA are Working for the Wrong Reasons!","summary":"  Existing Visual Question Answering (VQA) methods tend to exploit dataset\nbiases and spurious statistical correlations, instead of producing right\nanswers for the right reasons. To address this issue, recent bias mitigation\nmethods for VQA propose to incorporate visual cues (e.g., human attention maps)\nto better ground the VQA models, showcasing impressive gains. However, we show\nthat the performance improvements are not a result of improved visual\ngrounding, but a regularization effect which prevents over-fitting to\nlinguistic priors. For instance, we find that it is not actually necessary to\nprovide proper, human-based cues; random, insensible cues also result in\nsimilar improvements. Based on this observation, we propose a simpler\nregularization scheme that does not require any external annotations and yet\nachieves near state-of-the-art performance on VQA-CPv2.\n","authors":["Robik Shrestha","Kushal Kafle","Christopher Kanan"],"pdf_url":"https://arxiv.org/pdf/2004.05704v3.pdf","comment":"ACL 2020"},{"id":"http://arxiv.org/abs/2206.02171v3","updated":"2024-04-15T18:53:56Z","published":"2022-06-05T13:10:46Z","title":"Near-Term Advances in Quantum Natural Language Processing","summary":"  This paper describes experiments showing that some tasks in natural language\nprocessing (NLP) can already be performed using quantum computers, though so\nfar only with small datasets.\n  We demonstrate various approaches to topic classification. The first uses an\nexplicit word-based approach, in which word-topic scoring weights are\nimplemented as fractional rotations of individual qubit, and a new phrase is\nclassified based on the accumulation of these weights in a scoring qubit using\nentangling controlled-NOT gates. This is compared with more scalable quantum\nencodings of word embedding vectors, which are used in the computation of\nkernel values in a quantum support vector machine: this approach achieved an\naverage of 62% accuracy on classification tasks involving over 10000 words,\nwhich is the largest such quantum computing experiment to date.\n  We describe a quantum probability approach to bigram modeling that can be\napplied to sequences of words and formal concepts, investigating a generative\napproximation to these distributions using a quantum circuit Born machine, and\nan approach to ambiguity resolution in verb-noun composition using single-qubit\nrotations for simple nouns and 2-qubit controlled-NOT gates for simple verbs.\n  The smaller systems described have been run successfully on physical quantum\ncomputers, and the larger ones have been simulated. We show that statistically\nmeaningful results can be obtained using real datasets, but this is much more\ndifficult to predict than with easier artificial language examples used\npreviously in developing quantum NLP systems.\n  Other approaches to quantum NLP are compared, partly with respect to\ncontemporary issues including informal language, fluency, and truthfulness.\n","authors":["Dominic Widdows","Aaranya Alexander","Daiwei Zhu","Chase Zimmerman","Arunava Majumder"],"pdf_url":"https://arxiv.org/pdf/2206.02171v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.15561v2","updated":"2024-04-15T18:44:25Z","published":"2023-12-24T23:01:00Z","title":"README: Bridging Medical Jargon and Lay Understanding for Patient\n  Education through Data-Centric NLP","summary":"  The advancement in healthcare has shifted focus toward patient-centric\napproaches, particularly in self-care and patient education, facilitated by\naccess to Electronic Health Records (EHR). However, medical jargon in EHRs\nposes significant challenges in patient comprehension. To address this, we\nintroduce a new task of automatically generating lay definitions, aiming to\nsimplify complex medical terms into patient-friendly lay language. We first\ncreated the README dataset, an extensive collection of over 50,000 unique\n(medical term, lay definition) pairs and 300,000 mentions, each offering\ncontext-aware lay definitions manually annotated by domain experts. We have\nalso engineered a data-centric Human-AI pipeline that synergizes data\nfiltering, augmentation, and selection to improve data quality. We then used\nREADME as the training data for models and leveraged a Retrieval-Augmented\nGeneration method to reduce hallucinations and improve the quality of model\noutputs. Our extensive automatic and human evaluations demonstrate that\nopen-source mobile-friendly models, when fine-tuned with high-quality data, are\ncapable of matching or even surpassing the performance of state-of-the-art\nclosed-source large language models like ChatGPT. This research represents a\nsignificant stride in closing the knowledge gap in patient education and\nadvancing patient-centric healthcare solutions.\n","authors":["Zonghai Yao","Nandyala Siddharth Kantu","Guanghao Wei","Hieu Tran","Zhangqi Duan","Sunjae Kwon","Zhichao Yang","README annotation team","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2312.15561v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10003v4","updated":"2024-04-15T18:44:21Z","published":"2023-09-17T16:50:07Z","title":"A novel approach to measuring the scope of patent claims based on\n  probabilities obtained from (large) language models","summary":"  This work proposes to measure the scope of a patent claim as the reciprocal\nof self-information contained in this claim. Self-information is calculated\nbased on a probability of occurrence of the claim, where this probability is\nobtained from a language model. Grounded in information theory, this approach\nis based on the assumption that an unlikely concept is more informative than a\nusual concept, insofar as it is more surprising. In turn, the more surprising\nthe information required to define the claim, the narrower its scope. Seven\nlanguage models are considered, ranging from simplest models (each word or\ncharacter has an identical probability) to intermediate models (based on\naverage word or character frequencies), to large language models (LLMs) such as\nGPT2 and davinci-002. Remarkably, when using the simplest language models to\ncompute the probabilities, the scope becomes proportional to the reciprocal of\nthe number of words or characters involved in the claim, a metric already used\nin previous works. Application is made to multiple series of patent claims\ndirected to distinct inventions, where each series consists of claims devised\nto have a gradually decreasing scope. The performance of the language models is\nthen assessed through several ad hoc tests. The LLMs outperform models based on\nword and character frequencies, which themselves outdo the simplest models\nbased on word or character counts. Interestingly, however, the character count\nappears to be a more reliable indicator than the word count.\n","authors":["S√©bastien Ragot"],"pdf_url":"https://arxiv.org/pdf/2309.10003v4.pdf","comment":"58 pages, 6 tables, 6 figures. Substantial changes made to version 2:\n  New section 4.1 added (including a new table); Minor normalization issue\n  corrected in values listed in Appendix B; Content of former appendix C now\n  moved to Section 3; and new Appendix C added. Minor changes made to version 3\n  (style, typos, language). New calculations entered in version 4 (based on\n  davinci-002 and babgage-002)"},{"id":"http://arxiv.org/abs/2402.10688v2","updated":"2024-04-15T18:43:12Z","published":"2024-02-16T13:46:06Z","title":"Towards Uncovering How Large Language Model Works: An Explainability\n  Perspective","summary":"  Large language models (LLMs) have led to breakthroughs in language tasks, yet\nthe internal mechanisms that enable their remarkable generalization and\nreasoning abilities remain opaque. This lack of transparency presents\nchallenges such as hallucinations, toxicity, and misalignment with human\nvalues, hindering the safe and beneficial deployment of LLMs. This paper aims\nto uncover the mechanisms underlying LLM functionality through the lens of\nexplainability. First, we review how knowledge is architecturally composed\nwithin LLMs and encoded in their internal parameters via mechanistic\ninterpretability techniques. Then, we summarize how knowledge is embedded in\nLLM representations by leveraging probing techniques and representation\nengineering. Additionally, we investigate the training dynamics through a\nmechanistic perspective to explain phenomena such as grokking and memorization.\nLastly, we explore how the insights gained from these explanations can enhance\nLLM performance through model editing, improve efficiency through pruning, and\nbetter align with human values.\n","authors":["Haiyan Zhao","Fan Yang","Bo Shen","Himabindu Lakkaraju","Mengnan Du"],"pdf_url":"https://arxiv.org/pdf/2402.10688v2.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2309.08873v2","updated":"2024-04-15T18:39:01Z","published":"2023-09-16T04:34:55Z","title":"X-PARADE: Cross-Lingual Textual Entailment and Information Divergence\n  across Paragraphs","summary":"  Understanding when two pieces of text convey the same information is a goal\ntouching many subproblems in NLP, including textual entailment and\nfact-checking. This problem becomes more complex when those two pieces of text\nare in different languages. Here, we introduce X-PARADE (Cross-lingual\nParagraph-level Analysis of Divergences and Entailments), the first\ncross-lingual dataset of paragraph-level information divergences. Annotators\nlabel a paragraph in a target language at the span level and evaluate it with\nrespect to a corresponding paragraph in a source language, indicating whether a\ngiven piece of information is the same, new, or new but can be inferred. This\nlast notion establishes a link with cross-language NLI. Aligned paragraphs are\nsourced from Wikipedia pages in different languages, reflecting real\ninformation divergences observed in the wild. Armed with our dataset, we\ninvestigate a diverse set of approaches for this problem, including token\nalignment from machine translation, textual entailment methods that localize\ntheir decisions, and prompting LLMs. Our results show that these methods vary\nin their capability to handle inferable information, but they all fall short of\nhuman performance.\n","authors":["Juan Diego Rodriguez","Katrin Erk","Greg Durrett"],"pdf_url":"https://arxiv.org/pdf/2309.08873v2.pdf","comment":"To be published in NAACL 2024"},{"id":"http://arxiv.org/abs/2310.06627v4","updated":"2024-04-15T18:03:26Z","published":"2023-10-10T13:45:59Z","title":"What If the TV Was Off? Examining Counterfactual Reasoning Abilities of\n  Multi-modal Language Models","summary":"  Counterfactual reasoning, a fundamental aspect of human cognition, involves\ncontemplating alternatives to established facts or past events, significantly\nenhancing our abilities in planning and decision-making. In light of the\nadvancements in current multi-modal large language models, we explore their\neffectiveness in counterfactual reasoning. To facilitate this investigation, we\nintroduce a novel dataset, C-VQA, specifically designed to test the\ncounterfactual reasoning capabilities of modern multi-modal large language\nmodels. This dataset is constructed by infusing original questions with\ncounterfactual presuppositions, spanning various types such as numerical and\nboolean queries. It encompasses a mix of real and synthetic data, representing\na wide range of difficulty levels. Our thorough evaluations of contemporary\nvision-language models using this dataset have revealed substantial performance\ndrops, with some models showing up to a 40% decrease, highlighting a\nsignificant gap between current models and human-like vision reasoning\ncapabilities. We hope our dataset will serve as a vital benchmark for\nevaluating the counterfactual reasoning capabilities of models. Code and\ndataset are publicly available at https://bzhao.me/C-VQA/.\n","authors":["Letian Zhang","Xiaotong Zhai","Zhongkai Zhao","Yongshuo Zong","Xin Wen","Bingchen Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.06627v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10054v1","updated":"2024-04-15T18:00:30Z","published":"2024-04-15T18:00:30Z","title":"AIGeN: An Adversarial Approach for Instruction Generation in VLN","summary":"  In the last few years, the research interest in Vision-and-Language\nNavigation (VLN) has grown significantly. VLN is a challenging task that\ninvolves an agent following human instructions and navigating in a previously\nunknown environment to reach a specified goal. Recent work in literature\nfocuses on different ways to augment the available datasets of instructions for\nimproving navigation performance by exploiting synthetic training data. In this\nwork, we propose AIGeN, a novel architecture inspired by Generative Adversarial\nNetworks (GANs) that produces meaningful and well-formed synthetic instructions\nto improve navigation agents' performance. The model is composed of a\nTransformer decoder (GPT-2) and a Transformer encoder (BERT). During the\ntraining phase, the decoder generates sentences for a sequence of images\ndescribing the agent's path to a particular point while the encoder\ndiscriminates between real and fake instructions. Experimentally, we evaluate\nthe quality of the generated instructions and perform extensive ablation\nstudies. Additionally, we generate synthetic instructions for 217K trajectories\nusing AIGeN on Habitat-Matterport 3D Dataset (HM3D) and show an improvement in\nthe performance of an off-the-shelf VLN method. The validation analysis of our\nproposal is conducted on REVERIE and R2R and highlights the promising aspects\nof our proposal, achieving state-of-the-art performance.\n","authors":["Niyati Rawal","Roberto Bigazzi","Lorenzo Baraldi","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2404.10054v1.pdf","comment":"Accepted to 7th Multimodal Learning and Applications Workshop (MULA\n  2024) at the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  2024"},{"id":"http://arxiv.org/abs/2404.09992v1","updated":"2024-04-15T17:59:50Z","published":"2024-04-15T17:59:50Z","title":"MMInA: Benchmarking Multihop Multimodal Internet Agents","summary":"  Autonomous embodied agents live on an Internet of multimedia websites. Can\nthey hop around multimodal websites to complete complex user tasks? Existing\nbenchmarks fail to assess them in a realistic, evolving environment for their\nembodiment across websites. To answer this question, we present MMInA, a\nmultihop and multimodal benchmark to evaluate the embodied agents for\ncompositional Internet tasks, with several appealing properties: 1) Evolving\nreal-world multimodal websites. Our benchmark uniquely operates on evolving\nreal-world websites, ensuring a high degree of realism and applicability to\nnatural user tasks. Our data includes 1,050 human-written tasks covering\nvarious domains such as shopping and travel, with each task requiring the agent\nto autonomously extract multimodal information from web pages as observations;\n2) Multihop web browsing. Our dataset features naturally compositional tasks\nthat require information from or actions on multiple websites to solve, to\nassess long-range reasoning capabilities on web tasks; 3) Holistic evaluation.\nWe propose a novel protocol for evaluating an agent's progress in completing\nmultihop tasks. We experiment with both standalone (multimodal) language models\nand heuristic-based web agents. Extensive experiments demonstrate that while\nlong-chain multihop web tasks are easy for humans, they remain challenging for\nstate-of-the-art web agents. We identify that agents are more likely to fail on\nthe early hops when solving tasks of more hops, which results in lower task\nsuccess rates. To address this issue, we propose a simple memory augmentation\napproach replaying past action trajectories to reflect. Our method\nsignificantly improved both the single-hop and multihop web browsing abilities\nof agents. See our code and data at https://mmina.cliangyu.com\n","authors":["Ziniu Zhang","Shulin Tian","Liangyu Chen","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2404.09992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.16818v2","updated":"2024-04-15T17:58:01Z","published":"2024-01-30T08:45:08Z","title":"H2O-Danube-1.8B Technical Report","summary":"  We present H2O-Danube, a series of small 1.8B language models consisting of\nH2O-Danube-1.8B, trained on 1T tokens, and the incremental improved\nH2O-Danube2-1.8B trained on an additional 2T tokens. Our models exhibit highly\ncompetitive metrics across a multitude of benchmarks and, as of the time of\nthis writing, H2O-Danube2-1.8B achieves the top ranking on Open LLM Leaderboard\nfor all models below the 2B parameter range. The models follow core principles\nof LLama 2 and Mistral, and we leverage and refine various techniques for\npre-training large language models. We additionally release chat models trained\nwith supervised fine-tuning followed by direct preference optimization. We make\nall models openly available under Apache 2.0 license further democratizing LLMs\nto a wider audience economically.\n","authors":["Philipp Singer","Pascal Pfeiffer","Yauhen Babakhin","Maximilian Jeblick","Nischay Dhankhar","Gabor Fodor","Sri Satish Ambati"],"pdf_url":"https://arxiv.org/pdf/2401.16818v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09982v1","updated":"2024-04-15T17:57:30Z","published":"2024-04-15T17:57:30Z","title":"Memory Sharing for Large Language Model based Agents","summary":"  In the realm of artificial intelligence, the adaptation of Large Language\nModel (LLM)-based agents to execute tasks via natural language prompts\nrepresents a significant advancement, notably eliminating the need for explicit\nretraining or fine tuning for fixed-answer tasks such as common sense questions\nand yes/no queries. However, the application of In-context Learning to\nopen-ended challenges, such as poetry creation, reveals substantial limitations\ndue to the comprehensiveness of the provided examples and agent's ability to\nunderstand the content expressed in the problem, leading to outputs that often\ndiverge significantly from expected results. Addressing this gap, our study\nintroduces the Memory-Sharing (MS) framework for LLM multi-agents, which\nutilizes a real-time memory storage and retrieval system to enhance the\nIn-context Learning process. Each \"memory\" within this system captures both the\nposed query and the corresponding real-time response from an LLM-based agent,\naggregating these memories from a broad spectrum of similar agents to enrich\nthe memory pool shared by all agents. This framework not only aids agents in\nidentifying the most relevant examples for specific tasks but also evaluates\nthe potential utility of their memories for future applications by other\nagents. Empirical validation across three distinct domains involving\nspecialized functions of agents demonstrates that the MS framework\nsignificantly improve the agent's performance regrading the open-ended\nquestions. Furthermore, we also discuss what type of memory pool and what\nretrieval strategy in MS can better help agents, offering a future develop\ndirection of MS. The code and data are available at:\nhttps://github.com/GHupppp/MemorySharingLLM\n","authors":["Hang Gao","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.09982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00841v2","updated":"2024-04-15T17:56:58Z","published":"2024-02-01T18:31:34Z","title":"Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight\n  in the Real World for Meeting Summarization?","summary":"  Large Language Models (LLMs) have demonstrated impressive capabilities to\nsolve a wide range of tasks without being explicitly fine-tuned on\ntask-specific datasets. However, deploying LLMs in the real world is not\ntrivial, as it requires substantial computing resources. In this paper, we\ninvestigate whether smaller, compact LLMs are a good alternative to the\ncomparatively Larger LLMs2 to address significant costs associated with\nutilizing LLMs in the real world. In this regard, we study the meeting\nsummarization task in a real-world industrial environment and conduct extensive\nexperiments by comparing the performance of fine-tuned compact LLMs (e.g.,\nFLAN-T5, TinyLLaMA, LiteLLaMA) with zero-shot larger LLMs (e.g., LLaMA-2,\nGPT-3.5, PaLM-2). We observe that most smaller LLMs, even after fine-tuning,\nfail to outperform larger zero-shot LLMs in meeting summarization datasets.\nHowever, a notable exception is FLAN-T5 (780M parameters), which performs on\npar or even better than many zero-shot Larger LLMs (from 7B to above 70B\nparameters), while being significantly smaller. This makes compact LLMs like\nFLAN-T5 a suitable cost-efficient solution for real-world industrial\ndeployment.\n","authors":["Xue-Yong Fu","Md Tahmid Rahman Laskar","Elena Khasanova","Cheng Chen","Shashi Bhushan TN"],"pdf_url":"https://arxiv.org/pdf/2402.00841v2.pdf","comment":"Accepted by NAACL 2024 (Industry Track). The first two authors\n  contributed equally to this work"},{"id":"http://arxiv.org/abs/2404.09980v1","updated":"2024-04-15T17:56:39Z","published":"2024-04-15T17:56:39Z","title":"Context Does Matter: Implications for Crowdsourced Evaluation Labels in\n  Task-Oriented Dialogue Systems","summary":"  Crowdsourced labels play a crucial role in evaluating task-oriented dialogue\nsystems (TDSs). Obtaining high-quality and consistent ground-truth labels from\nannotators presents challenges. When evaluating a TDS, annotators must fully\ncomprehend the dialogue before providing judgments. Previous studies suggest\nusing only a portion of the dialogue context in the annotation process.\nHowever, the impact of this limitation on label quality remains unexplored.\nThis study investigates the influence of dialogue context on annotation\nquality, considering the truncated context for relevance and usefulness\nlabeling. We further propose to use large language models (LLMs) to summarize\nthe dialogue context to provide a rich and short description of the dialogue\ncontext and study the impact of doing so on the annotator's performance.\nReducing context leads to more positive ratings. Conversely, providing the\nentire dialogue context yields higher-quality relevance ratings but introduces\nambiguity in usefulness ratings. Using the first user utterance as context\nleads to consistent ratings, akin to those obtained using the entire dialogue,\nwith significantly reduced annotation effort. Our findings show how task\ndesign, particularly the availability of dialogue context, affects the quality\nand consistency of crowdsourced evaluation labels.\n","authors":["Clemencia Siro","Mohammad Aliannejadi","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2404.09980v1.pdf","comment":"Accepted at NAACL 2024 Findings"},{"id":"http://arxiv.org/abs/2404.09971v1","updated":"2024-04-15T17:48:46Z","published":"2024-04-15T17:48:46Z","title":"Constructing Benchmarks and Interventions for Combating Hallucinations\n  in LLMs","summary":"  Large language models (LLMs) are susceptible to hallucination, which sparked\na widespread effort to detect and prevent them. Recent work attempts to\nmitigate hallucinations by intervening in the model's computation during\ngeneration, using different setups and heuristics. Those works lack separation\nbetween different hallucination causes. In this work, we first introduce an\napproach for constructing datasets based on the model knowledge for detection\nand intervention methods in closed-book and open-book question-answering\nsettings. We then characterize the effect of different choices for\nintervention, such as the intervened components (MLPs, attention block,\nresidual stream, and specific heads), and how often and how strongly to\nintervene. We find that intervention success varies depending on the component,\nwith some components being detrimental to language modeling capabilities.\nFinally, we find that interventions can benefit from pre-hallucination steering\ndirection instead of post-hallucination. The code is available at\nhttps://github.com/technion-cs-nlp/hallucination-mitigation\n","authors":["Adi Simhi","Jonathan Herzig","Idan Szpektor","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2404.09971v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08565v3","updated":"2024-04-15T17:20:09Z","published":"2024-01-16T18:49:55Z","title":"Tuning Language Models by Proxy","summary":"  Despite the general capabilities of large pretrained language models, they\nconsistently benefit from further adaptation to better achieve desired\nbehaviors. However, tuning these models has become increasingly\nresource-intensive, or impossible when model weights are private. We introduce\nproxy-tuning, a lightweight decoding-time algorithm that operates on top of\nblack-box LMs to achieve the same end as direct tuning, but by accessing only\nits predictions over the output vocabulary, not its parameters. Our method\ntunes a smaller LM, then applies the difference between the predictions of the\nsmall tuned and untuned LMs to shift the original predictions of the larger\nuntuned model in the direction of tuning, while retaining the benefits of\nlarger-scale pretraining. In experiments, when we apply proxy-tuning to\nLlama2-70B using proxies of only 7B size, we can close 88% of the gap between\nLlama2-70B and its truly-tuned chat version, when evaluated across knowledge,\nreasoning, and safety benchmarks. Interestingly, on TruthfulQA, proxy-tuned\nmodels are actually more truthful than directly tuned models, possibly because\ndecoding-time guidance better retains the model's factual knowledge. We then\ndemonstrate the generality of proxy-tuning by applying it to domain adaptation\non code, and task-specific finetuning on question-answering and math problems.\nFinally, we show how to proxy-tune a truly black-box LM, GPT-3.5, for temporal\nadaptation, increasing its knowledge about recent events. Our work demonstrates\nthe promise of using small tuned LMs to efficiently customize large,\npotentially proprietary LMs through decoding-time guidance.\n","authors":["Alisa Liu","Xiaochuang Han","Yizhong Wang","Yulia Tsvetkov","Yejin Choi","Noah A. Smith"],"pdf_url":"https://arxiv.org/pdf/2401.08565v3.pdf","comment":"fix typo in Table 13, add acknowledgments section. code available at\n  https://github.com/alisawuffles/proxy-tuning"},{"id":"http://arxiv.org/abs/2403.15744v3","updated":"2024-04-15T17:10:46Z","published":"2024-03-23T07:16:23Z","title":"On the Fragility of Active Learners","summary":"  Active learning (AL) techniques aim to maximally utilize a labeling budget by\niteratively selecting instances that are most likely to improve prediction\naccuracy. However, their benefit compared to random sampling has not been\nconsistent across various setups, e.g., different datasets, classifiers. In\nthis empirical study, we examine how a combination of different factors might\nobscure any gains from an AL technique. Focusing on text classification, we\nrigorously evaluate AL techniques over around 1000 experiments that vary wrt\nthe dataset, batch size, text representation and the classifier. We show that\nAL is only effective in a narrow set of circumstances. We also address the\nproblem of using metrics that are better aligned with real world expectations.\nThe impact of this study is in its insights for a practitioner: (a) the choice\nof text representation and classifier is as important as that of an AL\ntechnique, (b) choice of the right metric is critical in assessment of the\nlatter, and, finally, (c) reported AL results must be holistically interpreted,\naccounting for variables other than just the query strategy.\n","authors":["Abhishek Ghose","Emma Thuong Nguyen"],"pdf_url":"https://arxiv.org/pdf/2403.15744v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09937v1","updated":"2024-04-15T17:03:41Z","published":"2024-04-15T17:03:41Z","title":"Compression Represents Intelligence Linearly","summary":"  There is a belief that learning to compress well will lead to intelligence.\nRecently, language modeling has been shown to be equivalent to compression,\nwhich offers a compelling rationale for the success of large language models\n(LLMs): the development of more advanced language models is essentially\nenhancing compression which facilitates intelligence. Despite such appealing\ndiscussions, little empirical evidence is present for the interplay between\ncompression and intelligence. In this work, we examine their relationship in\nthe context of LLMs, treating LLMs as data compressors. Given the abstract\nconcept of \"intelligence\", we adopt the average downstream benchmark scores as\na surrogate, specifically targeting intelligence related to knowledge and\ncommonsense, coding, and mathematical reasoning. Across 12 benchmarks, our\nstudy brings together 30 public LLMs that originate from diverse organizations.\nRemarkably, we find that LLMs' intelligence -- reflected by average benchmark\nscores -- almost linearly correlates with their ability to compress external\ntext corpora. These results provide concrete evidence supporting the belief\nthat superior compression indicates greater intelligence. Furthermore, our\nfindings suggest that compression efficiency, as an unsupervised metric derived\nfrom raw text corpora, serves as a reliable evaluation measure that is linearly\nassociated with the model capabilities. We open-source our compression datasets\nas well as our data collection pipelines to facilitate future researchers to\nassess compression properly.\n","authors":["Yuzhen Huang","Jinghan Zhang","Zifei Shan","Junxian He"],"pdf_url":"https://arxiv.org/pdf/2404.09937v1.pdf","comment":"Preprint. Data and code are available at\n  https://github.com/hkust-nlp/llm-compression-intelligence"},{"id":"http://arxiv.org/abs/2404.09932v1","updated":"2024-04-15T16:58:28Z","published":"2024-04-15T16:58:28Z","title":"Foundational Challenges in Assuring Alignment and Safety of Large\n  Language Models","summary":"  This work identifies 18 foundational challenges in assuring the alignment and\nsafety of large language models (LLMs). These challenges are organized into\nthree different categories: scientific understanding of LLMs, development and\ndeployment methods, and sociotechnical challenges. Based on the identified\nchallenges, we pose $200+$ concrete research questions.\n","authors":["Usman Anwar","Abulhair Saparov","Javier Rando","Daniel Paleka","Miles Turpin","Peter Hase","Ekdeep Singh Lubana","Erik Jenner","Stephen Casper","Oliver Sourbut","Benjamin L. Edelman","Zhaowei Zhang","Mario G√ºnther","Anton Korinek","Jose Hernandez-Orallo","Lewis Hammond","Eric Bigelow","Alexander Pan","Lauro Langosco","Tomasz Korbak","Heidi Zhang","Ruiqi Zhong","Se√°n √ì h√âigeartaigh","Gabriel Recchia","Giulio Corsi","Alan Chan","Markus Anderljung","Lilian Edwards","Yoshua Bengio","Danqi Chen","Samuel Albanie","Tegan Maharaj","Jakob Foerster","Florian Tramer","He He","Atoosa Kasirzadeh","Yejin Choi","David Krueger"],"pdf_url":"https://arxiv.org/pdf/2404.09932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10032v1","updated":"2024-04-15T16:37:44Z","published":"2024-04-15T16:37:44Z","title":"Detecting AI Generated Text Based on NLP and Machine Learning Approaches","summary":"  Recent advances in natural language processing (NLP) may enable artificial\nintelligence (AI) models to generate writing that is identical to human written\nform in the future. This might have profound ethical, legal, and social\nrepercussions. This study aims to address this problem by offering an accurate\nAI detector model that can differentiate between electronically produced text\nand human-written text. Our approach includes machine learning methods such as\nXGB Classifier, SVM, BERT architecture deep learning models. Furthermore, our\nresults show that the BERT performs better than previous models in identifying\ninformation generated by AI from information provided by humans. Provide a\ncomprehensive analysis of the current state of AI-generated text identification\nin our assessment of pertinent studies. Our testing yielded positive findings,\nshowing that our strategy is successful, with the BERT emerging as the most\nprobable answer. We analyze the research's societal implications, highlighting\nthe possible advantages for various industries while addressing sustainability\nissues pertaining to morality and the environment. The XGB classifier and SVM\ngive 0.84 and 0.81 accuracy in this article, respectively. The greatest\naccuracy in this research is provided by the BERT model, which provides 0.93%\naccuracy.\n","authors":["Nuzhat Prova"],"pdf_url":"https://arxiv.org/pdf/2404.10032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09911v1","updated":"2024-04-15T16:35:41Z","published":"2024-04-15T16:35:41Z","title":"ChatShop: Interactive Information Seeking with Language Agents","summary":"  The desire and ability to seek new information strategically are fundamental\nto human learning but often overlooked in current language agent development.\nUsing a web shopping task as an example, we show that it can be reformulated\nand solved as a retrieval task without a requirement of interactive information\nseeking. We then redesign the task to introduce a new role of shopper, serving\nas a realistically constrained communication channel. The agents in our\nproposed ChatShop task explore user preferences in open-ended conversation to\nmake informed decisions. Our experiments demonstrate that the proposed task can\neffectively evaluate the agent's ability to explore and gradually accumulate\ninformation through multi-turn interaction. We also show that LLM-simulated\nshoppers serve as a good proxy to real human shoppers and discover similar\nerror patterns of agents.\n","authors":["Sanxing Chen","Sam Wiseman","Bhuwan Dhingra"],"pdf_url":"https://arxiv.org/pdf/2404.09911v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03894v3","updated":"2024-04-15T16:29:41Z","published":"2024-03-06T17:52:08Z","title":"IRCoder: Intermediate Representations Make Language Models Robust\n  Multilingual Code Generators","summary":"  Code understanding and generation have fast become some of the most popular\napplications of language models (LMs). Nonetheless, research on multilingual\naspects of Code-LMs (i.e., LMs for code generation) such as cross-lingual\ntransfer between different programming languages, language-specific data\naugmentation, and post-hoc LM adaptation, alongside exploitation of data\nsources other than the original textual content, has been much sparser than for\ntheir natural language counterparts. In particular, most mainstream Code-LMs\nhave been pre-trained on source code files alone. In this work, we investigate\nthe prospect of leveraging readily available compiler intermediate\nrepresentations (IR) - shared across programming languages - to improve the\nmultilingual capabilities of Code-LMs and facilitate cross-lingual transfer.\n  To this end, we first compile SLTrans, a parallel dataset consisting of\nnearly 4M self-contained source code files coupled with respective intermediate\nrepresentations. Next, starting from various base Code-LMs (ranging in size\nfrom 1.1B to 7.3B parameters), we carry out continued causal language modelling\ntraining on SLTrans, forcing the Code-LMs to (1) learn the IR language and (2)\nalign the IR constructs with respective constructs of various programming\nlanguages. Our resulting models, dubbed IRCoder, display sizeable and\nconsistent gains across a wide variety of code generation tasks and metrics,\nincluding prompt robustness, multilingual code completion, code understanding,\nand instruction following.\n","authors":["Indraneil Paul","Goran Glava≈°","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2403.03894v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04481v3","updated":"2024-04-15T16:24:35Z","published":"2024-03-07T13:30:52Z","title":"Do Large Language Model Understand Multi-Intent Spoken Language ?","summary":"  This research signifies a considerable breakthrough in leveraging Large\nLanguage Models (LLMs) for multi-intent spoken language understanding (SLU).\nOur approach re-imagines the use of entity slots in multi-intent SLU\napplications, making the most of the generative potential of LLMs within the\nSLU landscape, leading to the development of the EN-LLM series. Furthermore, we\nintroduce the concept of Sub-Intent Instruction (SII) to amplify the analysis\nand interpretation of complex, multi-intent communications, which further\nsupports the creation of the ENSI-LLM models series. Our novel datasets,\nidentified as LM-MixATIS and LM-MixSNIPS, are synthesized from existing\nbenchmarks. The study evidences that LLMs may match or even surpass the\nperformance of the current best multi-intent SLU models. We also scrutinize the\nperformance of LLMs across a spectrum of intent configurations and dataset\ndistributions. On top of this, we present two revolutionary metrics - Entity\nSlot Accuracy (ESA) and Combined Semantic Accuracy (CSA) - to facilitate a\ndetailed assessment of LLM competence in this multifaceted field.\" Our code and\ndatasets are available at \\url{https://github.com/SJY8460/SLM}.\n","authors":["Shangjian Yin","Peijie Huang","Yuhong Xu","Haojing Huang","Jiatian Chen"],"pdf_url":"https://arxiv.org/pdf/2403.04481v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09897v1","updated":"2024-04-15T16:16:59Z","published":"2024-04-15T16:16:59Z","title":"Progressive Knowledge Graph Completion","summary":"  Knowledge Graph Completion (KGC) has emerged as a promising solution to\naddress the issue of incompleteness within Knowledge Graphs (KGs). Traditional\nKGC research primarily centers on triple classification and link prediction.\nNevertheless, we contend that these tasks do not align well with real-world\nscenarios and merely serve as surrogate benchmarks. In this paper, we\ninvestigate three crucial processes relevant to real-world construction\nscenarios: (a) the verification process, which arises from the necessity and\nlimitations of human verifiers; (b) the mining process, which identifies the\nmost promising candidates for verification; and (c) the training process, which\nharnesses verified data for subsequent utilization; in order to achieve a\ntransition toward more realistic challenges. By integrating these three\nprocesses, we introduce the Progressive Knowledge Graph Completion (PKGC) task,\nwhich simulates the gradual completion of KGs in real-world scenarios.\nFurthermore, to expedite PKGC processing, we propose two acceleration modules:\nOptimized Top-$k$ algorithm and Semantic Validity Filter. These modules\nsignificantly enhance the efficiency of the mining procedure. Our experiments\ndemonstrate that performance in link prediction does not accurately reflect\nperformance in PKGC. A more in-depth analysis reveals the key factors\ninfluencing the results and provides potential directions for future research.\n","authors":["Jiayi Li","Ruilin Luo","Jiaqi Sun","Jing Xiao","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2404.09897v1.pdf","comment":"14 pages, 10 figures"},{"id":"http://arxiv.org/abs/2402.06025v3","updated":"2024-04-15T16:11:50Z","published":"2024-02-08T19:57:29Z","title":"Doing Experiments and Revising Rules with Natural Language and\n  Probabilistic Reasoning","summary":"  We build a computational model of how humans actively infer hidden rules by\ndoing experiments. The basic principles behind the model is that, even if the\nrule is deterministic, the learner considers a broader space of fuzzy\nprobabilistic rules, which it represents in natural language, and updates its\nhypotheses online after each experiment according to approximately Bayesian\nprinciples. In the same framework we also model experiment design according to\ninformation-theoretic criteria. We find that the combination of these three\nprinciples -- explicit hypotheses, probabilistic rules, and online updates --\ncan explain human performance on a Zendo-style task, and that removing any of\nthese components leaves the model unable to account for the data.\n","authors":["Wasu Top Piriyakulkij","Kevin Ellis"],"pdf_url":"https://arxiv.org/pdf/2402.06025v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09188v2","updated":"2024-04-15T16:09:33Z","published":"2023-11-15T18:28:29Z","title":"Towards Verifiable Text Generation with Symbolic References","summary":"  LLMs are vulnerable to hallucinations, and thus their outputs generally\nrequire laborious human verification for high-stakes applications. To this end,\nwe propose symbolically grounded generation (SymGen) as a simple approach for\nenabling easier manual validation of an LLM's output. SymGen prompts an LLM to\ninterleave its regular output text with explicit symbolic references to fields\npresent in some conditioning data (e.g., a table in JSON format). The\nreferences can be used to display the provenance of different spans of text in\nthe generation, reducing the effort required for manual verification. Across a\nrange of data-to-text and question-answering experiments, we find that LLMs are\nable to directly output text that makes use of accurate symbolic references\nwhile maintaining fluency and factuality. In a human study we further find that\nsuch annotations can streamline human verification of machine-generated text.\nOur code will be available at http://symgen.github.io.\n","authors":["Lucas Torroba Hennigen","Shannon Shen","Aniruddha Nrusimha","Bernhard Gapp","David Sontag","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2311.09188v2.pdf","comment":"57 pages, 8 figures, 8 tables"},{"id":"http://arxiv.org/abs/2404.09889v1","updated":"2024-04-15T15:55:01Z","published":"2024-04-15T15:55:01Z","title":"Is Table Retrieval a Solved Problem? Join-Aware Multi-Table Retrieval","summary":"  Retrieving relevant tables containing the necessary information to accurately\nanswer a given question over tables is critical to open-domain\nquestion-answering (QA) systems. Previous methods assume the answer to such a\nquestion can be found either in a single table or multiple tables identified\nthrough question decomposition or rewriting. However, neither of these\napproaches is sufficient, as many questions require retrieving multiple tables\nand joining them through a join plan that cannot be discerned from the user\nquery itself. If the join plan is not considered in the retrieval stage, the\nsubsequent steps of reasoning and answering based on those retrieved tables are\nlikely to be incorrect. To address this problem, we introduce a method that\nuncovers useful join relations for any query and database during table\nretrieval. We use a novel re-ranking method formulated as a mixed-integer\nprogram that considers not only table-query relevance but also table-table\nrelevance that requires inferring join relationships. Our method outperforms\nthe state-of-the-art approaches for table retrieval by up to 9.3% in F1 score\nand for end-to-end QA by up to 5.4% in accuracy.\n","authors":["Peter Baile Chen","Yi Zhang","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2404.09889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05052v3","updated":"2024-04-15T15:54:51Z","published":"2023-07-11T07:03:29Z","title":"Towards Understanding In-Context Learning with Contrastive\n  Demonstrations and Saliency Maps","summary":"  We investigate the role of various demonstration components in the in-context\nlearning (ICL) performance of large language models (LLMs). Specifically, we\nexplore the impacts of ground-truth labels, input distribution, and\ncomplementary explanations, particularly when these are altered or perturbed.\nWe build on previous work, which offers mixed findings on how these elements\ninfluence ICL. To probe these questions, we employ explainable NLP (XNLP)\nmethods and utilize saliency maps of contrastive demonstrations for both\nqualitative and quantitative analysis. Our findings reveal that flipping\nground-truth labels significantly affects the saliency, though it's more\nnoticeable in larger LLMs. Our analysis of the input distribution at a granular\nlevel reveals that changing sentiment-indicative terms in a sentiment analysis\ntask to neutral ones does not have as substantial an impact as altering\nground-truth labels. Finally, we find that the effectiveness of complementary\nexplanations in boosting ICL performance is task-dependent, with limited\nbenefits seen in sentiment analysis tasks compared to symbolic reasoning tasks.\nThese insights are critical for understanding the functionality of LLMs and\nguiding the development of effective demonstrations, which is increasingly\nrelevant in light of the growing use of LLMs in applications such as ChatGPT.\nOur research code is publicly available at https://github.com/paihengxu/XICL.\n","authors":["Fuxiao Liu","Paiheng Xu","Zongxia Li","Yue Feng"],"pdf_url":"https://arxiv.org/pdf/2307.05052v3.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2311.10774v2","updated":"2024-04-15T15:48:48Z","published":"2023-11-15T23:36:42Z","title":"MMC: Advancing Multimodal Chart Understanding with Large-scale\n  Instruction Tuning","summary":"  With the rapid development of large language models (LLMs) and their\nintegration into large multimodal models (LMMs), there has been impressive\nprogress in zero-shot completion of user-oriented vision-language tasks.\nHowever, a gap remains in the domain of chart image understanding due to the\ndistinct abstract components in charts. To address this, we introduce a\nlarge-scale MultiModal Chart Instruction (\\textbf{MMC-Instruction}) dataset\ncomprising 600k instances supporting diverse tasks and chart types. Leveraging\nthis data, we develop MultiModal Chart Assistant (\\textbf{MMCA}), an LMM that\nachieves state-of-the-art performance on existing chart QA benchmarks.\nRecognizing the need for a comprehensive evaluation of LMM chart understanding,\nwe also propose a MultiModal Chart Benchmark (\\textbf{MMC-Benchmark}), a\ncomprehensive human-annotated benchmark with nine distinct tasks evaluating\nreasoning capabilities over charts. Extensive experiments on MMC-Benchmark\nreveal the limitations of existing LMMs on correctly interpreting charts, even\nfor the most recent GPT-4V model. Our work provides an instruction-tuning\nmethodology and benchmark to advance multimodal understanding of charts. Code\nand data are available at https://github.com/FuxiaoLiu/MMC.\n","authors":["Fuxiao Liu","Xiaoyang Wang","Wenlin Yao","Jianshu Chen","Kaiqiang Song","Sangwoo Cho","Yaser Yacoob","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2311.10774v2.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2404.09868v1","updated":"2024-04-15T15:33:29Z","published":"2024-04-15T15:33:29Z","title":"AI-Driven Statutory Reasoning via Software Engineering Methods","summary":"  The recent proliferation of generative artificial intelligence (GenAI)\ntechnologies such as pre-trained large language models (LLMs) has opened up new\nfrontiers in computational law. An exciting area of development is the use of\nAI to automate the rule-based reasoning inherent in statutory and contract law.\nWhile this form of reasoning has long been studied using classical techniques\nof natural language processing (NLP) and formal logic, recent solutions\nincreasingly make use of LLMs; though they are far from perfect.\n  The advent of GenAI has made it possible to treat many of these natural\nlanguage documents essentially as programs that compute a result given some set\nof facts. As such, it should be possible to understand, debug, maintain,\nevolve, and fix these documents using well-studied techniques from the field of\nsoftware engineering. This article introduces several concepts of automated\nsoftware testing and program analysis that could potentially be useful in\ncomputational law when applied to AI-driven analysis of statutes and contracts.\n","authors":["Rohan Padhye"],"pdf_url":"https://arxiv.org/pdf/2404.09868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04072v2","updated":"2024-04-15T15:25:53Z","published":"2023-11-07T15:36:40Z","title":"Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment","summary":"  Alignment with human preference is a desired property of large language\nmodels (LLMs). Currently, the main alignment approach is based on reinforcement\nlearning from human feedback (RLHF). Despite the effectiveness of RLHF, it is\nintricate to implement and train, thus recent studies explore how to develop\nalternative alignment approaches based on supervised fine-tuning (SFT). A major\nlimitation of SFT is that it essentially does imitation learning, which cannot\nfully understand what are the expected behaviors. To address this issue, we\npropose an improved alignment approach named FIGA. Different from prior\nmethods, we incorporate fine-grained (i.e., token or phrase level) quality\nsignals that are derived by contrasting good and bad responses. Our approach\nhas made two major contributions. Firstly, we curate a refined alignment\ndataset that pairs initial responses and the corresponding revised ones.\nSecondly, we devise a new loss function can leverage fine-grained quality\nsignals to instruct the learning of LLMs for alignment. Extensive experiments\nhave demonstrated the effectiveness of our approaches by comparing a number of\ncompetitive baselines.\n","authors":["Geyang Guo","Ranchi Zhao","Tianyi Tang","Wayne Xin Zhao","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2311.04072v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14530v3","updated":"2024-04-15T15:08:43Z","published":"2023-11-24T14:55:23Z","title":"Machine Translation for Ge'ez Language","summary":"  Machine translation (MT) for low-resource languages such as Ge'ez, an ancient\nlanguage that is no longer the native language of any community, faces\nchallenges such as out-of-vocabulary words, domain mismatches, and lack of\nsufficient labeled training data. In this work, we explore various methods to\nimprove Ge'ez MT, including transfer-learning from related languages,\noptimizing shared vocabulary and token segmentation approaches, finetuning\nlarge pre-trained models, and using large language models (LLMs) for few-shot\ntranslation with fuzzy matches. We develop a multilingual neural machine\ntranslation (MNMT) model based on languages relatedness, which brings an\naverage performance improvement of about 4 BLEU compared to standard bilingual\nmodels. We also attempt to finetune the NLLB-200 model, one of the most\nadvanced translation models available today, but find that it performs poorly\nwith only 4k training samples for Ge'ez. Furthermore, we experiment with using\nGPT-3.5, a state-of-the-art LLM, for few-shot translation with fuzzy matches,\nwhich leverages embedding similarity-based retrieval to find context examples\nfrom a parallel corpus. We observe that GPT-3.5 achieves a remarkable BLEU\nscore of 9.2 with no initial knowledge of Ge'ez, but still lower than the MNMT\nbaseline of 15.2. Our work provides insights into the potential and limitations\nof different approaches for low-resource and ancient language MT.\n","authors":["Aman Kassahun Wassie"],"pdf_url":"https://arxiv.org/pdf/2311.14530v3.pdf","comment":"8 pages, 1 figure"},{"id":"http://arxiv.org/abs/2401.12798v2","updated":"2024-04-15T14:47:12Z","published":"2024-01-23T14:31:12Z","title":"Gradient Flow of Energy: A General and Efficient Approach for Entity\n  Alignment Decoding","summary":"  Entity alignment (EA), a pivotal process in integrating multi-source\nKnowledge Graphs (KGs), seeks to identify equivalent entity pairs across these\ngraphs. Most existing approaches regard EA as a graph representation learning\ntask, concentrating on enhancing graph encoders. However, the decoding process\nin EA - essential for effective operation and alignment accuracy - has received\nlimited attention and remains tailored to specific datasets and model\narchitectures, necessitating both entity and additional explicit relation\nembeddings. This specificity limits its applicability, particularly in\nGNN-based models. To address this gap, we introduce a novel, generalized, and\nefficient decoding approach for EA, relying solely on entity embeddings. Our\nmethod optimizes the decoding process by minimizing Dirichlet energy, leading\nto the gradient flow within the graph, to maximize graph homophily. The\ndiscretization of the gradient flow produces a fast and scalable approach,\ntermed Triple Feature Propagation (TFP). TFP innovatively generalizes adjacency\nmatrices to multi-views matrices:entity-to-entity, entity-to-relation,\nrelation-to-entity, and relation-to-triple. The gradient flow through\ngeneralized matrices enables TFP to harness the multi-view structural\ninformation of KGs. Rigorous experimentation on diverse public datasets\ndemonstrates that our approach significantly enhances various EA methods.\nNotably, the approach achieves these advancements with less than 6 seconds of\nadditional computational time, establishing a new benchmark in efficiency and\nadaptability for future EA methods.\n","authors":["Yuanyi Wang","Haifeng Sun","Jingyu Wang","Qi Qi","Shaoling Sun","Jianxin Liao"],"pdf_url":"https://arxiv.org/pdf/2401.12798v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08669v2","updated":"2024-04-15T14:44:04Z","published":"2023-11-15T03:29:02Z","title":"On the Calibration of Multilingual Question Answering LLMs","summary":"  Multilingual pre-trained Large Language Models (LLMs) are incredibly\neffective at Question Answering (QA), a core task in Natural Language\nUnderstanding, achieving high accuracies on several multilingual benchmarks.\nHowever, little is known about how well their confidences are calibrated. In\nthis paper, we comprehensively benchmark the calibration of several\nmultilingual LLMs (MLLMs) on a variety of QA tasks. We perform extensive\nexperiments, spanning encoder-only, encoder-decoder, and decoder-only QA models\n(size varying from 110M to 7B parameters) and diverse languages, including both\nhigh- and low-resource ones. We study different dimensions of calibration in\nin-distribution, out-of-distribution, and cross-lingual transfer settings, and\ninvestigate strategies to improve it, including post-hoc methods and\nregularized fine-tuning. For decoder-only LLMs such as LlaMa2, we additionally\nfind that in-context learning improves confidence calibration on multilingual\ndata. We also conduct several ablation experiments to study the effect of\nlanguage distances, language corpus size, and model size on calibration, and\nhow multilingual models compare with their monolingual counterparts for diverse\ntasks and languages. Our experiments suggest that the multilingual QA models\nare poorly calibrated for languages other than English and incorporating a\nsmall set of cheaply translated multilingual samples during\nfine-tuning/calibration effectively enhances the calibration performance.\n","authors":["Yahan Yang","Soham Dan","Dan Roth","Insup Lee"],"pdf_url":"https://arxiv.org/pdf/2311.08669v2.pdf","comment":"Preprint. Under Submission"},{"id":"http://arxiv.org/abs/2404.09830v1","updated":"2024-04-15T14:28:33Z","published":"2024-04-15T14:28:33Z","title":"Negation Triplet Extraction with Syntactic Dependency and Semantic\n  Consistency","summary":"  Previous works of negation understanding mainly focus on negation cue\ndetection and scope resolution, without identifying negation subject which is\nalso significant to the downstream tasks. In this paper, we propose a new\nnegation triplet extraction (NTE) task which aims to extract negation subject\nalong with negation cue and scope. To achieve NTE, we devise a novel\nSyntax&Semantic-Enhanced Negation Extraction model, namely SSENE, which is\nbuilt based on a generative pretrained language model (PLM) {of Encoder-Decoder\narchitecture} with a multi-task learning framework. Specifically, the given\nsentence's syntactic dependency tree is incorporated into the PLM's encoder to\ndiscover the correlations between the negation subject, cue and scope.\nMoreover, the semantic consistency between the sentence and the extracted\ntriplet is ensured by an auxiliary task learning. Furthermore, we have\nconstructed a high-quality Chinese dataset NegComment based on the users'\nreviews from the real-world platform of Meituan, upon which our evaluations\nshow that SSENE achieves the best NTE performance compared to the baselines.\nOur ablation and case studies also demonstrate that incorporating the syntactic\ninformation helps the PLM's recognize the distant dependency between the\nsubject and cue, and the auxiliary task learning is helpful to extract the\nnegation triplets with more semantic consistency.\n","authors":["Yuchen Shi","Deqing Yang","Jingping Liu","Yanghua Xiao","Zongyu Wang","Huimin Xu"],"pdf_url":"https://arxiv.org/pdf/2404.09830v1.pdf","comment":"Accepted by COLING 2024"},{"id":"http://arxiv.org/abs/2402.12038v2","updated":"2024-04-15T14:22:05Z","published":"2024-02-19T10:47:09Z","title":"Self-AMPLIFY: Improving Small Language Models with Self Post Hoc\n  Explanations","summary":"  Incorporating natural language rationales in the prompt and In-Context\nLearning (ICL) has led to a significant improvement of Large Language Models\n(LLMs) performance. However, rationales currently require human-annotation or\nthe use of auxiliary proxy models to target promising samples or generate\nhigh-quality rationales. In this work, we propose Self-AMPLIFY to generate\nautomatically rationales from post hoc explanation methods applied to Small\nLanguage Models (SLMs) to improve their own performance. Self-AMPLIFY is a\n3-step method that targets samples, generates rationales and builds a final\nprompt to leverage ICL. Self-AMPLIFY performance is evaluated on two SLMs and\ntwo datasets requiring reasoning abilities: these experiments show that\nSelf-AMPLIFY achieves good results against competitors. Self-AMPLIFY is the\nfirst method to apply post hoc explanation methods to SLM to generate\nrationales to improve their own performance in a fully automated manner.\n","authors":["Milan Bhan","Jean-Noel Vittaut","Nicolas Chesneau","Marie-Jeanne Lesot"],"pdf_url":"https://arxiv.org/pdf/2402.12038v2.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2404.09752v1","updated":"2024-04-15T12:53:48Z","published":"2024-04-15T12:53:48Z","title":"Can We Break Free from Strong Data Augmentations in Self-Supervised\n  Learning?","summary":"  Self-supervised learning (SSL) has emerged as a promising solution for\naddressing the challenge of limited labeled data in deep neural networks\n(DNNs), offering scalability potential. However, the impact of design\ndependencies within the SSL framework remains insufficiently investigated. In\nthis study, we comprehensively explore SSL behavior across a spectrum of\naugmentations, revealing their crucial role in shaping SSL model performance\nand learning mechanisms. Leveraging these insights, we propose a novel learning\napproach that integrates prior knowledge, with the aim of curtailing the need\nfor extensive data augmentations and thereby amplifying the efficacy of learned\nrepresentations. Notably, our findings underscore that SSL models imbued with\nprior knowledge exhibit reduced texture bias, diminished reliance on shortcuts\nand augmentations, and improved robustness against both natural and adversarial\ncorruptions. These findings not only illuminate a new direction in SSL\nresearch, but also pave the way for enhancing DNN performance while\nconcurrently alleviating the imperative for intensive data augmentation,\nthereby enhancing scalability and real-world problem-solving capabilities.\n","authors":["Shruthi Gowda","Elahe Arani","Bahram Zonooz"],"pdf_url":"https://arxiv.org/pdf/2404.09752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09748v1","updated":"2024-04-15T12:50:44Z","published":"2024-04-15T12:50:44Z","title":"LetsGo: Large-Scale Garage Modeling and Rendering via LiDAR-Assisted\n  Gaussian Primitives","summary":"  Large garages are ubiquitous yet intricate scenes in our daily lives, posing\nchallenges characterized by monotonous colors, repetitive patterns, reflective\nsurfaces, and transparent vehicle glass. Conventional Structure from Motion\n(SfM) methods for camera pose estimation and 3D reconstruction fail in these\nenvironments due to poor correspondence construction. To address these\nchallenges, this paper introduces LetsGo, a LiDAR-assisted Gaussian splatting\napproach for large-scale garage modeling and rendering. We develop a handheld\nscanner, Polar, equipped with IMU, LiDAR, and a fisheye camera, to facilitate\naccurate LiDAR and image data scanning. With this Polar device, we present a\nGarageWorld dataset consisting of five expansive garage scenes with diverse\ngeometric structures and will release the dataset to the community for further\nresearch. We demonstrate that the collected LiDAR point cloud by the Polar\ndevice enhances a suite of 3D Gaussian splatting algorithms for garage scene\nmodeling and rendering. We also propose a novel depth regularizer for 3D\nGaussian splatting algorithm training, effectively eliminating floating\nartifacts in rendered images, and a lightweight Level of Detail (LOD) Gaussian\nrenderer for real-time viewing on web-based devices. Additionally, we explore a\nhybrid representation that combines the advantages of traditional mesh in\ndepicting simple geometry and colors (e.g., walls and the ground) with modern\n3D Gaussian representations capturing complex details and high-frequency\ntextures. This strategy achieves an optimal balance between memory performance\nand rendering quality. Experimental results on our dataset, along with\nScanNet++ and KITTI-360, demonstrate the superiority of our method in rendering\nquality and resource efficiency.\n","authors":["Jiadi Cui","Junming Cao","Yuhui Zhong","Liao Wang","Fuqiang Zhao","Penghao Wang","Yifan Chen","Zhipeng He","Lan Xu","Yujiao Shi","Yingliang Zhang","Jingyi Yu"],"pdf_url":"https://arxiv.org/pdf/2404.09748v1.pdf","comment":"Project Page: https://jdtsui.github.io/letsgo/"},{"id":"http://arxiv.org/abs/2404.09736v1","updated":"2024-04-15T12:37:26Z","published":"2024-04-15T12:37:26Z","title":"FSRT: Facial Scene Representation Transformer for Face Reenactment from\n  Factorized Appearance, Head-pose, and Facial Expression Features","summary":"  The task of face reenactment is to transfer the head motion and facial\nexpressions from a driving video to the appearance of a source image, which may\nbe of a different person (cross-reenactment). Most existing methods are\nCNN-based and estimate optical flow from the source image to the current\ndriving frame, which is then inpainted and refined to produce the output\nanimation. We propose a transformer-based encoder for computing a set-latent\nrepresentation of the source image(s). We then predict the output color of a\nquery pixel using a transformer-based decoder, which is conditioned with\nkeypoints and a facial expression vector extracted from the driving frame.\nLatent representations of the source person are learned in a self-supervised\nmanner that factorize their appearance, head pose, and facial expressions.\nThus, they are perfectly suited for cross-reenactment. In contrast to most\nrelated work, our method naturally extends to multiple source images and can\nthus adapt to person-specific facial dynamics. We also propose data\naugmentation and regularization schemes that are necessary to prevent\noverfitting and support generalizability of the learned representations. We\nevaluated our approach in a randomized user study. The results indicate\nsuperior performance compared to the state-of-the-art in terms of motion\ntransfer quality and temporal consistency.\n","authors":["Andre Rochow","Max Schwarz","Sven Behnke"],"pdf_url":"https://arxiv.org/pdf/2404.09736v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2404.09735v1","updated":"2024-04-15T12:35:10Z","published":"2024-04-15T12:35:10Z","title":"Equipping Diffusion Models with Differentiable Spatial Entropy for\n  Low-Light Image Enhancement","summary":"  Image restoration, which aims to recover high-quality images from their\ncorrupted counterparts, often faces the challenge of being an ill-posed problem\nthat allows multiple solutions for a single input. However, most deep learning\nbased works simply employ l1 loss to train their network in a deterministic\nway, resulting in over-smoothed predictions with inferior perceptual quality.\nIn this work, we propose a novel method that shifts the focus from a\ndeterministic pixel-by-pixel comparison to a statistical perspective,\nemphasizing the learning of distributions rather than individual pixel values.\nThe core idea is to introduce spatial entropy into the loss function to measure\nthe distribution difference between predictions and targets. To make this\nspatial entropy differentiable, we employ kernel density estimation (KDE) to\napproximate the probabilities for specific intensity values of each pixel with\ntheir neighbor areas. Specifically, we equip the entropy with diffusion models\nand aim for superior accuracy and enhanced perceptual quality over l1 based\nnoise matching loss. In the experiments, we evaluate the proposed method for\nlow light enhancement on two datasets and the NTIRE challenge 2024. All these\nresults illustrate the effectiveness of our statistic-based entropy loss. Code\nis available at https://github.com/shermanlian/spatial-entropy-loss.\n","authors":["Wenyi Lian","Wenjing Lian","Ziwei Luo"],"pdf_url":"https://arxiv.org/pdf/2404.09735v1.pdf","comment":"CVPRW 2024, best LPIPS in the NTIRE low light enhancement challenge\n  2024"},{"id":"http://arxiv.org/abs/2404.09732v1","updated":"2024-04-15T12:34:21Z","published":"2024-04-15T12:34:21Z","title":"Photo-Realistic Image Restoration in the Wild with Controlled\n  Vision-Language Models","summary":"  Though diffusion models have been successfully applied to various image\nrestoration (IR) tasks, their performance is sensitive to the choice of\ntraining datasets. Typically, diffusion models trained in specific datasets\nfail to recover images that have out-of-distribution degradations. To address\nthis problem, this work leverages a capable vision-language model and a\nsynthetic degradation pipeline to learn image restoration in the wild (wild\nIR). More specifically, all low-quality images are simulated with a synthetic\ndegradation pipeline that contains multiple common degradations such as blur,\nresize, noise, and JPEG compression. Then we introduce robust training for a\ndegradation-aware CLIP model to extract enriched image content features to\nassist high-quality image restoration. Our base diffusion model is the image\nrestoration SDE (IR-SDE). Built upon it, we further present a posterior\nsampling strategy for fast noise-free image generation. We evaluate our model\non both synthetic and real-world degradation datasets. Moreover, experiments on\nthe unified image restoration task illustrate that the proposed posterior\nsampling improves image generation quality for various degradations.\n","authors":["Ziwei Luo","Fredrik K. Gustafsson","Zheng Zhao","Jens Sj√∂lund","Thomas B. Sch√∂n"],"pdf_url":"https://arxiv.org/pdf/2404.09732v1.pdf","comment":"CVPRW 2024; Code: https://github.com/Algolzw/daclip-uir"},{"id":"http://arxiv.org/abs/2404.06913v2","updated":"2024-04-15T12:27:51Z","published":"2024-04-10T11:06:29Z","title":"Sparse Global Matching for Video Frame Interpolation with Large Motion","summary":"  Large motion poses a critical challenge in Video Frame Interpolation (VFI)\ntask. Existing methods are often constrained by limited receptive fields,\nresulting in sub-optimal performance when handling scenarios with large motion.\nIn this paper, we introduce a new pipeline for VFI, which can effectively\nintegrate global-level information to alleviate issues associated with large\nmotion. Specifically, we first estimate a pair of initial intermediate flows\nusing a high-resolution feature map for extracting local details. Then, we\nincorporate a sparse global matching branch to compensate for flow estimation,\nwhich consists of identifying flaws in initial flows and generating sparse flow\ncompensation with a global receptive field. Finally, we adaptively merge the\ninitial flow estimation with global flow compensation, yielding a more accurate\nintermediate flow. To evaluate the effectiveness of our method in handling\nlarge motion, we carefully curate a more challenging subset from commonly used\nbenchmarks. Our method demonstrates the state-of-the-art performance on these\nVFI subsets with large motion.\n","authors":["Chunxu Liu","Guozhen Zhang","Rui Zhao","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2404.06913v2.pdf","comment":"Accepted by CVPR 2024. Project page: https://sgm-vfi.github.io/.\n  Fixed some typos in the supplementary material"},{"id":"http://arxiv.org/abs/2402.06611v2","updated":"2024-04-15T12:13:42Z","published":"2024-02-09T18:42:30Z","title":"Image-based Deep Learning for the time-dependent prediction of fresh\n  concrete properties","summary":"  Increasing the degree of digitisation and automation in the concrete\nproduction process can play a crucial role in reducing the CO$_2$ emissions\nthat are associated with the production of concrete. In this paper, a method is\npresented that makes it possible to predict the properties of fresh concrete\nduring the mixing process based on stereoscopic image sequences of the\nconcretes flow behaviour. A Convolutional Neural Network (CNN) is used for the\nprediction, which receives the images supported by information on the mix\ndesign as input. In addition, the network receives temporal information in the\nform of the time difference between the time at which the images are taken and\nthe time at which the reference values of the concretes are carried out. With\nthis temporal information, the network implicitly learns the time-dependent\nbehaviour of the concretes properties. The network predicts the slump flow\ndiameter, the yield stress and the plastic viscosity. The time-dependent\nprediction potentially opens up the pathway to determine the temporal\ndevelopment of the fresh concrete properties already during mixing. This\nprovides a huge advantage for the concrete industry. As a result,\ncountermeasures can be taken in a timely manner. It is shown that an approach\nbased on depth and optical flow images, supported by information of the mix\ndesign, achieves the best results.\n","authors":["Max Meyer","Amadeus Langer","Max Mehltretter","Dries Beyer","Max Coenen","Tobias Schack","Michael Haist","Christian Heipke"],"pdf_url":"https://arxiv.org/pdf/2402.06611v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08212v2","updated":"2024-04-15T12:08:41Z","published":"2024-01-16T08:56:52Z","title":"Human vs. LMMs: Exploring the Discrepancy in Emoji Interpretation and\n  Usage in Digital Communication","summary":"  Leveraging Large Multimodal Models (LMMs) to simulate human behaviors when\nprocessing multimodal information, especially in the context of social media,\nhas garnered immense interest due to its broad potential and far-reaching\nimplications. Emojis, as one of the most unique aspects of digital\ncommunication, are pivotal in enriching and often clarifying the emotional and\ntonal dimensions. Yet, there is a notable gap in understanding how these\nadvanced models, such as GPT-4V, interpret and employ emojis in the nuanced\ncontext of online interaction. This study intends to bridge this gap by\nexamining the behavior of GPT-4V in replicating human-like use of emojis. The\nfindings reveal a discernible discrepancy between human and GPT-4V behaviors,\nlikely due to the subjective nature of human interpretation and the limitations\nof GPT-4V's English-centric training, suggesting cultural biases and inadequate\nrepresentation of non-English cultures.\n","authors":["Hanjia Lyu","Weihong Qi","Zhongyu Wei","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2401.08212v2.pdf","comment":"Accepted for publication in ICWSM 2024"},{"id":"http://arxiv.org/abs/2404.09707v1","updated":"2024-04-15T12:06:00Z","published":"2024-04-15T12:06:00Z","title":"Adaptive Patching for High-resolution Image Segmentation with\n  Transformers","summary":"  Attention-based models are proliferating in the space of image analytics,\nincluding segmentation. The standard method of feeding images to transformer\nencoders is to divide the images into patches and then feed the patches to the\nmodel as a linear sequence of tokens. For high-resolution images, e.g.\nmicroscopic pathology images, the quadratic compute and memory cost prohibits\nthe use of an attention-based model, if we are to use smaller patch sizes that\nare favorable in segmentation. The solution is to either use custom complex\nmulti-resolution models or approximate attention schemes. We take inspiration\nfrom Adapative Mesh Refinement (AMR) methods in HPC by adaptively patching the\nimages, as a pre-processing step, based on the image details to reduce the\nnumber of patches being fed to the model, by orders of magnitude. This method\nhas a negligible overhead, and works seamlessly with any attention-based model,\ni.e. it is a pre-processing step that can be adopted by any attention-based\nmodel without friction. We demonstrate superior segmentation quality over SoTA\nsegmentation models for real-world pathology datasets while gaining a geomean\nspeedup of $6.9\\times$ for resolutions up to $64K^2$, on up to $2,048$ GPUs.\n","authors":["Enzhi Zhang","Isaac Lyngaas","Peng Chen","Xiao Wang","Jun Igarashi","Yuankai Huo","Mohamed Wahib","Masaharu Munetomo"],"pdf_url":"https://arxiv.org/pdf/2404.09707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09697v1","updated":"2024-04-15T11:59:19Z","published":"2024-04-15T11:59:19Z","title":"HSIDMamba: Exploring Bidirectional State-Space Models for Hyperspectral\n  Denoising","summary":"  Effectively discerning spatial-spectral dependencies in HSI denoising is\ncrucial, but prevailing methods using convolution or transformers still face\ncomputational efficiency limitations. Recently, the emerging Selective State\nSpace Model(Mamba) has risen with its nearly linear computational complexity in\nprocessing natural language sequences, which inspired us to explore its\npotential in handling long spectral sequences. In this paper, we propose\nHSIDMamba(HSDM), tailored to exploit the linear complexity for effectively\ncapturing spatial-spectral dependencies in HSI denoising. In particular, HSDM\ncomprises multiple Hyperspectral Continuous Scan Blocks, incorporating\nBCSM(Bidirectional Continuous Scanning Mechanism), scale residual, and spectral\nattention mechanisms to enhance the capture of long-range and local\nspatial-spectral information. BCSM strengthens spatial-spectral interactions by\nlinking forward and backward scans and enhancing information from eight\ndirections through SSM, significantly enhancing the perceptual capability of\nHSDM and improving denoising performance more effectively. Extensive\nevaluations against HSI denoising benchmarks validate the superior performance\nof HSDM, achieving state-of-the-art results in performance and surpassing the\nefficiency of the latest transformer architectures by $30\\%$.\n","authors":["Yang Liu","Jiahua Xiao","Yu Guo","Peilin Jiang","Haiwei Yang","Fei Wang"],"pdf_url":"https://arxiv.org/pdf/2404.09697v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09692v1","updated":"2024-04-15T11:46:24Z","published":"2024-04-15T11:46:24Z","title":"XoFTR: Cross-modal Feature Matching Transformer","summary":"  We introduce, XoFTR, a cross-modal cross-view method for local feature\nmatching between thermal infrared (TIR) and visible images. Unlike visible\nimages, TIR images are less susceptible to adverse lighting and weather\nconditions but present difficulties in matching due to significant texture and\nintensity differences. Current hand-crafted and learning-based methods for\nvisible-TIR matching fall short in handling viewpoint, scale, and texture\ndiversities. To address this, XoFTR incorporates masked image modeling\npre-training and fine-tuning with pseudo-thermal image augmentation to handle\nthe modality differences. Additionally, we introduce a refined matching\npipeline that adjusts for scale discrepancies and enhances match reliability\nthrough sub-pixel level refinement. To validate our approach, we collect a\ncomprehensive visible-thermal dataset, and show that our method outperforms\nexisting methods on many benchmarks.\n","authors":["√ñnder Tuzcuoƒülu","Aybora K√∂ksal","Buƒüra Sofu","Sinan Kalkan","A. Aydƒ±n Alatan"],"pdf_url":"https://arxiv.org/pdf/2404.09692v1.pdf","comment":"CVPR Image Matching Workshop, 2024. 12 pages, 7 figures, 5 tables.\n  Codes and dataset are available at https://github.com/OnderT/XoFTR"},{"id":"http://arxiv.org/abs/2404.09690v1","updated":"2024-04-15T11:45:30Z","published":"2024-04-15T11:45:30Z","title":"Harnessing GPT-4V(ision) for Insurance: A Preliminary Exploration","summary":"  The emergence of Large Multimodal Models (LMMs) marks a significant milestone\nin the development of artificial intelligence. Insurance, as a vast and complex\ndiscipline, involves a wide variety of data forms in its operational processes,\nincluding text, images, and videos, thereby giving rise to diverse multimodal\ntasks. Despite this, there has been limited systematic exploration of\nmultimodal tasks specific to insurance, nor a thorough investigation into how\nLMMs can address these challenges. In this paper, we explore GPT-4V's\ncapabilities in the insurance domain. We categorize multimodal tasks by\nfocusing primarily on visual aspects based on types of insurance (e.g., auto,\nhousehold/commercial property, health, and agricultural insurance) and\ninsurance stages (e.g., risk assessment, risk monitoring, and claims\nprocessing). Our experiment reveals that GPT-4V exhibits remarkable abilities\nin insurance-related tasks, demonstrating not only a robust understanding of\nmultimodal content in the insurance domain but also a comprehensive knowledge\nof insurance scenarios. However, there are notable shortcomings: GPT-4V\nstruggles with detailed risk rating and loss assessment, suffers from\nhallucination in image understanding, and shows variable support for different\nlanguages. Through this work, we aim to bridge the insurance domain with\ncutting-edge LMM technology, facilitate interdisciplinary exchange and\ndevelopment, and provide a foundation for the continued advancement and\nevolution of future research endeavors.\n","authors":["Chenwei Lin","Hanjia Lyu","Jiebo Luo","Xian Xu"],"pdf_url":"https://arxiv.org/pdf/2404.09690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12796v3","updated":"2024-04-15T11:40:39Z","published":"2023-11-21T18:59:58Z","title":"Physics-guided Shape-from-Template: Monocular Video Perception through\n  Neural Surrogate Models","summary":"  3D reconstruction of dynamic scenes is a long-standing problem in computer\ngraphics and increasingly difficult the less information is available.\nShape-from-Template (SfT) methods aim to reconstruct a template-based geometry\nfrom RGB images or video sequences, often leveraging just a single monocular\ncamera without depth information, such as regular smartphone recordings.\nUnfortunately, existing reconstruction methods are either unphysical and noisy\nor slow in optimization. To solve this problem, we propose a novel SfT\nreconstruction algorithm for cloth using a pre-trained neural surrogate model\nthat is fast to evaluate, stable, and produces smooth reconstructions due to a\nregularizing physics simulation. Differentiable rendering of the simulated mesh\nenables pixel-wise comparisons between the reconstruction and a target video\nsequence that can be used for a gradient-based optimization procedure to\nextract not only shape information but also physical parameters such as\nstretching, shearing, or bending stiffness of the cloth. This allows to retain\na precise, stable, and smooth reconstructed geometry while reducing the runtime\nby a factor of 400-500 compared to $\\phi$-SfT, a state-of-the-art physics-based\nSfT approach.\n","authors":["David Stotko","Nils Wandel","Reinhard Klein"],"pdf_url":"https://arxiv.org/pdf/2311.12796v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09683v1","updated":"2024-04-15T11:36:31Z","published":"2024-04-15T11:36:31Z","title":"Post-Training Network Compression for 3D Medical Image Segmentation:\n  Reducing Computational Efforts via Tucker Decomposition","summary":"  We address the computational barrier of deploying advanced deep learning\nsegmentation models in clinical settings by studying the efficacy of network\ncompression through tensor decomposition. We propose a post-training Tucker\nfactorization that enables the decomposition of pre-existing models to reduce\ncomputational requirements without impeding segmentation accuracy. We applied\nTucker decomposition to the convolutional kernels of the TotalSegmentator (TS)\nmodel, an nnU-Net model trained on a comprehensive dataset for automatic\nsegmentation of 117 anatomical structures. Our approach reduced the\nfloating-point operations (FLOPs) and memory required during inference,\noffering an adjustable trade-off between computational efficiency and\nsegmentation quality. This study utilized the publicly available TS dataset,\nemploying various downsampling factors to explore the relationship between\nmodel size, inference speed, and segmentation performance. The application of\nTucker decomposition to the TS model substantially reduced the model parameters\nand FLOPs across various compression rates, with limited loss in segmentation\naccuracy. We removed up to 88% of the model's parameters with no significant\nperformance changes in the majority of classes after fine-tuning. Practical\nbenefits varied across different graphics processing unit (GPU) architectures,\nwith more distinct speed-ups on less powerful hardware. Post-hoc network\ncompression via Tucker decomposition presents a viable strategy for reducing\nthe computational demand of medical image segmentation models without\nsubstantially sacrificing accuracy. This approach enables the broader adoption\nof advanced deep learning technologies in clinical practice, offering a way to\nnavigate the constraints of hardware capabilities.\n","authors":["Tobias Weber","Jakob Dexl","David R√ºgamer","Michael Ingrisch"],"pdf_url":"https://arxiv.org/pdf/2404.09683v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00368v2","updated":"2024-04-15T11:18:00Z","published":"2024-03-30T13:41:57Z","title":"Towards Variable and Coordinated Holistic Co-Speech Motion Generation","summary":"  This paper addresses the problem of generating lifelike holistic co-speech\nmotions for 3D avatars, focusing on two key aspects: variability and\ncoordination. Variability allows the avatar to exhibit a wide range of motions\neven with similar speech content, while coordination ensures a harmonious\nalignment among facial expressions, hand gestures, and body poses. We aim to\nachieve both with ProbTalk, a unified probabilistic framework designed to\njointly model facial, hand, and body movements in speech. ProbTalk builds on\nthe variational autoencoder (VAE) architecture and incorporates three core\ndesigns. First, we introduce product quantization (PQ) to the VAE, which\nenriches the representation of complex holistic motion. Second, we devise a\nnovel non-autoregressive model that embeds 2D positional encoding into the\nproduct-quantized representation, thereby preserving essential structure\ninformation of the PQ codes. Last, we employ a secondary stage to refine the\npreliminary prediction, further sharpening the high-frequency details. Coupling\nthese three designs enables ProbTalk to generate natural and diverse holistic\nco-speech motions, outperforming several state-of-the-art methods in\nqualitative and quantitative evaluations, particularly in terms of realism. Our\ncode and model will be released for research purposes at\nhttps://feifeifeiliu.github.io/probtalk/.\n","authors":["Yifei Liu","Qiong Cao","Yandong Wen","Huaiguang Jiang","Changxing Ding"],"pdf_url":"https://arxiv.org/pdf/2404.00368v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2312.00362v2","updated":"2024-04-15T11:03:06Z","published":"2023-12-01T05:59:08Z","title":"Dancing with Still Images: Video Distillation via Static-Dynamic\n  Disentanglement","summary":"  Recently, dataset distillation has paved the way towards efficient machine\nlearning, especially for image datasets. However, the distillation for videos,\ncharacterized by an exclusive temporal dimension, remains an underexplored\ndomain. In this work, we provide the first systematic study of video\ndistillation and introduce a taxonomy to categorize temporal compression. Our\ninvestigation reveals that the temporal information is usually not well learned\nduring distillation, and the temporal dimension of synthetic data contributes\nlittle. The observations motivate our unified framework of disentangling the\ndynamic and static information in the videos. It first distills the videos into\nstill images as static memory and then compensates the dynamic and motion\ninformation with a learnable dynamic memory block. Our method achieves\nstate-of-the-art on video datasets at different scales, with a notably smaller\nmemory storage budget. Our code is available at\nhttps://github.com/yuz1wan/video_distillation.\n","authors":["Ziyu Wang","Yue Xu","Cewu Lu","Yong-Lu Li"],"pdf_url":"https://arxiv.org/pdf/2312.00362v2.pdf","comment":"CVPR 2024, project page: https://mvig-rhos.com/video-distill"},{"id":"http://arxiv.org/abs/2404.09666v1","updated":"2024-04-15T10:57:16Z","published":"2024-04-15T10:57:16Z","title":"Deformable MRI Sequence Registration for AI-based Prostate Cancer\n  Diagnosis","summary":"  The PI-CAI (Prostate Imaging: Cancer AI) challenge led to expert-level\ndiagnostic algorithms for clinically significant prostate cancer detection. The\nalgorithms receive biparametric MRI scans as input, which consist of\nT2-weighted and diffusion-weighted scans. These scans can be misaligned due to\nmultiple factors in the scanning process. Image registration can alleviate this\nissue by predicting the deformation between the sequences. We investigate the\neffect of image registration on the diagnostic performance of AI-based prostate\ncancer diagnosis. First, the image registration algorithm, developed in\nMeVisLab, is analyzed using a dataset with paired lesion annotations. Second,\nthe effect on diagnosis is evaluated by comparing case-level cancer diagnosis\nperformance between using the original dataset, rigidly aligned\ndiffusion-weighted scans, or deformably aligned diffusion-weighted scans. Rigid\nregistration showed no improvement. Deformable registration demonstrated a\nsubstantial improvement in lesion overlap (+10% median Dice score) and a\npositive yet non-significant improvement in diagnostic performance (+0.3%\nAUROC, p=0.18). Our investigation shows that a substantial improvement in\nlesion alignment does not directly lead to a significant improvement in\ndiagnostic performance. Qualitative analysis indicated that jointly developing\nimage registration methods and diagnostic AI algorithms could enhance\ndiagnostic accuracy and patient outcomes.\n","authors":["Alessa Hering","Sarah de Boer","Anindo Saha","Jasper J. Twilt","Derya Yakar","Maarten de Rooij","Henkjan Huisman","Joeran S. Bosma"],"pdf_url":"https://arxiv.org/pdf/2404.09666v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08285v2","updated":"2024-04-15T10:50:47Z","published":"2024-04-12T07:19:16Z","title":"A Survey of Neural Network Robustness Assessment in Image Recognition","summary":"  In recent years, there has been significant attention given to the robustness\nassessment of neural networks. Robustness plays a critical role in ensuring\nreliable operation of artificial intelligence (AI) systems in complex and\nuncertain environments. Deep learning's robustness problem is particularly\nsignificant, highlighted by the discovery of adversarial attacks on image\nclassification models. Researchers have dedicated efforts to evaluate\nrobustness in diverse perturbation conditions for image recognition tasks.\nRobustness assessment encompasses two main techniques: robustness verification/\ncertification for deliberate adversarial attacks and robustness testing for\nrandom data corruptions. In this survey, we present a detailed examination of\nboth adversarial robustness (AR) and corruption robustness (CR) in neural\nnetwork assessment. Analyzing current research papers and standards, we provide\nan extensive overview of robustness assessment in image recognition. Three\nessential aspects are analyzed: concepts, metrics, and assessment methods. We\ninvestigate the perturbation metrics and range representations used to measure\nthe degree of perturbations on images, as well as the robustness metrics\nspecifically for the robustness conditions of classification models. The\nstrengths and limitations of the existing methods are also discussed, and some\npotential directions for future research are provided.\n","authors":["Jie Wang","Jun Ai","Minyan Lu","Haoran Su","Dan Yu","Yutao Zhang","Junda Zhu","Jingyu Liu"],"pdf_url":"https://arxiv.org/pdf/2404.08285v2.pdf","comment":"Corrected typos and grammatical errors in Section 5"},{"id":"http://arxiv.org/abs/2404.09654v1","updated":"2024-04-15T10:42:22Z","published":"2024-04-15T10:42:22Z","title":"Do LLMs Understand Visual Anomalies? Uncovering LLM Capabilities in\n  Zero-shot Anomaly Detection","summary":"  Large vision-language models (LVLMs) are markedly proficient in deriving\nvisual representations guided by natural language. Recent explorations have\nutilized LVLMs to tackle zero-shot visual anomaly detection (VAD) challenges by\npairing images with textual descriptions indicative of normal and abnormal\nconditions, referred to as anomaly prompts. However, existing approaches depend\non static anomaly prompts that are prone to cross-semantic ambiguity, and\nprioritize global image-level representations over crucial local pixel-level\nimage-to-text alignment that is necessary for accurate anomaly localization. In\nthis paper, we present ALFA, a training-free approach designed to address these\nchallenges via a unified model. We propose a run-time prompt adaptation\nstrategy, which first generates informative anomaly prompts to leverage the\ncapabilities of a large language model (LLM). This strategy is enhanced by a\ncontextual scoring mechanism for per-image anomaly prompt adaptation and\ncross-semantic ambiguity mitigation. We further introduce a novel fine-grained\naligner to fuse local pixel-level semantics for precise anomaly localization,\nby projecting the image-text alignment from global to local semantic spaces.\nExtensive evaluations on the challenging MVTec and VisA datasets confirm ALFA's\neffectiveness in harnessing the language potential for zero-shot VAD, achieving\nsignificant PRO improvements of 12.1% on MVTec AD and 8.9% on VisA compared to\nstate-of-the-art zero-shot VAD approaches.\n","authors":["Jiaqi Zhu","Shaofeng Cai","Fang Deng","Junran Wu"],"pdf_url":"https://arxiv.org/pdf/2404.09654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.03453v2","updated":"2024-04-15T10:28:44Z","published":"2023-09-07T02:28:04Z","title":"SyncDreamer: Generating Multiview-consistent Images from a Single-view\n  Image","summary":"  In this paper, we present a novel diffusion model called that generates\nmultiview-consistent images from a single-view image. Using pretrained\nlarge-scale 2D diffusion models, recent work Zero123 demonstrates the ability\nto generate plausible novel views from a single-view image of an object.\nHowever, maintaining consistency in geometry and colors for the generated\nimages remains a challenge. To address this issue, we propose a synchronized\nmultiview diffusion model that models the joint probability distribution of\nmultiview images, enabling the generation of multiview-consistent images in a\nsingle reverse process. SyncDreamer synchronizes the intermediate states of all\nthe generated images at every step of the reverse process through a 3D-aware\nfeature attention mechanism that correlates the corresponding features across\ndifferent views. Experiments show that SyncDreamer generates images with high\nconsistency across different views, thus making it well-suited for various 3D\ngeneration tasks such as novel-view-synthesis, text-to-3D, and image-to-3D.\n","authors":["Yuan Liu","Cheng Lin","Zijiao Zeng","Xiaoxiao Long","Lingjie Liu","Taku Komura","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2309.03453v2.pdf","comment":"ICLR 2024 Spotlight. Project page:\n  https://liuyuan-pal.github.io/SyncDreamer/ Code:\n  https://github.com/liuyuan-pal/SyncDreamer"},{"id":"http://arxiv.org/abs/2404.09645v1","updated":"2024-04-15T10:24:32Z","published":"2024-04-15T10:24:32Z","title":"Real-world Instance-specific Image Goal Navigation for Service Robots:\n  Bridging the Domain Gap with Contrastive Learning","summary":"  Improving instance-specific image goal navigation (InstanceImageNav), which\nlocates the identical object in a real-world environment from a query image, is\nessential for robotic systems to assist users in finding desired objects. The\nchallenge lies in the domain gap between low-quality images observed by the\nmoving robot, characterized by motion blur and low-resolution, and high-quality\nquery images provided by the user. Such domain gaps could significantly reduce\nthe task success rate but have not been the focus of previous work. To address\nthis, we propose a novel method called Few-shot Cross-quality Instance-aware\nAdaptation (CrossIA), which employs contrastive learning with an instance\nclassifier to align features between massive low- and few high-quality images.\nThis approach effectively reduces the domain gap by bringing the latent\nrepresentations of cross-quality images closer on an instance basis.\nAdditionally, the system integrates an object image collection with a\npre-trained deblurring model to enhance the observed image quality. Our method\nfine-tunes the SimSiam model, pre-trained on ImageNet, using CrossIA. We\nevaluated our method's effectiveness through an InstanceImageNav task with 20\ndifferent types of instances, where the robot identifies the same instance in a\nreal-world environment as a high-quality query image. Our experiments showed\nthat our method improves the task success rate by up to three times compared to\nthe baseline, a conventional approach based on SuperGlue. These findings\nhighlight the potential of leveraging contrastive learning and image\nenhancement techniques to bridge the domain gap and improve object localization\nin robotic applications. The project website is\nhttps://emergentsystemlabstudent.github.io/DomainBridgingNav/.\n","authors":["Taichi Sakaguchi","Akira Taniguchi","Yoshinobu Hagiwara","Lotfi El Hafi","Shoichi Hasegawa","Tadahiro Taniguchi"],"pdf_url":"https://arxiv.org/pdf/2404.09645v1.pdf","comment":"See website at\n  https://emergentsystemlabstudent.github.io/DomainBridgingNav/. Submitted to\n  IROS2024"},{"id":"http://arxiv.org/abs/2404.09640v1","updated":"2024-04-15T10:19:39Z","published":"2024-04-15T10:19:39Z","title":"CREST: Cross-modal Resonance through Evidential Deep Learning for\n  Enhanced Zero-Shot Learning","summary":"  Zero-shot learning (ZSL) enables the recognition of novel classes by\nleveraging semantic knowledge transfer from known to unknown categories. This\nknowledge, typically encapsulated in attribute descriptions, aids in\nidentifying class-specific visual features, thus facilitating visual-semantic\nalignment and improving ZSL performance. However, real-world challenges such as\ndistribution imbalances and attribute co-occurrence among instances often\nhinder the discernment of local variances in images, a problem exacerbated by\nthe scarcity of fine-grained, region-specific attribute annotations. Moreover,\nthe variability in visual presentation within categories can also skew\nattribute-category associations. In response, we propose a bidirectional\ncross-modal ZSL approach CREST. It begins by extracting representations for\nattribute and visual localization and employs Evidential Deep Learning (EDL) to\nmeasure underlying epistemic uncertainty, thereby enhancing the model's\nresilience against hard negatives. CREST incorporates dual learning pathways,\nfocusing on both visual-category and attribute-category alignments, to ensure\nrobust correlation between latent and observable spaces. Moreover, we introduce\nan uncertainty-informed cross-modal fusion technique to refine visual-attribute\ninference. Extensive experiments demonstrate our model's effectiveness and\nunique explainability across multiple datasets. Our code and data are available\nat: Comments: Ongoing work; 10 pages, 2 Tables, 9 Figures; Repo is available at\nhttps://github.com/JethroJames/CREST.\n","authors":["Haojian Huang","Xiaozhen Qiao","Zhuo Chen","Haodong Chen","Bingyu Li","Zhe Sun","Mulin Chen","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2404.09640v1.pdf","comment":"Ongoing work; 10 pages, 2 Tables, 9 Figures; Repo is available at\n  https://github.com/JethroJames/CREST"},{"id":"http://arxiv.org/abs/2404.05468v2","updated":"2024-04-15T10:13:25Z","published":"2024-04-08T12:46:39Z","title":"Mind-to-Image: Projecting Visual Mental Imagination of the Brain from\n  fMRI","summary":"  The reconstruction of images observed by subjects from fMRI data collected\nduring visual stimuli has made significant strides in the past decade, thanks\nto the availability of extensive fMRI datasets and advancements in generative\nmodels for image generation. However, the application of visual reconstruction\nhas remained limited. Reconstructing visual imagination presents a greater\nchallenge, with potentially revolutionary applications ranging from aiding\nindividuals with disabilities to verifying witness accounts in court. The\nprimary hurdles in this field are the absence of data collection protocols for\nvisual imagery and the lack of datasets on the subject. Traditionally,\nfMRI-to-image relies on data collected from subjects exposed to visual stimuli,\nwhich poses issues for generating visual imagery based on the difference of\nbrain activity between visual stimulation and visual imagery. For the first\ntime, we have compiled a substantial dataset (around 6h of scans) on visual\nimagery along with a proposed data collection protocol. We then train a\nmodified version of an fMRI-to-image model and demonstrate the feasibility of\nreconstructing images from two modes of imagination: from memory and from pure\nimagination. This marks an important step towards creating a technology that\nallow direct reconstruction of visual imagery.\n","authors":["Hugo Caselles-Dupr√©","Charles Mellerio","Paul H√©rent","Aliz√©e Lopez-Persem","Benoit B√©ranger","Mathieu Soularue","Pierre Fautrel","Gauthier Vernier","Matthieu Cord"],"pdf_url":"https://arxiv.org/pdf/2404.05468v2.pdf","comment":"Pre-print to be updated"},{"id":"http://arxiv.org/abs/2403.16092v2","updated":"2024-04-15T10:06:41Z","published":"2024-03-24T11:09:41Z","title":"Are NeRFs ready for autonomous driving? Towards closing the\n  real-to-simulation gap","summary":"  Neural Radiance Fields (NeRFs) have emerged as promising tools for advancing\nautonomous driving (AD) research, offering scalable closed-loop simulation and\ndata augmentation capabilities. However, to trust the results achieved in\nsimulation, one needs to ensure that AD systems perceive real and rendered data\nin the same way. Although the performance of rendering methods is increasing,\nmany scenarios will remain inherently challenging to reconstruct faithfully. To\nthis end, we propose a novel perspective for addressing the real-to-simulated\ndata gap. Rather than solely focusing on improving rendering fidelity, we\nexplore simple yet effective methods to enhance perception model robustness to\nNeRF artifacts without compromising performance on real data. Moreover, we\nconduct the first large-scale investigation into the real-to-simulated data gap\nin an AD setting using a state-of-the-art neural rendering technique.\nSpecifically, we evaluate object detectors and an online mapping model on real\nand simulated data, and study the effects of different fine-tuning\nstrategies.Our results show notable improvements in model robustness to\nsimulated data, even improving real-world performance in some cases. Last, we\ndelve into the correlation between the real-to-simulated gap and image\nreconstruction metrics, identifying FID and LPIPS as strong indicators. See\nhttps://research.zenseact.com/publications/closing-real2sim-gap for our project\npage.\n","authors":["Carl Lindstr√∂m","Georg Hess","Adam Lilja","Maryam Fatemi","Lars Hammarstrand","Christoffer Petersson","Lennart Svensson"],"pdf_url":"https://arxiv.org/pdf/2403.16092v2.pdf","comment":"Accepted at Workshop on Autonomous Driving, CVPR 2024"},{"id":"http://arxiv.org/abs/2312.02244v3","updated":"2024-04-15T10:06:19Z","published":"2023-12-04T12:30:07Z","title":"Geometrically-driven Aggregation for Zero-shot 3D Point Cloud\n  Understanding","summary":"  Zero-shot 3D point cloud understanding can be achieved via 2D Vision-Language\nModels (VLMs). Existing strategies directly map Vision-Language Models from 2D\npixels of rendered or captured views to 3D points, overlooking the inherent and\nexpressible point cloud geometric structure. Geometrically similar or close\nregions can be exploited for bolstering point cloud understanding as they are\nlikely to share semantic information. To this end, we introduce the first\ntraining-free aggregation technique that leverages the point cloud's 3D\ngeometric structure to improve the quality of the transferred Vision-Language\nModels. Our approach operates iteratively, performing local-to-global\naggregation based on geometric and semantic point-level reasoning. We benchmark\nour approach on three downstream tasks, including classification, part\nsegmentation, and semantic segmentation, with a variety of datasets\nrepresenting both synthetic/real-world, and indoor/outdoor scenarios. Our\napproach achieves new state-of-the-art results in all benchmarks. Our approach\noperates iteratively, performing local-to-global aggregation based on geometric\nand semantic point-level reasoning. Code and dataset are available at\nhttps://luigiriz.github.io/geoze-website/\n","authors":["Guofeng Mei","Luigi Riz","Yiming Wang","Fabio Poiesi"],"pdf_url":"https://arxiv.org/pdf/2312.02244v3.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2404.09633v1","updated":"2024-04-15T10:05:36Z","published":"2024-04-15T10:05:36Z","title":"In-Context Translation: Towards Unifying Image Recognition, Processing,\n  and Generation","summary":"  We propose In-Context Translation (ICT), a general learning framework to\nunify visual recognition (e.g., semantic segmentation), low-level image\nprocessing (e.g., denoising), and conditional image generation (e.g.,\nedge-to-image synthesis). Thanks to unification, ICT significantly reduces the\ninherent inductive bias that comes with designing models for specific tasks,\nand it maximizes mutual enhancement across similar tasks. However, the\nunification across a large number of tasks is non-trivial due to various data\nformats and training pipelines. To this end, ICT introduces two designs.\nFirstly, it standardizes input-output data of different tasks into RGB image\npairs, e.g., semantic segmentation data pairs an RGB image with its\nsegmentation mask in the same RGB format. This turns different tasks into a\ngeneral translation task between two RGB images. Secondly, it standardizes the\ntraining of different tasks into a general in-context learning, where\n\"in-context\" means the input comprises an example input-output pair of the\ntarget task and a query image. The learning objective is to generate the\n\"missing\" data paired with the query. The implicit translation process is thus\nbetween the query and the generated image. In experiments, ICT unifies ten\nvision tasks and showcases impressive performance on their respective\nbenchmarks. Notably, compared to its competitors, e.g., Painter and\nPromptDiffusion, ICT trained on only 4 RTX 3090 GPUs is shown to be more\nefficient and less costly in training.\n","authors":["Han Xue","Qianru Sun","Li Song","Wenjun Zhang","Zhiwu Huang"],"pdf_url":"https://arxiv.org/pdf/2404.09633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09632v1","updated":"2024-04-15T10:04:15Z","published":"2024-04-15T10:04:15Z","title":"Bridging Vision and Language Spaces with Assignment Prediction","summary":"  This paper introduces VLAP, a novel approach that bridges pretrained vision\nmodels and large language models (LLMs) to make frozen LLMs understand the\nvisual world. VLAP transforms the embedding space of pretrained vision models\ninto the LLMs' word embedding space using a single linear layer for efficient\nand general-purpose visual and language understanding. Specifically, we harness\nwell-established word embeddings to bridge two modality embedding spaces. The\nvisual and text representations are simultaneously assigned to a set of word\nembeddings within pretrained LLMs by formulating the assigning procedure as an\noptimal transport problem. We predict the assignment of one modality from the\nrepresentation of another modality data, enforcing consistent assignments for\npaired multimodal data. This allows vision and language representations to\ncontain the same information, grounding the frozen LLMs' word embedding space\nin visual data. Moreover, a robust semantic taxonomy of LLMs can be preserved\nwith visual data since the LLMs interpret and reason linguistic information\nfrom correlations between word embeddings. Experimental results show that VLAP\nachieves substantial improvements over the previous linear transformation-based\napproaches across a range of vision-language tasks, including image captioning,\nvisual question answering, and cross-modal retrieval. We also demonstrate the\nlearned visual representations hold a semantic taxonomy of LLMs, making visual\nsemantic arithmetic possible.\n","authors":["Jungin Park","Jiyoung Lee","Kwanghoon Sohn"],"pdf_url":"https://arxiv.org/pdf/2404.09632v1.pdf","comment":"ICLR 2024 Camera-ready"},{"id":"http://arxiv.org/abs/2404.09624v1","updated":"2024-04-15T09:56:20Z","published":"2024-04-15T09:56:20Z","title":"AesExpert: Towards Multi-modality Foundation Model for Image Aesthetics\n  Perception","summary":"  The highly abstract nature of image aesthetics perception (IAP) poses\nsignificant challenge for current multimodal large language models (MLLMs). The\nlack of human-annotated multi-modality aesthetic data further exacerbates this\ndilemma, resulting in MLLMs falling short of aesthetics perception\ncapabilities. To address the above challenge, we first introduce a\ncomprehensively annotated Aesthetic Multi-Modality Instruction Tuning (AesMMIT)\ndataset, which serves as the footstone for building multi-modality aesthetics\nfoundation models. Specifically, to align MLLMs with human aesthetics\nperception, we construct a corpus-rich aesthetic critique database with 21,904\ndiverse-sourced images and 88K human natural language feedbacks, which are\ncollected via progressive questions, ranging from coarse-grained aesthetic\ngrades to fine-grained aesthetic descriptions. To ensure that MLLMs can handle\ndiverse queries, we further prompt GPT to refine the aesthetic critiques and\nassemble the large-scale aesthetic instruction tuning dataset, i.e. AesMMIT,\nwhich consists of 409K multi-typed instructions to activate stronger aesthetic\ncapabilities. Based on the AesMMIT database, we fine-tune the open-sourced\ngeneral foundation models, achieving multi-modality Aesthetic Expert models,\ndubbed AesExpert. Extensive experiments demonstrate that the proposed AesExpert\nmodels deliver significantly better aesthetic perception performances than the\nstate-of-the-art MLLMs, including the most advanced GPT-4V and\nGemini-Pro-Vision. Source data will be available at\nhttps://github.com/yipoh/AesExpert.\n","authors":["Yipo Huang","Xiangfei Sheng","Zhichao Yang","Quan Yuan","Zhichao Duan","Pengfei Chen","Leida Li","Weisi Lin","Guangming Shi"],"pdf_url":"https://arxiv.org/pdf/2404.09624v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03778v3","updated":"2024-04-15T09:55:50Z","published":"2024-04-04T19:50:57Z","title":"Flattening the Parent Bias: Hierarchical Semantic Segmentation in the\n  Poincar√© Ball","summary":"  Hierarchy is a natural representation of semantic taxonomies, including the\nones routinely used in image segmentation. Indeed, recent work on semantic\nsegmentation reports improved accuracy from supervised training leveraging\nhierarchical label structures. Encouraged by these results, we revisit the\nfundamental assumptions behind that work. We postulate and then empirically\nverify that the reasons for the observed improvement in segmentation accuracy\nmay be entirely unrelated to the use of the semantic hierarchy. To demonstrate\nthis, we design a range of cross-domain experiments with a representative\nhierarchical approach. We find that on the new testing domains, a flat\n(non-hierarchical) segmentation network, in which the parents are inferred from\nthe children, has superior segmentation accuracy to the hierarchical approach\nacross the board. Complementing these findings and inspired by the intrinsic\nproperties of hyperbolic spaces, we study a more principled approach to\nhierarchical segmentation using the Poincar\\'e ball model. The hyperbolic\nrepresentation largely outperforms the previous (Euclidean) hierarchical\napproach as well and is on par with our flat Euclidean baseline in terms of\nsegmentation accuracy. However, it additionally exhibits surprisingly strong\ncalibration quality of the parent nodes in the semantic hierarchy, especially\non the more challenging domains. Our combined analysis suggests that the\nestablished practice of hierarchical segmentation may be limited to in-domain\nsettings, whereas flat classifiers generalize substantially better, especially\nif they are modeled in the hyperbolic space.\n","authors":["Simon Weber","Barƒ±≈ü Z√∂ng√ºr","Nikita Araslanov","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2404.03778v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08339v2","updated":"2024-04-15T09:51:15Z","published":"2023-10-12T13:57:32Z","title":"TTK is Getting MPI-Ready","summary":"  This system paper documents the technical foundations for the extension of\nthe Topology ToolKit (TTK) to distributed-memory parallelism with the Message\nPassing Interface (MPI). While several recent papers introduced topology-based\napproaches for distributed-memory environments, these were reporting\nexperiments obtained with tailored, mono-algorithm implementations. In\ncontrast, we describe in this paper a versatile approach (supporting both\ntriangulated domains and regular grids) for the support of topological analysis\npipelines, i.e. a sequence of topological algorithms interacting together.\nWhile developing this extension, we faced several algorithmic and software\nengineering challenges, which we document in this paper. We describe an MPI\nextension of TTK's data structure for triangulation representation and\ntraversal, a central component to the global performance and generality of\nTTK's topological implementations. We also introduce an intermediate interface\nbetween TTK and MPI, both at the global pipeline level, and at the fine-grain\nalgorithmic level. We provide a taxonomy for the distributed-memory topological\nalgorithms supported by TTK, depending on their communication needs and provide\nexamples of hybrid MPI+thread parallelizations. Performance analyses show that\nparallel efficiencies range from 20% to 80% (depending on the algorithms), and\nthat the MPI-specific preconditioning introduced by our framework induces a\nnegligible computation time overhead. We illustrate the new distributed-memory\ncapabilities of TTK with an example of advanced analysis pipeline, combining\nmultiple algorithms, run on the largest publicly available dataset we have\nfound (120 billion vertices) on a cluster with 64 nodes (for a total of 1536\ncores). Finally, we provide a roadmap for the completion of TTK's MPI\nextension, along with generic recommendations for each algorithm communication\ncategory.\n","authors":["Eve Le Guillou","Michael Will","Pierre Guillou","Jonas Lukasczyk","Pierre Fortin","Christoph Garth","Julien Tierny"],"pdf_url":"https://arxiv.org/pdf/2310.08339v2.pdf","comment":"18 pages, 13 figures"},{"id":"http://arxiv.org/abs/2404.09619v1","updated":"2024-04-15T09:47:48Z","published":"2024-04-15T09:47:48Z","title":"UNIAA: A Unified Multi-modal Image Aesthetic Assessment Baseline and\n  Benchmark","summary":"  As an alternative to expensive expert evaluation, Image Aesthetic Assessment\n(IAA) stands out as a crucial task in computer vision. However, traditional IAA\nmethods are typically constrained to a single data source or task, restricting\nthe universality and broader application. In this work, to better align with\nhuman aesthetics, we propose a Unified Multi-modal Image Aesthetic Assessment\n(UNIAA) framework, including a Multi-modal Large Language Model (MLLM) named\nUNIAA-LLaVA and a comprehensive benchmark named UNIAA-Bench. We choose MLLMs\nwith both visual perception and language ability for IAA and establish a\nlow-cost paradigm for transforming the existing datasets into unified and\nhigh-quality visual instruction tuning data, from which the UNIAA-LLaVA is\ntrained. To further evaluate the IAA capability of MLLMs, we construct the\nUNIAA-Bench, which consists of three aesthetic levels: Perception, Description,\nand Assessment. Extensive experiments validate the effectiveness and\nrationality of UNIAA. UNIAA-LLaVA achieves competitive performance on all\nlevels of UNIAA-Bench, compared with existing MLLMs. Specifically, our model\nperforms better than GPT-4V in aesthetic perception and even approaches the\njunior-level human. We find MLLMs have great potential in IAA, yet there\nremains plenty of room for further improvement. The UNIAA-LLaVA and UNIAA-Bench\nwill be released.\n","authors":["Zhaokun Zhou","Qiulin Wang","Bin Lin","Yiwei Su","Rui Chen","Xin Tao","Amin Zheng","Li Yuan","Pengfei Wan","Di Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.09619v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09616v1","updated":"2024-04-15T09:40:44Z","published":"2024-04-15T09:40:44Z","title":"A Review and Efficient Implementation of Scene Graph Generation Metrics","summary":"  Scene graph generation has emerged as a prominent research field in computer\nvision, witnessing significant advancements in the recent years. However,\ndespite these strides, precise and thorough definitions for the metrics used to\nevaluate scene graph generation models are lacking. In this paper, we address\nthis gap in the literature by providing a review and precise definition of\ncommonly used metrics in scene graph generation. Our comprehensive examination\nclarifies the underlying principles of these metrics and can serve as a\nreference or introduction to scene graph metrics.\n  Furthermore, to facilitate the usage of these metrics, we introduce a\nstandalone Python package called SGBench that efficiently implements all\ndefined metrics, ensuring their accessibility to the research community.\nAdditionally, we present a scene graph benchmarking web service, that enables\nresearchers to compare scene graph generation methods and increase visibility\nof new methods in a central place.\n  All of our code can be found at https://lorjul.github.io/sgbench/.\n","authors":["Julian Lorenz","Robin Sch√∂n","Katja Ludwig","Rainer Lienhart"],"pdf_url":"https://arxiv.org/pdf/2404.09616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.11369v2","updated":"2024-04-15T09:33:21Z","published":"2023-06-20T08:19:51Z","title":"CrossKD: Cross-Head Knowledge Distillation for Object Detection","summary":"  Knowledge Distillation (KD) has been validated as an effective model\ncompression technique for learning compact object detectors. Existing\nstate-of-the-art KD methods for object detection are mostly based on feature\nimitation. In this paper, we present a general and effective prediction\nmimicking distillation scheme, called CrossKD, which delivers the intermediate\nfeatures of the student's detection head to the teacher's detection head. The\nresulting cross-head predictions are then forced to mimic the teacher's\npredictions. This manner relieves the student's head from receiving\ncontradictory supervision signals from the annotations and the teacher's\npredictions, greatly improving the student's detection performance. Moreover,\nas mimicking the teacher's predictions is the target of KD, CrossKD offers more\ntask-oriented information in contrast with feature imitation. On MS COCO, with\nonly prediction mimicking losses applied, our CrossKD boosts the average\nprecision of GFL ResNet-50 with 1x training schedule from 40.2 to 43.7,\noutperforming all existing KD methods. In addition, our method also works well\nwhen distilling detectors with heterogeneous backbones. Code is available at\nhttps://github.com/jbwang1997/CrossKD.\n","authors":["Jiabao Wang","Yuming Chen","Zhaohui Zheng","Xiang Li","Ming-Ming Cheng","Qibin Hou"],"pdf_url":"https://arxiv.org/pdf/2306.11369v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17648v3","updated":"2024-04-15T09:31:17Z","published":"2023-05-28T06:44:33Z","title":"Z-GMOT: Zero-shot Generic Multiple Object Tracking","summary":"  Despite recent significant progress, Multi-Object Tracking (MOT) faces\nlimitations such as reliance on prior knowledge and predefined categories and\nstruggles with unseen objects. To address these issues, Generic Multiple Object\nTracking (GMOT) has emerged as an alternative approach, requiring less prior\ninformation. However, current GMOT methods often rely on initial bounding boxes\nand struggle to handle variations in factors such as viewpoint, lighting,\nocclusion, and scale, among others. Our contributions commence with the\nintroduction of the \\textit{Referring GMOT dataset} a collection of videos,\neach accompanied by detailed textual descriptions of their attributes.\nSubsequently, we propose $\\mathtt{Z-GMOT}$, a cutting-edge tracking solution\ncapable of tracking objects from \\textit{never-seen categories} without the\nneed of initial bounding boxes or predefined categories. Within our\n$\\mathtt{Z-GMOT}$ framework, we introduce two novel components: (i)\n$\\mathtt{iGLIP}$, an improved Grounded language-image pretraining, for\naccurately detecting unseen objects with specific characteristics. (ii)\n$\\mathtt{MA-SORT}$, a novel object association approach that adeptly integrates\nmotion and appearance-based matching strategies to tackle the complex task of\ntracking objects with high similarity. Our contributions are benchmarked\nthrough extensive experiments conducted on the Referring GMOT dataset for GMOT\ntask. Additionally, to assess the generalizability of the proposed\n$\\mathtt{Z-GMOT}$, we conduct ablation studies on the DanceTrack and MOT20\ndatasets for the MOT task. Our dataset, code, and models are released at:\nhttps://fsoft-aic.github.io/Z-GMOT.\n","authors":["Kim Hoang Tran","Anh Duy Le Dinh","Tien Phat Nguyen","Thinh Phan","Pha Nguyen","Khoa Luu","Donald Adjeroh","Gianfranco Doretto","Ngan Hoang Le"],"pdf_url":"https://arxiv.org/pdf/2305.17648v3.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2307.03992v4","updated":"2024-04-15T09:19:01Z","published":"2023-07-08T14:59:41Z","title":"Stimulating the Diffusion Model for Image Denoising via Adaptive\n  Embedding and Ensembling","summary":"  Image denoising is a fundamental problem in computational photography, where\nachieving high perception with low distortion is highly demanding. Current\nmethods either struggle with perceptual quality or suffer from significant\ndistortion. Recently, the emerging diffusion model has achieved\nstate-of-the-art performance in various tasks and demonstrates great potential\nfor image denoising. However, stimulating diffusion models for image denoising\nis not straightforward and requires solving several critical problems. For one\nthing, the input inconsistency hinders the connection between diffusion models\nand image denoising. For another, the content inconsistency between the\ngenerated image and the desired denoised image introduces distortion. To tackle\nthese problems, we present a novel strategy called the Diffusion Model for\nImage Denoising (DMID) by understanding and rethinking the diffusion model from\na denoising perspective. Our DMID strategy includes an adaptive embedding\nmethod that embeds the noisy image into a pre-trained unconditional diffusion\nmodel and an adaptive ensembling method that reduces distortion in the denoised\nimage. Our DMID strategy achieves state-of-the-art performance on both\ndistortion-based and perception-based metrics, for both Gaussian and real-world\nimage denoising.The code is available at https://github.com/Li-Tong-621/DMID.\n","authors":["Tong Li","Hansen Feng","Lizhi Wang","Zhiwei Xiong","Hua Huang"],"pdf_url":"https://arxiv.org/pdf/2307.03992v4.pdf","comment":"18 pages,15 figures"},{"id":"http://arxiv.org/abs/2404.09601v1","updated":"2024-04-15T09:16:49Z","published":"2024-04-15T09:16:49Z","title":"Reactive Model Correction: Mitigating Harm to Task-Relevant Features via\n  Conditional Bias Suppression","summary":"  Deep Neural Networks are prone to learning and relying on spurious\ncorrelations in the training data, which, for high-risk applications, can have\nfatal consequences. Various approaches to suppress model reliance on harmful\nfeatures have been proposed that can be applied post-hoc without additional\ntraining. Whereas those methods can be applied with efficiency, they also tend\nto harm model performance by globally shifting the distribution of latent\nfeatures. To mitigate unintended overcorrection of model behavior, we propose a\nreactive approach conditioned on model-derived knowledge and eXplainable\nArtificial Intelligence (XAI) insights. While the reactive approach can be\napplied to many post-hoc methods, we demonstrate the incorporation of\nreactivity in particular for P-ClArC (Projective Class Artifact Compensation),\nintroducing a new method called R-ClArC (Reactive Class Artifact Compensation).\nThrough rigorous experiments in controlled settings (FunnyBirds) and with a\nreal-world dataset (ISIC2019), we show that introducing reactivity can minimize\nthe detrimental effect of the applied correction while simultaneously ensuring\nlow reliance on spurious features.\n","authors":["Dilyara Bareeva","Maximilian Dreyer","Frederik Pahde","Wojciech Samek","Sebastian Lapuschkin"],"pdf_url":"https://arxiv.org/pdf/2404.09601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11821v3","updated":"2024-04-15T09:10:56Z","published":"2024-03-18T14:24:20Z","title":"Evaluating Text-to-Image Synthesis: Survey and Taxonomy of Image Quality\n  Metrics","summary":"  Recent advances in text-to-image synthesis enabled through a combination of\nlanguage and vision foundation models have led to a proliferation of the tools\navailable and an increased attention to the field. When conducting\ntext-to-image synthesis, a central goal is to ensure that the content between\ntext and image is aligned. As such, there exist numerous evaluation metrics\nthat aim to mimic human judgement. However, it is often unclear which metric to\nuse for evaluating text-to-image synthesis systems as their evaluation is\nhighly nuanced. In this work, we provide a comprehensive overview of existing\ntext-to-image evaluation metrics. Based on our findings, we propose a new\ntaxonomy for categorizing these metrics. Our taxonomy is grounded in the\nassumption that there are two main quality criteria, namely compositionality\nand generality, which ideally map to human preferences. Ultimately, we derive\nguidelines for practitioners conducting text-to-image evaluation, discuss open\nchallenges of evaluation mechanisms, and surface limitations of current\nmetrics.\n","authors":["Sebastian Hartwig","Dominik Engel","Leon Sick","Hannah Kniesel","Tristan Payer","Poonam Poonam","Michael Gl√∂ckler","Alex B√§uerle","Timo Ropinski"],"pdf_url":"https://arxiv.org/pdf/2403.11821v3.pdf","comment":"preprint, 20 pages, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2404.09591v1","updated":"2024-04-15T09:01:47Z","published":"2024-04-15T09:01:47Z","title":"3D Gaussian Splatting as Markov Chain Monte Carlo","summary":"  While 3D Gaussian Splatting has recently become popular for neural rendering,\ncurrent methods rely on carefully engineered cloning and splitting strategies\nfor placing Gaussians, which does not always generalize and may lead to\npoor-quality renderings. In addition, for real-world scenes, they rely on a\ngood initial point cloud to perform well. In this work, we rethink 3D Gaussians\nas random samples drawn from an underlying probability distribution describing\nthe physical representation of the scene -- in other words, Markov Chain Monte\nCarlo (MCMC) samples. Under this view, we show that the 3D Gaussian updates are\nstrikingly similar to a Stochastic Langevin Gradient Descent (SGLD) update. As\nwith MCMC, samples are nothing but past visit locations, adding new Gaussians\nunder our framework can simply be realized without heuristics as placing\nGaussians at existing Gaussian locations. To encourage using fewer Gaussians\nfor efficiency, we introduce an L1-regularizer on the Gaussians. On various\nstandard evaluation scenes, we show that our method provides improved rendering\nquality, easy control over the number of Gaussians, and robustness to\ninitialization.\n","authors":["Shakiba Kheradmand","Daniel Rebain","Gopal Sharma","Weiwei Sun","Jeff Tseng","Hossam Isack","Abhishek Kar","Andrea Tagliasacchi","Kwang Moo Yi"],"pdf_url":"https://arxiv.org/pdf/2404.09591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09586v1","updated":"2024-04-15T08:54:33Z","published":"2024-04-15T08:54:33Z","title":"Mitigating the Curse of Dimensionality for Certified Robustness via Dual\n  Randomized Smoothing","summary":"  Randomized Smoothing (RS) has been proven a promising method for endowing an\narbitrary image classifier with certified robustness. However, the substantial\nuncertainty inherent in the high-dimensional isotropic Gaussian noise imposes\nthe curse of dimensionality on RS. Specifically, the upper bound of ${\\ell_2}$\ncertified robustness radius provided by RS exhibits a diminishing trend with\nthe expansion of the input dimension $d$, proportionally decreasing at a rate\nof $1/\\sqrt{d}$. This paper explores the feasibility of providing ${\\ell_2}$\ncertified robustness for high-dimensional input through the utilization of dual\nsmoothing in the lower-dimensional space. The proposed Dual Randomized\nSmoothing (DRS) down-samples the input image into two sub-images and smooths\nthe two sub-images in lower dimensions. Theoretically, we prove that DRS\nguarantees a tight ${\\ell_2}$ certified robustness radius for the original\ninput and reveal that DRS attains a superior upper bound on the ${\\ell_2}$\nrobustness radius, which decreases proportionally at a rate of $(1/\\sqrt m +\n1/\\sqrt n )$ with $m+n=d$. Extensive experiments demonstrate the\ngeneralizability and effectiveness of DRS, which exhibits a notable capability\nto integrate with established methodologies, yielding substantial improvements\nin both accuracy and ${\\ell_2}$ certified robustness baselines of RS on the\nCIFAR-10 and ImageNet datasets. Code is available at\nhttps://github.com/xiasong0501/DRS.\n","authors":["Song Xia","Yu Yi","Xudong Jiang","Henghui Ding"],"pdf_url":"https://arxiv.org/pdf/2404.09586v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09585v1","updated":"2024-04-15T08:52:51Z","published":"2024-04-15T08:52:51Z","title":"Pseudo-label Learning with Calibrated Confidence Using an Energy-based\n  Model","summary":"  In pseudo-labeling (PL), which is a type of semi-supervised learning,\npseudo-labels are assigned based on the confidence scores provided by the\nclassifier; therefore, accurate confidence is important for successful PL. In\nthis study, we propose a PL algorithm based on an energy-based model (EBM),\nwhich is referred to as the energy-based PL (EBPL). In EBPL, a neural\nnetwork-based classifier and an EBM are jointly trained by sharing their\nfeature extraction parts. This approach enables the model to learn both the\nclass decision boundary and input data distribution, enhancing confidence\ncalibration during network training. The experimental results demonstrate that\nEBPL outperforms the existing PL method in semi-supervised image classification\ntasks, with superior confidence calibration error and recognition accuracy.\n","authors":["Masahito Toba","Seiichi Uchida","Hideaki Hayashi"],"pdf_url":"https://arxiv.org/pdf/2404.09585v1.pdf","comment":"8 pages, 8 figures, Accepted at IJCNN 2024"},{"id":"http://arxiv.org/abs/2311.17955v2","updated":"2024-04-15T08:43:58Z","published":"2023-11-29T08:11:20Z","title":"PEAN: A Diffusion-Based Prior-Enhanced Attention Network for Scene Text\n  Image Super-Resolution","summary":"  Scene text image super-resolution (STISR) aims at simultaneously increasing\nthe resolution and readability of low-resolution scene text images, thus\nboosting the performance of the downstream recognition task. Two factors in\nscene text images, visual structure and semantic information, affect the\nrecognition performance significantly. To mitigate the effects from these\nfactors, this paper proposes a Prior-Enhanced Attention Network (PEAN).\nSpecifically, an attention-based modulation module is leveraged to understand\nscene text images by neatly perceiving the local and global dependence of\nimages, despite the shape of the text. Meanwhile, a diffusion-based module is\ndeveloped to enhance the text prior, hence offering better guidance for the SR\nnetwork to generate SR images with higher semantic accuracy. Additionally, a\nmulti-task learning paradigm is employed to optimize the network, enabling the\nmodel to generate legible SR images. As a result, PEAN establishes new SOTA\nresults on the TextZoom benchmark. Experiments are also conducted to analyze\nthe importance of the enhanced text prior as a means of improving the\nperformance of the SR network. Code will be made available at\nhttps://github.com/jdfxzzy/PEAN.\n","authors":["Zuoyan Zhao","Hui Xue","Pengfei Fang","Shipeng Zhu"],"pdf_url":"https://arxiv.org/pdf/2311.17955v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07662v3","updated":"2024-04-15T08:37:57Z","published":"2023-03-14T07:07:34Z","title":"Do More With What You Have: Transferring Depth-Scale from Labeled to\n  Unlabeled Domains","summary":"  Transferring the absolute depth prediction capabilities of an estimator to a\nnew domain is a task with significant real-world applications. This task is\nspecifically challenging when images from the new domain are collected without\nground-truth depth measurements, and possibly with sensors of different\nintrinsics. To overcome such limitations, a recent zero-shot solution was\ntrained on an extensive training dataset and encoded the various camera\nintrinsics. Other solutions generated synthetic data with depth labels that\nmatched the intrinsics of the new target data to enable depth-scale transfer\nbetween the domains.\n  In this work we present an alternative solution that can utilize any existing\nsynthetic or real dataset, that has a small number of images annotated with\nground truth depth labels. Specifically, we show that self-supervised depth\nestimators result in up-to-scale predictions that are linearly correlated to\ntheir absolute depth values across the domain, a property that we model in this\nwork using a single scalar. In addition, aligning the field-of-view of two\ndatasets prior to training, results in a common linear relationship for both\ndomains. We use this observed property to transfer the depth-scale from source\ndatasets that have absolute depth labels to new target datasets that lack these\nmeasurements, enabling absolute depth predictions in the target domain.\n  The suggested method was successfully demonstrated on the KITTI, DDAD and\nnuScenes datasets, while using other existing real or synthetic source\ndatasets, that have a different field-of-view, other image style or structural\ncontent, achieving comparable or better accuracy than other existing methods\nthat do not use target ground-truth depths.\n","authors":["Alexandra Dana","Nadav Carmel","Amit Shomer","Ofer Manela","Tomer Peleg"],"pdf_url":"https://arxiv.org/pdf/2303.07662v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09571v1","updated":"2024-04-15T08:32:41Z","published":"2024-04-15T08:32:41Z","title":"MTKD: Multi-Teacher Knowledge Distillation for Image Super-Resolution","summary":"  Knowledge distillation (KD) has emerged as a promising technique in deep\nlearning, typically employed to enhance a compact student network through\nlearning from their high-performance but more complex teacher variant. When\napplied in the context of image super-resolution, most KD approaches are\nmodified versions of methods developed for other computer vision tasks, which\nare based on training strategies with a single teacher and simple loss\nfunctions. In this paper, we propose a novel Multi-Teacher Knowledge\nDistillation (MTKD) framework specifically for image super-resolution. It\nexploits the advantages of multiple teachers by combining and enhancing the\noutputs of these teacher models, which then guides the learning process of the\ncompact student network. To achieve more effective learning performance, we\nhave also developed a new wavelet-based loss function for MTKD, which can\nbetter optimize the training process by observing differences in both the\nspatial and frequency domains. We fully evaluate the effectiveness of the\nproposed method by comparing it to five commonly used KD methods for image\nsuper-resolution based on three popular network architectures. The results show\nthat the proposed MTKD method achieves evident improvements in super-resolution\nperformance, up to 0.46dB (based on PSNR), over state-of-the-art KD approaches\nacross different network structures. The source code of MTKD will be made\navailable here for public evaluation.\n","authors":["Yuxuan Jiang","Chen Feng","Fan Zhang","David Bull"],"pdf_url":"https://arxiv.org/pdf/2404.09571v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09570v1","updated":"2024-04-15T08:32:18Z","published":"2024-04-15T08:32:18Z","title":"The revenge of BiSeNet: Efficient Multi-Task Image Segmentation","summary":"  Recent advancements in image segmentation have focused on enhancing the\nefficiency of the models to meet the demands of real-time applications,\nespecially on edge devices. However, existing research has primarily\nconcentrated on single-task settings, especially on semantic segmentation,\nleading to redundant efforts and specialized architectures for different tasks.\nTo address this limitation, we propose a novel architecture for efficient\nmulti-task image segmentation, capable of handling various segmentation tasks\nwithout sacrificing efficiency or accuracy. We introduce BiSeNetFormer, that\nleverages the efficiency of two-stream semantic segmentation architectures and\nit extends them into a mask classification framework. Our approach maintains\nthe efficient spatial and context paths to capture detailed and semantic\ninformation, respectively, while leveraging an efficient transformed-based\nsegmentation head that computes the binary masks and class probabilities. By\nseamlessly supporting multiple tasks, namely semantic and panoptic\nsegmentation, BiSeNetFormer offers a versatile solution for multi-task\nsegmentation. We evaluate our approach on popular datasets, Cityscapes and\nADE20K, demonstrating impressive inference speeds while maintaining competitive\naccuracy compared to state-of-the-art architectures. Our results indicate that\nBiSeNetFormer represents a significant advancement towards fast, efficient, and\nmulti-task segmentation networks, bridging the gap between model efficiency and\ntask adaptability.\n","authors":["Gabriele Rosi","Claudia Cuttano","Niccol√≤ Cavagnero","Giuseppe Averta","Fabio Cermelli"],"pdf_url":"https://arxiv.org/pdf/2404.09570v1.pdf","comment":"Accepted to ECV workshop at CVPR2024"},{"id":"http://arxiv.org/abs/2212.14855v3","updated":"2024-04-15T08:24:42Z","published":"2022-12-30T18:04:25Z","title":"Disentangled Explanations of Neural Network Predictions by Finding\n  Relevant Subspaces","summary":"  Explainable AI aims to overcome the black-box nature of complex ML models\nlike neural networks by generating explanations for their predictions.\nExplanations often take the form of a heatmap identifying input features (e.g.\npixels) that are relevant to the model's decision. These explanations, however,\nentangle the potentially multiple factors that enter into the overall complex\ndecision strategy. We propose to disentangle explanations by extracting at some\nintermediate layer of a neural network, subspaces that capture the multiple and\ndistinct activation patterns (e.g. visual concepts) that are relevant to the\nprediction. To automatically extract these subspaces, we propose two new\nanalyses, extending principles found in PCA or ICA to explanations. These novel\nanalyses, which we call principal relevant component analysis (PRCA) and\ndisentangled relevant subspace analysis (DRSA), maximize relevance instead of\ne.g. variance or kurtosis. This allows for a much stronger focus of the\nanalysis on what the ML model actually uses for predicting, ignoring\nactivations or concepts to which the model is invariant. Our approach is\ngeneral enough to work alongside common attribution techniques such as Shapley\nValue, Integrated Gradients, or LRP. Our proposed methods show to be\npractically useful and compare favorably to the state of the art as\ndemonstrated on benchmarks and three use cases.\n","authors":["Pattarawat Chormai","Jan Herrmann","Klaus-Robert M√ºller","Gr√©goire Montavon"],"pdf_url":"https://arxiv.org/pdf/2212.14855v3.pdf","comment":"17 pages + supplement"},{"id":"http://arxiv.org/abs/2303.14017v3","updated":"2024-04-15T08:22:49Z","published":"2023-03-24T14:18:40Z","title":"CF-Font: Content Fusion for Few-shot Font Generation","summary":"  Content and style disentanglement is an effective way to achieve few-shot\nfont generation. It allows to transfer the style of the font image in a source\ndomain to the style defined with a few reference images in a target domain.\nHowever, the content feature extracted using a representative font might not be\noptimal. In light of this, we propose a content fusion module (CFM) to project\nthe content feature into a linear space defined by the content features of\nbasis fonts, which can take the variation of content features caused by\ndifferent fonts into consideration. Our method also allows to optimize the\nstyle representation vector of reference images through a lightweight iterative\nstyle-vector refinement (ISR) strategy. Moreover, we treat the 1D projection of\na character image as a probability distribution and leverage the distance\nbetween two distributions as the reconstruction loss (namely projected\ncharacter loss, PCL). Compared to L2 or L1 reconstruction loss, the\ndistribution distance pays more attention to the global shape of characters. We\nhave evaluated our method on a dataset of 300 fonts with 6.5k characters each.\nExperimental results verify that our method outperforms existing\nstate-of-the-art few-shot font generation methods by a large margin. The source\ncode can be found at https://github.com/wangchi95/CF-Font.\n","authors":["Chi Wang","Min Zhou","Tiezheng Ge","Yuning Jiang","Hujun Bao","Weiwei Xu"],"pdf_url":"https://arxiv.org/pdf/2303.14017v3.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2404.09556v1","updated":"2024-04-15T08:19:08Z","published":"2024-04-15T08:19:08Z","title":"nnU-Net Revisited: A Call for Rigorous Validation in 3D Medical Image\n  Segmentation","summary":"  The release of nnU-Net marked a paradigm shift in 3D medical image\nsegmentation, demonstrating that a properly configured U-Net architecture could\nstill achieve state-of-the-art results. Despite this, the pursuit of novel\narchitectures, and the respective claims of superior performance over the U-Net\nbaseline, continued. In this study, we demonstrate that many of these recent\nclaims fail to hold up when scrutinized for common validation shortcomings,\nsuch as the use of inadequate baselines, insufficient datasets, and neglected\ncomputational resources. By meticulously avoiding these pitfalls, we conduct a\nthorough and comprehensive benchmarking of current segmentation methods\nincluding CNN-based, Transformer-based, and Mamba-based approaches. In contrast\nto current beliefs, we find that the recipe for state-of-the-art performance is\n1) employing CNN-based U-Net models, including ResNet and ConvNeXt variants, 2)\nusing the nnU-Net framework, and 3) scaling models to modern hardware\nresources. These results indicate an ongoing innovation bias towards novel\narchitectures in the field and underscore the need for more stringent\nvalidation standards in the quest for scientific progress.\n","authors":["Fabian Isensee","Tassilo Wald","Constantin Ulrich","Michael Baumgartner","Saikat Roy","Klaus Maier-Hein","Paul F. Jaeger"],"pdf_url":"https://arxiv.org/pdf/2404.09556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09555v1","updated":"2024-04-15T08:18:38Z","published":"2024-04-15T08:18:38Z","title":"AI-KD: Towards Alignment Invariant Face Image Quality Assessment Using\n  Knowledge Distillation","summary":"  Face Image Quality Assessment (FIQA) techniques have seen steady improvements\nover recent years, but their performance still deteriorates if the input face\nsamples are not properly aligned. This alignment sensitivity comes from the\nfact that most FIQA techniques are trained or designed using a specific face\nalignment procedure. If the alignment technique changes, the performance of\nmost existing FIQA techniques quickly becomes suboptimal. To address this\nproblem, we present in this paper a novel knowledge distillation approach,\ntermed AI-KD that can extend on any existing FIQA technique, improving its\nrobustness to alignment variations and, in turn, performance with different\nalignment procedures. To validate the proposed distillation approach, we\nconduct comprehensive experiments on 6 face datasets with 4 recent face\nrecognition models and in comparison to 7 state-of-the-art FIQA techniques. Our\nresults show that AI-KD consistently improves performance of the initial FIQA\ntechniques not only with misaligned samples, but also with properly aligned\nfacial images. Furthermore, it leads to a new state-of-the-art, when used with\na competitive initial FIQA approach. The code for AI-KD is made publicly\navailable from: https://github.com/LSIbabnikz/AI-KD.\n","authors":["≈Ωiga Babnik","Fadi Boutros","Naser Damer","Peter Peer","Vitomir ≈†truc"],"pdf_url":"https://arxiv.org/pdf/2404.09555v1.pdf","comment":"IEEE International Workshop on Biometrics and Forensics (IWBF) 2024,\n  pp. 6"},{"id":"http://arxiv.org/abs/2404.09540v1","updated":"2024-04-15T08:04:44Z","published":"2024-04-15T08:04:44Z","title":"Text-Driven Diverse Facial Texture Generation via Progressive\n  Latent-Space Refinement","summary":"  Automatic 3D facial texture generation has gained significant interest\nrecently. Existing approaches may not support the traditional physically based\nrendering pipeline or rely on 3D data captured by Light Stage. Our key\ncontribution is a progressive latent space refinement approach that can\nbootstrap from 3D Morphable Models (3DMMs)-based texture maps generated from\nfacial images to generate high-quality and diverse PBR textures, including\nalbedo, normal, and roughness. It starts with enhancing Generative Adversarial\nNetworks (GANs) for text-guided and diverse texture generation. To this end, we\ndesign a self-supervised paradigm to overcome the reliance on ground truth 3D\ntextures and train the generative model with only entangled texture maps.\nBesides, we foster mutual enhancement between GANs and Score Distillation\nSampling (SDS). SDS boosts GANs with more generative modes, while GANs promote\nmore efficient optimization of SDS. Furthermore, we introduce an edge-aware SDS\nfor multi-view consistent facial structure. Experiments demonstrate that our\nmethod outperforms existing 3D texture generation methods regarding\nphoto-realistic quality, diversity, and efficiency.\n","authors":["Chi Wang","Junming Huang","Rong Zhang","Qi Wang","Haotian Yang","Haibin Huang","Chongyang Ma","Weiwei Xu"],"pdf_url":"https://arxiv.org/pdf/2404.09540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05941v2","updated":"2024-04-15T07:59:37Z","published":"2023-12-10T17:07:37Z","title":"ASH: Animatable Gaussian Splats for Efficient and Photoreal Human\n  Rendering","summary":"  Real-time rendering of photorealistic and controllable human avatars stands\nas a cornerstone in Computer Vision and Graphics. While recent advances in\nneural implicit rendering have unlocked unprecedented photorealism for digital\navatars, real-time performance has mostly been demonstrated for static scenes\nonly. To address this, we propose ASH, an animatable Gaussian splatting\napproach for photorealistic rendering of dynamic humans in real-time. We\nparameterize the clothed human as animatable 3D Gaussians, which can be\nefficiently splatted into image space to generate the final rendering. However,\nnaively learning the Gaussian parameters in 3D space poses a severe challenge\nin terms of compute. Instead, we attach the Gaussians onto a deformable\ncharacter model, and learn their parameters in 2D texture space, which allows\nleveraging efficient 2D convolutional architectures that easily scale with the\nrequired number of Gaussians. We benchmark ASH with competing methods on\npose-controllable avatars, demonstrating that our method outperforms existing\nreal-time methods by a large margin and shows comparable or even better results\nthan offline methods.\n","authors":["Haokai Pang","Heming Zhu","Adam Kortylewski","Christian Theobalt","Marc Habermann"],"pdf_url":"https://arxiv.org/pdf/2312.05941v2.pdf","comment":"For project page, see https://vcai.mpi-inf.mpg.de/projects/ash/"},{"id":"http://arxiv.org/abs/2401.03522v2","updated":"2024-04-15T07:59:03Z","published":"2024-01-07T15:47:19Z","title":"Text-Driven Traffic Anomaly Detection with Temporal High-Frequency\n  Modeling in Driving Videos","summary":"  Traffic anomaly detection (TAD) in driving videos is critical for ensuring\nthe safety of autonomous driving and advanced driver assistance systems.\nPrevious single-stage TAD methods primarily rely on frame prediction, making\nthem vulnerable to interference from dynamic backgrounds induced by the rapid\nmovement of the dashboard camera. While two-stage TAD methods appear to be a\nnatural solution to mitigate such interference by pre-extracting\nbackground-independent features (such as bounding boxes and optical flow) using\nperceptual algorithms, they are susceptible to the performance of first-stage\nperceptual algorithms and may result in error propagation. In this paper, we\nintroduce TTHF, a novel single-stage method aligning video clips with text\nprompts, offering a new perspective on traffic anomaly detection. Unlike\nprevious approaches, the supervised signal of our method is derived from\nlanguages rather than orthogonal one-hot vectors, providing a more\ncomprehensive representation. Further, concerning visual representation, we\npropose to model the high frequency of driving videos in the temporal domain.\nThis modeling captures the dynamic changes of driving scenes, enhances the\nperception of driving behavior, and significantly improves the detection of\ntraffic anomalies. In addition, to better perceive various types of traffic\nanomalies, we carefully design an attentive anomaly focusing mechanism that\nvisually and linguistically guides the model to adaptively focus on the visual\ncontext of interest, thereby facilitating the detection of traffic anomalies.\nIt is shown that our proposed TTHF achieves promising performance,\noutperforming state-of-the-art competitors by +5.4% AUC on the DoTA dataset and\nachieving high generalization on the DADA dataset.\n","authors":["Rongqin Liang","Yuanman Li","Jiantao Zhou","Xia Li"],"pdf_url":"https://arxiv.org/pdf/2401.03522v2.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2404.09533v1","updated":"2024-04-15T07:53:07Z","published":"2024-04-15T07:53:07Z","title":"WiTUnet: A U-Shaped Architecture Integrating CNN and Transformer for\n  Improved Feature Alignment and Local Information Fusion","summary":"  Low-dose computed tomography (LDCT) has become the technology of choice for\ndiagnostic medical imaging, given its lower radiation dose compared to standard\nCT, despite increasing image noise and potentially affecting diagnostic\naccuracy. To address this, advanced deep learning-based LDCT denoising\nalgorithms have been developed, primarily using Convolutional Neural Networks\n(CNNs) or Transformer Networks with the Unet architecture. This architecture\nenhances image detail by integrating feature maps from the encoder and decoder\nvia skip connections. However, current methods often overlook enhancements to\nthe Unet architecture itself, focusing instead on optimizing encoder and\ndecoder structures. This approach can be problematic due to the significant\ndifferences in feature map characteristics between the encoder and decoder,\nwhere simple fusion strategies may not effectively reconstruct images.In this\npaper, we introduce WiTUnet, a novel LDCT image denoising method that utilizes\nnested, dense skip pathways instead of traditional skip connections to improve\nfeature integration. WiTUnet also incorporates a windowed Transformer structure\nto process images in smaller, non-overlapping segments, reducing computational\nload. Additionally, the integration of a Local Image Perception Enhancement\n(LiPe) module in both the encoder and decoder replaces the standard multi-layer\nperceptron (MLP) in Transformers, enhancing local feature capture and\nrepresentation. Through extensive experimental comparisons, WiTUnet has\ndemonstrated superior performance over existing methods in key metrics such as\nPeak Signal-to-Noise Ratio (PSNR), Structural Similarity (SSIM), and Root Mean\nSquare Error (RMSE), significantly improving noise removal and image quality.\n","authors":["Bin Wang","Fei Deng","Peifan Jiang","Shuang Wang","Xiao Han","Hongjie Zheng"],"pdf_url":"https://arxiv.org/pdf/2404.09533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09532v1","updated":"2024-04-15T07:51:40Z","published":"2024-04-15T07:51:40Z","title":"TMPQ-DM: Joint Timestep Reduction and Quantization Precision Selection\n  for Efficient Diffusion Models","summary":"  Diffusion models have emerged as preeminent contenders in the realm of\ngenerative models. Distinguished by their distinctive sequential generative\nprocesses, characterized by hundreds or even thousands of timesteps, diffusion\nmodels progressively reconstruct images from pure Gaussian noise, with each\ntimestep necessitating full inference of the entire model. However, the\nsubstantial computational demands inherent to these models present challenges\nfor deployment, quantization is thus widely used to lower the bit-width for\nreducing the storage and computing overheads. Current quantization\nmethodologies primarily focus on model-side optimization, disregarding the\ntemporal dimension, such as the length of the timestep sequence, thereby\nallowing redundant timesteps to continue consuming computational resources,\nleaving substantial scope for accelerating the generative process. In this\npaper, we introduce TMPQ-DM, which jointly optimizes timestep reduction and\nquantization to achieve a superior performance-efficiency trade-off, addressing\nboth temporal and model optimization aspects. For timestep reduction, we devise\na non-uniform grouping scheme tailored to the non-uniform nature of the\ndenoising process, thereby mitigating the explosive combinations of timesteps.\nIn terms of quantization, we adopt a fine-grained layer-wise approach to\nallocate varying bit-widths to different layers based on their respective\ncontributions to the final generative performance, thus rectifying performance\ndegradation observed in prior studies. To expedite the evaluation of\nfine-grained quantization, we further devise a super-network to serve as a\nprecision solver by leveraging shared quantization results. These two design\ncomponents are seamlessly integrated within our framework, enabling rapid joint\nexploration of the exponentially large decision space via a gradient-free\nevolutionary search algorithm.\n","authors":["Haojun Sun","Chen Tang","Zhi Wang","Yuan Meng","Jingyan jiang","Xinzhu Ma","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2404.09532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09531v1","updated":"2024-04-15T07:51:29Z","published":"2024-04-15T07:51:29Z","title":"Oblique-MERF: Revisiting and Improving MERF for Oblique Photography","summary":"  Neural implicit fields have established a new paradigm for scene\nrepresentation, with subsequent work achieving high-quality real-time\nrendering. However, reconstructing 3D scenes from oblique aerial photography\npresents unique challenges, such as varying spatial scale distributions and a\nconstrained range of tilt angles, often resulting in high memory consumption\nand reduced rendering quality at extrapolated viewpoints. In this paper, we\nenhance MERF to accommodate these data characteristics by introducing an\ninnovative adaptive occupancy plane optimized during the volume rendering\nprocess and a smoothness regularization term for view-dependent color to\naddress these issues. Our approach, termed Oblique-MERF, surpasses\nstate-of-the-art real-time methods by approximately 0.7 dB, reduces VRAM usage\nby about 40%, and achieves higher rendering frame rates with more realistic\nrendering outcomes across most viewpoints.\n","authors":["Xiaoyi Zeng","Kaiwen Song","Leyuan Yang","Bailin Deng","Juyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.09531v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09530v1","updated":"2024-04-15T07:50:15Z","published":"2024-04-15T07:50:15Z","title":"RanLayNet: A Dataset for Document Layout Detection used for Domain\n  Adaptation and Generalization","summary":"  Large ground-truth datasets and recent advances in deep learning techniques\nhave been useful for layout detection. However, because of the restricted\nlayout diversity of these datasets, training on them requires a sizable number\nof annotated instances, which is both expensive and time-consuming. As a\nresult, differences between the source and target domains may significantly\nimpact how well these models function. To solve this problem, domain adaptation\napproaches have been developed that use a small quantity of labeled data to\nadjust the model to the target domain. In this research, we introduced a\nsynthetic document dataset called RanLayNet, enriched with automatically\nassigned labels denoting spatial positions, ranges, and types of layout\nelements. The primary aim of this endeavor is to develop a versatile dataset\ncapable of training models with robustness and adaptability to diverse document\nformats. Through empirical experimentation, we demonstrate that a deep layout\nidentification model trained on our dataset exhibits enhanced performance\ncompared to a model trained solely on actual documents. Moreover, we conduct a\ncomparative analysis by fine-tuning inference models using both PubLayNet and\nIIIT-AR-13K datasets on the Doclaynet dataset. Our findings emphasize that\nmodels enriched with our dataset are optimal for tasks such as achieving 0.398\nand 0.588 mAP95 score in the scientific document domain for the TABLE class.\n","authors":["Avinash Anand","Raj Jaiswal","Mohit Gupta","Siddhesh S Bangar","Pijush Bhuyan","Naman Lal","Rajeev Singh","Ritika Jha","Rajiv Ratn Shah","Shin'ichi Satoh"],"pdf_url":"https://arxiv.org/pdf/2404.09530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09516v1","updated":"2024-04-15T07:24:45Z","published":"2024-04-15T07:24:45Z","title":"State Space Model for New-Generation Network Alternative to\n  Transformers: A Survey","summary":"  In the post-deep learning era, the Transformer architecture has demonstrated\nits powerful performance across pre-trained big models and various downstream\ntasks. However, the enormous computational demands of this architecture have\ndeterred many researchers. To further reduce the complexity of attention\nmodels, numerous efforts have been made to design more efficient methods. Among\nthem, the State Space Model (SSM), as a possible replacement for the\nself-attention based Transformer model, has drawn more and more attention in\nrecent years. In this paper, we give the first comprehensive review of these\nworks and also provide experimental comparisons and analysis to better\ndemonstrate the features and advantages of SSM. Specifically, we first give a\ndetailed description of principles to help the readers quickly capture the key\nideas of SSM. After that, we dive into the reviews of existing SSMs and their\nvarious applications, including natural language processing, computer vision,\ngraph, multi-modal and multi-media, point cloud/event stream, time series data,\nand other domains. In addition, we give statistical comparisons and analysis of\nthese models and hope it helps the readers to understand the effectiveness of\ndifferent structures on various tasks. Then, we propose possible research\npoints in this direction to better promote the development of the theoretical\nmodel and application of SSM. More related works will be continuously updated\non the following GitHub:\nhttps://github.com/Event-AHU/Mamba_State_Space_Model_Paper_List.\n","authors":["Xiao Wang","Shiao Wang","Yuhe Ding","Yuehang Li","Wentao Wu","Yao Rong","Weizhe Kong","Ju Huang","Shihao Li","Haoxiang Yang","Ziwen Wang","Bo Jiang","Chenglong Li","Yaowei Wang","Yonghong Tian","Jin Tang"],"pdf_url":"https://arxiv.org/pdf/2404.09516v1.pdf","comment":"The First review of State Space Model (SSM)/Mamba and their\n  applications in artificial intelligence, 33 pages"},{"id":"http://arxiv.org/abs/2404.09515v1","updated":"2024-04-15T07:20:09Z","published":"2024-04-15T07:20:09Z","title":"Deep image learning of quantitative structure-property relationships of\n  cooper alloys via feature augmentation on Geodesic curve in shape space","summary":"  Understanding how the structure of materials affects their properties is a\ncornerstone of materials science and engineering. However, traditional methods\nhave struggled to accurately describe the quantitative structure-property\nrelationships for complex structures. In our study, we bridge this gap by\nleveraging machine learning to analyze images of materials' microstructures,\nthus offering a novel way to understand and predict the properties of materials\nbased on their microstructures. We introduce a method known as FAGC (Feature\nAugmentation on Geodesic Curves), specifically demonstrated for Cu-Cr-Zr\nalloys. This approach utilizes machine learning to examine the shapes within\nimages of the alloys' microstructures and predict their mechanical and\nelectronic properties. This generative FAGC approach can effectively expand the\nrelatively small training datasets due to the limited availability of materials\nimages labeled with quantitative properties. The process begins with extracting\nfeatures from the images using neural networks. These features are then mapped\nonto the Pre-shape space to construct the Geodesic curves. Along these curves,\nnew features are generated, effectively increasing the dataset. Moreover, we\ndesign a pseudo-labeling mechanism for these newly generated features to\nfurther enhance the training dataset. Our FAGC method has shown remarkable\nresults, significantly improving the accuracy of predicting the electronic\nconductivity and hardness of Cu-Cr-Zr alloys, with R-squared values of 0.978\nand 0.998, respectively. These outcomes underscore the potential of FAGC to\naddress the challenge of limited image data in materials science, providing a\npowerful tool for establishing detailed and quantitative relationships between\ncomplex microstructures and material properties.\n","authors":["Yuexing Han","Guanxin Wan","Bing Wang","Yi Liu"],"pdf_url":"https://arxiv.org/pdf/2404.09515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.13783v2","updated":"2024-04-15T07:18:45Z","published":"2023-12-21T12:14:31Z","title":"Few Shot Part Segmentation Reveals Compositional Logic for Industrial\n  Anomaly Detection","summary":"  Logical anomalies (LA) refer to data violating underlying logical constraints\ne.g., the quantity, arrangement, or composition of components within an image.\nDetecting accurately such anomalies requires models to reason about various\ncomponent types through segmentation. However, curation of pixel-level\nannotations for semantic segmentation is both time-consuming and expensive.\nAlthough there are some prior few-shot or unsupervised co-part segmentation\nalgorithms, they often fail on images with industrial object. These images have\ncomponents with similar textures and shapes, and a precise differentiation\nproves challenging. In this study, we introduce a novel component segmentation\nmodel for LA detection that leverages a few labeled samples and unlabeled\nimages sharing logical constraints. To ensure consistent segmentation across\nunlabeled images, we employ a histogram matching loss in conjunction with an\nentropy loss. As segmentation predictions play a crucial role, we propose to\nenhance both local and global sample validity detection by capturing key\naspects from visual semantics via three memory banks: class histograms,\ncomponent composition embeddings and patch-level representations. For effective\nLA detection, we propose an adaptive scaling strategy to standardize anomaly\nscores from different memory banks in inference. Extensive experiments on the\npublic benchmark MVTec LOCO AD reveal our method achieves 98.1% AUROC in LA\ndetection vs. 89.6% from competing methods.\n","authors":["Soopil Kim","Sion An","Philip Chikontwe","Myeongkyun Kang","Ehsan Adeli","Kilian M. Pohl","Sang Hyun Park"],"pdf_url":"https://arxiv.org/pdf/2312.13783v2.pdf","comment":"Accepted in AAAI2024"},{"id":"http://arxiv.org/abs/2312.01897v2","updated":"2024-04-15T07:15:43Z","published":"2023-12-04T13:51:16Z","title":"Adapting Short-Term Transformers for Action Detection in Untrimmed\n  Videos","summary":"  Vision Transformer (ViT) has shown high potential in video recognition, owing\nto its flexible design, adaptable self-attention mechanisms, and the efficacy\nof masked pre-training. Yet, it remains unclear how to adapt these pre-trained\nshort-term ViTs for temporal action detection (TAD) in untrimmed videos. The\nexisting works treat them as off-the-shelf feature extractors for each\nshort-trimmed snippet without capturing the fine-grained relation among\ndifferent snippets in a broader temporal context. To mitigate this issue, this\npaper focuses on designing a new mechanism for adapting these pre-trained ViT\nmodels as a unified long-form video transformer to fully unleash its modeling\npower in capturing inter-snippet relation, while still keeping low computation\noverhead and memory consumption for efficient TAD. To this end, we design\neffective cross-snippet propagation modules to gradually exchange short-term\nvideo information among different snippets from two levels. For inner-backbone\ninformation propagation, we introduce a cross-snippet propagation strategy to\nenable multi-snippet temporal feature interaction inside the backbone.For\npost-backbone information propagation, we propose temporal transformer layers\nfor further clip-level modeling. With the plain ViT-B pre-trained with\nVideoMAE, our end-to-end temporal action detector (ViT-TAD) yields a very\ncompetitive performance to previous temporal action detectors, riching up to\n69.5 average mAP on THUMOS14, 37.40 average mAP on ActivityNet-1.3 and 17.20\naverage mAP on FineAction.\n","authors":["Min Yang","Huan Gao","Ping Guo","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2312.01897v2.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2404.09512v1","updated":"2024-04-15T07:15:39Z","published":"2024-04-15T07:15:39Z","title":"Magic Clothing: Controllable Garment-Driven Image Synthesis","summary":"  We propose Magic Clothing, a latent diffusion model (LDM)-based network\narchitecture for an unexplored garment-driven image synthesis task. Aiming at\ngenerating customized characters wearing the target garments with diverse text\nprompts, the image controllability is the most critical issue, i.e., to\npreserve the garment details and maintain faithfulness to the text prompts. To\nthis end, we introduce a garment extractor to capture the detailed garment\nfeatures, and employ self-attention fusion to incorporate them into the\npretrained LDMs, ensuring that the garment details remain unchanged on the\ntarget character. Then, we leverage the joint classifier-free guidance to\nbalance the control of garment features and text prompts over the generated\nresults. Meanwhile, the proposed garment extractor is a plug-in module\napplicable to various finetuned LDMs, and it can be combined with other\nextensions like ControlNet and IP-Adapter to enhance the diversity and\ncontrollability of the generated characters. Furthermore, we design\nMatched-Points-LPIPS (MP-LPIPS), a robust metric for evaluating the consistency\nof the target image to the source garment. Extensive experiments demonstrate\nthat our Magic Clothing achieves state-of-the-art results under various\nconditional controls for garment-driven image synthesis. Our source code is\navailable at https://github.com/ShineChen1024/MagicClothing.\n","authors":["Weifeng Chen","Tao Gu","Yuhao Xu","Chengcai Chen"],"pdf_url":"https://arxiv.org/pdf/2404.09512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01238v2","updated":"2024-04-15T07:12:20Z","published":"2024-03-02T15:47:42Z","title":"On the Road to Portability: Compressing End-to-End Motion Planner for\n  Autonomous Driving","summary":"  End-to-end motion planning models equipped with deep neural networks have\nshown great potential for enabling full autonomous driving. However, the\noversized neural networks render them impractical for deployment on\nresource-constrained systems, which unavoidably requires more computational\ntime and resources during reference.To handle this, knowledge distillation\noffers a promising approach that compresses models by enabling a smaller\nstudent model to learn from a larger teacher model. Nevertheless, how to apply\nknowledge distillation to compress motion planners has not been explored so\nfar. In this paper, we propose PlanKD, the first knowledge distillation\nframework tailored for compressing end-to-end motion planners. First,\nconsidering that driving scenes are inherently complex, often containing\nplanning-irrelevant or even noisy information, transferring such information is\nnot beneficial for the student planner. Thus, we design an information\nbottleneck based strategy to only distill planning-relevant information, rather\nthan transfer all information indiscriminately. Second, different waypoints in\nan output planned trajectory may hold varying degrees of importance for motion\nplanning, where a slight deviation in certain crucial waypoints might lead to a\ncollision. Therefore, we devise a safety-aware waypoint-attentive distillation\nmodule that assigns adaptive weights to different waypoints based on the\nimportance, to encourage the student to accurately mimic more crucial\nwaypoints, thereby improving overall safety. Experiments demonstrate that our\nPlanKD can boost the performance of smaller planners by a large margin, and\nsignificantly reduce their reference time.\n","authors":["Kaituo Feng","Changsheng Li","Dongchun Ren","Ye Yuan","Guoren Wang"],"pdf_url":"https://arxiv.org/pdf/2403.01238v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2402.00015v2","updated":"2024-04-15T07:06:54Z","published":"2023-12-28T14:14:31Z","title":"Maintaining User Trust Through Multistage Uncertainty Aware Inference","summary":"  This paper describes and evaluates a multistage approach to AI deployment.\nEach stage involves a more accurate method of inference, yet engaging each\ncomes with an increasing cost. In outlining the architecture, we present a\nmethod for quantifying model uncertainty that facilitates confident deferral\ndecisions. The architecture is currently under active deployment to thousands\nof cotton farmers across India. The broader idea however is applicable to a\ngrowing sector of AI deployments in challenging low resources settings.\n","authors":["Chandan Agrawal","Ashish Papanai","Jerome White"],"pdf_url":"https://arxiv.org/pdf/2402.00015v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09509v1","updated":"2024-04-15T07:05:14Z","published":"2024-04-15T07:05:14Z","title":"Fuse after Align: Improving Face-Voice Association Learning via\n  Multimodal Encoder","summary":"  Today, there have been many achievements in learning the association between\nvoice and face. However, most previous work models rely on cosine similarity or\nL2 distance to evaluate the likeness of voices and faces following contrastive\nlearning, subsequently applied to retrieval and matching tasks. This method\nonly considers the embeddings as high-dimensional vectors, utilizing a minimal\nscope of available information. This paper introduces a novel framework within\nan unsupervised setting for learning voice-face associations. By employing a\nmultimodal encoder after contrastive learning and addressing the problem\nthrough binary classification, we can learn the implicit information within the\nembeddings in a more effective and varied manner. Furthermore, by introducing\nan effective pair selection method, we enhance the learning outcomes of both\ncontrastive learning and the matching task. Empirical evidence demonstrates\nthat our framework achieves state-of-the-art results in voice-face matching,\nverification, and retrieval tasks, improving verification by approximately 3%,\nmatching by about 2.5%, and retrieval by around 1.3%.\n","authors":["Chong Peng","Liqiang He","Dan Su"],"pdf_url":"https://arxiv.org/pdf/2404.09509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09507v1","updated":"2024-04-15T06:58:09Z","published":"2024-04-15T06:58:09Z","title":"Clothes-Changing Person Re-Identification with Feasibility-Aware\n  Intermediary Matching","summary":"  Current clothes-changing person re-identification (re-id) approaches usually\nperform retrieval based on clothes-irrelevant features, while neglecting the\npotential of clothes-relevant features. However, we observe that relying solely\non clothes-irrelevant features for clothes-changing re-id is limited, since\nthey often lack adequate identity information and suffer from large intra-class\nvariations. On the contrary, clothes-relevant features can be used to discover\nsame-clothes intermediaries that possess informative identity clues. Based on\nthis observation, we propose a Feasibility-Aware Intermediary Matching (FAIM)\nframework to additionally utilize clothes-relevant features for retrieval.\nFirstly, an Intermediary Matching (IM) module is designed to perform an\nintermediary-assisted matching process. This process involves using\nclothes-relevant features to find informative intermediates, and then using\nclothes-irrelevant features of these intermediates to complete the matching.\nSecondly, in order to reduce the negative effect of low-quality intermediaries,\nan Intermediary-Based Feasibility Weighting (IBFW) module is designed to\nevaluate the feasibility of intermediary matching process by assessing the\nquality of intermediaries. Extensive experiments demonstrate that our method\noutperforms state-of-the-art methods on several widely-used clothes-changing\nre-id benchmarks.\n","authors":["Jiahe Zhao","Ruibing Hou","Hong Chang","Xinqian Gu","Bingpeng Ma","Shiguang Shan","Xilin Chen"],"pdf_url":"https://arxiv.org/pdf/2404.09507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09504v1","updated":"2024-04-15T06:50:58Z","published":"2024-04-15T06:50:58Z","title":"Learning Tracking Representations from Single Point Annotations","summary":"  Existing deep trackers are typically trained with largescale video frames\nwith annotated bounding boxes. However, these bounding boxes are expensive and\ntime-consuming to annotate, in particular for large scale datasets. In this\npaper, we propose to learn tracking representations from single point\nannotations (i.e., 4.5x faster to annotate than the traditional bounding box)\nin a weakly supervised manner. Specifically, we propose a soft contrastive\nlearning (SoCL) framework that incorporates target objectness prior into\nend-to-end contrastive learning. Our SoCL consists of adaptive positive and\nnegative sample generation, which is memory-efficient and effective for\nlearning tracking representations. We apply the learned representation of SoCL\nto visual tracking and show that our method can 1) achieve better performance\nthan the fully supervised baseline trained with box annotations under the same\nannotation time cost; 2) achieve comparable performance of the fully supervised\nbaseline by using the same number of training frames and meanwhile reducing\nannotation time cost by 78% and total fees by 85%; 3) be robust to annotation\nnoise.\n","authors":["Qiangqiang Wu","Antoni B. Chan"],"pdf_url":"https://arxiv.org/pdf/2404.09504v1.pdf","comment":"Accept to CVPR2024-L3DIVU"},{"id":"http://arxiv.org/abs/2403.13392v2","updated":"2024-04-15T06:46:04Z","published":"2024-03-20T08:33:40Z","title":"Robust image segmentation model based on binary level set","summary":"  In order to improve the robustness of traditional image segmentation models\nto noise, this paper models the illumination term in intensity inhomogeneity\nimages. Additionally, to enhance the model's robustness to noisy images, we\nincorporate the binary level set model into the proposed model. Compared to the\ntraditional level set, the binary level set eliminates the need for continuous\nreinitialization. Moreover, by introducing the variational operator GL, our\nmodel demonstrates better capability in segmenting noisy images. Finally, we\nemploy the three-step splitting operator method for solving, and the\neffectiveness of the proposed model is demonstrated on various images.\n","authors":["Wenqi Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.13392v2.pdf","comment":"SCI"},{"id":"http://arxiv.org/abs/2404.09502v1","updated":"2024-04-15T06:45:06Z","published":"2024-04-15T06:45:06Z","title":"SparseOcc: Rethinking Sparse Latent Representation for Vision-Based\n  Semantic Occupancy Prediction","summary":"  Vision-based perception for autonomous driving requires an explicit modeling\nof a 3D space, where 2D latent representations are mapped and subsequent 3D\noperators are applied. However, operating on dense latent spaces introduces a\ncubic time and space complexity, which limits scalability in terms of\nperception range or spatial resolution. Existing approaches compress the dense\nrepresentation using projections like Bird's Eye View (BEV) or Tri-Perspective\nView (TPV). Although efficient, these projections result in information loss,\nespecially for tasks like semantic occupancy prediction. To address this, we\npropose SparseOcc, an efficient occupancy network inspired by sparse point\ncloud processing. It utilizes a lossless sparse latent representation with\nthree key innovations. Firstly, a 3D sparse diffuser performs latent completion\nusing spatially decomposed 3D sparse convolutional kernels. Secondly, a feature\npyramid and sparse interpolation enhance scales with information from others.\nFinally, the transformer head is redesigned as a sparse variant. SparseOcc\nachieves a remarkable 74.9% reduction on FLOPs over the dense baseline.\nInterestingly, it also improves accuracy, from 12.8% to 14.1% mIOU, which in\npart can be attributed to the sparse representation's ability to avoid\nhallucinations on empty voxels.\n","authors":["Pin Tang","Zhongdao Wang","Guoqing Wang","Jilai Zheng","Xiangxuan Ren","Bailan Feng","Chao Ma"],"pdf_url":"https://arxiv.org/pdf/2404.09502v1.pdf","comment":"10 pages, 4 figures, accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2404.09499v1","updated":"2024-04-15T06:38:09Z","published":"2024-04-15T06:38:09Z","title":"Learning Human Motion from Monocular Videos via Cross-Modal Manifold\n  Alignment","summary":"  Learning 3D human motion from 2D inputs is a fundamental task in the realms\nof computer vision and computer graphics. Many previous methods grapple with\nthis inherently ambiguous task by introducing motion priors into the learning\nprocess. However, these approaches face difficulties in defining the complete\nconfigurations of such priors or training a robust model. In this paper, we\npresent the Video-to-Motion Generator (VTM), which leverages motion priors\nthrough cross-modal latent feature space alignment between 3D human motion and\n2D inputs, namely videos and 2D keypoints. To reduce the complexity of modeling\nmotion priors, we model the motion data separately for the upper and lower body\nparts. Additionally, we align the motion data with a scale-invariant virtual\nskeleton to mitigate the interference of human skeleton variations to the\nmotion priors. Evaluated on AIST++, the VTM showcases state-of-the-art\nperformance in reconstructing 3D human motion from monocular videos. Notably,\nour VTM exhibits the capabilities for generalization to unseen view angles and\nin-the-wild videos.\n","authors":["Shuaiying Hou","Hongyu Tao","Junheng Fang","Changqing Zou","Hujun Bao","Weiwei Xu"],"pdf_url":"https://arxiv.org/pdf/2404.09499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09498v1","updated":"2024-04-15T06:37:21Z","published":"2024-04-15T06:37:21Z","title":"FusionMamba: Dynamic Feature Enhancement for Multimodal Image Fusion\n  with Mamba","summary":"  Multi-modal image fusion aims to combine information from different modes to\ncreate a single image with comprehensive information and detailed textures.\nHowever, fusion models based on convolutional neural networks encounter\nlimitations in capturing global image features due to their focus on local\nconvolution operations. Transformer-based models, while excelling in global\nfeature modeling, confront computational challenges stemming from their\nquadratic complexity. Recently, the Selective Structured State Space Model has\nexhibited significant potential for long-range dependency modeling with linear\ncomplexity, offering a promising avenue to address the aforementioned dilemma.\nIn this paper, we propose FusionMamba, a novel dynamic feature enhancement\nmethod for multimodal image fusion with Mamba. Specifically, we devise an\nimproved efficient Mamba model for image fusion, integrating efficient visual\nstate space model with dynamic convolution and channel attention. This refined\nmodel not only upholds the performance of Mamba and global modeling capability\nbut also diminishes channel redundancy while enhancing local enhancement\ncapability. Additionally, we devise a dynamic feature fusion module (DFFM)\ncomprising two dynamic feature enhancement modules (DFEM) and a cross modality\nfusion mamba module (CMFM). The former serves for dynamic texture enhancement\nand dynamic difference perception, whereas the latter enhances correlation\nfeatures between modes and suppresses redundant intermodal information.\nFusionMamba has yielded state-of-the-art (SOTA) performance across various\nmultimodal medical image fusion tasks (CT-MRI, PET-MRI, SPECT-MRI), infrared\nand visible image fusion task (IR-VIS) and multimodal biomedical image fusion\ndataset (GFP-PC), which is proved that our model has generalization ability.\nThe code for FusionMamba is available at\nhttps://github.com/millieXie/FusionMamba.\n","authors":["Xinyu Xie","Yawen Cui","Chio-In Ieong","Tao Tan","Xiaozhi Zhang","Xubin Zheng","Zitong Yu"],"pdf_url":"https://arxiv.org/pdf/2404.09498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03883v2","updated":"2024-04-15T06:34:52Z","published":"2024-04-05T04:11:31Z","title":"LiDAR-Guided Cross-Attention Fusion for Hyperspectral Band Selection and\n  Image Classification","summary":"  The fusion of hyperspectral and LiDAR data has been an active research topic.\nExisting fusion methods have ignored the high-dimensionality and redundancy\nchallenges in hyperspectral images, despite that band selection methods have\nbeen intensively studied for hyperspectral image (HSI) processing. This paper\naddresses this significant gap by introducing a cross-attention mechanism from\nthe transformer architecture for the selection of HSI bands guided by LiDAR\ndata. LiDAR provides high-resolution vertical structural information, which can\nbe useful in distinguishing different types of land cover that may have similar\nspectral signatures but different structural profiles. In our approach, the\nLiDAR data are used as the \"query\" to search and identify the \"key\" from the\nHSI to choose the most pertinent bands for LiDAR. This method ensures that the\nselected HSI bands drastically reduce redundancy and computational requirements\nwhile working optimally with the LiDAR data. Extensive experiments have been\nundertaken on three paired HSI and LiDAR data sets: Houston 2013, Trento and\nMUUFL. The results highlight the superiority of the cross-attention mechanism,\nunderlining the enhanced classification accuracy of the identified HSI bands\nwhen fused with the LiDAR features. The results also show that the use of fewer\nbands combined with LiDAR surpasses the performance of state-of-the-art fusion\nmodels.\n","authors":["Judy X Yang","Jun Zhou","Jing Wang","Hui Tian","Alan Wee-Chung Liew"],"pdf_url":"https://arxiv.org/pdf/2404.03883v2.pdf","comment":"15 pages, 13 figures"},{"id":"http://arxiv.org/abs/2404.09496v1","updated":"2024-04-15T06:33:32Z","published":"2024-04-15T06:33:32Z","title":"Towards Collaborative Autonomous Driving: Simulation Platform and\n  End-to-End System","summary":"  Vehicle-to-everything-aided autonomous driving (V2X-AD) has a huge potential\nto provide a safer driving solution. Despite extensive researches in\ntransportation and communication to support V2X-AD, the actual utilization of\nthese infrastructures and communication resources in enhancing driving\nperformances remains largely unexplored. This highlights the necessity of\ncollaborative autonomous driving: a machine learning approach that optimizes\nthe information sharing strategy to improve the driving performance of each\nvehicle. This effort necessitates two key foundations: a platform capable of\ngenerating data to facilitate the training and testing of V2X-AD, and a\ncomprehensive system that integrates full driving-related functionalities with\nmechanisms for information sharing. From the platform perspective, we present\nV2Xverse, a comprehensive simulation platform for collaborative autonomous\ndriving. This platform provides a complete pipeline for collaborative driving.\nFrom the system perspective, we introduce CoDriving, a novel end-to-end\ncollaborative driving system that properly integrates V2X communication over\nthe entire autonomous pipeline, promoting driving with shared perceptual\ninformation. The core idea is a novel driving-oriented communication strategy.\nLeveraging this strategy, CoDriving improves driving performance while\noptimizing communication efficiency. We make comprehensive benchmarks with\nV2Xverse, analyzing both modular performance and closed-loop driving\nperformance. Experimental results show that CoDriving: i) significantly\nimproves the driving score by 62.49% and drastically reduces the pedestrian\ncollision rate by 53.50% compared to the SOTA end-to-end driving method, and\nii) achieves sustaining driving performance superiority over dynamic constraint\ncommunication conditions.\n","authors":["Genjia Liu","Yue Hu","Chenxin Xu","Weibo Mao","Junhao Ge","Zhengxiang Huang","Yifan Lu","Yinda Xu","Junkai Xia","Yafei Wang","Siheng Chen"],"pdf_url":"https://arxiv.org/pdf/2404.09496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09490v1","updated":"2024-04-15T06:24:56Z","published":"2024-04-15T06:24:56Z","title":"Leveraging Temporal Contextualization for Video Action Recognition","summary":"  Pretrained vision-language models have shown effectiveness in video\nunderstanding. However, recent studies have not sufficiently leveraged\nessential temporal information from videos, simply averaging frame-wise\nrepresentations or referencing consecutive frames. We introduce Temporally\nContextualized CLIP (TC-CLIP), a pioneering framework for video understanding\nthat effectively and efficiently leverages comprehensive video information. We\npropose Temporal Contextualization (TC), a novel layer-wise temporal\ninformation infusion mechanism for video that extracts core information from\neach frame, interconnects relevant information across the video to summarize\ninto context tokens, and ultimately leverages the context tokens during the\nfeature encoding process. Furthermore, our Video-conditional Prompting (VP)\nmodule manufactures context tokens to generate informative prompts in text\nmodality. We conduct extensive experiments in zero-shot, few-shot,\nbase-to-novel, and fully-supervised action recognition to validate the\nsuperiority of our TC-CLIP. Ablation studies for TC and VP guarantee our design\nchoices. Code is available at https://github.com/naver-ai/tc-clip\n","authors":["Minji Kim","Dongyoon Han","Taekyung Kim","Bohyung Han"],"pdf_url":"https://arxiv.org/pdf/2404.09490v1.pdf","comment":"24 pages, 10 figures, 12 tables"},{"id":"http://arxiv.org/abs/2404.09486v1","updated":"2024-04-15T06:15:46Z","published":"2024-04-15T06:15:46Z","title":"MMCode: Evaluating Multi-Modal Code Large Language Models with Visually\n  Rich Programming Problems","summary":"  Programming often involves converting detailed and complex specifications\ninto code, a process during which developers typically utilize visual aids to\nmore effectively convey concepts. While recent developments in Large Multimodal\nModels have demonstrated remarkable abilities in visual reasoning and\nmathematical tasks, there is little work on investigating whether these models\ncan effectively interpret visual elements for code generation. To this end, we\npresent MMCode, the first multi-modal coding dataset for evaluating algorithmic\nproblem-solving skills in visually rich contexts. MMCode contains 3,548\nquestions and 6,620 images collected from real-world programming challenges\nharvested from 10 code competition websites, presenting significant challenges\ndue to the extreme demand for reasoning abilities. Our experiment results show\nthat current state-of-the-art models struggle to solve these problems. The\nresults highlight the lack of powerful vision-code models, and we hope MMCode\ncan serve as an inspiration for future works in this domain. The data and code\nare publicly available at https://github.com/happylkx/MMCode.\n","authors":["Kaixin Li","Yuchen Tian","Qisheng Hu","Ziyang Luo","Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2404.09486v1.pdf","comment":"46 pages, 21 figures and 6 tables"},{"id":"http://arxiv.org/abs/2311.12198v3","updated":"2024-04-15T06:04:55Z","published":"2023-11-20T21:34:52Z","title":"PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics","summary":"  We introduce PhysGaussian, a new method that seamlessly integrates physically\ngrounded Newtonian dynamics within 3D Gaussians to achieve high-quality novel\nmotion synthesis. Employing a custom Material Point Method (MPM), our approach\nenriches 3D Gaussian kernels with physically meaningful kinematic deformation\nand mechanical stress attributes, all evolved in line with continuum mechanics\nprinciples. A defining characteristic of our method is the seamless integration\nbetween physical simulation and visual rendering: both components utilize the\nsame 3D Gaussian kernels as their discrete representations. This negates the\nnecessity for triangle/tetrahedron meshing, marching cubes, \"cage meshes,\" or\nany other geometry embedding, highlighting the principle of \"what you see is\nwhat you simulate (WS$^2$).\" Our method demonstrates exceptional versatility\nacross a wide variety of materials--including elastic entities, metals,\nnon-Newtonian fluids, and granular materials--showcasing its strong\ncapabilities in creating diverse visual content with novel viewpoints and\nmovements. Our project page is at: https://xpandora.github.io/PhysGaussian/\n","authors":["Tianyi Xie","Zeshun Zong","Yuxing Qiu","Xuan Li","Yutao Feng","Yin Yang","Chenfanfu Jiang"],"pdf_url":"https://arxiv.org/pdf/2311.12198v3.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2404.09476v1","updated":"2024-04-15T06:02:31Z","published":"2024-04-15T06:02:31Z","title":"FreqMamba: Viewing Mamba from a Frequency Perspective for Image\n  Deraining","summary":"  Images corrupted by rain streaks often lose vital frequency information for\nperception, and image deraining aims to solve this issue which relies on global\nand local degradation modeling. Recent studies have witnessed the effectiveness\nand efficiency of Mamba for perceiving global and local information based on\nits exploiting local correlation among patches, however, rarely attempts have\nbeen explored to extend it with frequency analysis for image deraining,\nlimiting its ability to perceive global degradation that is relevant to\nfrequency modeling (e.g. Fourier transform). In this paper, we propose\nFreqMamba, an effective and efficient paradigm that leverages the complementary\nbetween Mamba and frequency analysis for image deraining. The core of our\nmethod lies in extending Mamba with frequency analysis from two perspectives:\nextending it with frequency-band for exploiting frequency correlation, and\nconnecting it with Fourier transform for global degradation modeling.\nSpecifically, FreqMamba introduces complementary triple interaction structures\nincluding spatial Mamba, frequency band Mamba, and Fourier global modeling.\nFrequency band Mamba decomposes the image into sub-bands of different\nfrequencies to allow 2D scanning from the frequency dimension. Furthermore,\nleveraging Mamba's unique data-dependent properties, we use rainy images at\ndifferent scales to provide degradation priors to the network, thereby\nfacilitating efficient training. Extensive experiments show that our method\noutperforms state-of-the-art methods both visually and quantitatively.\n","authors":["Zou Zhen","Yu Hu","Zhao Feng"],"pdf_url":"https://arxiv.org/pdf/2404.09476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09475v1","updated":"2024-04-15T06:02:09Z","published":"2024-04-15T06:02:09Z","title":"Improving Weakly-Supervised Object Localization Using Adversarial\n  Erasing and Pseudo Label","summary":"  Weakly-supervised learning approaches have gained significant attention due\nto their ability to reduce the effort required for human annotations in\ntraining neural networks. This paper investigates a framework for\nweakly-supervised object localization, which aims to train a neural network\ncapable of predicting both the object class and its location using only images\nand their image-level class labels. The proposed framework consists of a shared\nfeature extractor, a classifier, and a localizer. The localizer predicts\npixel-level class probabilities, while the classifier predicts the object class\nat the image level. Since image-level class labels are insufficient for\ntraining the localizer, weakly-supervised object localization methods often\nencounter challenges in accurately localizing the entire object region. To\naddress this issue, the proposed method incorporates adversarial erasing and\npseudo labels to improve localization accuracy. Specifically, novel losses are\ndesigned to utilize adversarially erased foreground features and adversarially\nerased feature maps, reducing dependence on the most discriminative region.\nAdditionally, the proposed method employs pseudo labels to suppress activation\nvalues in the background while increasing them in the foreground. The proposed\nmethod is applied to two backbone networks (MobileNetV1 and InceptionV3) and is\nevaluated on three publicly available datasets (ILSVRC-2012, CUB-200-2011, and\nPASCAL VOC 2012). The experimental results demonstrate that the proposed method\noutperforms previous state-of-the-art methods across all evaluated metrics.\n","authors":["Byeongkeun Kang","Sinhae Cha","Yeejin Lee"],"pdf_url":"https://arxiv.org/pdf/2404.09475v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2404.09474v1","updated":"2024-04-15T06:01:48Z","published":"2024-04-15T06:01:48Z","title":"TCCT-Net: Two-Stream Network Architecture for Fast and Efficient\n  Engagement Estimation via Behavioral Feature Signals","summary":"  Engagement analysis finds various applications in healthcare, education,\nadvertisement, services. Deep Neural Networks, used for analysis, possess\ncomplex architecture and need large amounts of input data, computational power,\ninference time. These constraints challenge embedding systems into devices for\nreal-time use. To address these limitations, we present a novel two-stream\nfeature fusion \"Tensor-Convolution and Convolution-Transformer Network\"\n(TCCT-Net) architecture. To better learn the meaningful patterns in the\ntemporal-spatial domain, we design a \"CT\" stream that integrates a hybrid\nconvolutional-transformer. In parallel, to efficiently extract rich patterns\nfrom the temporal-frequency domain and boost processing speed, we introduce a\n\"TC\" stream that uses Continuous Wavelet Transform (CWT) to represent\ninformation in a 2D tensor form. Evaluated on the EngageNet dataset, the\nproposed method outperforms existing baselines, utilizing only two behavioral\nfeatures (head pose rotations) compared to the 98 used in baseline models.\nFurthermore, comparative analysis shows TCCT-Net's architecture offers an\norder-of-magnitude improvement in inference speed compared to state-of-the-art\nimage-based Recurrent Neural Network (RNN) methods. The code will be released\nat https://github.com/vedernikovphoto/TCCT_Net.\n","authors":["Alexander Vedernikov","Puneet Kumar","Haoyu Chen","Tapio Seppanen","Xiaobai Li"],"pdf_url":"https://arxiv.org/pdf/2404.09474v1.pdf","comment":"Accepted for the CVPR 2024 workshop (ABAW)"},{"id":"http://arxiv.org/abs/2404.09472v1","updated":"2024-04-15T05:53:26Z","published":"2024-04-15T05:53:26Z","title":"Q2A: Querying Implicit Fully Continuous Feature Pyramid to Align\n  Features for Medical Image Segmentation","summary":"  Recent medical image segmentation methods apply implicit neural\nrepresentation (INR) to the decoder for achieving a continuous coordinate\ndecoding to tackle the drawback of conventional discrete grid-based data\nrepresentations. However, the INR-based decoder cannot well handle the feature\nmisalignment problem brought about by the naive latent code acquisition\nstrategy in INR. Although there exist many feature alignment works, they all\nadopt a progressive multi-step aligning paradigm on a discrete feature pyramid,\nwhich is incompatible with the continuous one-step characteristics of INR-based\ndecoder, and thus fails to be the solution. Therefore, we propose Q2A, a novel\none-step query-based aligning paradigm, to solve the feature misalignment\nproblem in the INR-based decoder. Specifically, for each target coordinate, Q2A\nfirst generates several queries depicting the spatial offsets and the cell\nresolutions of the contextual features aligned to the coordinate, then\ncalculates the corresponding aligned features by feeding the queries into a\nnovel implicit fully continuous feature pyramid (FCFP), finally fuses the\naligned features to predict the class distribution. In FCFP, we further propose\na novel universal partition-and-aggregate strategy (P&A) to replace the naive\ninterpolation strategy for latent code acquisition in INR, which mitigates the\ninformation loss problem that occurs when the query cell resolution is\nrelatively large and achieves an effective feature decoding at arbitrary\ncontinuous resolution. We conduct extensive experiments on two medical\ndatasets, i.e. Glas and Synapse, and a universal dataset, i.e. Cityscapes, and\nthey show the superiority of the proposed Q2A.\n","authors":["Jiahao Yu","Li Chen"],"pdf_url":"https://arxiv.org/pdf/2404.09472v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2404.09469v1","updated":"2024-04-15T05:44:03Z","published":"2024-04-15T05:44:03Z","title":"Virtually Enriched NYU Depth V2 Dataset for Monocular Depth Estimation:\n  Do We Need Artificial Augmentation?","summary":"  We present ANYU, a new virtually augmented version of the NYU depth v2\ndataset, designed for monocular depth estimation. In contrast to the well-known\napproach where full 3D scenes of a virtual world are utilized to generate\nartificial datasets, ANYU was created by incorporating RGB-D representations of\nvirtual reality objects into the original NYU depth v2 images. We specifically\ndid not match each generated virtual object with an appropriate texture and a\nsuitable location within the real-world image. Instead, an assignment of\ntexture, location, lighting, and other rendering parameters was randomized to\nmaximize a diversity of the training data, and to show that it is randomness\nthat can improve the generalizing ability of a dataset. By conducting extensive\nexperiments with our virtually modified dataset and validating on the original\nNYU depth v2 and iBims-1 benchmarks, we show that ANYU improves the monocular\ndepth estimation performance and generalization of deep neural networks with\nconsiderably different architectures, especially for the current\nstate-of-the-art VPD model. To the best of our knowledge, this is the first\nwork that augments a real-world dataset with randomly generated virtual 3D\nobjects for monocular depth estimation. We make our ANYU dataset publicly\navailable in two training configurations with 10% and 100% additional\nsynthetically enriched RGB-D pairs of training images, respectively, for\nefficient training and empirical exploration of virtual augmentation at\nhttps://github.com/ABrain-One/ANYU\n","authors":["Dmitry Ignatov","Andrey Ignatov","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2404.09469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03124v2","updated":"2024-04-15T05:38:16Z","published":"2024-02-05T15:51:34Z","title":"Towards Eliminating Hard Label Constraints in Gradient Inversion Attacks","summary":"  Gradient inversion attacks aim to reconstruct local training data from\nintermediate gradients exposed in the federated learning framework. Despite\nsuccessful attacks, all previous methods, starting from reconstructing a single\ndata point and then relaxing the single-image limit to batch level, are only\ntested under hard label constraints. Even for single-image reconstruction, we\nstill lack an analysis-based algorithm to recover augmented soft labels. In\nthis work, we change the focus from enlarging batchsize to investigating the\nhard label constraints, considering a more realistic circumstance where label\nsmoothing and mixup techniques are used in the training process. In particular,\nwe are the first to initiate a novel algorithm to simultaneously recover the\nground-truth augmented label and the input feature of the last fully-connected\nlayer from single-input gradients, and provide a necessary condition for any\nanalytical-based label recovery methods. Extensive experiments testify to the\nlabel recovery accuracy, as well as the benefits to the following image\nreconstruction. We believe soft labels in classification tasks are worth\nfurther attention in gradient inversion attacks.\n","authors":["Yanbo Wang","Jian Liang","Ran He"],"pdf_url":"https://arxiv.org/pdf/2402.03124v2.pdf","comment":"ICLR2024 poster"},{"id":"http://arxiv.org/abs/2404.09465v1","updated":"2024-04-15T05:29:23Z","published":"2024-04-15T05:29:23Z","title":"PhyScene: Physically Interactable 3D Scene Synthesis for Embodied AI","summary":"  With recent developments in Embodied Artificial Intelligence (EAI) research,\nthere has been a growing demand for high-quality, large-scale interactive scene\ngeneration. While prior methods in scene synthesis have prioritized the\nnaturalness and realism of the generated scenes, the physical plausibility and\ninteractivity of scenes have been largely left unexplored. To address this\ndisparity, we introduce PhyScene, a novel method dedicated to generating\ninteractive 3D scenes characterized by realistic layouts, articulated objects,\nand rich physical interactivity tailored for embodied agents. Based on a\nconditional diffusion model for capturing scene layouts, we devise novel\nphysics- and interactivity-based guidance mechanisms that integrate constraints\nfrom object collision, room layout, and object reachability. Through extensive\nexperiments, we demonstrate that PhyScene effectively leverages these guidance\nfunctions for physically interactable scene synthesis, outperforming existing\nstate-of-the-art scene synthesis methods by a large margin. Our findings\nsuggest that the scenes generated by PhyScene hold considerable potential for\nfacilitating diverse skill acquisition among agents within interactive\nenvironments, thereby catalyzing further advancements in embodied AI research.\nProject website: http://physcene.github.io.\n","authors":["Yandan Yang","Baoxiong Jia","Peiyuan Zhi","Siyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2404.09465v1.pdf","comment":"Accepted by CVPR 2024, 18 pages"},{"id":"http://arxiv.org/abs/2404.09461v1","updated":"2024-04-15T05:00:40Z","published":"2024-04-15T05:00:40Z","title":"Improved Object-Based Style Transfer with Single Deep Network","summary":"  This research paper proposes a novel methodology for image-to-image style\ntransfer on objects utilizing a single deep convolutional neural network. The\nproposed approach leverages the You Only Look Once version 8 (YOLOv8)\nsegmentation model and the backbone neural network of YOLOv8 for style\ntransfer. The primary objective is to enhance the visual appeal of objects in\nimages by seamlessly transferring artistic styles while preserving the original\nobject characteristics. The proposed approach's novelty lies in combining\nsegmentation and style transfer in a single deep convolutional neural network.\nThis approach omits the need for multiple stages or models, thus resulting in\nsimpler training and deployment of the model for practical applications. The\nresults of this approach are shown on two content images by applying different\nstyle images. The paper also demonstrates the ability to apply style transfer\non multiple objects in the same image.\n","authors":["Harshmohan Kulkarni","Om Khare","Ninad Barve","Sunil Mane"],"pdf_url":"https://arxiv.org/pdf/2404.09461v1.pdf","comment":"In Proceedings of the Fourth International Conference on Innovations\n  in Computational Intelligence and Computer Vision"},{"id":"http://arxiv.org/abs/2303.09792v3","updated":"2024-04-15T04:58:07Z","published":"2023-03-17T06:26:55Z","title":"Exploring Sparse Visual Prompt for Domain Adaptive Dense Prediction","summary":"  The visual prompts have provided an efficient manner in addressing visual\ncross-domain problems. In previous works, Visual Domain Prompt (VDP) first\nintroduces domain prompts to tackle the classification Test-Time Adaptation\n(TTA) problem by warping image-level prompts on the input and fine-tuning\nprompts for each target domain. However, since the image-level prompts mask out\ncontinuous spatial details in the prompt-allocated region, it will suffer from\ninaccurate contextual information and limited domain knowledge extraction,\nparticularly when dealing with dense prediction TTA problems. To overcome these\nchallenges, we propose a novel Sparse Visual Domain Prompts (SVDP) approach,\nwhich holds minimal trainable parameters (e.g., 0.1\\%) in the image-level\nprompt and reserves more spatial information of the input. To better apply SVDP\nin extracting domain-specific knowledge, we introduce the Domain Prompt\nPlacement (DPP) method to adaptively allocates trainable parameters of SVDP on\nthe pixels with large distribution shifts. Furthermore, recognizing that each\ntarget domain sample exhibits a unique domain shift, we design Domain Prompt\nUpdating (DPU) strategy to optimize prompt parameters differently for each\nsample, facilitating efficient adaptation to the target domain. Extensive\nexperiments were conducted on widely-used TTA and continual TTA benchmarks, and\nour proposed method achieves state-of-the-art performance in both semantic\nsegmentation and depth estimation tasks.\n","authors":["Senqiao Yang","Jiarui Wu","Jiaming Liu","Xiaoqi Li","Qizhe Zhang","Mingjie Pan","Yulu Gan","Zehui Chen","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.09792v3.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2404.09458v1","updated":"2024-04-15T04:50:39Z","published":"2024-04-15T04:50:39Z","title":"CompGS: Efficient 3D Scene Representation via Compressed Gaussian\n  Splatting","summary":"  Gaussian splatting, renowned for its exceptional rendering quality and\nefficiency, has emerged as a prominent technique in 3D scene representation.\nHowever, the substantial data volume of Gaussian splatting impedes its\npractical utility in real-world applications. Herein, we propose an efficient\n3D scene representation, named Compressed Gaussian Splatting (CompGS), which\nharnesses compact Gaussian primitives for faithful 3D scene modeling with a\nremarkably reduced data size. To ensure the compactness of Gaussian primitives,\nwe devise a hybrid primitive structure that captures predictive relationships\nbetween each other. Then, we exploit a small set of anchor primitives for\nprediction, allowing the majority of primitives to be encapsulated into highly\ncompact residual forms. Moreover, we develop a rate-constrained optimization\nscheme to eliminate redundancies within such hybrid primitives, steering our\nCompGS towards an optimal trade-off between bitrate consumption and\nrepresentation efficacy. Experimental results show that the proposed CompGS\nsignificantly outperforms existing methods, achieving superior compactness in\n3D scene representation without compromising model accuracy and rendering\nquality. Our code will be released on GitHub for further research.\n","authors":["Xiangrui Liu","Xinju Wu","Pingping Zhang","Shiqi Wang","Zhu Li","Sam Kwong"],"pdf_url":"https://arxiv.org/pdf/2404.09458v1.pdf","comment":"Submitted to a conference"},{"id":"http://arxiv.org/abs/2404.09454v1","updated":"2024-04-15T04:43:53Z","published":"2024-04-15T04:43:53Z","title":"Utility-Fairness Trade-Offs and How to Find Them","summary":"  When building classification systems with demographic fairness\nconsiderations, there are two objectives to satisfy: 1) maximizing utility for\nthe specific task and 2) ensuring fairness w.r.t. a known demographic\nattribute. These objectives often compete, so optimizing both can lead to a\ntrade-off between utility and fairness. While existing works acknowledge the\ntrade-offs and study their limits, two questions remain unanswered: 1) What are\nthe optimal trade-offs between utility and fairness? and 2) How can we\nnumerically quantify these trade-offs from data for a desired prediction task\nand demographic attribute of interest? This paper addresses these questions. We\nintroduce two utility-fairness trade-offs: the Data-Space and Label-Space\nTrade-off. The trade-offs reveal three regions within the utility-fairness\nplane, delineating what is fully and partially possible and impossible. We\npropose U-FaTE, a method to numerically quantify the trade-offs for a given\nprediction task and group fairness definition from data samples. Based on the\ntrade-offs, we introduce a new scheme for evaluating representations. An\nextensive evaluation of fair representation learning methods and\nrepresentations from over 1000 pre-trained models revealed that most current\napproaches are far from the estimated and achievable fairness-utility\ntrade-offs across multiple datasets and prediction tasks.\n","authors":["Sepehr Dehdashtian","Bashir Sadeghi","Vishnu Naresh Boddeti"],"pdf_url":"https://arxiv.org/pdf/2404.09454v1.pdf","comment":"IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024"},{"id":"http://arxiv.org/abs/2404.05317v3","updated":"2024-04-15T04:37:44Z","published":"2024-04-08T09:08:43Z","title":"WebXR, A-Frame and Networked-Aframe as a Basis for an Open Metaverse: A\n  Conceptual Architecture","summary":"  This work proposes a WebXR-based cross-platform conceptual architecture,\nleveraging the A-Frame and Networked-Aframe frameworks, in order to facilitate\nthe development of an open, accessible, and interoperable metaverse. By\nintroducing the concept of spatial web app, this research contributes to the\ndiscourse on the metaverse, offering an architecture that democratizes access\nto virtual environments and extended reality through the web, and aligns with\nTim Berners-Lee's original vision of the World Wide Web as an open platform in\nthe digital realm.\n","authors":["Giuseppe Macario"],"pdf_url":"https://arxiv.org/pdf/2404.05317v3.pdf","comment":"updated section II-C (\"A-Frame\"), updated references"},{"id":"http://arxiv.org/abs/2404.09451v1","updated":"2024-04-15T04:31:24Z","published":"2024-04-15T04:31:24Z","title":"Contrastive Mean-Shift Learning for Generalized Category Discovery","summary":"  We address the problem of generalized category discovery (GCD) that aims to\npartition a partially labeled collection of images; only a small part of the\ncollection is labeled and the total number of target classes is unknown. To\naddress this generalized image clustering problem, we revisit the mean-shift\nalgorithm, i.e., a classic, powerful technique for mode seeking, and\nincorporate it into a contrastive learning framework. The proposed method,\ndubbed Contrastive Mean-Shift (CMS) learning, trains an image encoder to\nproduce representations with better clustering properties by an iterative\nprocess of mean shift and contrastive update. Experiments demonstrate that our\nmethod, both in settings with and without the total number of clusters being\nknown, achieves state-of-the-art performance on six public GCD benchmarks\nwithout bells and whistles.\n","authors":["Sua Choi","Dahyun Kang","Minsu Cho"],"pdf_url":"https://arxiv.org/pdf/2404.09451v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2404.09447v1","updated":"2024-04-15T04:20:01Z","published":"2024-04-15T04:20:01Z","title":"kNN-CLIP: Retrieval Enables Training-Free Segmentation on Continually\n  Expanding Large Vocabularies","summary":"  Rapid advancements in continual segmentation have yet to bridge the gap of\nscaling to large continually expanding vocabularies under compute-constrained\nscenarios. We discover that traditional continual training leads to\ncatastrophic forgetting under compute constraints, unable to outperform\nzero-shot segmentation methods. We introduce a novel strategy for semantic and\npanoptic segmentation with zero forgetting, capable of adapting to continually\ngrowing vocabularies without the need for retraining or large memory costs. Our\ntraining-free approach, kNN-CLIP, leverages a database of instance embeddings\nto enable open-vocabulary segmentation approaches to continually expand their\nvocabulary on any given domain with a single-pass through data, while only\nstoring embeddings minimizing both compute and memory costs. This method\nachieves state-of-the-art mIoU performance across large-vocabulary semantic and\npanoptic segmentation datasets. We hope kNN-CLIP represents a step forward in\nenabling more efficient and adaptable continual segmentation, paving the way\nfor advances in real-world large-vocabulary continual segmentation methods.\n","authors":["Zhongrui Gui","Shuyang Sun","Runjia Li","Jianhao Yuan","Zhaochong An","Karsten Roth","Ameya Prabhu","Philip Torr"],"pdf_url":"https://arxiv.org/pdf/2404.09447v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2404.09445v1","updated":"2024-04-15T04:14:42Z","published":"2024-04-15T04:14:42Z","title":"Exploring Text-to-Motion Generation with Human Preference","summary":"  This paper presents an exploration of preference learning in text-to-motion\ngeneration. We find that current improvements in text-to-motion generation\nstill rely on datasets requiring expert labelers with motion capture systems.\nInstead, learning from human preference data does not require motion capture\nsystems; a labeler with no expertise simply compares two generated motions.\nThis is particularly efficient because evaluating the model's output is easier\nthan gathering the motion that performs a desired task (e.g. backflip). To\npioneer the exploration of this paradigm, we annotate 3,528 preference pairs\ngenerated by MotionGPT, marking the first effort to investigate various\nalgorithms for learning from preference data. In particular, our exploration\nhighlights important design choices when using preference data. Additionally,\nour experimental results show that preference learning has the potential to\ngreatly improve current text-to-motion generative models. Our code and dataset\nare publicly available at\nhttps://github.com/THU-LYJ-Lab/InstructMotion}{https://github.com/THU-LYJ-Lab/InstructMotion\nto further facilitate research in this area.\n","authors":["Jenny Sheng","Matthieu Lin","Andrew Zhao","Kevin Pruvost","Yu-Hui Wen","Yangguang Li","Gao Huang","Yong-Jin Liu"],"pdf_url":"https://arxiv.org/pdf/2404.09445v1.pdf","comment":"Accepted to CVPR 2024 HuMoGen Workshop"},{"id":"http://arxiv.org/abs/2402.09055v3","updated":"2024-04-15T03:23:07Z","published":"2024-02-14T10:05:19Z","title":"Comment-aided Video-Language Alignment via Contrastive Pre-training for\n  Short-form Video Humor Detection","summary":"  The growing importance of multi-modal humor detection within affective\ncomputing correlates with the expanding influence of short-form video sharing\non social media platforms. In this paper, we propose a novel two-branch\nhierarchical model for short-form video humor detection (SVHD), named\nComment-aided Video-Language Alignment (CVLA) via data-augmented multi-modal\ncontrastive pre-training. Notably, our CVLA not only operates on raw signals\nacross various modal channels but also yields an appropriate multi-modal\nrepresentation by aligning the video and language components within a\nconsistent semantic space. The experimental results on two humor detection\ndatasets, including DY11k and UR-FUNNY, demonstrate that CVLA dramatically\noutperforms state-of-the-art and several competitive baseline approaches. Our\ndataset, code and model release at https://github.com/yliu-cs/CVLA.\n","authors":["Yang Liu","Tongfei Shen","Dong Zhang","Qingying Sun","Shoushan Li","Guodong Zhou"],"pdf_url":"https://arxiv.org/pdf/2402.09055v3.pdf","comment":"Accepted by ICMR 2024"},{"id":"http://arxiv.org/abs/2308.06603v3","updated":"2024-04-15T03:20:41Z","published":"2023-08-12T16:14:44Z","title":"LadleNet: A Two-Stage UNet for Infrared Image to Visible Image\n  Translation Guided by Semantic Segmentation","summary":"  The translation of thermal infrared (TIR) images into visible light (VI)\nimages plays a critical role in enhancing model performance and generalization\ncapability, particularly in various fields such as registration and fusion of\nTIR and VI images. However, current research in this field faces challenges of\ninsufficiently realistic image quality after translation and the difficulty of\nexisting models in adapting to unseen scenarios. In order to develop a more\ngeneralizable image translation architecture, we conducted an analysis of\nexisting translation architectures. By exploring the interpretability of\nintermediate modalities in existing translation architectures, we found that\nthe intermediate modality in the image translation process for street scene\nimages essentially performs semantic segmentation, distinguishing street images\nbased on background and foreground patterns before assigning color information.\nBased on these principles, we propose an improved algorithm based on U-net\ncalled LadleNet. This network utilizes a two-stage U-net concatenation\nstructure, consisting of Handle and Bowl modules. The Handle module is\nresponsible for constructing an abstract semantic space, while the Bowl module\ndecodes the semantic space to obtain the mapped VI image. Due to the\ncharacteristic of semantic segmentation, the Handle module has strong\nextensibility. Therefore, we also propose LadleNet+, which replaces the Handle\nmodule in LadleNet with a pre-trained DeepLabv3+ network, enabling the model to\nhave a more powerful capability in constructing semantic space. The proposed\nmethods were trained and tested on the KAIST dataset, followed by quantitative\nand qualitative analysis. Compared to existing methods, LadleNet and LadleNet+\nachieved an average improvement of 12.4% and 15.2% in SSIM metrics, and 37.9%\nand 50.6% in MS-SSIM metrics, respectively.\n","authors":["Tonghui Zou","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2308.06603v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09432v1","updated":"2024-04-15T03:12:17Z","published":"2024-04-15T03:12:17Z","title":"The 8th AI City Challenge","summary":"  The eighth AI City Challenge highlighted the convergence of computer vision\nand artificial intelligence in areas like retail, warehouse settings, and\nIntelligent Traffic Systems (ITS), presenting significant research\nopportunities. The 2024 edition featured five tracks, attracting unprecedented\ninterest from 726 teams in 47 countries and regions. Track 1 dealt with\nmulti-target multi-camera (MTMC) people tracking, highlighting significant\nenhancements in camera count, character number, 3D annotation, and camera\nmatrices, alongside new rules for 3D tracking and online tracking algorithm\nencouragement. Track 2 introduced dense video captioning for traffic safety,\nfocusing on pedestrian accidents using multi-camera feeds to improve insights\nfor insurance and prevention. Track 3 required teams to classify driver actions\nin a naturalistic driving analysis. Track 4 explored fish-eye camera analytics\nusing the FishEye8K dataset. Track 5 focused on motorcycle helmet rule\nviolation detection. The challenge utilized two leaderboards to showcase\nmethods, with participants setting new benchmarks, some surpassing existing\nstate-of-the-art achievements.\n","authors":["Shuo Wang","David C. Anastasiu","Zheng Tang","Ming-Ching Chang","Yue Yao","Liang Zheng","Mohammed Shaiqur Rahman","Meenakshi S. Arya","Anuj Sharma","Pranamesh Chakraborty","Sanjita Prajapati","Quan Kong","Norimasa Kobori","Munkhjargal Gochoo","Munkh-Erdene Otgonbold","Fady Alnajjar","Ganzorig Batnasan","Ping-Yang Chen","Jun-Wei Hsieh","Xunlei Wu","Sameer Satish Pusegaonkar","Yizhou Wang","Sujit Biswas","Rama Chellappa"],"pdf_url":"https://arxiv.org/pdf/2404.09432v1.pdf","comment":"Summary of the 8th AI City Challenge Workshop in conjunction with\n  CVPR 2024"},{"id":"http://arxiv.org/abs/2404.09431v1","updated":"2024-04-15T03:12:12Z","published":"2024-04-15T03:12:12Z","title":"VFMM3D: Releasing the Potential of Image by Vision Foundation Model for\n  Monocular 3D Object Detection","summary":"  Due to its cost-effectiveness and widespread availability, monocular 3D\nobject detection, which relies solely on a single camera during inference,\nholds significant importance across various applications, including autonomous\ndriving and robotics. Nevertheless, directly predicting the coordinates of\nobjects in 3D space from monocular images poses challenges. Therefore, an\neffective solution involves transforming monocular images into LiDAR-like\nrepresentations and employing a LiDAR-based 3D object detector to predict the\n3D coordinates of objects. The key step in this method is accurately converting\nthe monocular image into a reliable point cloud form. In this paper, we present\nVFMM3D, an innovative approach that leverages the capabilities of Vision\nFoundation Models (VFMs) to accurately transform single-view images into LiDAR\npoint cloud representations. VFMM3D utilizes the Segment Anything Model (SAM)\nand Depth Anything Model (DAM) to generate high-quality pseudo-LiDAR data\nenriched with rich foreground information. Specifically, the Depth Anything\nModel (DAM) is employed to generate dense depth maps. Subsequently, the Segment\nAnything Model (SAM) is utilized to differentiate foreground and background\nregions by predicting instance masks. These predicted instance masks and depth\nmaps are then combined and projected into 3D space to generate pseudo-LiDAR\npoints. Finally, any object detectors based on point clouds can be utilized to\npredict the 3D coordinates of objects. Comprehensive experiments are conducted\non the challenging 3D object detection dataset KITTI. Our VFMM3D establishes a\nnew state-of-the-art performance. Additionally, experimental results\ndemonstrate the generality of VFMM3D, showcasing its seamless integration into\nvarious LiDAR-based 3D object detectors.\n","authors":["Bonan Ding","Jin Xie","Jing Nie","Jiale Cao"],"pdf_url":"https://arxiv.org/pdf/2404.09431v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2307.09220v2","updated":"2024-04-15T02:47:01Z","published":"2023-07-18T12:52:49Z","title":"A Survey on Open-Vocabulary Detection and Segmentation: Past, Present,\n  and Future","summary":"  As the most fundamental scene understanding tasks, object detection and\nsegmentation have made tremendous progress in deep learning era. Due to the\nexpensive manual labeling cost, the annotated categories in existing datasets\nare often small-scale and pre-defined, i.e., state-of-the-art fully-supervised\ndetectors and segmentors fail to generalize beyond the closed vocabulary. To\nresolve this limitation, in the last few years, the community has witnessed an\nincreasing attention toward Open-Vocabulary Detection (OVD) and Segmentation\n(OVS). By ``open-vocabulary'', we mean that the models can classify objects\nbeyond pre-defined categories. In this survey, we provide a comprehensive\nreview on recent developments of OVD and OVS. A taxonomy is first developed to\norganize different tasks and methodologies. We find that the permission and\nusage of weak supervision signals can well discriminate different\nmethodologies, including: visual-semantic space mapping, novel visual feature\nsynthesis, region-aware training, pseudo-labeling, knowledge distillation, and\ntransfer learning. The proposed taxonomy is universal across different tasks,\ncovering object detection, semantic/instance/panoptic segmentation, 3D and\nvideo understanding. The main design principles, key challenges, development\nroutes, methodology strengths, and weaknesses are thoroughly analyzed. In\naddition, we benchmark each task along with the vital components of each method\nin appendix and updated online at\nhttps://github.com/seanzhuh/awesome-open-vocabulary-detection-and-segmentation.\nFinally, several promising directions are provided and discussed to stimulate\nfuture research.\n","authors":["Chaoyang Zhu","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2307.09220v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09426v1","updated":"2024-04-15T02:44:23Z","published":"2024-04-15T02:44:23Z","title":"ViFu: Multiple 360$^\\circ$ Objects Reconstruction with Clean Background\n  via Visible Part Fusion","summary":"  In this paper, we propose a method to segment and recover a static, clean\nbackground and multiple 360$^\\circ$ objects from observations of scenes at\ndifferent timestamps. Recent works have used neural radiance fields to model 3D\nscenes and improved the quality of novel view synthesis, while few studies have\nfocused on modeling the invisible or occluded parts of the training images.\nThese under-reconstruction parts constrain both scene editing and rendering\nview selection, thereby limiting their utility for synthetic data generation\nfor downstream tasks. Our basic idea is that, by observing the same set of\nobjects in various arrangement, so that parts that are invisible in one scene\nmay become visible in others. By fusing the visible parts from each scene,\nocclusion-free rendering of both background and foreground objects can be\nachieved.\n  We decompose the multi-scene fusion task into two main components: (1)\nobjects/background segmentation and alignment, where we leverage point\ncloud-based methods tailored to our novel problem formulation; (2) radiance\nfields fusion, where we introduce visibility field to quantify the visible\ninformation of radiance fields, and propose visibility-aware rendering for the\nfusion of series of scenes, ultimately obtaining clean background and\n360$^\\circ$ object rendering. Comprehensive experiments were conducted on\nsynthetic and real datasets, and the results demonstrate the effectiveness of\nour method.\n","authors":["Tianhan Xu","Takuya Ikeda","Koichi Nishiwaki"],"pdf_url":"https://arxiv.org/pdf/2404.09426v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2404.09425v1","updated":"2024-04-15T02:41:55Z","published":"2024-04-15T02:41:55Z","title":"Super-resolution of biomedical volumes with 2D supervision","summary":"  Volumetric biomedical microscopy has the potential to increase the diagnostic\ninformation extracted from clinical tissue specimens and improve the diagnostic\naccuracy of both human pathologists and computational pathology models.\nUnfortunately, barriers to integrating 3-dimensional (3D) volumetric microscopy\ninto clinical medicine include long imaging times, poor depth / z-axis\nresolution, and an insufficient amount of high-quality volumetric data.\nLeveraging the abundance of high-resolution 2D microscopy data, we introduce\nmasked slice diffusion for super-resolution (MSDSR), which exploits the\ninherent equivalence in the data-generating distribution across all spatial\ndimensions of biological specimens. This intrinsic characteristic allows for\nsuper-resolution models trained on high-resolution images from one plane (e.g.,\nXY) to effectively generalize to others (XZ, YZ), overcoming the traditional\ndependency on orientation. We focus on the application of MSDSR to stimulated\nRaman histology (SRH), an optical imaging modality for biological specimen\nanalysis and intraoperative diagnosis, characterized by its rapid acquisition\nof high-resolution 2D images but slow and costly optical z-sectioning. To\nevaluate MSDSR's efficacy, we introduce a new performance metric, SliceFID, and\ndemonstrate MSDSR's superior performance over baseline models through extensive\nevaluations. Our findings reveal that MSDSR not only significantly enhances the\nquality and resolution of 3D volumetric data, but also addresses major\nobstacles hindering the broader application of 3D volumetric microscopy in\nclinical diagnostics and biomedical research.\n","authors":["Cheng Jiang","Alexander Gedeon","Yiwei Lyu","Eric Landgraf","Yufeng Zhang","Xinhai Hou","Akhil Kondepudi","Asadur Chowdury","Honglak Lee","Todd Hollon"],"pdf_url":"https://arxiv.org/pdf/2404.09425v1.pdf","comment":"CVPR Workshop on Computer Vision for Microscopy Image Analysis 2024"},{"id":"http://arxiv.org/abs/2404.07487v2","updated":"2024-04-15T02:25:22Z","published":"2024-04-11T05:51:06Z","title":"Fine-Grained Side Information Guided Dual-Prompts for Zero-Shot Skeleton\n  Action Recognition","summary":"  Skeleton-based zero-shot action recognition aims to recognize unknown human\nactions based on the learned priors of the known skeleton-based actions and a\nsemantic descriptor space shared by both known and unknown categories. However,\nprevious works focus on establishing the bridges between the known skeleton\nrepresentation space and semantic descriptions space at the coarse-grained\nlevel for recognizing unknown action categories, ignoring the fine-grained\nalignment of these two spaces, resulting in suboptimal performance in\ndistinguishing high-similarity action categories. To address these challenges,\nwe propose a novel method via Side information and dual-prompts learning for\nskeleton-based zero-shot action recognition (STAR) at the fine-grained level.\nSpecifically, 1) we decompose the skeleton into several parts based on its\ntopology structure and introduce the side information concerning multi-part\ndescriptions of human body movements for alignment between the skeleton and the\nsemantic space at the fine-grained level; 2) we design the visual-attribute and\nsemantic-part prompts to improve the intra-class compactness within the\nskeleton space and inter-class separability within the semantic space,\nrespectively, to distinguish the high-similarity actions. Extensive experiments\nshow that our method achieves state-of-the-art performance in ZSL and GZSL\nsettings on NTU RGB+D, NTU RGB+D 120, and PKU-MMD datasets.\n","authors":["Yang Chen","Jingcai Guo","Tian He","Ling Wang"],"pdf_url":"https://arxiv.org/pdf/2404.07487v2.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2404.08449v2","updated":"2024-04-15T02:10:45Z","published":"2024-04-12T13:00:06Z","title":"OccGaussian: 3D Gaussian Splatting for Occluded Human Rendering","summary":"  Rendering dynamic 3D human from monocular videos is crucial for various\napplications such as virtual reality and digital entertainment. Most methods\nassume the people is in an unobstructed scene, while various objects may cause\nthe occlusion of body parts in real-life scenarios. Previous method utilizing\nNeRF for surface rendering to recover the occluded areas, but it requiring more\nthan one day to train and several seconds to render, failing to meet the\nrequirements of real-time interactive applications. To address these issues, we\npropose OccGaussian based on 3D Gaussian Splatting, which can be trained within\n6 minutes and produces high-quality human renderings up to 160 FPS with\noccluded input. OccGaussian initializes 3D Gaussian distributions in the\ncanonical space, and we perform occlusion feature query at occluded regions,\nthe aggregated pixel-align feature is extracted to compensate for the missing\ninformation. Then we use Gaussian Feature MLP to further process the feature\nalong with the occlusion-aware loss functions to better perceive the occluded\narea. Extensive experiments both in simulated and real-world occlusions,\ndemonstrate that our method achieves comparable or even superior performance\ncompared to the state-of-the-art method. And we improving training and\ninference speeds by 250x and 800x, respectively. Our code will be available for\nresearch purposes.\n","authors":["Jingrui Ye","Zongkai Zhang","Yujiao Jiang","Qingmin Liao","Wenming Yang","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2404.08449v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09415v1","updated":"2024-04-15T02:02:15Z","published":"2024-04-15T02:02:15Z","title":"A Review on Machine Learning Algorithms for Dust Aerosol Detection using\n  Satellite Data","summary":"  Dust storms are associated with certain respiratory illnesses across\ndifferent areas in the world. Researchers have devoted time and resources to\nstudy the elements surrounding dust storm phenomena. This paper reviews the\nefforts of those who have investigated dust aerosols using sensors onboard of\nsatellites using machine learning-based approaches. We have reviewed the most\ncommon issues revolving dust aerosol modeling using different datasets and\ndifferent sensors from a historical perspective. Our findings suggest that\nmulti-spectral approaches based on linear and non-linear combinations of\nspectral bands are some of the most successful for visualization and\nquantitative analysis; however, when researchers have leveraged machine\nlearning, performance has been improved and new opportunities to solve unique\nproblems arise.\n","authors":["Nurul Rafi","Pablo Rivas"],"pdf_url":"https://arxiv.org/pdf/2404.09415v1.pdf","comment":"The 23rd International Conference on Artificial Intelligence (ICAI\n  2021)"},{"id":"http://arxiv.org/abs/2404.09412v1","updated":"2024-04-15T01:58:54Z","published":"2024-04-15T01:58:54Z","title":"DeferredGS: Decoupled and Editable Gaussian Splatting with Deferred\n  Shading","summary":"  Reconstructing and editing 3D objects and scenes both play crucial roles in\ncomputer graphics and computer vision. Neural radiance fields (NeRFs) can\nachieve realistic reconstruction and editing results but suffer from\ninefficiency in rendering. Gaussian splatting significantly accelerates\nrendering by rasterizing Gaussian ellipsoids. However, Gaussian splatting\nutilizes a single Spherical Harmonic (SH) function to model both texture and\nlighting, limiting independent editing capabilities of these components.\nRecently, attempts have been made to decouple texture and lighting with the\nGaussian splatting representation but may fail to produce plausible geometry\nand decomposition results on reflective scenes. Additionally, the forward\nshading technique they employ introduces noticeable blending artifacts during\nrelighting, as the geometry attributes of Gaussians are optimized under the\noriginal illumination and may not be suitable for novel lighting conditions. To\naddress these issues, we introduce DeferredGS, a method for decoupling and\nediting the Gaussian splatting representation using deferred shading. To\nachieve successful decoupling, we model the illumination with a learnable\nenvironment map and define additional attributes such as texture parameters and\nnormal direction on Gaussians, where the normal is distilled from a jointly\ntrained signed distance function. More importantly, we apply deferred shading,\nresulting in more realistic relighting effects compared to previous methods.\nBoth qualitative and quantitative experiments demonstrate the superior\nperformance of DeferredGS in novel view synthesis and editing tasks.\n","authors":["Tong Wu","Jia-Mu Sun","Yu-Kun Lai","Yuewen Ma","Leif Kobbelt","Lin Gao"],"pdf_url":"https://arxiv.org/pdf/2404.09412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.13313v2","updated":"2024-04-15T01:49:23Z","published":"2023-12-20T09:16:47Z","title":"ParamISP: Learned Forward and Inverse ISPs using Camera Parameters","summary":"  RAW images are rarely shared mainly due to its excessive data size compared\nto their sRGB counterparts obtained by camera ISPs. Learning the forward and\ninverse processes of camera ISPs has been recently demonstrated, enabling\nphysically-meaningful RAW-level image processing on input sRGB images. However,\nexisting learning-based ISP methods fail to handle the large variations in the\nISP processes with respect to camera parameters such as ISO and exposure time,\nand have limitations when used for various applications. In this paper, we\npropose ParamISP, a learning-based method for forward and inverse conversion\nbetween sRGB and RAW images, that adopts a novel neural-network module to\nutilize camera parameters, which is dubbed as ParamNet. Given the camera\nparameters provided in the EXIF data, ParamNet converts them into a feature\nvector to control the ISP networks. Extensive experiments demonstrate that\nParamISP achieve superior RAW and sRGB reconstruction results compared to\nprevious methods and it can be effectively used for a variety of applications\nsuch as deblurring dataset synthesis, raw deblurring, HDR reconstruction, and\ncamera-to-camera transfer.\n","authors":["Woohyeok Kim","Geonu Kim","Junyong Lee","Seungyong Lee","Seung-Hwan Baek","Sunghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2312.13313v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09406v1","updated":"2024-04-15T01:47:44Z","published":"2024-04-15T01:47:44Z","title":"Human-in-the-Loop Segmentation of Multi-species Coral Imagery","summary":"  Broad-scale marine surveys performed by underwater vehicles significantly\nincrease the availability of coral reef imagery, however it is costly and\ntime-consuming for domain experts to label images. Point label propagation is\nan approach used to leverage existing image data labeled with sparse point\nlabels. The resulting augmented ground truth generated is then used to train a\nsemantic segmentation model. Here, we first demonstrate that recent advances in\nfoundation models enable generation of multi-species coral augmented ground\ntruth masks using denoised DINOv2 features and K-Nearest Neighbors (KNN),\nwithout the need for any pre-training or custom-designed algorithms. For\nextremely sparsely labeled images, we propose a labeling regime based on\nhuman-in-the-loop principles, resulting in significant improvement in\nannotation efficiency: If only 5 point labels per image are available, our\nproposed human-in-the-loop approach improves on the state-of-the-art by 17.3%\nfor pixel accuracy and 22.6% for mIoU; and by 10.6% and 19.1% when 10 point\nlabels per image are available. Even if the human-in-the-loop labeling regime\nis not used, the denoised DINOv2 features with a KNN outperforms the prior\nstate-of-the-art by 3.5% for pixel accuracy and 5.7% for mIoU (5 grid points).\nWe also provide a detailed analysis of how point labeling style and the\nquantity of points per image affects the point label propagation quality and\nprovide general recommendations on maximizing point label efficiency.\n","authors":["Scarlett Raine","Ross Marchant","Brano Kusy","Frederic Maire","Niko Suenderhauf","Tobias Fischer"],"pdf_url":"https://arxiv.org/pdf/2404.09406v1.pdf","comment":"10 pages, 6 figures, an additional 4 pages of supplementary material"},{"id":"http://arxiv.org/abs/2307.11259v2","updated":"2024-04-15T01:31:57Z","published":"2023-07-20T22:35:27Z","title":"Investigating Low Data, Confidence Aware Image Prediction on Smooth\n  Repetitive Videos using Gaussian Processes","summary":"  The ability to predict future states is crucial to informed decision-making\nwhile interacting with dynamic environments. With cameras providing a prevalent\nand information-rich sensing modality, the problem of predicting future states\nfrom image sequences has garnered a lot of attention. Current state-of-the-art\nmethods typically train large parametric models for their predictions. Though\noften able to predict with accuracy these models often fail to provide\ninterpretable confidence metrics around their predictions. Additionally these\nmethods are reliant on the availability of large training datasets to converge\nto useful solutions. In this paper, we focus on the problem of predicting\nfuture images of an image sequence with interpretable confidence bounds from\nvery little training data. To approach this problem, we use non-parametric\nmodels to take a probabilistic approach to image prediction. We generate\nprobability distributions over sequentially predicted images, and propagate\nuncertainty through time to generate a confidence metric for our predictions.\nGaussian Processes are used for their data efficiency and ability to readily\nincorporate new training data online. Our methods predictions are evaluated on\na smooth fluid simulation environment. We showcase the capabilities of our\napproach on real world data by predicting pedestrian flows and weather patterns\nfrom satellite imagery.\n","authors":["Nikhil U. Shinde","Xiao Liang","Florian Richter","Michael C. Yip"],"pdf_url":"https://arxiv.org/pdf/2307.11259v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09401v1","updated":"2024-04-15T01:27:07Z","published":"2024-04-15T01:27:07Z","title":"Watermark-embedded Adversarial Examples for Copyright Protection against\n  Diffusion Models","summary":"  Diffusion Models (DMs) have shown remarkable capabilities in various\nimage-generation tasks. However, there are growing concerns that DMs could be\nused to imitate unauthorized creations and thus raise copyright issues. To\naddress this issue, we propose a novel framework that embeds personal\nwatermarks in the generation of adversarial examples. Such examples can force\nDMs to generate images with visible watermarks and prevent DMs from imitating\nunauthorized images. We construct a generator based on conditional adversarial\nnetworks and design three losses (adversarial loss, GAN loss, and perturbation\nloss) to generate adversarial examples that have subtle perturbation but can\neffectively attack DMs to prevent copyright violations. Training a generator\nfor a personal watermark by our method only requires 5-10 samples within 2-3\nminutes, and once the generator is trained, it can generate adversarial\nexamples with that watermark significantly fast (0.2s per image). We conduct\nextensive experiments in various conditional image-generation scenarios.\nCompared to existing methods that generate images with chaotic textures, our\nmethod adds visible watermarks on the generated images, which is a more\nstraightforward way to indicate copyright violations. We also observe that our\nadversarial examples exhibit good transferability across unknown generative\nmodels. Therefore, this work provides a simple yet powerful way to protect\ncopyright from DM-based imitation.\n","authors":["Peifei Zhu","Tsubasa Takahashi","Hirokatsu Kataoka"],"pdf_url":"https://arxiv.org/pdf/2404.09401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00513v3","updated":"2024-04-15T01:15:34Z","published":"2024-03-31T01:20:16Z","title":"Transformer based Pluralistic Image Completion with Reduced Information\n  Loss","summary":"  Transformer based methods have achieved great success in image inpainting\nrecently. However, we find that these solutions regard each pixel as a token,\nthus suffering from an information loss issue from two aspects: 1) They\ndownsample the input image into much lower resolutions for efficiency\nconsideration. 2) They quantize $256^3$ RGB values to a small number (such as\n512) of quantized color values. The indices of quantized pixels are used as\ntokens for the inputs and prediction targets of the transformer. To mitigate\nthese issues, we propose a new transformer based framework called \"PUT\".\nSpecifically, to avoid input downsampling while maintaining computation\nefficiency, we design a patch-based auto-encoder P-VQVAE. The encoder converts\nthe masked image into non-overlapped patch tokens and the decoder recovers the\nmasked regions from the inpainted tokens while keeping the unmasked regions\nunchanged. To eliminate the information loss caused by input quantization, an\nUn-quantized Transformer is applied. It directly takes features from the\nP-VQVAE encoder as input without any quantization and only regards the\nquantized tokens as prediction targets. Furthermore, to make the inpainting\nprocess more controllable, we introduce semantic and structural conditions as\nextra guidance. Extensive experiments show that our method greatly outperforms\nexisting transformer based methods on image fidelity and achieves much higher\ndiversity and better fidelity than state-of-the-art pluralistic inpainting\nmethods on complex large-scale datasets (e.g., ImageNet). Codes are available\nat https://github.com/liuqk3/PUT.\n","authors":["Qiankun Liu","Yuqi Jiang","Zhentao Tan","Dongdong Chen","Ying Fu","Qi Chu","Gang Hua","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2404.00513v3.pdf","comment":"Accepted by TPAMI (2024). arXiv admin note: text overlap with\n  arXiv:2205.05076"},{"id":"http://arxiv.org/abs/2104.00170v3","updated":"2024-04-15T01:03:11Z","published":"2021-04-01T00:14:45Z","title":"Are Bias Mitigation Techniques for Deep Learning Effective?","summary":"  A critical problem in deep learning is that systems learn inappropriate\nbiases, resulting in their inability to perform well on minority groups. This\nhas led to the creation of multiple algorithms that endeavor to mitigate bias.\nHowever, it is not clear how effective these methods are. This is because study\nprotocols differ among papers, systems are tested on datasets that fail to test\nmany forms of bias, and systems have access to hidden knowledge or are tuned\nspecifically to the test set. To address this, we introduce an improved\nevaluation protocol, sensible metrics, and a new dataset, which enables us to\nask and answer critical questions about bias mitigation algorithms. We evaluate\nseven state-of-the-art algorithms using the same network architecture and\nhyperparameter selection policy across three benchmark datasets. We introduce a\nnew dataset called Biased MNIST that enables assessment of robustness to\nmultiple bias sources. We use Biased MNIST and a visual question answering\n(VQA) benchmark to assess robustness to hidden biases. Rather than only tuning\nto the test set distribution, we study robustness across different tuning\ndistributions, which is critical because for many applications the test\ndistribution may not be known during development. We find that algorithms\nexploit hidden biases, are unable to scale to multiple forms of bias, and are\nhighly sensitive to the choice of tuning set. Based on our findings, we implore\nthe community to adopt more rigorous assessment of future bias mitigation\nmethods. All data, code, and results are publicly available at:\nhttps://github.com/erobic/bias-mitigators.\n","authors":["Robik Shrestha","Kushal Kafle","Christopher Kanan"],"pdf_url":"https://arxiv.org/pdf/2104.00170v3.pdf","comment":"WACV 2022"},{"id":"http://arxiv.org/abs/2404.09389v1","updated":"2024-04-15T00:19:47Z","published":"2024-04-15T00:19:47Z","title":"Masked and Shuffled Blind Spot Denoising for Real-World Images","summary":"  We introduce a novel approach to single image denoising based on the Blind\nSpot Denoising principle, which we call MAsked and SHuffled Blind Spot\nDenoising (MASH). We focus on the case of correlated noise, which often plagues\nreal images. MASH is the result of a careful analysis to determine the\nrelationships between the level of blindness (masking) of the input and the\n(unknown) noise correlation. Moreover, we introduce a shuffling technique to\nweaken the local correlation of noise, which in turn yields an additional\ndenoising performance improvement. We evaluate MASH via extensive experiments\non real-world noisy image datasets. We demonstrate on par or better results\ncompared to existing self-supervised denoising methods.\n","authors":["Hamadi Chihaoui","Paolo Favaro"],"pdf_url":"https://arxiv.org/pdf/2404.09389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09387v1","updated":"2024-04-15T00:12:27Z","published":"2024-04-15T00:12:27Z","title":"RankCLIP: Ranking-Consistent Language-Image Pretraining","summary":"  Among the ever-evolving development of vision-language models, contrastive\nlanguage-image pretraining (CLIP) has set new benchmarks in many downstream\ntasks such as zero-shot classifications by leveraging self-supervised\ncontrastive learning on large amounts of text-image pairs. However, its\ndependency on rigid one-to-one mappings overlooks the complex and often\nmultifaceted relationships between and within texts and images. To this end, we\nintroduce RankCLIP, a novel pretraining method that extends beyond the rigid\none-to-one matching framework of CLIP and its variants. By leveraging both\nin-modal and cross-modal ranking consistency, RankCLIP improves the alignment\nprocess, enabling it to capture the nuanced many-to-many relationships between\nand within each modality. Through comprehensive experiments, we demonstrate the\nenhanced capability of RankCLIP to effectively improve performance across\nvarious downstream tasks, notably achieving significant gains in zero-shot\nclassifications over state-of-the-art methods, underscoring the potential of\nRankCLIP in further advancing vision-language pretraining.\n","authors":["Yiming Zhang","Zhuokai Zhao","Zhaorun Chen","Zhili Feng","Zenghui Ding","Yining Sun"],"pdf_url":"https://arxiv.org/pdf/2404.09387v1.pdf","comment":"10 pages, 3 figures, 6 tables. Code and model checkpoints are\n  available at https://github.com/Jam1ezhang/RankCLIP"},{"id":"http://arxiv.org/abs/2310.17347v3","updated":"2024-04-15T23:52:11Z","published":"2023-10-26T12:27:56Z","title":"CADS: Unleashing the Diversity of Diffusion Models through\n  Condition-Annealed Sampling","summary":"  While conditional diffusion models are known to have good coverage of the\ndata distribution, they still face limitations in output diversity,\nparticularly when sampled with a high classifier-free guidance scale for\noptimal image quality or when trained on small datasets. We attribute this\nproblem to the role of the conditioning signal in inference and offer an\nimproved sampling strategy for diffusion models that can increase generation\ndiversity, especially at high guidance scales, with minimal loss of sample\nquality. Our sampling strategy anneals the conditioning signal by adding\nscheduled, monotonically decreasing Gaussian noise to the conditioning vector\nduring inference to balance diversity and condition alignment. Our\nCondition-Annealed Diffusion Sampler (CADS) can be used with any pretrained\nmodel and sampling algorithm, and we show that it boosts the diversity of\ndiffusion models in various conditional generation tasks. Further, using an\nexisting pretrained diffusion model, CADS achieves a new state-of-the-art FID\nof 1.70 and 2.31 for class-conditional ImageNet generation at 256$\\times$256\nand 512$\\times$512 respectively.\n","authors":["Seyedmorteza Sadat","Jakob Buhmann","Derek Bradley","Otmar Hilliges","Romann M. Weber"],"pdf_url":"https://arxiv.org/pdf/2310.17347v3.pdf","comment":"Published as a conference paper at ICLR 2024"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2404.09709v1","updated":"2024-04-15T12:08:44Z","published":"2024-04-15T12:08:44Z","title":"Scenario-Adaptive Fine-Grained Personalization Network: Tailoring User\n  Behavior Representation to the Scenario Context","summary":"  Existing methods often adjust representations adaptively only after\naggregating user behavior sequences. This coarse-grained approach to\nre-weighting the entire user sequence hampers the model's ability to accurately\nmodel the user interest migration across different scenarios. To enhance the\nmodel's capacity to capture user interests from historical behavior sequences\nin each scenario, we develop a ranking framework named the Scenario-Adaptive\nFine-Grained Personalization Network (SFPNet), which designs a kind of\nfine-grained method for multi-scenario personalized recommendations.\nSpecifically, SFPNet comprises a series of blocks named as Scenario-Tailoring\nBlock, stacked sequentially. Each block initially deploys a parameter\npersonalization unit to integrate scenario information at a coarse-grained\nlevel by redefining fundamental features. Subsequently, we consolidate\nscenario-adaptively adjusted feature representations to serve as context\ninformation. By employing residual connection, we incorporate this context into\nthe representation of each historical behavior, allowing for context-aware\nfine-grained customization of the behavior representations at the\nscenario-level, which in turn supports scenario-aware user interest modeling.\n","authors":["Moyu Zhang","Yongxiang Tang","Jinxin Hu","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.09709v1.pdf","comment":"Accepted by SIGIR 2024, 10 pages, 5 figures, 5 tables"},{"id":"http://arxiv.org/abs/2312.09591v2","updated":"2024-04-15T11:42:36Z","published":"2023-12-15T08:01:55Z","title":"Explicitly Integrating Judgment Prediction with Legal Document\n  Retrieval: A Law-Guided Generative Approach","summary":"  Legal document retrieval and judgment prediction are crucial tasks in\nintelligent legal systems. In practice, determining whether two documents share\nthe same judgments is essential for establishing their relevance in legal\nretrieval. However, existing legal retrieval studies either ignore the vital\nrole of judgment prediction or rely on implicit training objectives, expecting\na proper alignment of legal documents in vector space based on their judgments.\nNeither approach provides explicit evidence of judgment consistency for\nrelevance modeling, leading to inaccuracies and a lack of transparency in\nretrieval. To address this issue, we propose a law-guided method, namely GEAR,\nwithin the generative retrieval framework. GEAR explicitly integrates judgment\nprediction with legal document retrieval in a sequence-to-sequence manner.\nExperiments on two Chinese legal case retrieval datasets show the superiority\nof GEAR over state-of-the-art methods while maintaining competitive judgment\nprediction performance. Moreover, we validate its robustness across languages\nand domains on a French statutory article retrieval dataset.\n","authors":["Weicong Qin","Zelin Cao","Weijie Yu","Zihua Si","Sirui Chen","Jun Xu"],"pdf_url":"https://arxiv.org/pdf/2312.09591v2.pdf","comment":"Accepted by SIGIR'2024"},{"id":"http://arxiv.org/abs/2404.09578v1","updated":"2024-04-15T08:39:52Z","published":"2024-04-15T08:39:52Z","title":"Recall-Augmented Ranking: Enhancing Click-Through Rate Prediction\n  Accuracy with Cross-Stage Data","summary":"  Click-through rate (CTR) prediction plays an indispensable role in online\nplatforms. Numerous models have been proposed to capture users' shifting\npreferences by leveraging user behavior sequences. However, these historical\nsequences often suffer from severe homogeneity and scarcity compared to the\nextensive item pool. Relying solely on such sequences for user representations\nis inherently restrictive, as user interests extend beyond the scope of items\nthey have previously engaged with. To address this challenge, we propose a\ndata-driven approach to enrich user representations. We recognize user\nprofiling and recall items as two ideal data sources within the cross-stage\nframework, encompassing the u2u (user-to-user) and i2i (item-to-item) aspects\nrespectively. In this paper, we propose a novel architecture named\nRecall-Augmented Ranking (RAR). RAR consists of two key sub-modules, which\nsynergistically gather information from a vast pool of look-alike users and\nrecall items, resulting in enriched user representations. Notably, RAR is\northogonal to many existing CTR models, allowing for consistent performance\nimprovements in a plug-and-play manner. Extensive experiments are conducted,\nwhich verify the efficacy and compatibility of RAR against the SOTA methods.\n","authors":["Junjie Huang","Guohao Cai","Jieming Zhu","Zhenhua Dong","Ruiming Tang","Weinan Zhang","Yong Yu"],"pdf_url":"https://arxiv.org/pdf/2404.09578v1.pdf","comment":"4 pages, accepted by WWW 2024 Short Track"},{"id":"http://arxiv.org/abs/2404.09520v1","updated":"2024-04-15T07:31:19Z","published":"2024-04-15T07:31:19Z","title":"UniSAR: Modeling User Transition Behaviors between Search and\n  Recommendation","summary":"  Nowadays, many platforms provide users with both search and recommendation\nservices as important tools for accessing information. The phenomenon has led\nto a correlation between user search and recommendation behaviors, providing an\nopportunity to model user interests in a fine-grained way. Existing approaches\neither model user search and recommendation behaviors separately or overlook\nthe different transitions between user search and recommendation behaviors. In\nthis paper, we propose a framework named UniSAR that effectively models the\ndifferent types of fine-grained behavior transitions for providing users a\nUnified Search And Recommendation service. Specifically, UniSAR models the user\ntransition behaviors between search and recommendation through three steps:\nextraction, alignment, and fusion, which are respectively implemented by\ntransformers equipped with pre-defined masks, contrastive learning that aligns\nthe extracted fine-grained user transitions, and cross-attentions that fuse\ndifferent transitions. To provide users with a unified service, the learned\nrepresentations are fed into the downstream search and recommendation models.\nJoint learning on both search and recommendation data is employed to utilize\nthe knowledge and enhance each other. Experimental results on two public\ndatasets demonstrated the effectiveness of UniSAR in terms of enhancing both\nsearch and recommendation simultaneously. The experimental analysis further\nvalidates that UniSAR enhances the results by successfully modeling the user\ntransition behaviors between search and recommendation.\n","authors":["Teng Shi","Zihua Si","Jun Xu","Xiao Zhang","Xiaoxue Zang","Kai Zheng","Dewei Leng","Yanan Niu","Yang Song"],"pdf_url":"https://arxiv.org/pdf/2404.09520v1.pdf","comment":"Accepted by SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.17416v2","updated":"2024-04-15T06:05:50Z","published":"2024-03-26T06:14:19Z","title":"AFDGCF: Adaptive Feature De-correlation Graph Collaborative Filtering\n  for Recommendations","summary":"  Collaborative filtering methods based on graph neural networks (GNNs) have\nwitnessed significant success in recommender systems (RS), capitalizing on\ntheir ability to capture collaborative signals within intricate user-item\nrelationships via message-passing mechanisms. However, these GNN-based RS\ninadvertently introduce excess linear correlation between user and item\nembeddings, contradicting the goal of providing personalized recommendations.\nWhile existing research predominantly ascribes this flaw to the over-smoothing\nproblem, this paper underscores the critical, often overlooked role of the\nover-correlation issue in diminishing the effectiveness of GNN representations\nand subsequent recommendation performance. Up to now, the over-correlation\nissue remains unexplored in RS. Meanwhile, how to mitigate the impact of\nover-correlation while preserving collaborative filtering signals is a\nsignificant challenge. To this end, this paper aims to address the\naforementioned gap by undertaking a comprehensive study of the over-correlation\nissue in graph collaborative filtering models. Firstly, we present empirical\nevidence to demonstrate the widespread prevalence of over-correlation in these\nmodels. Subsequently, we dive into a theoretical analysis which establishes a\npivotal connection between the over-correlation and over-smoothing issues.\nLeveraging these insights, we introduce the Adaptive Feature De-correlation\nGraph Collaborative Filtering (AFDGCF) framework, which dynamically applies\ncorrelation penalties to the feature dimensions of the representation matrix,\neffectively alleviating both over-correlation and over-smoothing issues. The\nefficacy of the proposed framework is corroborated through extensive\nexperiments conducted with four representative graph collaborative filtering\nmodels across four publicly available datasets.\n","authors":["Wei Wu","Chao Wang","Dazhong Shen","Chuan Qin","Liyi Chen","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2403.17416v2.pdf","comment":"Accepted by SIGIR2024"},{"id":"http://arxiv.org/abs/2404.09473v1","updated":"2024-04-15T05:56:13Z","published":"2024-04-15T05:56:13Z","title":"Exploring the Nexus Between Retrievability and Query Generation\n  Strategies","summary":"  Quantifying bias in retrieval functions through document retrievability\nscores is vital for assessing recall-oriented retrieval systems. However, many\nstudies investigating retrieval model bias lack validation of their query\ngeneration methods as accurate representations of retrievability for real users\nand their queries. This limitation results from the absence of established\ncriteria for query generation in retrievability assessments. Typically,\nresearchers resort to using frequent collocations from document corpora when no\nquery log is available. In this study, we address the issue of reproducibility\nand seek to validate query generation methods by comparing retrievability\nscores generated from artificially generated queries to those derived from\nquery logs. Our findings demonstrate a minimal or negligible correlation\nbetween retrievability scores from artificial queries and those from query\nlogs. This suggests that artificially generated queries may not accurately\nreflect retrievability scores as derived from query logs. We further explore\nalternative query generation techniques, uncovering a variation that exhibits\nthe highest correlation. This alternative approach holds promise for improving\nreproducibility when query logs are unavailable.\n","authors":["Aman Sinha","Priyanshu Raj Mall","Dwaipayan Roy"],"pdf_url":"https://arxiv.org/pdf/2404.09473v1.pdf","comment":"Accepted at ECIR 2024"},{"id":"http://arxiv.org/abs/2305.13859v3","updated":"2024-04-15T05:42:31Z","published":"2023-05-23T09:30:36Z","title":"Generative Retrieval via Term Set Generation","summary":"  Recently, generative retrieval emerges as a promising alternative to\ntraditional retrieval paradigms. It assigns each document a unique identifier,\nknown as DocID, and employs a generative model to directly generate the\nrelevant DocID for the input query. A common choice for DocID is one or several\nnatural language sequences, e.g. the title or n-grams, so that the pre-trained\nknowledge of the generative model can be utilized. However, a sequence is\ngenerated token by token, where only the most likely candidates are kept and\nthe rest are pruned at each decoding step, thus, retrieval fails if any token\nwithin the relevant DocID is falsely pruned. What's worse, during decoding, the\nmodel can only perceive preceding tokens in DocID while being blind to\nsubsequent ones, hence is prone to make such errors. To address this problem,\nwe present a novel framework for generative retrieval, dubbed Term-Set\nGeneration (TSGen). Instead of sequences, we use a set of terms as DocID, which\nare automatically selected to concisely summarize the document's semantics and\ndistinguish it from others. On top of the term-set DocID, we propose a\npermutation-invariant decoding algorithm, with which the term set can be\ngenerated in any permutation yet will always lead to the corresponding\ndocument. Remarkably, TSGen perceives all valid terms rather than only the\npreceding ones at each decoding step. Given the constant decoding space, it can\nmake more reliable decisions due to the broader perspective. TSGen is also\nresilient to errors: the relevant DocID will not be pruned as long as the\ndecoded term belongs to it. Lastly, we design an iterative optimization\nprocedure to incentivize the model to generate the relevant term set in its\nfavorable permutation. We conduct extensive experiments on popular benchmarks,\nwhich validate the effectiveness, the generalizability, the scalability, and\nthe efficiency of TSGen.\n","authors":["Peitian Zhang","Zheng Liu","Yujia Zhou","Zhicheng Dou","Fangchao Liu","Zhao Cao"],"pdf_url":"https://arxiv.org/pdf/2305.13859v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.02046v3","updated":"2024-04-15T04:18:34Z","published":"2023-07-05T06:03:40Z","title":"Recommender Systems in the Era of Large Language Models (LLMs)","summary":"  With the prosperity of e-commerce and web applications, Recommender Systems\n(RecSys) have become an important component of our daily life, providing\npersonalized suggestions that cater to user preferences. While Deep Neural\nNetworks (DNNs) have made significant advancements in enhancing recommender\nsystems by modeling user-item interactions and incorporating textual side\ninformation, DNN-based methods still face limitations, such as difficulties in\nunderstanding users' interests and capturing textual side information,\ninabilities in generalizing to various recommendation scenarios and reasoning\non their predictions, etc. Meanwhile, the emergence of Large Language Models\n(LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural\nLanguage Processing (NLP) and Artificial Intelligence (AI), due to their\nremarkable abilities in fundamental responsibilities of language understanding\nand generation, as well as impressive generalization and reasoning\ncapabilities. As a result, recent studies have attempted to harness the power\nof LLMs to enhance recommender systems. Given the rapid evolution of this\nresearch direction in recommender systems, there is a pressing need for a\nsystematic overview that summarizes existing LLM-empowered recommender systems,\nto provide researchers in relevant fields with an in-depth understanding.\nTherefore, in this paper, we conduct a comprehensive review of LLM-empowered\nrecommender systems from various aspects including Pre-training, Fine-tuning,\nand Prompting. More specifically, we first introduce representative methods to\nharness the power of LLMs (as a feature encoder) for learning representations\nof users and items. Then, we review recent techniques of LLMs for enhancing\nrecommender systems from three paradigms, namely pre-training, fine-tuning, and\nprompting. Finally, we comprehensively discuss future directions in this\nemerging field.\n","authors":["Wenqi Fan","Zihuai Zhao","Jiatong Li","Yunqing Liu","Xiaowei Mei","Yiqi Wang","Zhen Wen","Fei Wang","Xiangyu Zhao","Jiliang Tang","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2307.02046v3.pdf","comment":"Accepted by IEEE TKDE"},{"id":"http://arxiv.org/abs/2404.07581v2","updated":"2024-04-15T03:42:01Z","published":"2024-04-11T09:13:52Z","title":"M-scan: A Multi-Scenario Causal-driven Adaptive Network for\n  Recommendation","summary":"  We primarily focus on the field of multi-scenario recommendation, which poses\na significant challenge in effectively leveraging data from different scenarios\nto enhance predictions in scenarios with limited data. Current mainstream\nefforts mainly center around innovative model network architectures, with the\naim of enabling the network to implicitly acquire knowledge from diverse\nscenarios. However, the uncertainty of implicit learning in networks arises\nfrom the absence of explicit modeling, leading to not only difficulty in\ntraining but also incomplete user representation and suboptimal performance.\nFurthermore, through causal graph analysis, we have discovered that the\nscenario itself directly influences click behavior, yet existing approaches\ndirectly incorporate data from other scenarios during the training of the\ncurrent scenario, leading to prediction biases when they directly utilize click\nbehaviors from other scenarios to train models. To address these problems, we\npropose the Multi-Scenario Causal-driven Adaptive Network M-scan). This model\nincorporates a Scenario-Aware Co-Attention mechanism that explicitly extracts\nuser interests from other scenarios that align with the current scenario.\nAdditionally, it employs a Scenario Bias Eliminator module utilizing causal\ncounterfactual inference to mitigate biases introduced by data from other\nscenarios. Extensive experiments on two public datasets demonstrate the\nefficacy of our M-scan compared to the existing baseline models.\n","authors":["Jiachen Zhu","Yichao Wang","Jianghao Lin","Jiarui Qin","Ruiming Tang","Weinan Zhang","Yong Yu"],"pdf_url":"https://arxiv.org/pdf/2404.07581v2.pdf","comment":"This paper has been accepted by WWW'24"},{"id":"http://arxiv.org/abs/2404.08361v2","updated":"2024-04-15T01:50:25Z","published":"2024-04-12T09:57:17Z","title":"Large-Scale Multi-Domain Recommendation: an Automatic Domain Feature\n  Extraction and Personalized Integration Framework","summary":"  Feed recommendation is currently the mainstream mode for many real-world\napplications (e.g., TikTok, Dianping), it is usually necessary to model and\npredict user interests in multiple scenarios (domains) within and even outside\nthe application. Multi-domain learning is a typical solution in this regard.\nWhile considerable efforts have been made in this regard, there are still two\nlong-standing challenges: (1) Accurately depicting the differences among\ndomains using domain features is crucial for enhancing the performance of each\ndomain. However, manually designing domain features and models for numerous\ndomains can be a laborious task. (2) Users typically have limited impressions\nin only a few domains. Extracting features automatically from other domains and\nleveraging them to improve the predictive capabilities of each domain has\nconsistently posed a challenging problem. In this paper, we propose an\nAutomatic Domain Feature Extraction and Personalized Integration (DFEI)\nframework for the large-scale multi-domain recommendation. The framework\nautomatically transforms the behavior of each individual user into an\naggregation of all user behaviors within the domain, which serves as the domain\nfeatures. Unlike offline feature engineering methods, the extracted domain\nfeatures are higher-order representations and directly related to the target\nlabel. Besides, by personalized integration of domain features from other\ndomains for each user and the innovation in the training mode, the DFEI\nframework can yield more accurate conversion identification. Experimental\nresults on both public and industrial datasets, consisting of over 20 domains,\nclearly demonstrate that the proposed framework achieves significantly better\nperformance compared with SOTA baselines. Furthermore, we have released the\nsource code of the proposed framework at https://github.com/xidongbo/DFEI.\n","authors":["Dongbo Xi","Zhen Chen","Yuexian Wang","He Cui","Chong Peng","Fuzhen Zhuang","Peng Yan"],"pdf_url":"https://arxiv.org/pdf/2404.08361v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2311.05800v2","updated":"2024-04-15T22:11:33Z","published":"2023-11-10T00:17:10Z","title":"Leveraging LLMs for Synthesizing Training Data Across Many Languages in\n  Multilingual Dense Retrieval","summary":"  There has been limited success for dense retrieval models in multilingual\nretrieval, due to uneven and scarce training data available across multiple\nlanguages. Synthetic training data generation is promising (e.g., InPars or\nPromptagator), but has been investigated only for English. Therefore, to study\nmodel capabilities across both cross-lingual and monolingual retrieval tasks,\nwe develop SWIM-IR, a synthetic retrieval training dataset containing 33 (high\nto very-low resource) languages for fine-tuning multilingual dense retrievers\nwithout requiring any human supervision. To construct SWIM-IR, we propose SAP\n(summarize-then-ask prompting), where the large language model (LLM) generates\na textual summary prior to the query generation step. SAP assists the LLM in\ngenerating informative queries in the target language. Using SWIM-IR, we\nexplore synthetic fine-tuning of multilingual dense retrieval models and\nevaluate them robustly on three retrieval benchmarks: XOR-Retrieve\n(cross-lingual), MIRACL (monolingual) and XTREME-UP (cross-lingual). Our\nmodels, called SWIM-X, are competitive with human-supervised dense retrieval\nmodels, e.g., mContriever-X, finding that SWIM-IR can cheaply substitute for\nexpensive human-labeled retrieval training data. SWIM-IR dataset and SWIM-X\nmodels are available at https://github.com/google-research-datasets/SWIM-IR.\n","authors":["Nandan Thakur","Jianmo Ni","Gustavo Hern√°ndez √Åbrego","John Wieting","Jimmy Lin","Daniel Cer"],"pdf_url":"https://arxiv.org/pdf/2311.05800v2.pdf","comment":"Accepted at NAACL 2024. Data released at\n  https://github.com/google-research-datasets/swim-ir"},{"id":"http://arxiv.org/abs/2404.10150v1","updated":"2024-04-15T21:42:20Z","published":"2024-04-15T21:42:20Z","title":"TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table\n  Decomposition","summary":"  Table reasoning is a challenging task that requires understanding both\nnatural language questions and structured tabular data. Large language models\n(LLMs) have shown impressive capabilities in natural language understanding and\ngeneration, but they often struggle with large tables due to their limited\ninput length. In this paper, we propose TabSQLify, a novel method that\nleverages text-to-SQL generation to decompose tables into smaller and relevant\nsub-tables, containing only essential information for answering questions or\nverifying statements, before performing the reasoning task. In our\ncomprehensive evaluation on four challenging datasets, our approach\ndemonstrates comparable or superior performance compared to prevailing methods\nreliant on full tables as input. Moreover, our method can reduce the input\ncontext length significantly, making it more scalable and efficient for\nlarge-scale table reasoning applications. Our method performs remarkably well\non the WikiTQ benchmark, achieving an accuracy of 64.7%. Additionally, on the\nTabFact benchmark, it achieves a high accuracy of 79.5%. These results surpass\nother LLM-based baseline models on gpt-3.5-turbo (chatgpt). TabSQLify can\nreduce the table size significantly alleviating the computational load on LLMs\nwhen handling large tables without compromising performance.\n","authors":["Md Mahadi Hasan Nahid","Davood Rafiei"],"pdf_url":"https://arxiv.org/pdf/2404.10150v1.pdf","comment":"Accepted to NAACL 2024 (long, main)"},{"id":"http://arxiv.org/abs/2404.10097v1","updated":"2024-04-15T19:08:48Z","published":"2024-04-15T19:08:48Z","title":"LegalPro-BERT: Classification of Legal Provisions by fine-tuning BERT\n  Large Language Model","summary":"  A contract is a type of legal document commonly used in organizations.\nContract review is an integral and repetitive process to avoid business risk\nand liability. Contract analysis requires the identification and classification\nof key provisions and paragraphs within an agreement. Identification and\nvalidation of contract clauses can be a time-consuming and challenging task\ndemanding the services of trained and expensive lawyers, paralegals or other\nlegal assistants. Classification of legal provisions in contracts using\nartificial intelligence and natural language processing is complex due to the\nrequirement of domain-specialized legal language for model training and the\nscarcity of sufficient labeled data in the legal domain. Using general-purpose\nmodels is not effective in this context due to the use of specialized legal\nvocabulary in contracts which may not be recognized by a general model. To\naddress this problem, we propose the use of a pre-trained large language model\nwhich is subsequently calibrated on legal taxonomy. We propose LegalPro-BERT, a\nBERT transformer architecture model that we fine- tune to efficiently handle\nclassification task for legal provisions. We conducted experiments to measure\nand compare metrics with current benchmark results. We found that LegalPro-BERT\noutperforms the previous benchmark used for comparison in this research.\n","authors":["Amit Tewari"],"pdf_url":"https://arxiv.org/pdf/2404.10097v1.pdf","comment":"17 pages, 4 figures"},{"id":"http://arxiv.org/abs/2404.09980v1","updated":"2024-04-15T17:56:39Z","published":"2024-04-15T17:56:39Z","title":"Context Does Matter: Implications for Crowdsourced Evaluation Labels in\n  Task-Oriented Dialogue Systems","summary":"  Crowdsourced labels play a crucial role in evaluating task-oriented dialogue\nsystems (TDSs). Obtaining high-quality and consistent ground-truth labels from\nannotators presents challenges. When evaluating a TDS, annotators must fully\ncomprehend the dialogue before providing judgments. Previous studies suggest\nusing only a portion of the dialogue context in the annotation process.\nHowever, the impact of this limitation on label quality remains unexplored.\nThis study investigates the influence of dialogue context on annotation\nquality, considering the truncated context for relevance and usefulness\nlabeling. We further propose to use large language models (LLMs) to summarize\nthe dialogue context to provide a rich and short description of the dialogue\ncontext and study the impact of doing so on the annotator's performance.\nReducing context leads to more positive ratings. Conversely, providing the\nentire dialogue context yields higher-quality relevance ratings but introduces\nambiguity in usefulness ratings. Using the first user utterance as context\nleads to consistent ratings, akin to those obtained using the entire dialogue,\nwith significantly reduced annotation effort. Our findings show how task\ndesign, particularly the availability of dialogue context, affects the quality\nand consistency of crowdsourced evaluation labels.\n","authors":["Clemencia Siro","Mohammad Aliannejadi","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2404.09980v1.pdf","comment":"Accepted at NAACL 2024 Findings"},{"id":"http://arxiv.org/abs/2404.09889v1","updated":"2024-04-15T15:55:01Z","published":"2024-04-15T15:55:01Z","title":"Is Table Retrieval a Solved Problem? Join-Aware Multi-Table Retrieval","summary":"  Retrieving relevant tables containing the necessary information to accurately\nanswer a given question over tables is critical to open-domain\nquestion-answering (QA) systems. Previous methods assume the answer to such a\nquestion can be found either in a single table or multiple tables identified\nthrough question decomposition or rewriting. However, neither of these\napproaches is sufficient, as many questions require retrieving multiple tables\nand joining them through a join plan that cannot be discerned from the user\nquery itself. If the join plan is not considered in the retrieval stage, the\nsubsequent steps of reasoning and answering based on those retrieved tables are\nlikely to be incorrect. To address this problem, we introduce a method that\nuncovers useful join relations for any query and database during table\nretrieval. We use a novel re-ranking method formulated as a mixed-integer\nprogram that considers not only table-query relevance but also table-table\nrelevance that requires inferring join relationships. Our method outperforms\nthe state-of-the-art approaches for table retrieval by up to 9.3% in F1 score\nand for end-to-end QA by up to 5.4% in accuracy.\n","authors":["Peter Baile Chen","Yi Zhang","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2404.09889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.12798v2","updated":"2024-04-15T14:47:12Z","published":"2024-01-23T14:31:12Z","title":"Gradient Flow of Energy: A General and Efficient Approach for Entity\n  Alignment Decoding","summary":"  Entity alignment (EA), a pivotal process in integrating multi-source\nKnowledge Graphs (KGs), seeks to identify equivalent entity pairs across these\ngraphs. Most existing approaches regard EA as a graph representation learning\ntask, concentrating on enhancing graph encoders. However, the decoding process\nin EA - essential for effective operation and alignment accuracy - has received\nlimited attention and remains tailored to specific datasets and model\narchitectures, necessitating both entity and additional explicit relation\nembeddings. This specificity limits its applicability, particularly in\nGNN-based models. To address this gap, we introduce a novel, generalized, and\nefficient decoding approach for EA, relying solely on entity embeddings. Our\nmethod optimizes the decoding process by minimizing Dirichlet energy, leading\nto the gradient flow within the graph, to maximize graph homophily. The\ndiscretization of the gradient flow produces a fast and scalable approach,\ntermed Triple Feature Propagation (TFP). TFP innovatively generalizes adjacency\nmatrices to multi-views matrices:entity-to-entity, entity-to-relation,\nrelation-to-entity, and relation-to-triple. The gradient flow through\ngeneralized matrices enables TFP to harness the multi-view structural\ninformation of KGs. Rigorous experimentation on diverse public datasets\ndemonstrates that our approach significantly enhances various EA methods.\nNotably, the approach achieves these advancements with less than 6 seconds of\nadditional computational time, establishing a new benchmark in efficiency and\nadaptability for future EA methods.\n","authors":["Yuanyi Wang","Haifeng Sun","Jingyu Wang","Qi Qi","Shaoling Sun","Jianxin Liao"],"pdf_url":"https://arxiv.org/pdf/2401.12798v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10097v1","updated":"2024-04-15T19:08:48Z","published":"2024-04-15T19:08:48Z","title":"LegalPro-BERT: Classification of Legal Provisions by fine-tuning BERT\n  Large Language Model","summary":"  A contract is a type of legal document commonly used in organizations.\nContract review is an integral and repetitive process to avoid business risk\nand liability. Contract analysis requires the identification and classification\nof key provisions and paragraphs within an agreement. Identification and\nvalidation of contract clauses can be a time-consuming and challenging task\ndemanding the services of trained and expensive lawyers, paralegals or other\nlegal assistants. Classification of legal provisions in contracts using\nartificial intelligence and natural language processing is complex due to the\nrequirement of domain-specialized legal language for model training and the\nscarcity of sufficient labeled data in the legal domain. Using general-purpose\nmodels is not effective in this context due to the use of specialized legal\nvocabulary in contracts which may not be recognized by a general model. To\naddress this problem, we propose the use of a pre-trained large language model\nwhich is subsequently calibrated on legal taxonomy. We propose LegalPro-BERT, a\nBERT transformer architecture model that we fine-tune to efficiently handle\nclassification task for legal provisions. We conducted experiments to measure\nand compare metrics with current benchmark results. We found that LegalPro-BERT\noutperforms the previous benchmark used for comparison in this research.\n","authors":["Amit Tewari"],"pdf_url":"https://arxiv.org/pdf/2404.10097v1.pdf","comment":"17 pages, 4 figures"},{"id":"http://arxiv.org/abs/2009.10128v2","updated":"2024-04-15T14:22:57Z","published":"2020-09-21T18:46:00Z","title":"Claraprint: a chord and melody based fingerprint for western classical\n  music cover detection","summary":"  Cover song detection has been an active field in the Music Information\nRetrieval (MIR) community during the past decades. Most of the research\ncommunity focused in solving it for a wide range of music genres with diverse\ncharacteristics. Western classical music, a genre heavily based on the\nrecording of \"cover songs\", or musical works, represents a large heritage,\noffering immediate application for an efficient fingerprint algorithm. We\npropose an engineering approach for retrieving a cover song from a reference\ndatabase thanks to a fingerprint designed for classical musical works. We open\na new data set to encourage the scientific community to use it for further\nresearches regarding this genre.\n","authors":["Micka√´l Arcos"],"pdf_url":"https://arxiv.org/pdf/2009.10128v2.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2404.09753v1","updated":"2024-04-15T12:54:31Z","published":"2024-04-15T12:54:31Z","title":"Personalized Collaborative Fine-Tuning for On-Device Large Language\n  Models","summary":"  We explore on-device self-supervised collaborative fine-tuning of large\nlanguage models with limited local data availability. Taking inspiration from\nthe collaborative learning community, we introduce three distinct\ntrust-weighted gradient aggregation schemes: weight similarity-based,\nprediction similarity-based and validation performance-based. To minimize\ncommunication overhead, we integrate Low-Rank Adaptation (LoRA) and only\nexchange LoRA weight updates. Our protocols, driven by prediction and\nperformance metrics, surpass both FedAvg and local fine-tuning methods, which\nis particularly evident in realistic scenarios with more diverse local data\ndistributions. The results underscore the effectiveness of our approach in\naddressing heterogeneity and scarcity within local datasets.\n","authors":["Nicolas Wagner","Dongyang Fan","Martin Jaggi"],"pdf_url":"https://arxiv.org/pdf/2404.09753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09752v1","updated":"2024-04-15T12:53:48Z","published":"2024-04-15T12:53:48Z","title":"Can We Break Free from Strong Data Augmentations in Self-Supervised\n  Learning?","summary":"  Self-supervised learning (SSL) has emerged as a promising solution for\naddressing the challenge of limited labeled data in deep neural networks\n(DNNs), offering scalability potential. However, the impact of design\ndependencies within the SSL framework remains insufficiently investigated. In\nthis study, we comprehensively explore SSL behavior across a spectrum of\naugmentations, revealing their crucial role in shaping SSL model performance\nand learning mechanisms. Leveraging these insights, we propose a novel learning\napproach that integrates prior knowledge, with the aim of curtailing the need\nfor extensive data augmentations and thereby amplifying the efficacy of learned\nrepresentations. Notably, our findings underscore that SSL models imbued with\nprior knowledge exhibit reduced texture bias, diminished reliance on shortcuts\nand augmentations, and improved robustness against both natural and adversarial\ncorruptions. These findings not only illuminate a new direction in SSL\nresearch, but also pave the way for enhancing DNN performance while\nconcurrently alleviating the imperative for intensive data augmentation,\nthereby enhancing scalability and real-world problem-solving capabilities.\n","authors":["Shruthi Gowda","Elahe Arani","Bahram Zonooz"],"pdf_url":"https://arxiv.org/pdf/2404.09752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09737v1","updated":"2024-04-15T12:38:46Z","published":"2024-04-15T12:38:46Z","title":"Quantization of Large Language Models with an Overdetermined Basis","summary":"  In this paper, we introduce an algorithm for data quantization based on the\nprinciples of Kashin representation. This approach hinges on decomposing any\ngiven vector, matrix, or tensor into two factors. The first factor maintains a\nsmall infinity norm, while the second exhibits a similarly constrained norm\nwhen multiplied by an orthogonal matrix. Surprisingly, the entries of factors\nafter decomposition are well-concentrated around several peaks, which allows us\nto efficiently replace them with corresponding centroids for quantization\npurposes. We study the theoretical properties of the proposed approach and\nrigorously evaluate our compression algorithm in the context of next-word\nprediction tasks and on a set of downstream tasks for text classification. Our\nfindings demonstrate that Kashin Quantization achieves competitive or superior\nquality in model performance while ensuring data compression, marking a\nsignificant advancement in the field of data quantization.\n","authors":["Daniil Merkulov","Daria Cherniuk","Alexander Rudikov","Ivan Oseledets","Ekaterina Muravleva","Aleksandr Mikhalev","Boris Kashin"],"pdf_url":"https://arxiv.org/pdf/2404.09737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09730v1","updated":"2024-04-15T12:29:28Z","published":"2024-04-15T12:29:28Z","title":"Convergence Analysis of Probability Flow ODE for Score-based Generative\n  Models","summary":"  Score-based generative models have emerged as a powerful approach for\nsampling high-dimensional probability distributions. Despite their\neffectiveness, their theoretical underpinnings remain relatively\nunderdeveloped. In this work, we study the convergence properties of\ndeterministic samplers based on probability flow ODEs from both theoretical and\nnumerical perspectives. Assuming access to $L^2$-accurate estimates of the\nscore function, we prove the total variation between the target and the\ngenerated data distributions can be bounded above by\n$\\mathcal{O}(d\\sqrt{\\delta})$ in the continuous time level, where $d$ denotes\nthe data dimension and $\\delta$ represents the $L^2$-score matching error. For\npractical implementations using a $p$-th order Runge-Kutta integrator with step\nsize $h$, we establish error bounds of $\\mathcal{O}(d(\\sqrt{\\delta} + (dh)^p))$\nat the discrete level. Finally, we present numerical studies on problems up to\n$128$ dimensions to verify our theory, which indicate a better score matching\nerror and dimension dependence.\n","authors":["Daniel Zhengyu Huang","Jiaoyang Huang","Zhengjiang Lin"],"pdf_url":"https://arxiv.org/pdf/2404.09730v1.pdf","comment":"33 pages, 7 figures"},{"id":"http://arxiv.org/abs/2404.09729v1","updated":"2024-04-15T12:29:16Z","published":"2024-04-15T12:29:16Z","title":"Amplitude-Phase Fusion for Enhanced Electrocardiogram Morphological\n  Analysis","summary":"  Considering the variability of amplitude and phase patterns in\nelectrocardiogram (ECG) signals due to cardiac activity and individual\ndifferences, existing entropy-based studies have not fully utilized these two\npatterns and lack integration. To address this gap, this paper proposes a novel\nfusion entropy metric, morphological ECG entropy (MEE) for the first time,\nspecifically designed for ECG morphology, to comprehensively describe the\nfusion of amplitude and phase patterns. MEE is computed based on beat-level\nsamples, enabling detailed analysis of each cardiac cycle. Experimental results\ndemonstrate that MEE achieves rapid, accurate, and label-free localization of\nabnormal ECG arrhythmia regions. Furthermore, MEE provides a method for\nassessing sample diversity, facilitating compression of imbalanced training\nsets (via representative sample selection), and outperforms random pruning.\nAdditionally, MEE exhibits the ability to describe areas of poor quality. By\ndiscussing, it proves the robustness of MEE value calculation to noise\ninterference and its low computational complexity. Finally, we integrate this\nmethod into a clinical interactive interface to provide a more convenient and\nintuitive user experience. These findings indicate that MEE serves as a\nvaluable clinical descriptor for ECG characterization. The implementation code\ncan be referenced at the following link:\nhttps://github.com/fdu-harry/ECG-MEE-metric.\n","authors":["Shuaicong Hu","Yanan Wang","Jian Liu","Jingyu Lin","Shengmei Qin","Zhenning Nie","Zhifeng Yao","Wenjie Cai","Cuiwei Yang"],"pdf_url":"https://arxiv.org/pdf/2404.09729v1.pdf","comment":"16 pages, 12 figures"},{"id":"http://arxiv.org/abs/2404.09722v1","updated":"2024-04-15T12:25:41Z","published":"2024-04-15T12:25:41Z","title":"VFLGAN: Vertical Federated Learning-based Generative Adversarial Network\n  for Vertically Partitioned Data Publication","summary":"  In the current artificial intelligence (AI) era, the scale and quality of the\ndataset play a crucial role in training a high-quality AI model. However, good\ndata is not a free lunch and is always hard to access due to privacy\nregulations like the General Data Protection Regulation (GDPR). A potential\nsolution is to release a synthetic dataset with a similar distribution to that\nof the private dataset. Nevertheless, in some scenarios, it has been found that\nthe attributes needed to train an AI model belong to different parties, and\nthey cannot share the raw data for synthetic data publication due to privacy\nregulations. In PETS 2023, Xue et al. proposed the first generative adversary\nnetwork-based model, VertiGAN, for vertically partitioned data publication.\nHowever, after thoroughly investigating, we found that VertiGAN is less\neffective in preserving the correlation among the attributes of different\nparties. This article proposes a Vertical Federated Learning-based Generative\nAdversarial Network, VFLGAN, for vertically partitioned data publication to\naddress the above issues. Our experimental results show that compared with\nVertiGAN, VFLGAN significantly improves the quality of synthetic data. Taking\nthe MNIST dataset as an example, the quality of the synthetic dataset generated\nby VFLGAN is 3.2 times better than that generated by VertiGAN w.r.t. the\nFr\\'echet Distance. We also designed a more efficient and effective Gaussian\nmechanism for the proposed VFLGAN to provide the synthetic dataset with a\ndifferential privacy guarantee. On the other hand, differential privacy only\ngives the upper bound of the worst-case privacy guarantee. This article also\nproposes a practical auditing scheme that applies membership inference attacks\nto estimate privacy leakage through the synthetic dataset.\n","authors":["Xun Yuan","Yang Yang","Prosanta Gope","Aryan Pasikhani","Biplab Sikdar"],"pdf_url":"https://arxiv.org/pdf/2404.09722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09717v1","updated":"2024-04-15T12:20:09Z","published":"2024-04-15T12:20:09Z","title":"Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\n  Large Language Model","summary":"  Many recent studies endeavor to improve open-source language models through\nimitation learning, and re-training on the synthetic instruction data from\nstate-of-the-art proprietary models like ChatGPT and GPT-4. However, the innate\nnature of synthetic data inherently contains noisy data, giving rise to a\nsubstantial presence of low-quality data replete with erroneous responses, and\nflawed reasoning. Although we intuitively grasp the potential harm of noisy\ndata, we lack a quantitative understanding of its impact. To this end, this\npaper explores the correlation between the degree of noise and its impact on\nlanguage models through instruction tuning. We first introduce the\nFalsity-Controllable (FACO) dataset, which comprises pairs of true answers with\ncorresponding reasoning, as well as false pairs to manually control the falsity\nratio of the dataset.Through our extensive experiments, we found multiple\nintriguing findings of the correlation between the factuality of the dataset\nand instruction tuning: Specifically, we verified falsity of the instruction is\nhighly relevant to various benchmark scores. Moreover, when LLMs are trained\nwith false instructions, they learn to lie and generate fake unfaithful\nanswers, even though they know the correct answer for the user request.\nAdditionally, we noted that once the language model is trained with a dataset\ncontaminated by noise, restoring its original performance is possible, but it\nfailed to reach full performance.\n","authors":["Hyunsoo Cho"],"pdf_url":"https://arxiv.org/pdf/2404.09717v1.pdf","comment":"Under review @ *ACL"},{"id":"http://arxiv.org/abs/2404.09715v1","updated":"2024-04-15T12:18:09Z","published":"2024-04-15T12:18:09Z","title":"Higher Replay Ratio Empowers Sample-Efficient Multi-Agent Reinforcement\n  Learning","summary":"  One of the notorious issues for Reinforcement Learning (RL) is poor sample\nefficiency. Compared to single agent RL, the sample efficiency for Multi-Agent\nReinforcement Learning (MARL) is more challenging because of its inherent\npartial observability, non-stationary training, and enormous strategy space.\nAlthough much effort has been devoted to developing new methods and enhancing\nsample efficiency, we look at the widely used episodic training mechanism. In\neach training step, tens of frames are collected, but only one gradient step is\nmade. We argue that this episodic training could be a source of poor sample\nefficiency. To better exploit the data already collected, we propose to\nincrease the frequency of the gradient updates per environment interaction\n(a.k.a. Replay Ratio or Update-To-Data ratio). To show its generality, we\nevaluate $3$ MARL methods on $6$ SMAC tasks. The empirical results validate\nthat a higher replay ratio significantly improves the sample efficiency for\nMARL algorithms. The codes to reimplement the results presented in this paper\nare open-sourced at https://anonymous.4open.science/r/rr_for_MARL-0D83/.\n","authors":["Linjie Xu","Zichuan Liu","Alexander Dockhorn","Diego Perez-Liebana","Jinyu Wang","Lei Song","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2404.09715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09709v1","updated":"2024-04-15T12:08:44Z","published":"2024-04-15T12:08:44Z","title":"Scenario-Adaptive Fine-Grained Personalization Network: Tailoring User\n  Behavior Representation to the Scenario Context","summary":"  Existing methods often adjust representations adaptively only after\naggregating user behavior sequences. This coarse-grained approach to\nre-weighting the entire user sequence hampers the model's ability to accurately\nmodel the user interest migration across different scenarios. To enhance the\nmodel's capacity to capture user interests from historical behavior sequences\nin each scenario, we develop a ranking framework named the Scenario-Adaptive\nFine-Grained Personalization Network (SFPNet), which designs a kind of\nfine-grained method for multi-scenario personalized recommendations.\nSpecifically, SFPNet comprises a series of blocks named as Scenario-Tailoring\nBlock, stacked sequentially. Each block initially deploys a parameter\npersonalization unit to integrate scenario information at a coarse-grained\nlevel by redefining fundamental features. Subsequently, we consolidate\nscenario-adaptively adjusted feature representations to serve as context\ninformation. By employing residual connection, we incorporate this context into\nthe representation of each historical behavior, allowing for context-aware\nfine-grained customization of the behavior representations at the\nscenario-level, which in turn supports scenario-aware user interest modeling.\n","authors":["Moyu Zhang","Yongxiang Tang","Jinxin Hu","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.09709v1.pdf","comment":"Accepted by SIGIR 2024, 10 pages, 5 figures, 5 tables"},{"id":"http://arxiv.org/abs/2404.09708v1","updated":"2024-04-15T12:06:22Z","published":"2024-04-15T12:06:22Z","title":"Kernel-based learning with guarantees for multi-agent applications","summary":"  This paper addresses a kernel-based learning problem for a network of agents\nlocally observing a latent multidimensional, nonlinear phenomenon in a noisy\nenvironment. We propose a learning algorithm that requires only mild a priori\nknowledge about the phenomenon under investigation and delivers a model with\ncorresponding non-asymptotic high probability error bounds. Both non-asymptotic\nanalysis of the method and numerical simulation results are presented and\ndiscussed in the paper.\n","authors":["Krzysztof Kowalczyk","Pawe≈Ç Wachel","Cristian R. Rojas"],"pdf_url":"https://arxiv.org/pdf/2404.09708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09707v1","updated":"2024-04-15T12:06:00Z","published":"2024-04-15T12:06:00Z","title":"Adaptive Patching for High-resolution Image Segmentation with\n  Transformers","summary":"  Attention-based models are proliferating in the space of image analytics,\nincluding segmentation. The standard method of feeding images to transformer\nencoders is to divide the images into patches and then feed the patches to the\nmodel as a linear sequence of tokens. For high-resolution images, e.g.\nmicroscopic pathology images, the quadratic compute and memory cost prohibits\nthe use of an attention-based model, if we are to use smaller patch sizes that\nare favorable in segmentation. The solution is to either use custom complex\nmulti-resolution models or approximate attention schemes. We take inspiration\nfrom Adapative Mesh Refinement (AMR) methods in HPC by adaptively patching the\nimages, as a pre-processing step, based on the image details to reduce the\nnumber of patches being fed to the model, by orders of magnitude. This method\nhas a negligible overhead, and works seamlessly with any attention-based model,\ni.e. it is a pre-processing step that can be adopted by any attention-based\nmodel without friction. We demonstrate superior segmentation quality over SoTA\nsegmentation models for real-world pathology datasets while gaining a geomean\nspeedup of $6.9\\times$ for resolutions up to $64K^2$, on up to $2,048$ GPUs.\n","authors":["Enzhi Zhang","Isaac Lyngaas","Peng Chen","Xiao Wang","Jun Igarashi","Yuankai Huo","Mohamed Wahib","Masaharu Munetomo"],"pdf_url":"https://arxiv.org/pdf/2404.09707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09703v1","updated":"2024-04-15T12:01:42Z","published":"2024-04-15T12:01:42Z","title":"AI Competitions and Benchmarks: Dataset Development","summary":"  Machine learning is now used in many applications thanks to its ability to\npredict, generate, or discover patterns from large quantities of data. However,\nthe process of collecting and transforming data for practical use is intricate.\nEven in today's digital era, where substantial data is generated daily, it is\nuncommon for it to be readily usable; most often, it necessitates meticulous\nmanual data preparation. The haste in developing new models can frequently\nresult in various shortcomings, potentially posing risks when deployed in\nreal-world scenarios (eg social discrimination, critical failures), leading to\nthe failure or substantial escalation of costs in AI-based projects. This\nchapter provides a comprehensive overview of established methodological tools,\nenriched by our practical experience, in the development of datasets for\nmachine learning. Initially, we develop the tasks involved in dataset\ndevelopment and offer insights into their effective management (including\nrequirements, design, implementation, evaluation, distribution, and\nmaintenance). Then, we provide more details about the implementation process\nwhich includes data collection, transformation, and quality evaluation.\nFinally, we address practical considerations regarding dataset distribution and\nmaintenance.\n","authors":["Romain Egele","Julio C. S. Jacques Junior","Jan N. van Rijn","Isabelle Guyon","Xavier Bar√≥","Albert Clap√©s","Prasanna Balaprakash","Sergio Escalera","Thomas Moeslund","Jun Wan"],"pdf_url":"https://arxiv.org/pdf/2404.09703v1.pdf","comment":"Preprint version of the 3rd Chapter of the book: Competitions and\n  Benchmarks, the science behind the contests\n  (https://sites.google.com/chalearn.org/book/home)"},{"id":"http://arxiv.org/abs/2404.09695v1","updated":"2024-04-15T11:53:22Z","published":"2024-04-15T11:53:22Z","title":"LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\n  Compression for Large Language Models","summary":"  Large language models (LLMs) show excellent performance in difficult tasks,\nbut they often require massive memories and computational resources. How to\nreduce the parameter scale of LLMs has become research hotspots. In this study,\nwe make an important observation that the multi-head self-attention (MHA)\nsub-layer of Transformer exhibits noticeable low-rank structure, while the\nfeed-forward network (FFN) sub-layer does not. With this regard, we design a\nmixed compression model, which organically combines Low-Rank matrix\napproximation And structured Pruning (LoRAP). For the MHA sub-layer, we propose\nan input activation weighted singular value decomposition method to strengthen\nthe low-rank characteristic. Furthermore, we discover that the weight matrices\nin MHA sub-layer have different low-rank degrees. Thus, a novel parameter\nallocation scheme according to the discrepancy of low-rank degrees is devised.\nFor the FFN sub-layer, we propose a gradient-free structured channel pruning\nmethod. During the pruning, we get an interesting finding that the least\nimportant 1% of parameter actually play a vital role in model performance.\nExtensive evaluations on zero-shot perplexity and zero-shot task classification\nindicate that our proposal is superior to previous structured compression\nrivals under multiple compression ratios.\n","authors":["Guangyan Li","Yongqiang Tang","Wensheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.09695v1.pdf","comment":"8 pages,4 figures"},{"id":"http://arxiv.org/abs/2404.02484v2","updated":"2024-04-15T11:48:37Z","published":"2024-04-03T05:44:03Z","title":"New methods for drug synergy prediction: a mini-review","summary":"  In this mini-review, we explore the new prediction methods for drug\ncombination synergy relying on high-throughput combinatorial screens. The fast\nprogress of the field is witnessed in the more than thirty original machine\nlearning methods published since 2021, a clear majority of them based on deep\nlearning techniques. We aim to put these papers under a unifying lens by\nhighlighting the core technologies, the data sources, the input data types and\nsynergy scores used in the methods, as well as the prediction scenarios and\nevaluation protocols that the papers deal with. Our finding is that the best\nmethods accurately solve the synergy prediction scenarios involving known drugs\nor cell lines while the scenarios involving new drugs or cell lines still fall\nshort of an accurate prediction level.\n","authors":["Fatemeh Abbasi","Juho Rousu"],"pdf_url":"https://arxiv.org/pdf/2404.02484v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09690v1","updated":"2024-04-15T11:45:30Z","published":"2024-04-15T11:45:30Z","title":"Harnessing GPT-4V(ision) for Insurance: A Preliminary Exploration","summary":"  The emergence of Large Multimodal Models (LMMs) marks a significant milestone\nin the development of artificial intelligence. Insurance, as a vast and complex\ndiscipline, involves a wide variety of data forms in its operational processes,\nincluding text, images, and videos, thereby giving rise to diverse multimodal\ntasks. Despite this, there has been limited systematic exploration of\nmultimodal tasks specific to insurance, nor a thorough investigation into how\nLMMs can address these challenges. In this paper, we explore GPT-4V's\ncapabilities in the insurance domain. We categorize multimodal tasks by\nfocusing primarily on visual aspects based on types of insurance (e.g., auto,\nhousehold/commercial property, health, and agricultural insurance) and\ninsurance stages (e.g., risk assessment, risk monitoring, and claims\nprocessing). Our experiment reveals that GPT-4V exhibits remarkable abilities\nin insurance-related tasks, demonstrating not only a robust understanding of\nmultimodal content in the insurance domain but also a comprehensive knowledge\nof insurance scenarios. However, there are notable shortcomings: GPT-4V\nstruggles with detailed risk rating and loss assessment, suffers from\nhallucination in image understanding, and shows variable support for different\nlanguages. Through this work, we aim to bridge the insurance domain with\ncutting-edge LMM technology, facilitate interdisciplinary exchange and\ndevelopment, and provide a foundation for the continued advancement and\nevolution of future research endeavors.\n","authors":["Chenwei Lin","Hanjia Lyu","Jiebo Luo","Xian Xu"],"pdf_url":"https://arxiv.org/pdf/2404.09690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12796v3","updated":"2024-04-15T11:40:39Z","published":"2023-11-21T18:59:58Z","title":"Physics-guided Shape-from-Template: Monocular Video Perception through\n  Neural Surrogate Models","summary":"  3D reconstruction of dynamic scenes is a long-standing problem in computer\ngraphics and increasingly difficult the less information is available.\nShape-from-Template (SfT) methods aim to reconstruct a template-based geometry\nfrom RGB images or video sequences, often leveraging just a single monocular\ncamera without depth information, such as regular smartphone recordings.\nUnfortunately, existing reconstruction methods are either unphysical and noisy\nor slow in optimization. To solve this problem, we propose a novel SfT\nreconstruction algorithm for cloth using a pre-trained neural surrogate model\nthat is fast to evaluate, stable, and produces smooth reconstructions due to a\nregularizing physics simulation. Differentiable rendering of the simulated mesh\nenables pixel-wise comparisons between the reconstruction and a target video\nsequence that can be used for a gradient-based optimization procedure to\nextract not only shape information but also physical parameters such as\nstretching, shearing, or bending stiffness of the cloth. This allows to retain\na precise, stable, and smooth reconstructed geometry while reducing the runtime\nby a factor of 400-500 compared to $\\phi$-SfT, a state-of-the-art physics-based\nSfT approach.\n","authors":["David Stotko","Nils Wandel","Reinhard Klein"],"pdf_url":"https://arxiv.org/pdf/2311.12796v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09686v1","updated":"2024-04-15T11:37:40Z","published":"2024-04-15T11:37:40Z","title":"AntBatchInfer: Elastic Batch Inference in the Kubernetes Cluster","summary":"  Offline batch inference is a common task in the industry for deep learning\napplications, but it can be challenging to ensure stability and performance\nwhen dealing with large amounts of data and complicated inference pipelines.\nThis paper demonstrated AntBatchInfer, an elastic batch inference framework,\nwhich is specially optimized for the non-dedicated cluster. AntBatchInfer\naddresses these challenges by providing multi-level fault-tolerant\ncapabilities, enabling the stable execution of versatile and long-running\ninference tasks. It also improves inference efficiency by pipelining,\nintra-node, and inter-node scaling. It further optimizes the performance in\ncomplicated multiple-model batch inference scenarios. Through extensive\nexperiments and real-world statistics, we demonstrate the superiority of our\nframework in terms of stability and efficiency. In the experiment, it\noutperforms the baseline by at least $2\\times$ and $6\\times$ in the\nsingle-model or multiple-model batch inference. Also, it is widely used at Ant\nGroup, with thousands of daily jobs from various scenarios, including DLRM, CV,\nand NLP, which proves its practicability in the industry.\n","authors":["Siyuan Li","Youshao Xiao","Fanzhuang Meng","Lin Ju","Lei Liang","Lin Wang","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2404.09686v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09683v1","updated":"2024-04-15T11:36:31Z","published":"2024-04-15T11:36:31Z","title":"Post-Training Network Compression for 3D Medical Image Segmentation:\n  Reducing Computational Efforts via Tucker Decomposition","summary":"  We address the computational barrier of deploying advanced deep learning\nsegmentation models in clinical settings by studying the efficacy of network\ncompression through tensor decomposition. We propose a post-training Tucker\nfactorization that enables the decomposition of pre-existing models to reduce\ncomputational requirements without impeding segmentation accuracy. We applied\nTucker decomposition to the convolutional kernels of the TotalSegmentator (TS)\nmodel, an nnU-Net model trained on a comprehensive dataset for automatic\nsegmentation of 117 anatomical structures. Our approach reduced the\nfloating-point operations (FLOPs) and memory required during inference,\noffering an adjustable trade-off between computational efficiency and\nsegmentation quality. This study utilized the publicly available TS dataset,\nemploying various downsampling factors to explore the relationship between\nmodel size, inference speed, and segmentation performance. The application of\nTucker decomposition to the TS model substantially reduced the model parameters\nand FLOPs across various compression rates, with limited loss in segmentation\naccuracy. We removed up to 88% of the model's parameters with no significant\nperformance changes in the majority of classes after fine-tuning. Practical\nbenefits varied across different graphics processing unit (GPU) architectures,\nwith more distinct speed-ups on less powerful hardware. Post-hoc network\ncompression via Tucker decomposition presents a viable strategy for reducing\nthe computational demand of medical image segmentation models without\nsubstantially sacrificing accuracy. This approach enables the broader adoption\nof advanced deep learning technologies in clinical practice, offering a way to\nnavigate the constraints of hardware capabilities.\n","authors":["Tobias Weber","Jakob Dexl","David R√ºgamer","Michael Ingrisch"],"pdf_url":"https://arxiv.org/pdf/2404.09683v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09679v1","updated":"2024-04-15T11:20:44Z","published":"2024-04-15T11:20:44Z","title":"AntDT: A Self-Adaptive Distributed Training Framework for Leader and\n  Straggler Nodes","summary":"  Many distributed training techniques like Parameter Server and AllReduce have\nbeen proposed to take advantage of the increasingly large data and rich\nfeatures. However, stragglers frequently occur in distributed training due to\nresource contention and hardware heterogeneity, which significantly hampers the\ntraining efficiency. Previous works only address part of the stragglers and\ncould not adaptively solve various stragglers in practice. Additionally, it is\nchallenging to use a systematic framework to address all stragglers because\ndifferent stragglers require diverse data allocation and fault-tolerance\nmechanisms. Therefore, this paper proposes a unified distributed training\nframework called AntDT (Ant Distributed Training Framework) to adaptively solve\nthe straggler problems. Firstly, the framework consists of four components,\nincluding the Stateful Dynamic Data Sharding service, Monitor, Controller, and\nAgent. These components work collaboratively to efficiently distribute\nworkloads and provide a range of pre-defined straggler mitigation methods with\nfault tolerance, thereby hiding messy details of data allocation and fault\nhandling. Secondly, the framework provides a high degree of flexibility,\nallowing for the customization of straggler mitigation solutions based on the\nspecific circumstances of the cluster. Leveraging this flexibility, we\nintroduce two straggler mitigation solutions, namely AntDT-ND for non-dedicated\nclusters and AntDT-DD for dedicated clusters, as practical examples to resolve\nvarious types of stragglers at Ant Group. Justified by our comprehensive\nexperiments and industrial deployment statistics, AntDT outperforms other SOTA\nmethods more than 3x in terms of training efficiency. Additionally, in Alipay's\nhomepage recommendation scenario, using AntDT reduces the training duration of\nthe ranking model from 27.8 hours to just 5.4 hours.\n","authors":["Youshao Xiao","Lin Ju","Zhenglei Zhou","Siyuan Li","Zhaoxin Huan","Dalong Zhang","Rujie Jiang","Lin Wang","Xiaolu Zhang","Lei Liang","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2404.09679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00362v2","updated":"2024-04-15T11:03:06Z","published":"2023-12-01T05:59:08Z","title":"Dancing with Still Images: Video Distillation via Static-Dynamic\n  Disentanglement","summary":"  Recently, dataset distillation has paved the way towards efficient machine\nlearning, especially for image datasets. However, the distillation for videos,\ncharacterized by an exclusive temporal dimension, remains an underexplored\ndomain. In this work, we provide the first systematic study of video\ndistillation and introduce a taxonomy to categorize temporal compression. Our\ninvestigation reveals that the temporal information is usually not well learned\nduring distillation, and the temporal dimension of synthetic data contributes\nlittle. The observations motivate our unified framework of disentangling the\ndynamic and static information in the videos. It first distills the videos into\nstill images as static memory and then compensates the dynamic and motion\ninformation with a learnable dynamic memory block. Our method achieves\nstate-of-the-art on video datasets at different scales, with a notably smaller\nmemory storage budget. Our code is available at\nhttps://github.com/yuz1wan/video_distillation.\n","authors":["Ziyu Wang","Yue Xu","Cewu Lu","Yong-Lu Li"],"pdf_url":"https://arxiv.org/pdf/2312.00362v2.pdf","comment":"CVPR 2024, project page: https://mvig-rhos.com/video-distill"},{"id":"http://arxiv.org/abs/2312.07950v4","updated":"2024-04-15T10:57:16Z","published":"2023-12-13T07:56:27Z","title":"CBQ: Cross-Block Quantization for Large Language Models","summary":"  Post-training quantization (PTQ) has played a key role in compressing large\nlanguage models (LLMs) with ultra-low costs. However, existing PTQ methods only\nfocus on handling the outliers within one layer or one block, which ignores the\ndependency of blocks and leads to severe performance degradation in low-bit\nsettings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ\nmethod for LLMs. CBQ employs a cross-block dependency using a homologous\nreconstruction scheme, establishing long-range dependencies across multiple\nblocks to minimize error accumulation. Furthermore, CBQ incorporates a\ncoarse-to-fine preprocessing (CFP) strategy for suppressing weight and\nactivation outliers, coupled with an adaptive LoRA-Rounding technique for\nprecise weight quantization. These innovations enable CBQ to not only handle\nextreme outliers effectively but also improve overall quantization accuracy.\nExtensive experiments show that CBQ achieves superior low-bit quantization\n(W4A4, W4A8, W2A16) and outperforms existing state-of-the-art methods across\nvarious LLMs and datasets. Notably, CBQ quantizes the 4-bit LLAMA1-65B model\nwithin only 4.3 hours on a single GPU, achieving a commendable tradeoff between\nperformance and quantization efficiency.\n","authors":["Xin Ding","Xiaoyu Liu","Zhijun Tu","Yun Zhang","Wei Li","Jie Hu","Hanting Chen","Yehui Tang","Zhiwei Xiong","Baoqun Yin","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2312.07950v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09664v1","updated":"2024-04-15T10:54:47Z","published":"2024-04-15T10:54:47Z","title":"Closing the Gap in the Trade-off between Fair Representations and\n  Accuracy","summary":"  The rapid developments of various machine learning models and their\ndeployments in several applications has led to discussions around the\nimportance of looking beyond the accuracies of these models. Fairness of such\nmodels is one such aspect that is deservedly gaining more attention. In this\nwork, we analyse the natural language representations of documents and\nsentences (i.e., encodings) for any embedding-level bias that could potentially\nalso affect the fairness of the downstream tasks that rely on them. We identify\nbias in these encodings either towards or against different sub-groups based on\nthe difference in their reconstruction errors along various subsets of\nprincipal components. We explore and recommend ways to mitigate such bias in\nthe encodings while also maintaining a decent accuracy in classification models\nthat use them.\n","authors":["Biswajit Rout","Ananya B. Sai","Arun Rajkumar"],"pdf_url":"https://arxiv.org/pdf/2404.09664v1.pdf","comment":"DAI-24"},{"id":"http://arxiv.org/abs/2404.09657v1","updated":"2024-04-15T10:45:12Z","published":"2024-04-15T10:45:12Z","title":"Sampling for Model Predictive Trajectory Planning in Autonomous Driving\n  using Normalizing Flows","summary":"  Alongside optimization-based planners, sampling-based approaches are often\nused in trajectory planning for autonomous driving due to their simplicity.\nModel predictive path integral control is a framework that builds upon\noptimization principles while incorporating stochastic sampling of input\ntrajectories. This paper investigates several sampling approaches for\ntrajectory generation. In this context, normalizing flows originating from the\nfield of variational inference are considered for the generation of sampling\ndistributions, as they model transformations of simple to more complex\ndistributions. Accordingly, learning-based normalizing flow models are trained\nfor a more efficient exploration of the input domain for the task at hand. The\ndeveloped algorithm and the proposed sampling distributions are evaluated in\ntwo simulation scenarios.\n","authors":["Georg Rabenstein","Lars Ullrich","Knut Graichen"],"pdf_url":"https://arxiv.org/pdf/2404.09657v1.pdf","comment":"Accepted to be published as part of the 2024 IEEE Intelligent\n  Vehicles Symposium (IV), Jeju Shinhwa World, Jeju Island, Korea, June 2-5,\n  2024"},{"id":"http://arxiv.org/abs/2404.09656v1","updated":"2024-04-15T10:44:31Z","published":"2024-04-15T10:44:31Z","title":"Learn Your Reference Model for Real Good Alignment","summary":"  The complexity of the alignment problem stems from the fact that existing\nmethods are unstable. Researchers continuously invent various tricks to address\nthis shortcoming. For instance, in the fundamental Reinforcement Learning From\nHuman Feedback (RLHF) technique of Language Model alignment, in addition to\nreward maximization, the Kullback-Leibler divergence between the trainable\npolicy and the SFT policy is minimized. This addition prevents the model from\nbeing overfitted to the Reward Model (RM) and generating texts that are\nout-of-domain for the RM. The Direct Preference Optimization (DPO) method\nreformulates the optimization task of RLHF and eliminates the Reward Model\nwhile tacitly maintaining the requirement for the policy to be close to the SFT\npolicy. In our paper, we argue that this implicit limitation in the DPO method\nleads to sub-optimal results. We propose a new method called Trust Region DPO\n(TR-DPO), which updates the reference policy during training. With such a\nstraightforward update, we demonstrate the effectiveness of TR-DPO against DPO\non the Anthropic HH and TLDR datasets. We show that TR-DPO outperforms DPO by\nup to 19%, measured by automatic evaluation with GPT-4. The new alignment\napproach that we propose allows us to improve the quality of models across\nseveral parameters at once, such as coherence, correctness, level of detail,\nhelpfulness, and harmlessness.\n","authors":["Alexey Gorbatovski","Boris Shaposhnikov","Alexey Malakhov","Nikita Surnachev","Yaroslav Aksenov","Ian Maksimov","Nikita Balagansky","Daniil Gavrilov"],"pdf_url":"https://arxiv.org/pdf/2404.09656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05468v2","updated":"2024-04-15T10:13:25Z","published":"2024-04-08T12:46:39Z","title":"Mind-to-Image: Projecting Visual Mental Imagination of the Brain from\n  fMRI","summary":"  The reconstruction of images observed by subjects from fMRI data collected\nduring visual stimuli has made significant strides in the past decade, thanks\nto the availability of extensive fMRI datasets and advancements in generative\nmodels for image generation. However, the application of visual reconstruction\nhas remained limited. Reconstructing visual imagination presents a greater\nchallenge, with potentially revolutionary applications ranging from aiding\nindividuals with disabilities to verifying witness accounts in court. The\nprimary hurdles in this field are the absence of data collection protocols for\nvisual imagery and the lack of datasets on the subject. Traditionally,\nfMRI-to-image relies on data collected from subjects exposed to visual stimuli,\nwhich poses issues for generating visual imagery based on the difference of\nbrain activity between visual stimulation and visual imagery. For the first\ntime, we have compiled a substantial dataset (around 6h of scans) on visual\nimagery along with a proposed data collection protocol. We then train a\nmodified version of an fMRI-to-image model and demonstrate the feasibility of\nreconstructing images from two modes of imagination: from memory and from pure\nimagination. This marks an important step towards creating a technology that\nallow direct reconstruction of visual imagery.\n","authors":["Hugo Caselles-Dupr√©","Charles Mellerio","Paul H√©rent","Aliz√©e Lopez-Persem","Benoit B√©ranger","Mathieu Soularue","Pierre Fautrel","Gauthier Vernier","Matthieu Cord"],"pdf_url":"https://arxiv.org/pdf/2404.05468v2.pdf","comment":"Pre-print to be updated"},{"id":"http://arxiv.org/abs/2404.09636v1","updated":"2024-04-15T10:12:33Z","published":"2024-04-15T10:12:33Z","title":"All-in-one simulation-based inference","summary":"  Amortized Bayesian inference trains neural networks to solve stochastic\ninference problems using model simulations, thereby making it possible to\nrapidly perform Bayesian inference for any newly observed data. However,\ncurrent simulation-based amortized inference methods are simulation-hungry and\ninflexible: They require the specification of a fixed parametric prior,\nsimulator, and inference tasks ahead of time. Here, we present a new amortized\ninference method -- the Simformer -- which overcomes these limitations. By\ntraining a probabilistic diffusion model with transformer architectures, the\nSimformer outperforms current state-of-the-art amortized inference approaches\non benchmark tasks and is substantially more flexible: It can be applied to\nmodels with function-valued parameters, it can handle inference scenarios with\nmissing or unstructured data, and it can sample arbitrary conditionals of the\njoint distribution of parameters and data, including both posterior and\nlikelihood. We showcase the performance and flexibility of the Simformer on\nsimulators from ecology, epidemiology, and neuroscience, and demonstrate that\nit opens up new possibilities and application domains for amortized Bayesian\ninference on simulation-based models.\n","authors":["Manuel Gloeckler","Michael Deistler","Christian Weilbach","Frank Wood","Jakob H. Macke"],"pdf_url":"https://arxiv.org/pdf/2404.09636v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09632v1","updated":"2024-04-15T10:04:15Z","published":"2024-04-15T10:04:15Z","title":"Bridging Vision and Language Spaces with Assignment Prediction","summary":"  This paper introduces VLAP, a novel approach that bridges pretrained vision\nmodels and large language models (LLMs) to make frozen LLMs understand the\nvisual world. VLAP transforms the embedding space of pretrained vision models\ninto the LLMs' word embedding space using a single linear layer for efficient\nand general-purpose visual and language understanding. Specifically, we harness\nwell-established word embeddings to bridge two modality embedding spaces. The\nvisual and text representations are simultaneously assigned to a set of word\nembeddings within pretrained LLMs by formulating the assigning procedure as an\noptimal transport problem. We predict the assignment of one modality from the\nrepresentation of another modality data, enforcing consistent assignments for\npaired multimodal data. This allows vision and language representations to\ncontain the same information, grounding the frozen LLMs' word embedding space\nin visual data. Moreover, a robust semantic taxonomy of LLMs can be preserved\nwith visual data since the LLMs interpret and reason linguistic information\nfrom correlations between word embeddings. Experimental results show that VLAP\nachieves substantial improvements over the previous linear transformation-based\napproaches across a range of vision-language tasks, including image captioning,\nvisual question answering, and cross-modal retrieval. We also demonstrate the\nlearned visual representations hold a semantic taxonomy of LLMs, making visual\nsemantic arithmetic possible.\n","authors":["Jungin Park","Jiyoung Lee","Kwanghoon Sohn"],"pdf_url":"https://arxiv.org/pdf/2404.09632v1.pdf","comment":"ICLR 2024 Camera-ready"},{"id":"http://arxiv.org/abs/2404.09625v1","updated":"2024-04-15T09:56:36Z","published":"2024-04-15T09:56:36Z","title":"Privacy-Preserving Intrusion Detection using Convolutional Neural\n  Networks","summary":"  Privacy-preserving analytics is designed to protect valuable assets. A common\nservice provision involves the input data from the client and the model on the\nanalyst's side. The importance of the privacy preservation is fuelled by legal\nobligations and intellectual property concerns. We explore the use case of a\nmodel owner providing an analytic service on customer's private data. No\ninformation about the data shall be revealed to the analyst and no information\nabout the model shall be leaked to the customer. Current methods involve costs:\naccuracy deterioration and computational complexity. The complexity, in turn,\nresults in a longer processing time, increased requirement on computing\nresources, and involves data communication between the client and the server.\nIn order to deploy such service architecture, we need to evaluate the optimal\nsetting that fits the constraints. And that is what this paper addresses. In\nthis work, we enhance an attack detection system based on Convolutional Neural\nNetworks with privacy-preserving technology based on PriMIA framework that is\ninitially designed for medical data.\n","authors":["Martin Kodys","Zhongmin Dai","Vrizlynn L. L. Thing"],"pdf_url":"https://arxiv.org/pdf/2404.09625v1.pdf","comment":"Accepted at IEEE Conference on Artificial Intelligence (CAI) 2024"},{"id":"http://arxiv.org/abs/2310.08339v2","updated":"2024-04-15T09:51:15Z","published":"2023-10-12T13:57:32Z","title":"TTK is Getting MPI-Ready","summary":"  This system paper documents the technical foundations for the extension of\nthe Topology ToolKit (TTK) to distributed-memory parallelism with the Message\nPassing Interface (MPI). While several recent papers introduced topology-based\napproaches for distributed-memory environments, these were reporting\nexperiments obtained with tailored, mono-algorithm implementations. In\ncontrast, we describe in this paper a versatile approach (supporting both\ntriangulated domains and regular grids) for the support of topological analysis\npipelines, i.e. a sequence of topological algorithms interacting together.\nWhile developing this extension, we faced several algorithmic and software\nengineering challenges, which we document in this paper. We describe an MPI\nextension of TTK's data structure for triangulation representation and\ntraversal, a central component to the global performance and generality of\nTTK's topological implementations. We also introduce an intermediate interface\nbetween TTK and MPI, both at the global pipeline level, and at the fine-grain\nalgorithmic level. We provide a taxonomy for the distributed-memory topological\nalgorithms supported by TTK, depending on their communication needs and provide\nexamples of hybrid MPI+thread parallelizations. Performance analyses show that\nparallel efficiencies range from 20% to 80% (depending on the algorithms), and\nthat the MPI-specific preconditioning introduced by our framework induces a\nnegligible computation time overhead. We illustrate the new distributed-memory\ncapabilities of TTK with an example of advanced analysis pipeline, combining\nmultiple algorithms, run on the largest publicly available dataset we have\nfound (120 billion vertices) on a cluster with 64 nodes (for a total of 1536\ncores). Finally, we provide a roadmap for the completion of TTK's MPI\nextension, along with generic recommendations for each algorithm communication\ncategory.\n","authors":["Eve Le Guillou","Michael Will","Pierre Guillou","Jonas Lukasczyk","Pierre Fortin","Christoph Garth","Julien Tierny"],"pdf_url":"https://arxiv.org/pdf/2310.08339v2.pdf","comment":"18 pages, 13 figures"},{"id":"http://arxiv.org/abs/2404.09617v1","updated":"2024-04-15T09:46:12Z","published":"2024-04-15T09:46:12Z","title":"Safeguarding adaptive methods: global convergence of Barzilai-Borwein\n  and other stepsize choices","summary":"  Leveraging on recent advancements on adaptive methods for convex minimization\nproblems, this paper provides a linesearch-free proximal gradient framework for\nglobalizing the convergence of popular stepsize choices such as\nBarzilai-Borwein and one-dimensional Anderson acceleration. This framework can\ncope with problems in which the gradient of the differentiable function is\nmerely locally H\\\"older continuous. Our analysis not only encompasses but also\nrefines existing results upon which it builds. The theory is corroborated by\nnumerical evidence that showcases the synergetic interplay between fast\nstepsize selections and adaptive methods.\n","authors":["Ou Hongjia","Andreas Themelis"],"pdf_url":"https://arxiv.org/pdf/2404.09617v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09616v1","updated":"2024-04-15T09:40:44Z","published":"2024-04-15T09:40:44Z","title":"A Review and Efficient Implementation of Scene Graph Generation Metrics","summary":"  Scene graph generation has emerged as a prominent research field in computer\nvision, witnessing significant advancements in the recent years. However,\ndespite these strides, precise and thorough definitions for the metrics used to\nevaluate scene graph generation models are lacking. In this paper, we address\nthis gap in the literature by providing a review and precise definition of\ncommonly used metrics in scene graph generation. Our comprehensive examination\nclarifies the underlying principles of these metrics and can serve as a\nreference or introduction to scene graph metrics.\n  Furthermore, to facilitate the usage of these metrics, we introduce a\nstandalone Python package called SGBench that efficiently implements all\ndefined metrics, ensuring their accessibility to the research community.\nAdditionally, we present a scene graph benchmarking web service, that enables\nresearchers to compare scene graph generation methods and increase visibility\nof new methods in a central place.\n  All of our code can be found at https://lorjul.github.io/sgbench/.\n","authors":["Julian Lorenz","Robin Sch√∂n","Katja Ludwig","Rainer Lienhart"],"pdf_url":"https://arxiv.org/pdf/2404.09616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09610v1","updated":"2024-04-15T09:32:12Z","published":"2024-04-15T09:32:12Z","title":"LoRA Dropout as a Sparsity Regularizer for Overfitting Control","summary":"  Parameter-efficient fine-tuning methods, represented by LoRA, play an\nessential role in adapting large-scale pre-trained models to downstream tasks.\nHowever, fine-tuning LoRA-series models also faces the risk of overfitting on\nthe training dataset, and yet there's still a lack of theoretical guidance and\npractical mechanism to control overfitting on LoRA-based PEFT methods. In this\npaper, we propose a LoRA Dropout mechanism for the LoRA-based methods by\nintroducing random noises to the learnable low-rank matrices and increasing\nparameter sparsity. We then demonstrate the theoretical mechanism of our LoRA\nDropout mechanism from the perspective of sparsity regularization by providing\na generalization error bound under this framework. Theoretical results show\nthat appropriate sparsity would help tighten the gap between empirical and\ngeneralization risks and thereby control overfitting. Furthermore, based on the\nLoRA Dropout framework, we introduce a test-time ensemble strategy and provide\ntheoretical evidence demonstrating that the ensemble method can further\ncompress the error bound, and lead to better performance during inference time.\nExtensive experiments on various NLP tasks provide practical validations of the\neffectiveness of our LoRA Dropout framework in improving model accuracy and\ncalibration.\n","authors":["Yang Lin","Xinyu Ma","Xu Chu","Yujie Jin","Zhibang Yang","Yasha Wang","Hong Mei"],"pdf_url":"https://arxiv.org/pdf/2404.09610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09606v1","updated":"2024-04-15T09:26:33Z","published":"2024-04-15T09:26:33Z","title":"A Self-feedback Knowledge Elicitation Approach for Chemical Reaction\n  Predictions","summary":"  The task of chemical reaction predictions (CRPs) plays a pivotal role in\nadvancing drug discovery and material science. However, its effectiveness is\nconstrained by the vast and uncertain chemical reaction space and challenges in\ncapturing reaction selectivity, particularly due to existing methods'\nlimitations in exploiting the data's inherent knowledge. To address these\nchallenges, we introduce a data-curated self-feedback knowledge elicitation\napproach. This method starts from iterative optimization of molecular\nrepresentations and facilitates the extraction of knowledge on chemical\nreaction types (RTs). Then, we employ adaptive prompt learning to infuse the\nprior knowledge into the large language model (LLM). As a result, we achieve\nsignificant enhancements: a 14.2% increase in retrosynthesis prediction\naccuracy, a 74.2% rise in reagent prediction accuracy, and an expansion in the\nmodel's capability for handling multi-task chemical reactions. This research\noffers a novel paradigm for knowledge elicitation in scientific research and\nshowcases the untapped potential of LLMs in CRPs.\n","authors":["Pengfei Liu","Jun Tao","Zhixiang Ren"],"pdf_url":"https://arxiv.org/pdf/2404.09606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09604v1","updated":"2024-04-15T09:22:46Z","published":"2024-04-15T09:22:46Z","title":"Machine learning-based optimization workflow of the homogeneity of\n  spunbond nonwovens with human validation","summary":"  In the last ten years, the average annual growth rate of nonwoven production\nwas 4%. In 2020 and 2021, nonwoven production has increased even further due to\nthe huge demand for nonwoven products needed for protective clothing such as\nFFP2 masks to combat the COVID19 pandemic. Optimizing the production process is\nstill a challenge due to its high nonlinearity. In this paper, we present a\nmachine learning-based optimization workflow aimed at improving the homogeneity\nof spunbond nonwovens. The optimization workflow is based on a mathematical\nmodel that simulates the microstructures of nonwovens. Based on trainingy data\ncoming from this simulator, different machine learning algorithms are trained\nin order to find a surrogate model for the time-consuming simulator. Human\nvalidation is employed to verify the outputs of machine learning algorithms by\nassessing the aesthetics of the nonwovens. We include scientific and expert\nknowledge into the training data to reduce the computational costs involved in\nthe optimization process. We demonstrate the necessity and effectiveness of our\nworkflow in optimizing the homogeneity of nonwovens.\n","authors":["Viny Saajan Victor","Andre Schmei√üer","Heike Leitte","Simone Gramsch"],"pdf_url":"https://arxiv.org/pdf/2404.09604v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09601v1","updated":"2024-04-15T09:16:49Z","published":"2024-04-15T09:16:49Z","title":"Reactive Model Correction: Mitigating Harm to Task-Relevant Features via\n  Conditional Bias Suppression","summary":"  Deep Neural Networks are prone to learning and relying on spurious\ncorrelations in the training data, which, for high-risk applications, can have\nfatal consequences. Various approaches to suppress model reliance on harmful\nfeatures have been proposed that can be applied post-hoc without additional\ntraining. Whereas those methods can be applied with efficiency, they also tend\nto harm model performance by globally shifting the distribution of latent\nfeatures. To mitigate unintended overcorrection of model behavior, we propose a\nreactive approach conditioned on model-derived knowledge and eXplainable\nArtificial Intelligence (XAI) insights. While the reactive approach can be\napplied to many post-hoc methods, we demonstrate the incorporation of\nreactivity in particular for P-ClArC (Projective Class Artifact Compensation),\nintroducing a new method called R-ClArC (Reactive Class Artifact Compensation).\nThrough rigorous experiments in controlled settings (FunnyBirds) and with a\nreal-world dataset (ISIC2019), we show that introducing reactivity can minimize\nthe detrimental effect of the applied correction while simultaneously ensuring\nlow reliance on spurious features.\n","authors":["Dilyara Bareeva","Maximilian Dreyer","Frederik Pahde","Wojciech Samek","Sebastian Lapuschkin"],"pdf_url":"https://arxiv.org/pdf/2404.09601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09586v1","updated":"2024-04-15T08:54:33Z","published":"2024-04-15T08:54:33Z","title":"Mitigating the Curse of Dimensionality for Certified Robustness via Dual\n  Randomized Smoothing","summary":"  Randomized Smoothing (RS) has been proven a promising method for endowing an\narbitrary image classifier with certified robustness. However, the substantial\nuncertainty inherent in the high-dimensional isotropic Gaussian noise imposes\nthe curse of dimensionality on RS. Specifically, the upper bound of ${\\ell_2}$\ncertified robustness radius provided by RS exhibits a diminishing trend with\nthe expansion of the input dimension $d$, proportionally decreasing at a rate\nof $1/\\sqrt{d}$. This paper explores the feasibility of providing ${\\ell_2}$\ncertified robustness for high-dimensional input through the utilization of dual\nsmoothing in the lower-dimensional space. The proposed Dual Randomized\nSmoothing (DRS) down-samples the input image into two sub-images and smooths\nthe two sub-images in lower dimensions. Theoretically, we prove that DRS\nguarantees a tight ${\\ell_2}$ certified robustness radius for the original\ninput and reveal that DRS attains a superior upper bound on the ${\\ell_2}$\nrobustness radius, which decreases proportionally at a rate of $(1/\\sqrt m +\n1/\\sqrt n )$ with $m+n=d$. Extensive experiments demonstrate the\ngeneralizability and effectiveness of DRS, which exhibits a notable capability\nto integrate with established methodologies, yielding substantial improvements\nin both accuracy and ${\\ell_2}$ certified robustness baselines of RS on the\nCIFAR-10 and ImageNet datasets. Code is available at\nhttps://github.com/xiasong0501/DRS.\n","authors":["Song Xia","Yu Yi","Xudong Jiang","Henghui Ding"],"pdf_url":"https://arxiv.org/pdf/2404.09586v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.06913v2","updated":"2024-04-15T08:54:09Z","published":"2023-06-12T07:34:21Z","title":"A Graph Transformer-Driven Approach for Network Robustness Learning","summary":"  Learning and analysis of network robustness, including controllability\nrobustness and connectivity robustness, is critical for various networked\nsystems against attacks. Traditionally, network robustness is determined by\nattack simulations, which is very time-consuming and even incapable for\nlarge-scale networks. Network Robustness Learning, which is dedicated to\nlearning network robustness with high precision and high speed, provides a\npowerful tool to analyze network robustness by replacing simulations. In this\npaper, a novel versatile and unified robustness learning approach via graph\ntransformer (NRL-GT) is proposed, which accomplishes the task of\ncontrollability robustness learning and connectivity robustness learning from\nmultiple aspects including robustness curve learning, overall robustness\nlearning, and synthetic network classification. Numerous experiments show that:\n1) NRL-GT is a unified learning framework for controllability robustness and\nconnectivity robustness, demonstrating a strong generalization ability to\nensure high precision when training and test sets are distributed differently;\n2) Compared to the cutting-edge methods, NRL-GT can simultaneously perform\nnetwork robustness learning from multiple aspects and obtains superior results\nin less time. NRL-GT is also able to deal with complex networks of different\nsize with low learning error and high efficiency; 3) It is worth mentioning\nthat the backbone of NRL-GT can serve as a transferable feature learning module\nfor complex networks of different size and different downstream tasks.\n","authors":["Yu Zhang","Jia Li","Jie Ding","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2306.06913v2.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2312.05044v2","updated":"2024-04-15T08:45:16Z","published":"2023-12-08T13:52:16Z","title":"Backward Learning for Goal-Conditioned Policies","summary":"  Can we learn policies in reinforcement learning without rewards? Can we learn\na policy just by trying to reach a goal state? We answer these questions\npositively by proposing a multi-step procedure that first learns a world model\nthat goes backward in time, secondly generates goal-reaching backward\ntrajectories, thirdly improves those sequences using shortest path finding\nalgorithms, and finally trains a neural network policy by imitation learning.\nWe evaluate our method on a deterministic maze environment where the\nobservations are $64\\times 64$ pixel bird's eye images and can show that it\nconsistently reaches several goals.\n","authors":["Marc H√∂ftmann","Jan Robine","Stefan Harmeling"],"pdf_url":"https://arxiv.org/pdf/2312.05044v2.pdf","comment":"World Models, Goal-conditioned, Reward-free, Workshop on\n  Goal-Conditioned Reinforcement Learning - NeurIPS 2023"},{"id":"http://arxiv.org/abs/2404.05555v2","updated":"2024-04-15T08:44:13Z","published":"2024-04-08T14:28:27Z","title":"On the Convergence of Continual Learning with Adaptive Methods","summary":"  One of the objectives of continual learning is to prevent catastrophic\nforgetting in learning multiple tasks sequentially, and the existing solutions\nhave been driven by the conceptualization of the plasticity-stability dilemma.\nHowever, the convergence of continual learning for each sequential task is less\nstudied so far. In this paper, we provide a convergence analysis of\nmemory-based continual learning with stochastic gradient descent and empirical\nevidence that training current tasks causes the cumulative degradation of\nprevious tasks. We propose an adaptive method for nonconvex continual learning\n(NCCL), which adjusts step sizes of both previous and current tasks with the\ngradients. The proposed method can achieve the same convergence rate as the SGD\nmethod when the catastrophic forgetting term which we define in the paper is\nsuppressed at each iteration. Further, we demonstrate that the proposed\nalgorithm improves the performance of continual learning over existing methods\nfor several image classification tasks.\n","authors":["Seungyub Han","Yeongmo Kim","Taehyun Cho","Jungwoo Lee"],"pdf_url":"https://arxiv.org/pdf/2404.05555v2.pdf","comment":"Proceedings of the Thirty-Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI 2023), see\n  https://proceedings.mlr.press/v216/han23a.html"},{"id":"http://arxiv.org/abs/2404.09574v1","updated":"2024-04-15T08:36:40Z","published":"2024-04-15T08:36:40Z","title":"Predicting and Analyzing Pedestrian Crossing Behavior at Unsignalized\n  Crossings","summary":"  Understanding and predicting pedestrian crossing behavior is essential for\nenhancing automated driving and improving driving safety. Predicting gap\nselection behavior and the use of zebra crossing enables driving systems to\nproactively respond and prevent potential conflicts. This task is particularly\nchallenging at unsignalized crossings due to the ambiguous right of way,\nrequiring pedestrians to constantly interact with vehicles and other\npedestrians. This study addresses these challenges by utilizing simulator data\nto investigate scenarios involving multiple vehicles and pedestrians. We\npropose and evaluate machine learning models to predict gap selection in\nnon-zebra scenarios and zebra crossing usage in zebra scenarios. We investigate\nand discuss how pedestrians' behaviors are influenced by various factors,\nincluding pedestrian waiting time, walking speed, the number of unused gaps,\nthe largest missed gap, and the influence of other pedestrians. This research\ncontributes to the evolution of intelligent vehicles by providing predictive\nmodels and valuable insights into pedestrian crossing behavior.\n","authors":["Chi Zhang","Janis Sprenger","Zhongjun Ni","Christian Berger"],"pdf_url":"https://arxiv.org/pdf/2404.09574v1.pdf","comment":"8 pages, 10 figures, 4 tables. Accepted in 2024 IEEE Intelligent\n  Vehicles Symposium (IV)"},{"id":"http://arxiv.org/abs/2404.09565v1","updated":"2024-04-15T08:27:47Z","published":"2024-04-15T08:27:47Z","title":"Reliability Estimation of News Media Sources: Birds of a Feather Flock\n  Together","summary":"  Evaluating the reliability of news sources is a routine task for journalists\nand organizations committed to acquiring and disseminating accurate\ninformation. Recent research has shown that predicting sources' reliability\nrepresents an important first-prior step in addressing additional challenges\nsuch as fake news detection and fact-checking. In this paper, we introduce a\nnovel approach for source reliability estimation that leverages reinforcement\nlearning strategies for estimating the reliability degree of news sources.\nContrary to previous research, our proposed approach models the problem as the\nestimation of a reliability degree, and not a reliability label, based on how\nall the news media sources interact with each other on the Web. We validated\nthe effectiveness of our method on a news media reliability dataset that is an\norder of magnitude larger than comparable existing datasets. Results show that\nthe estimated reliability degrees strongly correlates with journalists-provided\nscores (Spearman=0.80) and can effectively predict reliability labels\n(macro-avg. F$_1$ score=81.05). We release our implementation and dataset,\naiming to provide a valuable resource for the NLP community working on\ninformation verification.\n","authors":["Sergio Burdisso","Dairazalia S√°nchez-Cort√©s","Esa√∫ Villatoro-Tello","Petr Motlicek"],"pdf_url":"https://arxiv.org/pdf/2404.09565v1.pdf","comment":"Accepted to NAACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2212.14855v3","updated":"2024-04-15T08:24:42Z","published":"2022-12-30T18:04:25Z","title":"Disentangled Explanations of Neural Network Predictions by Finding\n  Relevant Subspaces","summary":"  Explainable AI aims to overcome the black-box nature of complex ML models\nlike neural networks by generating explanations for their predictions.\nExplanations often take the form of a heatmap identifying input features (e.g.\npixels) that are relevant to the model's decision. These explanations, however,\nentangle the potentially multiple factors that enter into the overall complex\ndecision strategy. We propose to disentangle explanations by extracting at some\nintermediate layer of a neural network, subspaces that capture the multiple and\ndistinct activation patterns (e.g. visual concepts) that are relevant to the\nprediction. To automatically extract these subspaces, we propose two new\nanalyses, extending principles found in PCA or ICA to explanations. These novel\nanalyses, which we call principal relevant component analysis (PRCA) and\ndisentangled relevant subspace analysis (DRSA), maximize relevance instead of\ne.g. variance or kurtosis. This allows for a much stronger focus of the\nanalysis on what the ML model actually uses for predicting, ignoring\nactivations or concepts to which the model is invariant. Our approach is\ngeneral enough to work alongside common attribution techniques such as Shapley\nValue, Integrated Gradients, or LRP. Our proposed methods show to be\npractically useful and compare favorably to the state of the art as\ndemonstrated on benchmarks and three use cases.\n","authors":["Pattarawat Chormai","Jan Herrmann","Klaus-Robert M√ºller","Gr√©goire Montavon"],"pdf_url":"https://arxiv.org/pdf/2212.14855v3.pdf","comment":"17 pages + supplement"},{"id":"http://arxiv.org/abs/2404.09562v1","updated":"2024-04-15T08:22:47Z","published":"2024-04-15T08:22:47Z","title":"œÉ-GPTs: A New Approach to Autoregressive Models","summary":"  Autoregressive models, such as the GPT family, use a fixed order, usually\nleft-to-right, to generate sequences. However, this is not a necessity. In this\npaper, we challenge this assumption and show that by simply adding a positional\nencoding for the output, this order can be modulated on-the-fly per-sample\nwhich offers key advantageous properties. It allows for the sampling of and\nconditioning on arbitrary subsets of tokens, and it also allows sampling in one\nshot multiple tokens dynamically according to a rejection strategy, leading to\na sub-linear number of model evaluations. We evaluate our method across various\ndomains, including language modeling, path-solving, and aircraft vertical rate\nprediction, decreasing the number of steps required for generation by an order\nof magnitude.\n","authors":["Arnaud Pannatier","Evann Courdier","Fran√ßois Fleuret"],"pdf_url":"https://arxiv.org/pdf/2404.09562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15865v2","updated":"2024-04-15T08:18:47Z","published":"2023-11-27T14:33:21Z","title":"A precise symbolic emulator of the linear matter power spectrum","summary":"  Computing the matter power spectrum, $P(k)$, as a function of cosmological\nparameters can be prohibitively slow in cosmological analyses, hence emulating\nthis calculation is desirable. Previous analytic approximations are\ninsufficiently accurate for modern applications, so black-box, uninterpretable\nemulators are often used. We utilise an efficient genetic programming based\nsymbolic regression framework to explore the space of potential mathematical\nexpressions which can approximate the power spectrum and $\\sigma_8$. We learn\nthe ratio between an existing low-accuracy fitting function for $P(k)$ and that\nobtained by solving the Boltzmann equations and thus still incorporate the\nphysics which motivated this earlier approximation. We obtain an analytic\napproximation to the linear power spectrum with a root mean squared fractional\nerror of 0.2% between $k = 9\\times10^{-3} - 9 \\, h{\\rm \\, Mpc^{-1}}$ and across\na wide range of cosmological parameters, and we provide physical\ninterpretations for various terms in the expression. Our analytic approximation\nis 950 times faster to evaluate than camb and 36 times faster than the neural\nnetwork based matter power spectrum emulator BACCO. We also provide a simple\nanalytic approximation for $\\sigma_8$ with a similar accuracy, with a root mean\nsquared fractional error of just 0.1% when evaluated across the same range of\ncosmologies. This function is easily invertible to obtain $A_{\\rm s}$ as a\nfunction of $\\sigma_8$ and the other cosmological parameters, if preferred. It\nis possible to obtain symbolic approximations to a seemingly complex function\nat a precision required for current and future cosmological analyses without\nresorting to deep-learning techniques, thus avoiding their black-box nature and\nlarge number of parameters. Our emulator will be usable long after the codes on\nwhich numerical approximations are built become outdated.\n","authors":["Deaglan J. Bartlett","Lukas Kammerer","Gabriel Kronberger","Harry Desmond","Pedro G. Ferreira","Benjamin D. Wandelt","Bogdan Burlacu","David Alonso","Matteo Zennaro"],"pdf_url":"https://arxiv.org/pdf/2311.15865v2.pdf","comment":"9 pages, 5 figures. Accepted for publication in A&A"},{"id":"http://arxiv.org/abs/2402.17492v2","updated":"2024-04-15T08:12:04Z","published":"2024-02-27T13:18:00Z","title":"syren-halofit: A fast, interpretable, high-precision formula for the\n  $Œõ$CDM nonlinear matter power spectrum","summary":"  Rapid and accurate evaluation of the nonlinear matter power spectrum, $P(k)$,\nas a function of cosmological parameters and redshift is of fundamental\nimportance in cosmology. Analytic approximations provide an interpretable\nsolution, yet current approximations are neither fast nor accurate relative to\nnumerical emulators. We use symbolic regression to obtain simple analytic\napproximations to the nonlinear scale, $k_\\sigma$, the effective spectral\nindex, $n_{\\rm eff}$, and the curvature, $C$, which are required for the\nhalofit model. We then re-optimise the coefficients of halofit to fit a wide\nrange of cosmologies and redshifts. We explore the space of analytic\nexpressions to fit the residuals between $P(k)$ and the optimised predictions\nof halofit. Our results are designed to match the predictions of\nEuclidEmulator2, but are validated against $N$-body simulations. Our symbolic\nexpressions for $k_\\sigma$, $n_{\\rm eff}$ and $C$ have root mean squared\nfractional errors of 0.8%, 0.2% and 0.3%, respectively, for redshifts below 3\nand a wide range of cosmologies. The re-optimised halofit parameters reduce the\nroot mean squared fractional error (compared to EuclidEmulator2) from 3% to\nbelow 2% for wavenumbers $k=9\\times10^{-3}-9 \\, h{\\rm Mpc^{-1}}$. We introduce\nsyren-halofit (symbolic-regression-enhanced halofit), an extension to halofit\ncontaining a short symbolic correction which improves this error to 1%. Our\nmethod is 2350 and 3170 times faster than current halofit and hmcode\nimplementations, respectively, and 2680 and 64 times faster than\nEuclidEmulator2 (which requires running class) and the BACCO emulator. We\nobtain comparable accuracy to EuclidEmulator2 and BACCO when tested on $N$-body\nsimulations. Our work greatly increases the speed and accuracy of symbolic\napproximations to $P(k)$, making them significantly faster than their numerical\ncounterparts without loss of accuracy.\n","authors":["Deaglan J. Bartlett","Benjamin D. Wandelt","Matteo Zennaro","Pedro G. Ferreira","Harry Desmond"],"pdf_url":"https://arxiv.org/pdf/2402.17492v2.pdf","comment":"11 pages, 8 figures. Accepted for publication in A&A"},{"id":"http://arxiv.org/abs/2404.09544v1","updated":"2024-04-15T08:11:21Z","published":"2024-04-15T08:11:21Z","title":"GNNavigator: Towards Adaptive Training of Graph Neural Networks via\n  Automatic Guideline Exploration","summary":"  Graph Neural Networks (GNNs) succeed significantly in many applications\nrecently. However, balancing GNNs training runtime cost, memory consumption,\nand attainable accuracy for various applications is non-trivial. Previous\ntraining methodologies suffer from inferior adaptability and lack a unified\ntraining optimization solution. To address the problem, this work proposes\nGNNavigator, an adaptive GNN training configuration optimization framework.\nGNNavigator meets diverse GNN application requirements due to our unified\nsoftware-hardware co-abstraction, proposed GNNs training performance model, and\npractical design space exploration solution. Experimental results show that\nGNNavigator can achieve up to 3.1x speedup and 44.9% peak memory reduction with\ncomparable accuracy to state-of-the-art approaches.\n","authors":["Tong Qiao","Jianlei Yang","Yingjie Qi","Ao Zhou","Chen Bai","Bei Yu","Weisheng Zhao","Chunming Hu"],"pdf_url":"https://arxiv.org/pdf/2404.09544v1.pdf","comment":"Accepted by DAC'24"},{"id":"http://arxiv.org/abs/2302.02568v4","updated":"2024-04-15T08:11:18Z","published":"2023-02-06T05:11:27Z","title":"Less is More: Understanding Word-level Textual Adversarial Attack via\n  n-gram Frequency Descend","summary":"  Word-level textual adversarial attacks have demonstrated notable efficacy in\nmisleading Natural Language Processing (NLP) models. Despite their success, the\nunderlying reasons for their effectiveness and the fundamental characteristics\nof adversarial examples (AEs) remain obscure. This work aims to interpret\nword-level attacks by examining their $n$-gram frequency patterns. Our\ncomprehensive experiments reveal that in approximately 90\\% of cases,\nword-level attacks lead to the generation of examples where the frequency of\n$n$-grams decreases, a tendency we term as the $n$-gram Frequency Descend\n($n$-FD). This finding suggests a straightforward strategy to enhance model\nrobustness: training models using examples with $n$-FD. To examine the\nfeasibility of this strategy, we employed the $n$-gram frequency information,\nas an alternative to conventional loss gradients, to generate perturbed\nexamples in adversarial training. The experiment results indicate that the\nfrequency-based approach performs comparably with the gradient-based approach\nin improving model robustness. Our research offers a novel and more intuitive\nperspective for understanding word-level textual adversarial attacks and\nproposes a new direction to improve model robustness.\n","authors":["Ning Lu","Shengcai Liu","Zhirui Zhang","Qi Wang","Haifeng Liu","Ke Tang"],"pdf_url":"https://arxiv.org/pdf/2302.02568v4.pdf","comment":"To be published in: 2024 IEEE Conference on Artificial Intelligence\n  (CAI 2024)"},{"id":"http://arxiv.org/abs/2404.09541v1","updated":"2024-04-15T08:06:54Z","published":"2024-04-15T08:06:54Z","title":"Application of the representative measure approach to assess the\n  reliability of decision trees in dealing with unseen vehicle collision data","summary":"  Machine learning algorithms are fundamental components of novel data-informed\nArtificial Intelligence architecture. In this domain, the imperative role of\nrepresentative datasets is a cornerstone in shaping the trajectory of\nartificial intelligence (AI) development. Representative datasets are needed to\ntrain machine learning components properly. Proper training has multiple\nimpacts: it reduces the final model's complexity, power, and uncertainties. In\nthis paper, we investigate the reliability of the\n$\\varepsilon$-representativeness method to assess the dataset similarity from a\ntheoretical perspective for decision trees. We decided to focus on the family\nof decision trees because it includes a wide variety of models known to be\nexplainable. Thus, in this paper, we provide a result guaranteeing that if two\ndatasets are related by $\\varepsilon$-representativeness, i.e., both of them\nhave points closer than $\\varepsilon$, then the predictions by the classic\ndecision tree are similar. Experimentally, we have also tested that\n$\\varepsilon$-representativeness presents a significant correlation with the\nordering of the feature importance. Moreover, we extend the results\nexperimentally in the context of unseen vehicle collision data for XGboost, a\nmachine-learning component widely adopted for dealing with tabular data.\n","authors":["Javier Perera-Lago","V√≠ctor Toscano-Dur√°n","Eduardo Paluzo-Hidalgo","Sara Narteni","Matteo Rucco"],"pdf_url":"https://arxiv.org/pdf/2404.09541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09536v1","updated":"2024-04-15T07:59:11Z","published":"2024-04-15T07:59:11Z","title":"Beyond Noise: Privacy-Preserving Decentralized Learning with Virtual\n  Nodes","summary":"  Decentralized learning (DL) enables collaborative learning without a server\nand without training data leaving the users' devices. However, the models\nshared in DL can still be used to infer training data. Conventional privacy\ndefenses such as differential privacy and secure aggregation fall short in\neffectively safeguarding user privacy in DL. We introduce Shatter, a novel DL\napproach in which nodes create virtual nodes (VNs) to disseminate chunks of\ntheir full model on their behalf. This enhances privacy by (i) preventing\nattackers from collecting full models from other nodes, and (ii) hiding the\nidentity of the original node that produced a given model chunk. We\ntheoretically prove the convergence of Shatter and provide a formal analysis\ndemonstrating how Shatter reduces the efficacy of attacks compared to when\nexchanging full models between participating nodes. We evaluate the convergence\nand attack resilience of Shatter with existing DL algorithms, with\nheterogeneous datasets, and against three standard privacy attacks, including\ngradient inversion. Our evaluation shows that Shatter not only renders these\nprivacy attacks infeasible when each node operates 16 VNs but also exhibits a\npositive impact on model convergence compared to standard DL. This enhanced\nprivacy comes with a manageable increase in communication volume.\n","authors":["Sayan Biswas","Mathieu Even","Anne-Marie Kermarrec","Laurent Massoulie","Rafael Pires","Rishi Sharma","Martijn de Vos"],"pdf_url":"https://arxiv.org/pdf/2404.09536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09533v1","updated":"2024-04-15T07:53:07Z","published":"2024-04-15T07:53:07Z","title":"WiTUnet: A U-Shaped Architecture Integrating CNN and Transformer for\n  Improved Feature Alignment and Local Information Fusion","summary":"  Low-dose computed tomography (LDCT) has become the technology of choice for\ndiagnostic medical imaging, given its lower radiation dose compared to standard\nCT, despite increasing image noise and potentially affecting diagnostic\naccuracy. To address this, advanced deep learning-based LDCT denoising\nalgorithms have been developed, primarily using Convolutional Neural Networks\n(CNNs) or Transformer Networks with the Unet architecture. This architecture\nenhances image detail by integrating feature maps from the encoder and decoder\nvia skip connections. However, current methods often overlook enhancements to\nthe Unet architecture itself, focusing instead on optimizing encoder and\ndecoder structures. This approach can be problematic due to the significant\ndifferences in feature map characteristics between the encoder and decoder,\nwhere simple fusion strategies may not effectively reconstruct images.In this\npaper, we introduce WiTUnet, a novel LDCT image denoising method that utilizes\nnested, dense skip pathways instead of traditional skip connections to improve\nfeature integration. WiTUnet also incorporates a windowed Transformer structure\nto process images in smaller, non-overlapping segments, reducing computational\nload. Additionally, the integration of a Local Image Perception Enhancement\n(LiPe) module in both the encoder and decoder replaces the standard multi-layer\nperceptron (MLP) in Transformers, enhancing local feature capture and\nrepresentation. Through extensive experimental comparisons, WiTUnet has\ndemonstrated superior performance over existing methods in key metrics such as\nPeak Signal-to-Noise Ratio (PSNR), Structural Similarity (SSIM), and Root Mean\nSquare Error (RMSE), significantly improving noise removal and image quality.\n","authors":["Bin Wang","Fei Deng","Peifan Jiang","Shuang Wang","Xiao Han","Hongjie Zheng"],"pdf_url":"https://arxiv.org/pdf/2404.09533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09532v1","updated":"2024-04-15T07:51:40Z","published":"2024-04-15T07:51:40Z","title":"TMPQ-DM: Joint Timestep Reduction and Quantization Precision Selection\n  for Efficient Diffusion Models","summary":"  Diffusion models have emerged as preeminent contenders in the realm of\ngenerative models. Distinguished by their distinctive sequential generative\nprocesses, characterized by hundreds or even thousands of timesteps, diffusion\nmodels progressively reconstruct images from pure Gaussian noise, with each\ntimestep necessitating full inference of the entire model. However, the\nsubstantial computational demands inherent to these models present challenges\nfor deployment, quantization is thus widely used to lower the bit-width for\nreducing the storage and computing overheads. Current quantization\nmethodologies primarily focus on model-side optimization, disregarding the\ntemporal dimension, such as the length of the timestep sequence, thereby\nallowing redundant timesteps to continue consuming computational resources,\nleaving substantial scope for accelerating the generative process. In this\npaper, we introduce TMPQ-DM, which jointly optimizes timestep reduction and\nquantization to achieve a superior performance-efficiency trade-off, addressing\nboth temporal and model optimization aspects. For timestep reduction, we devise\na non-uniform grouping scheme tailored to the non-uniform nature of the\ndenoising process, thereby mitigating the explosive combinations of timesteps.\nIn terms of quantization, we adopt a fine-grained layer-wise approach to\nallocate varying bit-widths to different layers based on their respective\ncontributions to the final generative performance, thus rectifying performance\ndegradation observed in prior studies. To expedite the evaluation of\nfine-grained quantization, we further devise a super-network to serve as a\nprecision solver by leveraging shared quantization results. These two design\ncomponents are seamlessly integrated within our framework, enabling rapid joint\nexploration of the exponentially large decision space via a gradient-free\nevolutionary search algorithm.\n","authors":["Haojun Sun","Chen Tang","Zhi Wang","Yuan Meng","Jingyan jiang","Xinzhu Ma","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2404.09532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.03409v3","updated":"2024-04-15T07:50:32Z","published":"2023-09-07T00:07:15Z","title":"Large Language Models as Optimizers","summary":"  Optimization is ubiquitous. While derivative-based algorithms have been\npowerful tools for various problems, the absence of gradient imposes challenges\non many real-world applications. In this work, we propose Optimization by\nPROmpting (OPRO), a simple and effective approach to leverage large language\nmodels (LLMs) as optimizers, where the optimization task is described in\nnatural language. In each optimization step, the LLM generates new solutions\nfrom the prompt that contains previously generated solutions with their values,\nthen the new solutions are evaluated and added to the prompt for the next\noptimization step. We first showcase OPRO on linear regression and traveling\nsalesman problems, then move on to our main application in prompt optimization,\nwhere the goal is to find instructions that maximize the task accuracy. With a\nvariety of LLMs, we demonstrate that the best prompts optimized by OPRO\noutperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on\nBig-Bench Hard tasks. Code at https://github.com/google-deepmind/opro.\n","authors":["Chengrun Yang","Xuezhi Wang","Yifeng Lu","Hanxiao Liu","Quoc V. Le","Denny Zhou","Xinyun Chen"],"pdf_url":"https://arxiv.org/pdf/2309.03409v3.pdf","comment":"ICLR 2024; 42 pages, 26 figures, 15 tables. Code at\n  https://github.com/google-deepmind/opro"},{"id":"http://arxiv.org/abs/2404.09529v1","updated":"2024-04-15T07:49:10Z","published":"2024-04-15T07:49:10Z","title":"Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\n  in Large Language Models","summary":"  During inference for transformer-based large language models (LLM),\nprefilling is the computation of the key-value (KV) cache for input tokens in\nthe prompt prior to autoregressive generation. For longer input prompt lengths,\nprefilling will incur a significant overhead on decoding time. In this work, we\nhighlight the following pitfall of prefilling: for batches containing\nhigh-varying prompt lengths, significant computation is wasted by the standard\npractice of padding sequences to the maximum length. As LLMs increasingly\nsupport longer context lengths, potentially up to 10 million tokens, variations\nin prompt lengths within a batch become more pronounced. To address this, we\npropose Prepacking, a simple yet effective method to optimize prefilling\ncomputation. To avoid redundant computation on pad tokens, prepacking combines\nprompts of varying lengths into a sequence and packs multiple sequences into a\ncompact batch using a bin-packing algorithm. It then modifies the attention\nmask and positional encoding to compute multiple prefilled KV-caches for\nmultiple prompts within a single sequence. On standard curated dataset\ncontaining prompts with varying lengths, we obtain a significant speed and\nmemory efficiency improvements as compared to the default padding-based\nprefilling computation within Huggingface across a range of base model\nconfigurations and inference serving scenarios.\n","authors":["Siyan Zhao","Daniel Israel","Guy Van den Broeck","Aditya Grover"],"pdf_url":"https://arxiv.org/pdf/2404.09529v1.pdf","comment":"18 pages, code in https://github.com/siyan-zhao/prepacking"},{"id":"http://arxiv.org/abs/2404.09526v1","updated":"2024-04-15T07:45:04Z","published":"2024-04-15T07:45:04Z","title":"LoongServe: Efficiently Serving Long-context Large Language Models with\n  Elastic Sequence Parallelism","summary":"  The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation.\n","authors":["Bingyang Wu","Shengyu Liu","Yinmin Zhong","Peng Sun","Xuanzhe Liu","Xin Jin"],"pdf_url":"https://arxiv.org/pdf/2404.09526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09524v1","updated":"2024-04-15T07:41:35Z","published":"2024-04-15T07:41:35Z","title":"Dynamic fault detection and diagnosis of industrial alkaline water\n  electrolyzer process with variational Bayesian dictionary learning","summary":"  Alkaline Water Electrolysis (AWE) is one of the simplest green hydrogen\nproduction method using renewable energy.\n  AWE system typically yields process variables that are serially correlated\nand contaminated by measurement uncertainty.\n  A novel robust dynamic variational Bayesian dictionary learning (RDVDL)\nmonitoring approach is proposed to improve the reliability and safety of AWE\noperation.\n  RDVDL employs a sparse Bayesian dictionary learning to preserve the dynamic\nmechanism information of AWE process which allows the easy interpretation of\nfault detection results.\n  To improve the robustness to measurement uncertainty, a low-rank vector\nautoregressive (VAR) method is derived to reliably extract the serial\ncorrelation from process variables.\n  The effectiveness of the proposed approach is demonstrated with an industrial\nhydrogen production process, and RDVDL can efficiently detect and diagnose\ncritical AWE faults.\n","authors":["Qi Zhang","Lei Xie","Weihua Xu","Hongye Su"],"pdf_url":"https://arxiv.org/pdf/2404.09524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16786v2","updated":"2024-04-15T07:36:00Z","published":"2023-10-25T17:20:19Z","title":"Optimal Inflationary Potentials","summary":"  Inflation is a highly favoured theory for the early Universe. It is\ncompatible with current observations of the cosmic microwave background and\nlarge scale structure and is a driver in the quest to detect primordial\ngravitational waves. It is also, given the current quality of the data, highly\nunder-determined with a large number of candidate implementations. We use a new\nmethod in symbolic regression to generate all possible simple scalar field\npotentials for one of two possible basis sets of operators. Treating these as\nsingle-field, slow-roll inflationary models we then score them with an\ninformation-theoretic metric (\"minimum description length\") that quantifies\ntheir efficiency in compressing the information in current data. We explore two\npossible priors on the parameter space of potentials, one related to the\nfunctions' structural complexity and one that uses a Katz back-off language\nmodel to prefer functions that may be theoretically motivated. This enables us\nto identify the inflaton potentials that optimally balance simplicity with\naccuracy at explaining current data, which may subsequently find theoretical\nmotivation. Our exploratory study opens the door to extraction of fundamental\nphysics directly from data, and may be augmented with more refined theoretical\npriors in the quest for a complete understanding of the early Universe.\n","authors":["Tom√°s Sousa","Deaglan J. Bartlett","Harry Desmond","Pedro G. Ferreira"],"pdf_url":"https://arxiv.org/pdf/2310.16786v2.pdf","comment":"13+4 pages, 4 figures; Accepted for publication in Physical Review D"},{"id":"http://arxiv.org/abs/2404.09521v1","updated":"2024-04-15T07:31:48Z","published":"2024-04-15T07:31:48Z","title":"Inferring Behavior-Specific Context Improves Zero-Shot Generalization in\n  Reinforcement Learning","summary":"  In this work, we address the challenge of zero-shot generalization (ZSG) in\nReinforcement Learning (RL), where agents must adapt to entirely novel\nenvironments without additional training. We argue that understanding and\nutilizing contextual cues, such as the gravity level of the environment, is\ncritical for robust generalization, and we propose to integrate the learning of\ncontext representations directly with policy learning. Our algorithm\ndemonstrates improved generalization on various simulated domains,\noutperforming prior context-learning techniques in zero-shot settings. By\njointly learning policy and context, our method acquires behavior-specific\ncontext representations, enabling adaptation to unseen environments and marks\nprogress towards reinforcement learning systems that generalize across diverse\nreal-world tasks. Our code and experiments are available at\nhttps://github.com/tidiane-camaret/contextual_rl_zero_shot.\n","authors":["Tidiane Camaret Ndir","Andr√© Biedenkapp","Noor Awad"],"pdf_url":"https://arxiv.org/pdf/2404.09521v1.pdf","comment":"https://github.com/tidiane-camaret/contextual_rl_zero_shot"},{"id":"http://arxiv.org/abs/2310.06743v2","updated":"2024-04-15T07:31:06Z","published":"2023-10-10T16:12:17Z","title":"Geographic Location Encoding with Spherical Harmonics and Sinusoidal\n  Representation Networks","summary":"  Learning representations of geographical space is vital for any machine\nlearning model that integrates geolocated data, spanning application domains\nsuch as remote sensing, ecology, or epidemiology. Recent work embeds\ncoordinates using sine and cosine projections based on Double Fourier Sphere\n(DFS) features. These embeddings assume a rectangular data domain even on\nglobal data, which can lead to artifacts, especially at the poles. At the same\ntime, little attention has been paid to the exact design of the neural network\narchitectures with which these functional embeddings are combined. This work\nproposes a novel location encoder for globally distributed geographic data that\ncombines spherical harmonic basis functions, natively defined on spherical\nsurfaces, with sinusoidal representation networks (SirenNets) that can be\ninterpreted as learned Double Fourier Sphere embedding. We systematically\nevaluate positional embeddings and neural network architectures across various\nbenchmarks and synthetic evaluation datasets. In contrast to previous\napproaches that require the combination of both positional encoding and neural\nnetworks to learn meaningful representations, we show that both spherical\nharmonics and sinusoidal representation networks are competitive on their own\nbut set state-of-the-art performances across tasks when combined. The model\ncode and experiments are available at\nhttps://github.com/marccoru/locationencoder.\n","authors":["Marc Ru√üwurm","Konstantin Klemmer","Esther Rolf","Robin Zbinden","Devis Tuia"],"pdf_url":"https://arxiv.org/pdf/2310.06743v2.pdf","comment":"Camera-ready version"},{"id":"http://arxiv.org/abs/2312.05356v3","updated":"2024-04-15T07:31:00Z","published":"2023-12-08T20:28:08Z","title":"Neuron-level LLM Patching for Code Generation","summary":"  Large Language Models (LLMs) have found widespread adoption in software\nengineering, particularly in code generation tasks. However, updating these\nmodels with new knowledge can be prohibitively expensive, yet it is essential\nfor maximizing their utility. In this paper, we propose a novel and effective\nmodel editing approach, \\textsc{MENT}, to patch LLMs in coding tasks.\n\\textsc{MENT} is effective, efficient, and reliable. It can correct a neural\nmodel by patching 1 or 2 neurons. As the pioneer work on neuron-level model\nediting of generative models, we formalize the editing process and introduce\nthe involved concepts. Besides, we also introduce new measures to evaluate its\ngeneralization ability, and build a benchmark for further study. Our approach\nis evaluated on three coding tasks, including API-seq recommendation,\nline-level code generation, and pseudocode-to-code transaction. The\nexperimental results show that the proposed approach outperforms the state of\nthe arts by a significant margin in both effectiveness and efficiency measures.\nIn addition, we demonstrate the usages of \\textsc{MENT} for LLM reasoning in\nsoftware engineering. By editing LLM knowledge, the directly or indirectly\ndependent behaviors of API invocation in the chain-of-thought will change\naccordingly. It explained the significance of repairing LLMs.\n","authors":["Jian Gu","Aldeida Aleti","Chunyang Chen","Hongyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.05356v3.pdf","comment":"12 pages, 6 figures, 6 tables, under peer-review"},{"id":"http://arxiv.org/abs/2404.09519v1","updated":"2024-04-15T07:30:26Z","published":"2024-04-15T07:30:26Z","title":"Nonlinear sparse variational Bayesian learning based model predictive\n  control with application to PEMFC temperature control","summary":"  The accuracy of the underlying model predictions is crucial for the success\nof model predictive control (MPC) applications. If the model is unable to\naccurately analyze the dynamics of the controlled system, the performance and\nstability guarantees provided by MPC may not be achieved. Learning-based MPC\ncan learn models from data, improving the applicability and reliability of MPC.\nThis study develops a nonlinear sparse variational Bayesian learning based MPC\n(NSVB-MPC) for nonlinear systems, where the model is learned by the developed\nNSVB method. Variational inference is used by NSVB-MPC to assess the predictive\naccuracy and make the necessary corrections to quantify system uncertainty. The\nsuggested approach ensures input-to-state (ISS) and the feasibility of\nrecursive constraints in accordance with the concept of an invariant terminal\nregion. Finally, a PEMFC temperature control model experiment confirms the\neffectiveness of the NSVB-MPC method.\n","authors":["Qi Zhang","Lei Wang","Weihua Xu","Hongye Su","Lei Xie"],"pdf_url":"https://arxiv.org/pdf/2404.09519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09516v1","updated":"2024-04-15T07:24:45Z","published":"2024-04-15T07:24:45Z","title":"State Space Model for New-Generation Network Alternative to\n  Transformers: A Survey","summary":"  In the post-deep learning era, the Transformer architecture has demonstrated\nits powerful performance across pre-trained big models and various downstream\ntasks. However, the enormous computational demands of this architecture have\ndeterred many researchers. To further reduce the complexity of attention\nmodels, numerous efforts have been made to design more efficient methods. Among\nthem, the State Space Model (SSM), as a possible replacement for the\nself-attention based Transformer model, has drawn more and more attention in\nrecent years. In this paper, we give the first comprehensive review of these\nworks and also provide experimental comparisons and analysis to better\ndemonstrate the features and advantages of SSM. Specifically, we first give a\ndetailed description of principles to help the readers quickly capture the key\nideas of SSM. After that, we dive into the reviews of existing SSMs and their\nvarious applications, including natural language processing, computer vision,\ngraph, multi-modal and multi-media, point cloud/event stream, time series data,\nand other domains. In addition, we give statistical comparisons and analysis of\nthese models and hope it helps the readers to understand the effectiveness of\ndifferent structures on various tasks. Then, we propose possible research\npoints in this direction to better promote the development of the theoretical\nmodel and application of SSM. More related works will be continuously updated\non the following GitHub:\nhttps://github.com/Event-AHU/Mamba_State_Space_Model_Paper_List.\n","authors":["Xiao Wang","Shiao Wang","Yuhe Ding","Yuehang Li","Wentao Wu","Yao Rong","Weizhe Kong","Ju Huang","Shihao Li","Haoxiang Yang","Ziwen Wang","Bo Jiang","Chenglong Li","Yaowei Wang","Yonghong Tian","Jin Tang"],"pdf_url":"https://arxiv.org/pdf/2404.09516v1.pdf","comment":"The First review of State Space Model (SSM)/Mamba and their\n  applications in artificial intelligence, 33 pages"},{"id":"http://arxiv.org/abs/2312.13783v2","updated":"2024-04-15T07:18:45Z","published":"2023-12-21T12:14:31Z","title":"Few Shot Part Segmentation Reveals Compositional Logic for Industrial\n  Anomaly Detection","summary":"  Logical anomalies (LA) refer to data violating underlying logical constraints\ne.g., the quantity, arrangement, or composition of components within an image.\nDetecting accurately such anomalies requires models to reason about various\ncomponent types through segmentation. However, curation of pixel-level\nannotations for semantic segmentation is both time-consuming and expensive.\nAlthough there are some prior few-shot or unsupervised co-part segmentation\nalgorithms, they often fail on images with industrial object. These images have\ncomponents with similar textures and shapes, and a precise differentiation\nproves challenging. In this study, we introduce a novel component segmentation\nmodel for LA detection that leverages a few labeled samples and unlabeled\nimages sharing logical constraints. To ensure consistent segmentation across\nunlabeled images, we employ a histogram matching loss in conjunction with an\nentropy loss. As segmentation predictions play a crucial role, we propose to\nenhance both local and global sample validity detection by capturing key\naspects from visual semantics via three memory banks: class histograms,\ncomponent composition embeddings and patch-level representations. For effective\nLA detection, we propose an adaptive scaling strategy to standardize anomaly\nscores from different memory banks in inference. Extensive experiments on the\npublic benchmark MVTec LOCO AD reveal our method achieves 98.1% AUROC in LA\ndetection vs. 89.6% from competing methods.\n","authors":["Soopil Kim","Sion An","Philip Chikontwe","Myeongkyun Kang","Ehsan Adeli","Kilian M. Pohl","Sang Hyun Park"],"pdf_url":"https://arxiv.org/pdf/2312.13783v2.pdf","comment":"Accepted in AAAI2024"},{"id":"http://arxiv.org/abs/2310.02579v2","updated":"2024-04-15T07:11:03Z","published":"2023-10-04T04:48:55Z","title":"On the Stability of Expressive Positional Encodings for Graphs","summary":"  Designing effective positional encodings for graphs is key to building\npowerful graph transformers and enhancing message-passing graph neural\nnetworks. Although widespread, using Laplacian eigenvectors as positional\nencodings faces two fundamental challenges: (1) \\emph{Non-uniqueness}: there\nare many different eigendecompositions of the same Laplacian, and (2)\n\\emph{Instability}: small perturbations to the Laplacian could result in\ncompletely different eigenspaces, leading to unpredictable changes in\npositional encoding. Despite many attempts to address non-uniqueness, most\nmethods overlook stability, leading to poor generalization on unseen graph\nstructures. We identify the cause of instability to be a ``hard partition'' of\neigenspaces. Hence, we introduce Stable and Expressive Positional Encodings\n(SPE), an architecture for processing eigenvectors that uses eigenvalues to\n``softly partition'' eigenspaces. SPE is the first architecture that is (1)\nprovably stable, and (2) universally expressive for basis invariant functions\nwhilst respecting all symmetries of eigenvectors. Besides guaranteed stability,\nwe prove that SPE is at least as expressive as existing methods, and highly\ncapable of counting graph structures. Finally, we evaluate the effectiveness of\nour method on molecular property prediction, and out-of-distribution\ngeneralization tasks, finding improved generalization compared to existing\npositional encoding methods. Our code is available at\n\\url{https://github.com/Graph-COM/SPE}.\n","authors":["Yinan Huang","William Lu","Joshua Robinson","Yu Yang","Muhan Zhang","Stefanie Jegelka","Pan Li"],"pdf_url":"https://arxiv.org/pdf/2310.02579v2.pdf","comment":"ICLR 2023"},{"id":"http://arxiv.org/abs/2404.09510v1","updated":"2024-04-15T07:06:47Z","published":"2024-04-15T07:06:47Z","title":"Listen to the Waves: Using a Neuronal Model of the Human Auditory System\n  to Predict Ocean Waves","summary":"  Artificial neural networks (ANNs) have evolved from the 1940s primitive\nmodels of brain function to become tools for artificial intelligence. They\ncomprise many units, artificial neurons, interlinked through weighted\nconnections. ANNs are trained to perform tasks through learning rules that\nmodify the connection weights. With these rules being in the focus of research,\nANNs have become a branch of machine learning developing independently from\nneuroscience. Although likely required for the development of truly intelligent\nmachines, the integration of neuroscience into ANNs has remained a neglected\nproposition.\n  Here, we demonstrate that designing an ANN along biological principles\nresults in drastically improved task performance. As a challenging real-world\nproblem, we choose real-time ocean-wave prediction which is essential for\nvarious maritime operations. Motivated by the similarity of ocean waves\nmeasured at a single location to sound waves arriving at the eardrum, we\nredesign an echo state network to resemble the brain's auditory system. This\nyields a powerful predictive tool which is computationally lean, robust with\nrespect to network parameters, and works efficiently across a wide range of sea\nstates. Our results demonstrate the advantages of integrating neuroscience with\nmachine learning and offer a tool for use in the production of green energy\nfrom ocean waves.\n","authors":["Artur Matysiak","Volker Roeber","Henrik Kalisch","Reinhard K√∂nig","Patrick J. C. May"],"pdf_url":"https://arxiv.org/pdf/2404.09510v1.pdf","comment":"23 pages, 6 figures"},{"id":"http://arxiv.org/abs/2404.09494v1","updated":"2024-04-15T06:32:28Z","published":"2024-04-15T06:32:28Z","title":"On the Necessity of Collaboration in Online Model Selection with\n  Decentralized Data","summary":"  We consider online model selection with decentralized data over $M$ clients,\nand study a fundamental problem: the necessity of collaboration. Previous work\ngave a negative answer from the perspective of worst-case regret minimization,\nwhile we give a different answer from the perspective of regret-computational\ncost trade-off. We separately propose a federated algorithm with and without\ncommunication constraint and prove regret bounds that show (i) collaboration is\nunnecessary if we do not limit the computational cost on each client; (ii)\ncollaboration is necessary if we limit the computational cost on each client to\n$o(K)$, where $K$ is the number of candidate hypothesis spaces. As a\nby-product, we improve the regret bounds of algorithms for distributed online\nmulti-kernel learning at a smaller computational and communication cost. Our\nalgorithms rely on three new techniques, i.e., an improved Bernstein's\ninequality for martingale, a federated algorithmic framework, named FOMD-No-LU,\nand decoupling model selection and predictions, which might be of independent\ninterest.\n","authors":["Junfan Li","Zenglin Xu","Zheshun Wu","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2404.09494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09491v1","updated":"2024-04-15T06:26:08Z","published":"2024-04-15T06:26:08Z","title":"Large Language Models Can Automatically Engineer Features for Few-Shot\n  Tabular Learning","summary":"  Large Language Models (LLMs), with their remarkable ability to tackle\nchallenging and unseen reasoning problems, hold immense potential for tabular\nlearning, that is vital for many real-world applications. In this paper, we\npropose a novel in-context learning framework, FeatLLM, which employs LLMs as\nfeature engineers to produce an input data set that is optimally suited for\ntabular predictions. The generated features are used to infer class likelihood\nwith a simple downstream machine learning model, such as linear regression and\nyields high performance few-shot learning. The proposed FeatLLM framework only\nuses this simple predictive model with the discovered features at inference\ntime. Compared to existing LLM-based approaches, FeatLLM eliminates the need to\nsend queries to the LLM for each sample at inference time. Moreover, it merely\nrequires API-level access to LLMs, and overcomes prompt size limitations. As\ndemonstrated across numerous tabular datasets from a wide range of domains,\nFeatLLM generates high-quality rules, significantly (10% on average)\noutperforming alternatives such as TabLLM and STUNT.\n","authors":["Sungwon Han","Jinsung Yoon","Sercan O Arik","Tomas Pfister"],"pdf_url":"https://arxiv.org/pdf/2404.09491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.02810v2","updated":"2024-04-15T06:15:32Z","published":"2022-12-06T08:01:40Z","title":"Data Imputation with Iterative Graph Reconstruction","summary":"  Effective data imputation demands rich latent ``structure\" discovery\ncapabilities from ``plain\" tabular data. Recent advances in graph neural\nnetworks-based data imputation solutions show their strong structure learning\npotential by directly translating tabular data as bipartite graphs. However,\ndue to a lack of relations between samples, those solutions treat all samples\nequally which is against one important observation: ``similar sample should\ngive more information about missing values.\" This paper presents a novel\nIterative graph Generation and Reconstruction framework for Missing data\nimputation(IGRM). Instead of treating all samples equally, we introduce the\nconcept: ``friend networks\" to represent different relations among samples. To\ngenerate an accurate friend network with missing data, an end-to-end friend\nnetwork reconstruction solution is designed to allow for continuous friend\nnetwork optimization during imputation learning. The representation of the\noptimized friend network, in turn, is used to further optimize the data\nimputation process with differentiated message passing. Experiment results on\neight benchmark datasets show that IGRM yields 39.13% lower mean absolute error\ncompared with nine baselines and 9.04% lower than the second-best. Our code is\navailable at https://github.com/G-AILab/IGRM.\n","authors":["Jiajun Zhong","Weiwei Ye","Ning Gui"],"pdf_url":"https://arxiv.org/pdf/2212.02810v2.pdf","comment":"Published in AAAI2023"},{"id":"http://arxiv.org/abs/2404.08010v2","updated":"2024-04-15T06:08:51Z","published":"2024-04-10T03:22:58Z","title":"Differentiable Search for Finding Optimal Quantization Strategy","summary":"  To accelerate and compress deep neural networks (DNNs), many network\nquantization algorithms have been proposed. Although the quantization strategy\nof any algorithm from the state-of-the-arts may outperform others in some\nnetwork architectures, it is hard to prove the strategy is always better than\nothers, and even cannot judge that the strategy is always the best choice for\nall layers in a network. In other words, existing quantization algorithms are\nsuboptimal as they ignore the different characteristics of different layers and\nquantize all layers by a uniform quantization strategy. To solve the issue, in\nthis paper, we propose a differentiable quantization strategy search (DQSS) to\nassign optimal quantization strategy for individual layer by taking advantages\nof the benefits of different quantization algorithms. Specifically, we\nformulate DQSS as a differentiable neural architecture search problem and adopt\nan efficient convolution to efficiently explore the mixed quantization\nstrategies from a global perspective by gradient-based optimization. We conduct\nDQSS for post-training quantization to enable their performance to be\ncomparable with that in full precision models. We also employ DQSS in\nquantization-aware training for further validating the effectiveness of DQSS.\nTo circumvent the expensive optimization cost when employing DQSS in\nquantization-aware training, we update the hyper-parameters and the network\nparameters in a single forward-backward pass. Besides, we adjust the\noptimization process to avoid the potential under-fitting problem.\nComprehensive experiments on high level computer vision task, i.e., image\nclassification, and low level computer vision task, i.e., image\nsuper-resolution, with various network architectures show that DQSS could\noutperform the state-of-the-arts.\n","authors":["Lianqiang Li","Chenqian Yan","Yefei Chen"],"pdf_url":"https://arxiv.org/pdf/2404.08010v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09481v1","updated":"2024-04-15T06:07:10Z","published":"2024-04-15T06:07:10Z","title":"SpamDam: Towards Privacy-Preserving and Adversary-Resistant SMS Spam\n  Detection","summary":"  In this study, we introduce SpamDam, a SMS spam detection framework designed\nto overcome key challenges in detecting and understanding SMS spam, such as the\nlack of public SMS spam datasets, increasing privacy concerns of collecting SMS\ndata, and the need for adversary-resistant detection models. SpamDam comprises\nfour innovative modules: an SMS spam radar that identifies spam messages from\nonline social networks(OSNs); an SMS spam inspector for statistical analysis;\nSMS spam detectors(SSDs) that enable both central training and federated\nlearning; and an SSD analyzer that evaluates model resistance against\nadversaries in realistic scenarios. Leveraging SpamDam, we have compiled over\n76K SMS spam messages from Twitter and Weibo between 2018 and 2023, forming the\nlargest dataset of its kind. This dataset has enabled new insights into recent\nspam campaigns and the training of high-performing binary and multi-label\nclassifiers for spam detection. Furthermore, effectiveness of federated\nlearning has been well demonstrated to enable privacy-preserving SMS spam\ndetection. Additionally, we have rigorously tested the adversarial robustness\nof SMS spam detection models, introducing the novel reverse backdoor attack,\nwhich has shown effectiveness and stealthiness in practical tests.\n","authors":["Yekai Li","Rufan Zhang","Wenxin Rong","Xianghang Mi"],"pdf_url":"https://arxiv.org/pdf/2404.09481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12198v3","updated":"2024-04-15T06:04:55Z","published":"2023-11-20T21:34:52Z","title":"PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics","summary":"  We introduce PhysGaussian, a new method that seamlessly integrates physically\ngrounded Newtonian dynamics within 3D Gaussians to achieve high-quality novel\nmotion synthesis. Employing a custom Material Point Method (MPM), our approach\nenriches 3D Gaussian kernels with physically meaningful kinematic deformation\nand mechanical stress attributes, all evolved in line with continuum mechanics\nprinciples. A defining characteristic of our method is the seamless integration\nbetween physical simulation and visual rendering: both components utilize the\nsame 3D Gaussian kernels as their discrete representations. This negates the\nnecessity for triangle/tetrahedron meshing, marching cubes, \"cage meshes,\" or\nany other geometry embedding, highlighting the principle of \"what you see is\nwhat you simulate (WS$^2$).\" Our method demonstrates exceptional versatility\nacross a wide variety of materials--including elastic entities, metals,\nnon-Newtonian fluids, and granular materials--showcasing its strong\ncapabilities in creating diverse visual content with novel viewpoints and\nmovements. Our project page is at: https://xpandora.github.io/PhysGaussian/\n","authors":["Tianyi Xie","Zeshun Zong","Yuxing Qiu","Xuan Li","Yutao Feng","Yin Yang","Chenfanfu Jiang"],"pdf_url":"https://arxiv.org/pdf/2311.12198v3.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2402.04051v3","updated":"2024-04-15T05:57:26Z","published":"2024-02-06T14:53:28Z","title":"Analysis of Linear Mode Connectivity via Permutation-Based Weight\n  Matching","summary":"  Recently, Ainsworth et al. showed that using weight matching (WM) to minimize\nthe $L_2$ distance in a permutation search of model parameters effectively\nidentifies permutations that satisfy linear mode connectivity (LMC), in which\nthe loss along a linear path between two independently trained models with\ndifferent seeds remains nearly constant. This paper provides a theoretical\nanalysis of LMC using WM, which is crucial for understanding stochastic\ngradient descent's effectiveness and its application in areas like model\nmerging. We first experimentally and theoretically show that permutations found\nby WM do not significantly reduce the $L_2$ distance between two models and the\noccurrence of LMC is not merely due to distance reduction by WM in itself. We\nthen provide theoretical insights showing that permutations can change the\ndirections of the singular vectors, but not the singular values, of the weight\nmatrices in each layer. This finding shows that permutations found by WM mainly\nalign the directions of singular vectors associated with large singular values\nacross models. This alignment brings the singular vectors with large singular\nvalues, which determine the model functionality, closer between pre-merged and\npost-merged models, so that the post-merged model retains functionality similar\nto the pre-merged models, making it easy to satisfy LMC. Finally, we analyze\nthe difference between WM and straight-through estimator (STE), a\ndataset-dependent permutation search method, and show that WM outperforms STE,\nespecially when merging three or more models.\n","authors":["Akira Ito","Masanori Yamada","Atsutoshi Kumagai"],"pdf_url":"https://arxiv.org/pdf/2402.04051v3.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2404.09470v1","updated":"2024-04-15T05:50:46Z","published":"2024-04-15T05:50:46Z","title":"LatticeML: A data-driven application for predicting the effective Young\n  Modulus of high temperature graph based architected materials","summary":"  Architected materials with their unique topology and geometry offer the\npotential to modify physical and mechanical properties. Machine learning can\naccelerate the design and optimization of these materials by identifying\noptimal designs and forecasting performance. This work presents LatticeML, a\ndata-driven application for predicting the effective Young's Modulus of\nhigh-temperature graph-based architected materials. The study considers eleven\ngraph-based lattice structures with two high-temperature alloys, Ti-6Al-4V and\nInconel 625. Finite element simulations were used to compute the effective\nYoung's Modulus of the 2x2x2 unit cell configurations. A machine learning\nframework was developed to predict Young's Modulus, involving data collection,\npreprocessing, implementation of regression models, and deployment of the\nbest-performing model. Five supervised learning algorithms were evaluated, with\nthe XGBoost Regressor achieving the highest accuracy (MSE = 2.7993, MAE =\n1.1521, R-squared = 0.9875). The application uses the Streamlit framework to\ncreate an interactive web interface, allowing users to input material and\ngeometric parameters and obtain predicted Young's Modulus values.\n","authors":["Akshansh Mishra"],"pdf_url":"https://arxiv.org/pdf/2404.09470v1.pdf","comment":"32 pages, 11 figures"},{"id":"http://arxiv.org/abs/2404.09469v1","updated":"2024-04-15T05:44:03Z","published":"2024-04-15T05:44:03Z","title":"Virtually Enriched NYU Depth V2 Dataset for Monocular Depth Estimation:\n  Do We Need Artificial Augmentation?","summary":"  We present ANYU, a new virtually augmented version of the NYU depth v2\ndataset, designed for monocular depth estimation. In contrast to the well-known\napproach where full 3D scenes of a virtual world are utilized to generate\nartificial datasets, ANYU was created by incorporating RGB-D representations of\nvirtual reality objects into the original NYU depth v2 images. We specifically\ndid not match each generated virtual object with an appropriate texture and a\nsuitable location within the real-world image. Instead, an assignment of\ntexture, location, lighting, and other rendering parameters was randomized to\nmaximize a diversity of the training data, and to show that it is randomness\nthat can improve the generalizing ability of a dataset. By conducting extensive\nexperiments with our virtually modified dataset and validating on the original\nNYU depth v2 and iBims-1 benchmarks, we show that ANYU improves the monocular\ndepth estimation performance and generalization of deep neural networks with\nconsiderably different architectures, especially for the current\nstate-of-the-art VPD model. To the best of our knowledge, this is the first\nwork that augments a real-world dataset with randomly generated virtual 3D\nobjects for monocular depth estimation. We make our ANYU dataset publicly\navailable in two training configurations with 10% and 100% additional\nsynthetically enriched RGB-D pairs of training images, respectively, for\nefficient training and empirical exploration of virtual augmentation at\nhttps://github.com/ABrain-One/ANYU\n","authors":["Dmitry Ignatov","Andrey Ignatov","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2404.09469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03124v2","updated":"2024-04-15T05:38:16Z","published":"2024-02-05T15:51:34Z","title":"Towards Eliminating Hard Label Constraints in Gradient Inversion Attacks","summary":"  Gradient inversion attacks aim to reconstruct local training data from\nintermediate gradients exposed in the federated learning framework. Despite\nsuccessful attacks, all previous methods, starting from reconstructing a single\ndata point and then relaxing the single-image limit to batch level, are only\ntested under hard label constraints. Even for single-image reconstruction, we\nstill lack an analysis-based algorithm to recover augmented soft labels. In\nthis work, we change the focus from enlarging batchsize to investigating the\nhard label constraints, considering a more realistic circumstance where label\nsmoothing and mixup techniques are used in the training process. In particular,\nwe are the first to initiate a novel algorithm to simultaneously recover the\nground-truth augmented label and the input feature of the last fully-connected\nlayer from single-input gradients, and provide a necessary condition for any\nanalytical-based label recovery methods. Extensive experiments testify to the\nlabel recovery accuracy, as well as the benefits to the following image\nreconstruction. We believe soft labels in classification tasks are worth\nfurther attention in gradient inversion attacks.\n","authors":["Yanbo Wang","Jian Liang","Ran He"],"pdf_url":"https://arxiv.org/pdf/2402.03124v2.pdf","comment":"ICLR2024 poster"},{"id":"http://arxiv.org/abs/2404.09466v1","updated":"2024-04-15T05:35:09Z","published":"2024-04-15T05:35:09Z","title":"Scoring Intervals using Non-hierarchical Transformer For Automatic Piano\n  Transcription","summary":"  The neural semi-Markov Conditional Random Field (semi-CRF) framework has\ndemonstrated promise for event-based piano transcription. In this framework,\nall events (notes or pedals) are represented as closed intervals tied to\nspecific event types. The neural semi-CRF approach requires an interval scoring\nmatrix that assigns a score for every candidate interval. However, designing an\nefficient and expressive architecture for scoring intervals is not trivial. In\nthis paper, we introduce a simple method for scoring intervals using scaled\ninner product operations that resemble how attention scoring is done in\ntransformers. We show theoretically that, due to the special structure from\nencoding the non-overlapping intervals, under a mild condition, the inner\nproduct operations are expressive enough to represent an ideal scoring matrix\nthat can yield the correct transcription result. We then demonstrate that an\nencoder-only non-hierarchical transformer backbone, operating only on a\nlow-time-resolution feature map, is capable of transcribing piano notes and\npedals with high accuracy and time precision. The experiment shows that our\napproach achieves the new state-of-the-art performance across all subtasks in\nterms of the F1 measure on the Maestro dataset.\n","authors":["Yujia Yan","Zhiyao Duan"],"pdf_url":"https://arxiv.org/pdf/2404.09466v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09465v1","updated":"2024-04-15T05:29:23Z","published":"2024-04-15T05:29:23Z","title":"PhyScene: Physically Interactable 3D Scene Synthesis for Embodied AI","summary":"  With recent developments in Embodied Artificial Intelligence (EAI) research,\nthere has been a growing demand for high-quality, large-scale interactive scene\ngeneration. While prior methods in scene synthesis have prioritized the\nnaturalness and realism of the generated scenes, the physical plausibility and\ninteractivity of scenes have been largely left unexplored. To address this\ndisparity, we introduce PhyScene, a novel method dedicated to generating\ninteractive 3D scenes characterized by realistic layouts, articulated objects,\nand rich physical interactivity tailored for embodied agents. Based on a\nconditional diffusion model for capturing scene layouts, we devise novel\nphysics- and interactivity-based guidance mechanisms that integrate constraints\nfrom object collision, room layout, and object reachability. Through extensive\nexperiments, we demonstrate that PhyScene effectively leverages these guidance\nfunctions for physically interactable scene synthesis, outperforming existing\nstate-of-the-art scene synthesis methods by a large margin. Our findings\nsuggest that the scenes generated by PhyScene hold considerable potential for\nfacilitating diverse skill acquisition among agents within interactive\nenvironments, thereby catalyzing further advancements in embodied AI research.\nProject website: http://physcene.github.io.\n","authors":["Yandan Yang","Baoxiong Jia","Peiyuan Zhi","Siyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2404.09465v1.pdf","comment":"Accepted by CVPR 2024, 18 pages"},{"id":"http://arxiv.org/abs/2404.09463v1","updated":"2024-04-15T05:14:52Z","published":"2024-04-15T05:14:52Z","title":"PRIME: A CyberGIS Platform for Resilience Inference Measurement and\n  Enhancement","summary":"  In an era of increased climatic disasters, there is an urgent need to develop\nreliable frameworks and tools for evaluating and improving community resilience\nto climatic hazards at multiple geographical and temporal scales. Defining and\nquantifying resilience in the social domain is relatively subjective due to the\nintricate interplay of socioeconomic factors with disaster resilience.\nMeanwhile, there is a lack of computationally rigorous, user-friendly tools\nthat can support customized resilience assessment considering local conditions.\nThis study aims to address these gaps through the power of CyberGIS with three\nobjectives: 1) To develop an empirically validated disaster resilience model -\nCustomized Resilience Inference Measurement designed for multi-scale community\nresilience assessment and influential socioeconomic factors identification, 2)\nTo implement a Platform for Resilience Inference Measurement and Enhancement\nmodule in the CyberGISX platform backed by high-performance computing, 3) To\ndemonstrate the utility of PRIME through a representative study. CRIM generates\nvulnerability, adaptability, and overall resilience scores derived from\nempirical hazard parameters. Computationally intensive Machine Learning methods\nare employed to explain the intricate relationships between these scores and\nsocioeconomic driving factors. PRIME provides a web-based notebook interface\nguiding users to select study areas, configure parameters, calculate and\ngeo-visualize resilience scores, and interpret socioeconomic factors shaping\nresilience capacities. A representative study showcases the efficiency of the\nplatform while explaining how the visual results obtained may be interpreted.\nThe essence of this work lies in its comprehensive architecture that\nencapsulates the requisite data, analytical and geo-visualization functions,\nand ML models for resilience assessment.\n","authors":["Debayan Mandal","Dr. Lei Zou","Rohan Singh Wilkho","Joynal Abedin","Bing Zhou","Dr. Heng Cai","Dr. Furqan Baig","Dr. Nasir Gharaibeh","Dr. Nina Lam"],"pdf_url":"https://arxiv.org/pdf/2404.09463v1.pdf","comment":"28 pages, 6 figures"},{"id":"http://arxiv.org/abs/2306.03401v3","updated":"2024-04-15T05:13:25Z","published":"2023-06-06T04:32:10Z","title":"A Lightweight Method for Tackling Unknown Participation Statistics in\n  Federated Averaging","summary":"  In federated learning (FL), clients usually have diverse participation\nstatistics that are unknown a priori, which can significantly harm the\nperformance of FL if not handled properly. Existing works aiming at addressing\nthis problem are usually based on global variance reduction, which requires a\nsubstantial amount of additional memory in a multiplicative factor equal to the\ntotal number of clients. An important open problem is to find a lightweight\nmethod for FL in the presence of clients with unknown participation rates. In\nthis paper, we address this problem by adapting the aggregation weights in\nfederated averaging (FedAvg) based on the participation history of each client.\nWe first show that, with heterogeneous participation statistics, FedAvg with\nnon-optimal aggregation weights can diverge from the optimal solution of the\noriginal FL objective, indicating the need of finding optimal aggregation\nweights. However, it is difficult to compute the optimal weights when the\nparticipation statistics are unknown. To address this problem, we present a new\nalgorithm called FedAU, which improves FedAvg by adaptively weighting the\nclient updates based on online estimates of the optimal weights without knowing\nthe statistics of client participation. We provide a theoretical convergence\nanalysis of FedAU using a novel methodology to connect the estimation error and\nconvergence. Our theoretical results reveal important and interesting insights,\nwhile showing that FedAU converges to an optimal solution of the original\nobjective and has desirable properties such as linear speedup. Our experimental\nresults also verify the advantage of FedAU over baseline methods with various\nparticipation patterns.\n","authors":["Shiqiang Wang","Mingyue Ji"],"pdf_url":"https://arxiv.org/pdf/2306.03401v3.pdf","comment":"Accepted to ICLR 2024"},{"id":"http://arxiv.org/abs/2404.09461v1","updated":"2024-04-15T05:00:40Z","published":"2024-04-15T05:00:40Z","title":"Improved Object-Based Style Transfer with Single Deep Network","summary":"  This research paper proposes a novel methodology for image-to-image style\ntransfer on objects utilizing a single deep convolutional neural network. The\nproposed approach leverages the You Only Look Once version 8 (YOLOv8)\nsegmentation model and the backbone neural network of YOLOv8 for style\ntransfer. The primary objective is to enhance the visual appeal of objects in\nimages by seamlessly transferring artistic styles while preserving the original\nobject characteristics. The proposed approach's novelty lies in combining\nsegmentation and style transfer in a single deep convolutional neural network.\nThis approach omits the need for multiple stages or models, thus resulting in\nsimpler training and deployment of the model for practical applications. The\nresults of this approach are shown on two content images by applying different\nstyle images. The paper also demonstrates the ability to apply style transfer\non multiple objects in the same image.\n","authors":["Harshmohan Kulkarni","Om Khare","Ninad Barve","Sunil Mane"],"pdf_url":"https://arxiv.org/pdf/2404.09461v1.pdf","comment":"In Proceedings of the Fourth International Conference on Innovations\n  in Computational Intelligence and Computer Vision"},{"id":"http://arxiv.org/abs/2404.09456v1","updated":"2024-04-15T04:45:49Z","published":"2024-04-15T04:45:49Z","title":"Hyperbolic Heterogeneous Graph Attention Networks","summary":"  Most previous heterogeneous graph embedding models represent elements in a\nheterogeneous graph as vector representations in a low-dimensional Euclidean\nspace. However, because heterogeneous graphs inherently possess complex\nstructures, such as hierarchical or power-law structures, distortions can occur\nwhen representing them in Euclidean space. To overcome this limitation, we\npropose Hyperbolic Heterogeneous Graph Attention Networks (HHGAT) that learn\nvector representations in hyperbolic spaces with meta-path instances. We\nconducted experiments on three real-world heterogeneous graph datasets,\ndemonstrating that HHGAT outperforms state-of-the-art heterogeneous graph\nembedding models in node classification and clustering tasks.\n","authors":["Jongmin Park","Seunghoon Han","Soohwan Jeong","Sungsu Lim"],"pdf_url":"https://arxiv.org/pdf/2404.09456v1.pdf","comment":"Accepted in ACM THE WEB CONFERENCE 2024 short paper track"},{"id":"http://arxiv.org/abs/2404.09454v1","updated":"2024-04-15T04:43:53Z","published":"2024-04-15T04:43:53Z","title":"Utility-Fairness Trade-Offs and How to Find Them","summary":"  When building classification systems with demographic fairness\nconsiderations, there are two objectives to satisfy: 1) maximizing utility for\nthe specific task and 2) ensuring fairness w.r.t. a known demographic\nattribute. These objectives often compete, so optimizing both can lead to a\ntrade-off between utility and fairness. While existing works acknowledge the\ntrade-offs and study their limits, two questions remain unanswered: 1) What are\nthe optimal trade-offs between utility and fairness? and 2) How can we\nnumerically quantify these trade-offs from data for a desired prediction task\nand demographic attribute of interest? This paper addresses these questions. We\nintroduce two utility-fairness trade-offs: the Data-Space and Label-Space\nTrade-off. The trade-offs reveal three regions within the utility-fairness\nplane, delineating what is fully and partially possible and impossible. We\npropose U-FaTE, a method to numerically quantify the trade-offs for a given\nprediction task and group fairness definition from data samples. Based on the\ntrade-offs, we introduce a new scheme for evaluating representations. An\nextensive evaluation of fair representation learning methods and\nrepresentations from over 1000 pre-trained models revealed that most current\napproaches are far from the estimated and achievable fairness-utility\ntrade-offs across multiple datasets and prediction tasks.\n","authors":["Sepehr Dehdashtian","Bashir Sadeghi","Vishnu Naresh Boddeti"],"pdf_url":"https://arxiv.org/pdf/2404.09454v1.pdf","comment":"IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024"},{"id":"http://arxiv.org/abs/2404.09453v1","updated":"2024-04-15T04:41:53Z","published":"2024-04-15T04:41:53Z","title":"Towards Greener Nights: Exploring AI-Driven Solutions for Light\n  Pollution Management","summary":"  This research endeavors to address the pervasive issue of light pollution\nthrough an interdisciplinary approach, leveraging data science and machine\nlearning techniques. By analyzing extensive datasets and research findings, we\naim to develop predictive models capable of estimating the degree of sky glow\nobserved in various locations and times. Our research seeks to inform\nevidence-based interventions and promote responsible outdoor lighting practices\nto mitigate the adverse impacts of light pollution on ecosystems, energy\nconsumption, and human well-being.\n","authors":["Paras Varshney","Niral Desai","Uzair Ahmed"],"pdf_url":"https://arxiv.org/pdf/2404.09453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04003v2","updated":"2024-04-15T04:40:08Z","published":"2023-10-06T04:13:10Z","title":"The Role of Federated Learning in a Wireless World with Foundation\n  Models","summary":"  Foundation models (FMs) are general-purpose artificial intelligence (AI)\nmodels that have recently enabled multiple brand-new generative AI\napplications. The rapid advances in FMs serve as an important contextual\nbackdrop for the vision of next-generation wireless networks, where federated\nlearning (FL) is a key enabler of distributed network intelligence. Currently,\nthe exploration of the interplay between FMs and FL is still in its nascent\nstage. Naturally, FMs are capable of boosting the performance of FL, and FL\ncould also leverage decentralized data and computing resources to assist in the\ntraining of FMs. However, the exceptionally high requirements that FMs have for\ncomputing resources, storage, and communication overhead would pose critical\nchallenges to FL-enabled wireless networks. In this article, we explore the\nextent to which FMs are suitable for FL over wireless networks, including a\nbroad overview of research challenges and opportunities. In particular, we\ndiscuss multiple new paradigms for realizing future intelligent networks that\nintegrate FMs and FL. We also consolidate several broad research directions\nassociated with these paradigms.\n","authors":["Zihan Chen","Howard H. Yang","Y. C. Tay","Kai Fong Ernest Chong","Tony Q. S. Quek"],"pdf_url":"https://arxiv.org/pdf/2310.04003v2.pdf","comment":"8 pages, 4 figures, 2 tables. This version has been accepted by IEEE\n  Wireless Communiactions"},{"id":"http://arxiv.org/abs/2404.09447v1","updated":"2024-04-15T04:20:01Z","published":"2024-04-15T04:20:01Z","title":"kNN-CLIP: Retrieval Enables Training-Free Segmentation on Continually\n  Expanding Large Vocabularies","summary":"  Rapid advancements in continual segmentation have yet to bridge the gap of\nscaling to large continually expanding vocabularies under compute-constrained\nscenarios. We discover that traditional continual training leads to\ncatastrophic forgetting under compute constraints, unable to outperform\nzero-shot segmentation methods. We introduce a novel strategy for semantic and\npanoptic segmentation with zero forgetting, capable of adapting to continually\ngrowing vocabularies without the need for retraining or large memory costs. Our\ntraining-free approach, kNN-CLIP, leverages a database of instance embeddings\nto enable open-vocabulary segmentation approaches to continually expand their\nvocabulary on any given domain with a single-pass through data, while only\nstoring embeddings minimizing both compute and memory costs. This method\nachieves state-of-the-art mIoU performance across large-vocabulary semantic and\npanoptic segmentation datasets. We hope kNN-CLIP represents a step forward in\nenabling more efficient and adaptable continual segmentation, paving the way\nfor advances in real-world large-vocabulary continual segmentation methods.\n","authors":["Zhongrui Gui","Shuyang Sun","Runjia Li","Jianhao Yuan","Zhaochong An","Karsten Roth","Ameya Prabhu","Philip Torr"],"pdf_url":"https://arxiv.org/pdf/2404.09447v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2404.09445v1","updated":"2024-04-15T04:14:42Z","published":"2024-04-15T04:14:42Z","title":"Exploring Text-to-Motion Generation with Human Preference","summary":"  This paper presents an exploration of preference learning in text-to-motion\ngeneration. We find that current improvements in text-to-motion generation\nstill rely on datasets requiring expert labelers with motion capture systems.\nInstead, learning from human preference data does not require motion capture\nsystems; a labeler with no expertise simply compares two generated motions.\nThis is particularly efficient because evaluating the model's output is easier\nthan gathering the motion that performs a desired task (e.g. backflip). To\npioneer the exploration of this paradigm, we annotate 3,528 preference pairs\ngenerated by MotionGPT, marking the first effort to investigate various\nalgorithms for learning from preference data. In particular, our exploration\nhighlights important design choices when using preference data. Additionally,\nour experimental results show that preference learning has the potential to\ngreatly improve current text-to-motion generative models. Our code and dataset\nare publicly available at\nhttps://github.com/THU-LYJ-Lab/InstructMotion}{https://github.com/THU-LYJ-Lab/InstructMotion\nto further facilitate research in this area.\n","authors":["Jenny Sheng","Matthieu Lin","Andrew Zhao","Kevin Pruvost","Yu-Hui Wen","Yangguang Li","Gao Huang","Yong-Jin Liu"],"pdf_url":"https://arxiv.org/pdf/2404.09445v1.pdf","comment":"Accepted to CVPR 2024 HuMoGen Workshop"},{"id":"http://arxiv.org/abs/2404.09443v1","updated":"2024-04-15T04:02:39Z","published":"2024-04-15T04:02:39Z","title":"Hybrid FedGraph: An efficient hybrid federated learning algorithm using\n  graph convolutional neural network","summary":"  Federated learning is an emerging paradigm for decentralized training of\nmachine learning models on distributed clients, without revealing the data to\nthe central server. Most existing works have focused on horizontal or vertical\ndata distributions, where each client possesses different samples with shared\nfeatures, or each client fully shares only sample indices, respectively.\nHowever, the hybrid scheme is much less studied, even though it is much more\ncommon in the real world. Therefore, in this paper, we propose a generalized\nalgorithm, FedGraph, that introduces a graph convolutional neural network to\ncapture feature-sharing information while learning features from a subset of\nclients. We also develop a simple but effective clustering algorithm that\naggregates features produced by the deep neural networks of each client while\npreserving data privacy.\n","authors":["Jaeyeon Jang","Diego Klabjan","Veena Mendiratta","Fanfei Meng"],"pdf_url":"https://arxiv.org/pdf/2404.09443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02181v2","updated":"2024-04-15T03:57:44Z","published":"2024-03-04T16:23:58Z","title":"Not all Layers of LLMs are Necessary during Inference","summary":"  The inference phase of Large Language Models (LLMs) is very expensive. An\nideal inference stage of LLMs could utilize fewer computational resources while\nstill maintaining its capabilities (e.g., generalization and in-context\nlearning ability). In this paper, we try to answer the question, \"During LLM\ninference, can we use shallow layers for easy instances; and deep layers for\nhard ones?\" To answer this question, we first indicate that Not all Layers are\nNecessary during Inference by statistically analyzing the activated layers\nacross tasks. Then, we propose a simple algorithm named AdaInfer to determine\nthe inference termination moment based on the input instance adaptively. More\nimportantly, AdaInfer does not alter LLM parameters and maintains\ngeneralizability across tasks. Experiments on well-known LLMs (i.e., Llama2\nseries and OPT) show that AdaInfer saves an average of 14.8% of computational\nresources, even up to 50% on sentiment tasks, while maintaining comparable\nperformance. Additionally, this method is orthogonal to other model\nacceleration techniques, potentially boosting inference efficiency further.\n","authors":["Siqi Fan","Xin Jiang","Xiang Li","Xuying Meng","Peng Han","Shuo Shang","Aixin Sun","Yequan Wang","Zhongyuan Wang"],"pdf_url":"https://arxiv.org/pdf/2403.02181v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09438v1","updated":"2024-04-15T03:50:47Z","published":"2024-04-15T03:50:47Z","title":"Developing Lagrangian-based Methods for Nonsmooth Nonconvex Optimization","summary":"  In this paper, we consider the minimization of a nonsmooth nonconvex\nobjective function $f(x)$ over a closed convex subset $\\mathcal{X}$ of\n$\\mathbb{R}^n$, with additional nonsmooth nonconvex constraints $c(x) = 0$. We\ndevelop a unified framework for developing Lagrangian-based methods, which\ntakes a single-step update to the primal variables by some subgradient methods\nin each iteration. These subgradient methods are ``embedded'' into our\nframework, in the sense that they are incorporated as black-box updates to the\nprimal variables. We prove that our proposed framework inherits the global\nconvergence guarantees from these embedded subgradient methods under mild\nconditions. In addition, we show that our framework can be extended to solve\nconstrained optimization problems with expectation constraints. Based on the\nproposed framework, we show that a wide range of existing stochastic\nsubgradient methods, including the proximal SGD, proximal momentum SGD, and\nproximal ADAM, can be embedded into Lagrangian-based methods. Preliminary\nnumerical experiments on deep learning tasks illustrate that our proposed\nframework yields efficient variants of Lagrangian-based methods with\nconvergence guarantees for nonconvex nonsmooth constrained optimization\nproblems.\n","authors":["Nachuan Xiao","Kuangyu Ding","Xiaoyin Hu","Kim-Chuan Toh"],"pdf_url":"https://arxiv.org/pdf/2404.09438v1.pdf","comment":"30 pages, 4 figures"},{"id":"http://arxiv.org/abs/2308.06603v3","updated":"2024-04-15T03:20:41Z","published":"2023-08-12T16:14:44Z","title":"LadleNet: A Two-Stage UNet for Infrared Image to Visible Image\n  Translation Guided by Semantic Segmentation","summary":"  The translation of thermal infrared (TIR) images into visible light (VI)\nimages plays a critical role in enhancing model performance and generalization\ncapability, particularly in various fields such as registration and fusion of\nTIR and VI images. However, current research in this field faces challenges of\ninsufficiently realistic image quality after translation and the difficulty of\nexisting models in adapting to unseen scenarios. In order to develop a more\ngeneralizable image translation architecture, we conducted an analysis of\nexisting translation architectures. By exploring the interpretability of\nintermediate modalities in existing translation architectures, we found that\nthe intermediate modality in the image translation process for street scene\nimages essentially performs semantic segmentation, distinguishing street images\nbased on background and foreground patterns before assigning color information.\nBased on these principles, we propose an improved algorithm based on U-net\ncalled LadleNet. This network utilizes a two-stage U-net concatenation\nstructure, consisting of Handle and Bowl modules. The Handle module is\nresponsible for constructing an abstract semantic space, while the Bowl module\ndecodes the semantic space to obtain the mapped VI image. Due to the\ncharacteristic of semantic segmentation, the Handle module has strong\nextensibility. Therefore, we also propose LadleNet+, which replaces the Handle\nmodule in LadleNet with a pre-trained DeepLabv3+ network, enabling the model to\nhave a more powerful capability in constructing semantic space. The proposed\nmethods were trained and tested on the KAIST dataset, followed by quantitative\nand qualitative analysis. Compared to existing methods, LadleNet and LadleNet+\nachieved an average improvement of 12.4% and 15.2% in SSIM metrics, and 37.9%\nand 50.6% in MS-SSIM metrics, respectively.\n","authors":["Tonghui Zou","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2308.06603v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.15395v5","updated":"2024-04-15T03:20:29Z","published":"2023-09-27T04:33:09Z","title":"Model-Free, Regret-Optimal Best Policy Identification in Online CMDPs","summary":"  This paper considers the best policy identification (BPI) problem in online\nConstrained Markov Decision Processes (CMDPs). We are interested in algorithms\nthat are model-free, have low regret, and identify an approximately optimal\npolicy with a high probability. Existing model-free algorithms for online CMDPs\nwith sublinear regret and constraint violation do not provide any convergence\nguarantee to an optimal policy and provide only average performance guarantees\nwhen a policy is uniformly sampled at random from all previously used policies.\nIn this paper, we develop a new algorithm, named\nPruning-Refinement-Identification (PRI), based on a fundamental structural\nproperty of CMDPs proved before, which we call limited stochasticity. The\nproperty says for a CMDP with $N$ constraints, there exists an optimal policy\nwith at most $N$ stochastic decisions. The proposed algorithm first identifies\nat which step and in which state a stochastic decision has to be taken and then\nfine-tunes the distributions of these stochastic decisions. PRI achieves trio\nobjectives: (i) PRI is a model-free algorithm; and (ii) it outputs an\napproximately optimal policy with a high probability at the end of learning;\nand (iii) PRI guarantees $\\tilde{\\mathcal{O}}(H\\sqrt{K})$ regret and constraint\nviolation, which significantly improves the best existing regret bound\n$\\tilde{\\mathcal{O}}(H^4 \\sqrt{SA}K^{\\frac{4}{5}})$ under a model-free\nalgorithm, where $H$ is the length of each episode, $S$ is the number of\nstates, $A$ is the number of actions, and the total number of episodes during\nlearning is $2K+\\tilde{\\cal O}(K^{0.25}).$ We further present a matching lower\nvia an example that shows under any online learning algorithm, there exists a\nwell-separated CMDP instance such that either the regret or violation has to be\n$\\Omega(H\\sqrt{K}),$ which matches the upper bound by a polylogarithmic factor.\n","authors":["Zihan Zhou","Honghao Wei","Lei Ying"],"pdf_url":"https://arxiv.org/pdf/2309.15395v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09432v1","updated":"2024-04-15T03:12:17Z","published":"2024-04-15T03:12:17Z","title":"The 8th AI City Challenge","summary":"  The eighth AI City Challenge highlighted the convergence of computer vision\nand artificial intelligence in areas like retail, warehouse settings, and\nIntelligent Traffic Systems (ITS), presenting significant research\nopportunities. The 2024 edition featured five tracks, attracting unprecedented\ninterest from 726 teams in 47 countries and regions. Track 1 dealt with\nmulti-target multi-camera (MTMC) people tracking, highlighting significant\nenhancements in camera count, character number, 3D annotation, and camera\nmatrices, alongside new rules for 3D tracking and online tracking algorithm\nencouragement. Track 2 introduced dense video captioning for traffic safety,\nfocusing on pedestrian accidents using multi-camera feeds to improve insights\nfor insurance and prevention. Track 3 required teams to classify driver actions\nin a naturalistic driving analysis. Track 4 explored fish-eye camera analytics\nusing the FishEye8K dataset. Track 5 focused on motorcycle helmet rule\nviolation detection. The challenge utilized two leaderboards to showcase\nmethods, with participants setting new benchmarks, some surpassing existing\nstate-of-the-art achievements.\n","authors":["Shuo Wang","David C. Anastasiu","Zheng Tang","Ming-Ching Chang","Yue Yao","Liang Zheng","Mohammed Shaiqur Rahman","Meenakshi S. Arya","Anuj Sharma","Pranamesh Chakraborty","Sanjita Prajapati","Quan Kong","Norimasa Kobori","Munkhjargal Gochoo","Munkh-Erdene Otgonbold","Fady Alnajjar","Ganzorig Batnasan","Ping-Yang Chen","Jun-Wei Hsieh","Xunlei Wu","Sameer Satish Pusegaonkar","Yizhou Wang","Sujit Biswas","Rama Chellappa"],"pdf_url":"https://arxiv.org/pdf/2404.09432v1.pdf","comment":"Summary of the 8th AI City Challenge Workshop in conjunction with\n  CVPR 2024"},{"id":"http://arxiv.org/abs/2404.09430v1","updated":"2024-04-15T03:04:37Z","published":"2024-04-15T03:04:37Z","title":"On the Efficiency of Privacy Attacks in Federated Learning","summary":"  Recent studies have revealed severe privacy risks in federated learning,\nrepresented by Gradient Leakage Attacks. However, existing studies mainly aim\nat increasing the privacy attack success rate and overlook the high computation\ncosts for recovering private data, making the privacy attack impractical in\nreal applications. In this study, we examine privacy attacks from the\nperspective of efficiency and propose a framework for improving the Efficiency\nof Privacy Attacks in Federated Learning (EPAFL). We make three novel\ncontributions. First, we systematically evaluate the computational costs for\nrepresentative privacy attacks in federated learning, which exhibits a high\npotential to optimize efficiency. Second, we propose three early-stopping\ntechniques to effectively reduce the computational costs of these privacy\nattacks. Third, we perform experiments on benchmark datasets and show that our\nproposed method can significantly reduce computational costs and maintain\ncomparable attack success rates for state-of-the-art privacy attacks in\nfederated learning. We provide the codes on GitHub at\nhttps://github.com/mlsysx/EPAFL.\n","authors":["Nawrin Tabassum","Ka-Ho Chow","Xuyu Wang","Wenbin Zhang","Yanzhao Wu"],"pdf_url":"https://arxiv.org/pdf/2404.09430v1.pdf","comment":"To appear on FedVision 2024. EPAFL (https://github.com/mlsysx/EPAFL)"},{"id":"http://arxiv.org/abs/2309.13788v4","updated":"2024-04-15T03:01:09Z","published":"2023-09-25T00:45:07Z","title":"Can LLM-Generated Misinformation Be Detected?","summary":"  The advent of Large Language Models (LLMs) has made a transformative impact.\nHowever, the potential that LLMs such as ChatGPT can be exploited to generate\nmisinformation has posed a serious concern to online safety and public trust. A\nfundamental research question is: will LLM-generated misinformation cause more\nharm than human-written misinformation? We propose to tackle this question from\nthe perspective of detection difficulty. We first build a taxonomy of\nLLM-generated misinformation. Then we categorize and validate the potential\nreal-world methods for generating misinformation with LLMs. Then, through\nextensive empirical investigation, we discover that LLM-generated\nmisinformation can be harder to detect for humans and detectors compared to\nhuman-written misinformation with the same semantics, which suggests it can\nhave more deceptive styles and potentially cause more harm. We also discuss the\nimplications of our discovery on combating misinformation in the age of LLMs\nand the countermeasures.\n","authors":["Canyu Chen","Kai Shu"],"pdf_url":"https://arxiv.org/pdf/2309.13788v4.pdf","comment":"Accepted to Proceedings of ICLR 2024. 9 pages for main paper, 38\n  pages including appendix. The code, results, dataset for this paper and more\n  resources on \"LLMs Meet Misinformation\" have been released on the project\n  website: https://llm-misinformation.github.io/"},{"id":"http://arxiv.org/abs/2403.13269v2","updated":"2024-04-15T02:55:19Z","published":"2024-03-20T03:07:50Z","title":"AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient\n  Fine-Tuning of Large Models","summary":"  We present a novel Parameter-Efficient Fine-Tuning (PEFT) method, dubbed as\nAdaptive Freezing of Low Rank Adaptation (AFLoRA). Specifically, for each\npre-trained frozen weight tensor, we add a parallel path of trainable low-rank\nmatrices, namely a down-projection and an up-projection matrix, each of which\nis followed by a feature transformation vector. Based on a novel freezing\nscore, we the incrementally freeze these projection matrices during fine-tuning\nto reduce the computation and alleviate over-fitting. Our experimental results\ndemonstrate that we can achieve state-of-the-art performance with an average\nimprovement of up to $0.85\\%$ as evaluated on GLUE benchmark while yeilding up\nto $9.5\\times$ fewer average trainable parameters. While compared in terms of\nruntime, AFLoRA can yield up to $1.86\\times$ improvement as opposed to similar\nPEFT alternatives. Besides the practical utility of our approach, we provide\ninsights on the trainability requirements of LoRA paths at different modules\nand the freezing schedule for the different projection matrices. Code will be\nreleased.\n","authors":["Zeyu Liu","Souvik Kundu","Anni Li","Junrui Wan","Lianghao Jiang","Peter Anthony Beerel"],"pdf_url":"https://arxiv.org/pdf/2403.13269v2.pdf","comment":"5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2402.11858v3","updated":"2024-04-15T02:53:41Z","published":"2024-02-19T06:00:35Z","title":"Stochastic Hessian Fittings with Lie Groups","summary":"  This paper studies the fitting of Hessian or its inverse for stochastic\noptimizations using a Hessian fitting criterion from the preconditioned\nstochastic gradient descent (PSGD) method, which is intimately related to many\ncommonly used second order and adaptive gradient optimizers, e.g., BFGS,\nGaussian-Newton and natural gradient descent, AdaGrad, etc. Our analyses reveal\nthe efficiency and reliability differences among a wide range of preconditioner\nfitting methods, from closed-form to iterative solutions, using Hessian-vector\nproducts or stochastic gradients only, with Hessian fittings in the Euclidean\nspace, the manifold of symmetric positive definite (SPL) matrices, to a variety\nof Lie groups. The most intriguing discovery is that the Hessian fitting itself\nas an optimization problem is strongly convex under mild conditions with a\nspecific yet general enough Lie group. This discovery turns Hessian fitting\ninto a well behaved optimization problem, and facilitates the designs of highly\nefficient and elegant Lie group sparse preconditioner fitting methods for large\nscale stochastic optimizations.\n","authors":["Xi-Lin Li"],"pdf_url":"https://arxiv.org/pdf/2402.11858v3.pdf","comment":"13 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2202.05193v3","updated":"2024-04-15T02:46:34Z","published":"2022-02-10T17:50:26Z","title":"Suboptimal Performance of the Bayes Optimal Algorithm in Frequentist\n  Best Arm Identification","summary":"  We consider the fixed-budget best arm identification problem with rewards\nfollowing normal distributions. In this problem, the forecaster is given $K$\narms (or treatments) and $T$ time steps. The forecaster attempts to find the\narm with the largest mean, via an adaptive experiment conducted using an\nalgorithm. The algorithm's performance is evaluated by simple regret,\nreflecting the quality of the estimated best arm. While frequentist simple\nregret can decrease exponentially with respect to $T$, Bayesian simple regret\ndecreases polynomially. This paper demonstrates that the Bayes optimal\nalgorithm, which minimizes the Bayesian simple regret, does not yield an\nexponential decrease in simple regret under certain parameter settings. This\ncontrasts with the numerous findings that suggest the asymptotic equivalence of\nBayesian and frequentist approaches in fixed sampling regimes. Although the\nBayes optimal algorithm is formulated as a recursive equation that is virtually\nimpossible to compute exactly, we lay the groundwork for future research by\nintroducing a novel concept termed the expected Bellman improvement.\n","authors":["Junpei Komiyama"],"pdf_url":"https://arxiv.org/pdf/2202.05193v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18376v3","updated":"2024-04-15T02:26:43Z","published":"2023-10-27T00:13:59Z","title":"SQLformer: Deep Auto-Regressive Query Graph Generation for Text-to-SQL\n  Translation","summary":"  In recent years, there has been growing interest in text-to-SQL translation,\nwhich is the task of converting natural language questions into executable SQL\nqueries. This technology is important for its potential to democratize data\nextraction from databases. However, some of its key hurdles include domain\ngeneralisation, which is the ability to adapt to previously unseen databases,\nand alignment of natural language questions with the corresponding SQL queries.\nTo overcome these challenges, we introduce SQLformer, a novel Transformer\narchitecture specifically crafted to perform text-to-SQL translation tasks. Our\nmodel predicts SQL queries as abstract syntax trees (ASTs) in an autoregressive\nway, incorporating structural inductive bias in the encoder and decoder layers.\nThis bias, guided by database table and column selection, aids the decoder in\ngenerating SQL query ASTs represented as graphs in a Breadth-First Search\ncanonical order. Comprehensive experiments show the state-of-the-art\nperformance of SQLformer across five widely used text-to-SQL benchmarks. Our\nimplementation is available at https://github.com/AdrianBZG/SQLformer.\n","authors":["Adri√°n Bazaga","Pietro Li√≤","Gos Micklem"],"pdf_url":"https://arxiv.org/pdf/2310.18376v3.pdf","comment":"13 pages, 4 figures, 8 tables"},{"id":"http://arxiv.org/abs/2404.09415v1","updated":"2024-04-15T02:02:15Z","published":"2024-04-15T02:02:15Z","title":"A Review on Machine Learning Algorithms for Dust Aerosol Detection using\n  Satellite Data","summary":"  Dust storms are associated with certain respiratory illnesses across\ndifferent areas in the world. Researchers have devoted time and resources to\nstudy the elements surrounding dust storm phenomena. This paper reviews the\nefforts of those who have investigated dust aerosols using sensors onboard of\nsatellites using machine learning-based approaches. We have reviewed the most\ncommon issues revolving dust aerosol modeling using different datasets and\ndifferent sensors from a historical perspective. Our findings suggest that\nmulti-spectral approaches based on linear and non-linear combinations of\nspectral bands are some of the most successful for visualization and\nquantitative analysis; however, when researchers have leveraged machine\nlearning, performance has been improved and new opportunities to solve unique\nproblems arise.\n","authors":["Nurul Rafi","Pablo Rivas"],"pdf_url":"https://arxiv.org/pdf/2404.09415v1.pdf","comment":"The 23rd International Conference on Artificial Intelligence (ICAI\n  2021)"},{"id":"http://arxiv.org/abs/2404.09413v1","updated":"2024-04-15T02:00:24Z","published":"2024-04-15T02:00:24Z","title":"On the Optimal Regret of Locally Private Linear Contextual Bandit","summary":"  Contextual bandit with linear reward functions is among one of the most\nextensively studied models in bandit and online learning research. Recently,\nthere has been increasing interest in designing \\emph{locally private} linear\ncontextual bandit algorithms, where sensitive information contained in contexts\nand rewards is protected against leakage to the general public. While the\nclassical linear contextual bandit algorithm admits cumulative regret upper\nbounds of $\\tilde O(\\sqrt{T})$ via multiple alternative methods, it has\nremained open whether such regret bounds are attainable in the presence of\nlocal privacy constraints, with the state-of-the-art result being $\\tilde\nO(T^{3/4})$. In this paper, we show that it is indeed possible to achieve an\n$\\tilde O(\\sqrt{T})$ regret upper bound for locally private linear contextual\nbandit. Our solution relies on several new algorithmic and analytical ideas,\nsuch as the analysis of mean absolute deviation errors and layered principal\ncomponent regression in order to achieve small mean absolute deviation errors.\n","authors":["Jiachun Li","David Simchi-Levi","Yining Wang"],"pdf_url":"https://arxiv.org/pdf/2404.09413v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09411v1","updated":"2024-04-15T01:58:18Z","published":"2024-04-15T01:58:18Z","title":"Wasserstein Wormhole: Scalable Optimal Transport Distance with\n  Transformers","summary":"  Optimal transport (OT) and the related Wasserstein metric (W) are powerful\nand ubiquitous tools for comparing distributions. However, computing pairwise\nWasserstein distances rapidly becomes intractable as cohort size grows. An\nattractive alternative would be to find an embedding space in which pairwise\nEuclidean distances map to OT distances, akin to standard multidimensional\nscaling (MDS). We present Wasserstein Wormhole, a transformer-based autoencoder\nthat embeds empirical distributions into a latent space wherein Euclidean\ndistances approximate OT distances. Extending MDS theory, we show that our\nobjective function implies a bound on the error incurred when embedding\nnon-Euclidean distances. Empirically, distances between Wormhole embeddings\nclosely match Wasserstein distances, enabling linear time computation of OT\ndistances. Along with an encoder that maps distributions to embeddings,\nWasserstein Wormhole includes a decoder that maps embeddings back to\ndistributions, allowing for operations in the embedding space to generalize to\nOT spaces, such as Wasserstein barycenter estimation and OT interpolation. By\nlending scalability and interpretability to OT approaches, Wasserstein Wormhole\nunlocks new avenues for data analysis in the fields of computational geometry\nand single-cell biology.\n","authors":["Doron Haviv","Russell Zhang Kunes","Thomas Dougherty","Cassandra Burdziak","Tal Nawy","Anna Gilbert","Dana Pe'er"],"pdf_url":"https://arxiv.org/pdf/2404.09411v1.pdf","comment":"23 Figures, 7 main figures, 2 supplemental figures"},{"id":"http://arxiv.org/abs/2404.08495v2","updated":"2024-04-15T01:56:27Z","published":"2024-04-12T14:25:49Z","title":"Dataset Reset Policy Optimization for RLHF","summary":"  Reinforcement Learning (RL) from Human Preference-based feedback is a popular\nparadigm for fine-tuning generative models, which has produced impressive\nmodels such as GPT-4 and Claude3 Opus. This framework often consists of two\nsteps: learning a reward model from an offline preference dataset followed by\nrunning online RL to optimize the learned reward model. In this work,\nleveraging the idea of reset, we propose a new RLHF algorithm with provable\nguarantees. Motivated by the fact that offline preference dataset provides\ninformative states (i.e., data that is preferred by the labelers), our new\nalgorithm, Dataset Reset Policy Optimization (DR-PO), integrates the existing\noffline preference dataset into the online policy training procedure via\ndataset reset: it directly resets the policy optimizer to the states in the\noffline dataset, instead of always starting from the initial state\ndistribution. In theory, we show that DR-PO learns to perform at least as good\nas any policy that is covered by the offline dataset under general function\napproximation with finite sample complexity. In experiments, we demonstrate\nthat on both the TL;DR summarization and the Anthropic Helpful Harmful (HH)\ndataset, the generation from DR-PO is better than that from Proximal Policy\nOptimization (PPO) and Direction Preference Optimization (DPO), under the\nmetric of GPT4 win-rate. Code for this work can be found at\nhttps://github.com/Cornell-RL/drpo.\n","authors":["Jonathan D. Chang","Wenhao Zhan","Owen Oertell","Kiant√© Brantley","Dipendra Misra","Jason D. Lee","Wen Sun"],"pdf_url":"https://arxiv.org/pdf/2404.08495v2.pdf","comment":"28 pages, 6 tables, 3 Figures, 3 Algorithms"},{"id":"http://arxiv.org/abs/2404.09406v1","updated":"2024-04-15T01:47:44Z","published":"2024-04-15T01:47:44Z","title":"Human-in-the-Loop Segmentation of Multi-species Coral Imagery","summary":"  Broad-scale marine surveys performed by underwater vehicles significantly\nincrease the availability of coral reef imagery, however it is costly and\ntime-consuming for domain experts to label images. Point label propagation is\nan approach used to leverage existing image data labeled with sparse point\nlabels. The resulting augmented ground truth generated is then used to train a\nsemantic segmentation model. Here, we first demonstrate that recent advances in\nfoundation models enable generation of multi-species coral augmented ground\ntruth masks using denoised DINOv2 features and K-Nearest Neighbors (KNN),\nwithout the need for any pre-training or custom-designed algorithms. For\nextremely sparsely labeled images, we propose a labeling regime based on\nhuman-in-the-loop principles, resulting in significant improvement in\nannotation efficiency: If only 5 point labels per image are available, our\nproposed human-in-the-loop approach improves on the state-of-the-art by 17.3%\nfor pixel accuracy and 22.6% for mIoU; and by 10.6% and 19.1% when 10 point\nlabels per image are available. Even if the human-in-the-loop labeling regime\nis not used, the denoised DINOv2 features with a KNN outperforms the prior\nstate-of-the-art by 3.5% for pixel accuracy and 5.7% for mIoU (5 grid points).\nWe also provide a detailed analysis of how point labeling style and the\nquantity of points per image affects the point label propagation quality and\nprovide general recommendations on maximizing point label efficiency.\n","authors":["Scarlett Raine","Ross Marchant","Brano Kusy","Frederic Maire","Niko Suenderhauf","Tobias Fischer"],"pdf_url":"https://arxiv.org/pdf/2404.09406v1.pdf","comment":"10 pages, 6 figures, an additional 4 pages of supplementary material"},{"id":"http://arxiv.org/abs/2404.09403v1","updated":"2024-04-15T01:34:44Z","published":"2024-04-15T01:34:44Z","title":"Neuro-Inspired Information-Theoretic Hierarchical Perception for\n  Multimodal Learning","summary":"  Integrating and processing information from various sources or modalities are\ncritical for obtaining a comprehensive and accurate perception of the real\nworld in autonomous systems and cyber-physical systems. Drawing inspiration\nfrom neuroscience, we develop the Information-Theoretic Hierarchical Perception\n(ITHP) model, which utilizes the concept of information bottleneck. Different\nfrom most traditional fusion models that incorporate all modalities identically\nin neural networks, our model designates a prime modality and regards the\nremaining modalities as detectors in the information pathway, serving to\ndistill the flow of information. Our proposed perception model focuses on\nconstructing an effective and compact information flow by achieving a balance\nbetween the minimization of mutual information between the latent state and the\ninput modal state, and the maximization of mutual information between the\nlatent states and the remaining modal states. This approach leads to compact\nlatent state representations that retain relevant information while minimizing\nredundancy, thereby substantially enhancing the performance of multimodal\nrepresentation learning. Experimental evaluations on the MUStARD, CMU-MOSI, and\nCMU-MOSEI datasets demonstrate that our model consistently distills crucial\ninformation in multimodal learning scenarios, outperforming state-of-the-art\nbenchmarks. Remarkably, on the CMU-MOSI dataset, ITHP surpasses human-level\nperformance in the multimodal sentiment binary classification task across all\nevaluation metrics (i.e., Binary Accuracy, F1 Score, Mean Absolute Error, and\nPearson Correlation).\n","authors":["Xiongye Xiao","Gengshuo Liu","Gaurav Gupta","Defu Cao","Shixuan Li","Yaxing Li","Tianqing Fang","Mingxi Cheng","Paul Bogdan"],"pdf_url":"https://arxiv.org/pdf/2404.09403v1.pdf","comment":"The Twelfth International Conference on Learning Representations.\n  arXiv admin note: text overlap with arXiv:2309.15877"},{"id":"http://arxiv.org/abs/2404.09402v1","updated":"2024-04-15T01:28:16Z","published":"2024-04-15T01:28:16Z","title":"Neural McKean-Vlasov Processes: Distributional Dependence in Diffusion\n  Processes","summary":"  McKean-Vlasov stochastic differential equations (MV-SDEs) provide a\nmathematical description of the behavior of an infinite number of interacting\nparticles by imposing a dependence on the particle density. As such, we study\nthe influence of explicitly including distributional information in the\nparameterization of the SDE. We propose a series of semi-parametric methods for\nrepresenting MV-SDEs, and corresponding estimators for inferring parameters\nfrom data based on the properties of the MV-SDE. We analyze the characteristics\nof the different architectures and estimators, and consider their applicability\nin relevant machine learning problems. We empirically compare the performance\nof the different architectures and estimators on real and synthetic datasets\nfor time series and probabilistic modeling. The results suggest that explicitly\nincluding distributional dependence in the parameterization of the SDE is\neffective in modeling temporal data with interaction under an exchangeability\nassumption while maintaining strong performance for standard It\\^o-SDEs due to\nthe richer class of probability flows associated with MV-SDEs.\n","authors":["Haoming Yang","Ali Hasan","Yuting Ng","Vahid Tarokh"],"pdf_url":"https://arxiv.org/pdf/2404.09402v1.pdf","comment":"Appears in AISTATS 2024"},{"id":"http://arxiv.org/abs/2204.02426v5","updated":"2024-04-15T01:11:48Z","published":"2022-04-05T18:06:49Z","title":"OccamNets: Mitigating Dataset Bias by Favoring Simpler Hypotheses","summary":"  Dataset bias and spurious correlations can significantly impair\ngeneralization in deep neural networks. Many prior efforts have addressed this\nproblem using either alternative loss functions or sampling strategies that\nfocus on rare patterns. We propose a new direction: modifying the network\narchitecture to impose inductive biases that make the network robust to dataset\nbias. Specifically, we propose OccamNets, which are biased to favor simpler\nsolutions by design. OccamNets have two inductive biases. First, they are\nbiased to use as little network depth as needed for an individual example.\nSecond, they are biased toward using fewer image locations for prediction.\nWhile OccamNets are biased toward simpler hypotheses, they can learn more\ncomplex hypotheses if necessary. In experiments, OccamNets outperform or rival\nstate-of-the-art methods run on architectures that do not incorporate these\ninductive biases. Furthermore, we demonstrate that when the state-of-the-art\ndebiasing methods are combined with OccamNets results further improve.\n","authors":["Robik Shrestha","Kushal Kafle","Christopher Kanan"],"pdf_url":"https://arxiv.org/pdf/2204.02426v5.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2104.00170v3","updated":"2024-04-15T01:03:11Z","published":"2021-04-01T00:14:45Z","title":"Are Bias Mitigation Techniques for Deep Learning Effective?","summary":"  A critical problem in deep learning is that systems learn inappropriate\nbiases, resulting in their inability to perform well on minority groups. This\nhas led to the creation of multiple algorithms that endeavor to mitigate bias.\nHowever, it is not clear how effective these methods are. This is because study\nprotocols differ among papers, systems are tested on datasets that fail to test\nmany forms of bias, and systems have access to hidden knowledge or are tuned\nspecifically to the test set. To address this, we introduce an improved\nevaluation protocol, sensible metrics, and a new dataset, which enables us to\nask and answer critical questions about bias mitigation algorithms. We evaluate\nseven state-of-the-art algorithms using the same network architecture and\nhyperparameter selection policy across three benchmark datasets. We introduce a\nnew dataset called Biased MNIST that enables assessment of robustness to\nmultiple bias sources. We use Biased MNIST and a visual question answering\n(VQA) benchmark to assess robustness to hidden biases. Rather than only tuning\nto the test set distribution, we study robustness across different tuning\ndistributions, which is critical because for many applications the test\ndistribution may not be known during development. We find that algorithms\nexploit hidden biases, are unable to scale to multiple forms of bias, and are\nhighly sensitive to the choice of tuning set. Based on our findings, we implore\nthe community to adopt more rigorous assessment of future bias mitigation\nmethods. All data, code, and results are publicly available at:\nhttps://github.com/erobic/bias-mitigators.\n","authors":["Robik Shrestha","Kushal Kafle","Christopher Kanan"],"pdf_url":"https://arxiv.org/pdf/2104.00170v3.pdf","comment":"WACV 2022"},{"id":"http://arxiv.org/abs/2404.08003v2","updated":"2024-04-15T00:59:59Z","published":"2024-04-09T04:21:13Z","title":"Asynchronous Federated Reinforcement Learning with Policy Gradient\n  Updates: Algorithm Design and Convergence Analysis","summary":"  To improve the efficiency of reinforcement learning, we propose a novel\nasynchronous federated reinforcement learning framework termed AFedPG, which\nconstructs a global model through collaboration among $N$ agents using policy\ngradient (PG) updates. To handle the challenge of lagged policies in\nasynchronous settings, we design delay-adaptive lookahead and normalized update\ntechniques that can effectively handle the heterogeneous arrival times of\npolicy gradients. We analyze the theoretical global convergence bound of\nAFedPG, and characterize the advantage of the proposed algorithm in terms of\nboth the sample complexity and time complexity. Specifically, our AFedPG method\nachieves $\\mathcal{O}(\\frac{{\\epsilon}^{-2.5}}{N})$ sample complexity at each\nagent on average. Compared to the single agent setting with\n$\\mathcal{O}(\\epsilon^{-2.5})$ sample complexity, it enjoys a linear speedup\nwith respect to the number of agents. Moreover, compared to synchronous FedPG,\nAFedPG improves the time complexity from $\\mathcal{O}(\\frac{t_{\\max}}{N})$ to\n$\\mathcal{O}(\\frac{1}{\\sum_{i=1}^{N} \\frac{1}{t_{i}}})$, where $t_{i}$ denotes\nthe time consumption in each iteration at the agent $i$, and $t_{\\max}$ is the\nlargest one. The latter complexity $\\mathcal{O}(\\frac{1}{\\sum_{i=1}^{N}\n\\frac{1}{t_{i}}})$ is always smaller than the former one, and this improvement\nbecomes significant in large-scale federated settings with heterogeneous\ncomputing powers ($t_{\\max}\\gg t_{\\min}$). Finally, we empirically verify the\nimproved performances of AFedPG in three MuJoCo environments with varying\nnumbers of agents. We also demonstrate the improvements with different\ncomputing heterogeneity.\n","authors":["Guangchen Lan","Dong-Jun Han","Abolfazl Hashemi","Vaneet Aggarwal","Christopher G. Brinton"],"pdf_url":"https://arxiv.org/pdf/2404.08003v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09392v1","updated":"2024-04-15T00:25:12Z","published":"2024-04-15T00:25:12Z","title":"An Autoencoder-Based Constellation Design for AirComp in Wireless\n  Federated Learning","summary":"  Wireless federated learning (FL) relies on efficient uplink communications to\naggregate model updates across distributed edge devices. Over-the-air\ncomputation (a.k.a. AirComp) has emerged as a promising approach for addressing\nthe scalability challenge of FL over wireless links with limited communication\nresources. Unlike conventional methods, AirComp allows multiple edge devices to\ntransmit uplink signals simultaneously, enabling the parameter server to\ndirectly decode the average global model. However, existing AirComp solutions\nare intrinsically analog, while modern wireless systems predominantly adopt\ndigital modulations. Consequently, careful constellation designs are necessary\nto accurately decode the sum model updates without ambiguity. In this paper, we\npropose an end-to-end communication system supporting AirComp with digital\nmodulation, aiming to overcome the challenges associated with accurate decoding\nof the sum signal with constellation designs. We leverage autoencoder network\nstructures and explore the joint optimization of transmitter and receiver\ncomponents. Our approach fills an important gap in the context of accurately\ndecoding the sum signal in digital modulation-based AirComp, which can advance\nthe deployment of FL in contemporary wireless systems.\n","authors":["Yujia Mu","Xizixiang Wei","Cong Shen"],"pdf_url":"https://arxiv.org/pdf/2404.09392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09391v1","updated":"2024-04-15T00:23:41Z","published":"2024-04-15T00:23:41Z","title":"Privacy at a Price: Exploring its Dual Impact on AI Fairness","summary":"  The worldwide adoption of machine learning (ML) and deep learning models,\nparticularly in critical sectors, such as healthcare and finance, presents\nsubstantial challenges in maintaining individual privacy and fairness. These\ntwo elements are vital to a trustworthy environment for learning systems. While\nnumerous studies have concentrated on protecting individual privacy through\ndifferential privacy (DP) mechanisms, emerging research indicates that\ndifferential privacy in machine learning models can unequally impact separate\ndemographic subgroups regarding prediction accuracy. This leads to a fairness\nconcern, and manifests as biased performance. Although the prevailing view is\nthat enhancing privacy intensifies fairness disparities, a smaller, yet\nsignificant, subset of research suggests the opposite view. In this article,\nwith extensive evaluation results, we demonstrate that the impact of\ndifferential privacy on fairness is not monotonous. Instead, we observe that\nthe accuracy disparity initially grows as more DP noise (enhanced privacy) is\nadded to the ML process, but subsequently diminishes at higher privacy levels\nwith even more noise. Moreover, implementing gradient clipping in the\ndifferentially private stochastic gradient descent ML method can mitigate the\nnegative impact of DP noise on fairness. This mitigation is achieved by\nmoderating the disparity growth through a lower clipping threshold.\n","authors":["Mengmeng Yang","Ming Ding","Youyang Qu","Wei Ni","David Smith","Thierry Rakotoarivelo"],"pdf_url":"https://arxiv.org/pdf/2404.09391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09389v1","updated":"2024-04-15T00:19:47Z","published":"2024-04-15T00:19:47Z","title":"Masked and Shuffled Blind Spot Denoising for Real-World Images","summary":"  We introduce a novel approach to single image denoising based on the Blind\nSpot Denoising principle, which we call MAsked and SHuffled Blind Spot\nDenoising (MASH). We focus on the case of correlated noise, which often plagues\nreal images. MASH is the result of a careful analysis to determine the\nrelationships between the level of blindness (masking) of the input and the\n(unknown) noise correlation. Moreover, we introduce a shuffling technique to\nweaken the local correlation of noise, which in turn yields an additional\ndenoising performance improvement. We evaluate MASH via extensive experiments\non real-world noisy image datasets. We demonstrate on par or better results\ncompared to existing self-supervised denoising methods.\n","authors":["Hamadi Chihaoui","Paolo Favaro"],"pdf_url":"https://arxiv.org/pdf/2404.09389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.05337v2","updated":"2024-04-15T00:19:41Z","published":"2022-11-10T04:40:31Z","title":"Spatiotemporal k-means","summary":"  Spatiotemporal data is increasingly available due to emerging sensor and data\nacquisition technologies that track moving objects. Spatiotemporal clustering\naddresses the need to efficiently discover patterns and trends in moving object\nbehavior without human supervision. One application of interest is the\ndiscovery of moving clusters, where clusters have a static identity, but their\nlocation and content can change over time. We propose a two phase\nspatiotemporal clustering method called spatiotemporal k-means (STkM) that is\nable to analyze the multi-scale relationships within spatiotemporal data. By\noptimizing an objective function that is unified over space and time, the\nmethod can track dynamic clusters at both short and long timescales with\nminimal parameter tuning and no post-processing. We begin by proposing a\ntheoretical generating model for spatiotemporal data and prove the efficacy of\nSTkM in this setting. We then evaluate STkM on a recently developed collective\nanimal behavior benchmark dataset and show that STkM outperforms baseline\nmethods in the low-data limit, which is a critical regime of consideration in\nmany emerging applications. Finally, we showcase how STkM can be extended to\nmore complex machine learning tasks, particularly unsupervised region of\ninterest detection and tracking in videos.\n","authors":["Olga Dorabiala","Devavrat Vivek Dabke","Jennifer Webster","Nathan Kutz","Aleksandr Aravkin"],"pdf_url":"https://arxiv.org/pdf/2211.05337v2.pdf","comment":"18 pages, 5 figures"},{"id":"http://arxiv.org/abs/2404.09387v1","updated":"2024-04-15T00:12:27Z","published":"2024-04-15T00:12:27Z","title":"RankCLIP: Ranking-Consistent Language-Image Pretraining","summary":"  Among the ever-evolving development of vision-language models, contrastive\nlanguage-image pretraining (CLIP) has set new benchmarks in many downstream\ntasks such as zero-shot classifications by leveraging self-supervised\ncontrastive learning on large amounts of text-image pairs. However, its\ndependency on rigid one-to-one mappings overlooks the complex and often\nmultifaceted relationships between and within texts and images. To this end, we\nintroduce RankCLIP, a novel pretraining method that extends beyond the rigid\none-to-one matching framework of CLIP and its variants. By leveraging both\nin-modal and cross-modal ranking consistency, RankCLIP improves the alignment\nprocess, enabling it to capture the nuanced many-to-many relationships between\nand within each modality. Through comprehensive experiments, we demonstrate the\nenhanced capability of RankCLIP to effectively improve performance across\nvarious downstream tasks, notably achieving significant gains in zero-shot\nclassifications over state-of-the-art methods, underscoring the potential of\nRankCLIP in further advancing vision-language pretraining.\n","authors":["Yiming Zhang","Zhuokai Zhao","Zhaorun Chen","Zhili Feng","Zenghui Ding","Yining Sun"],"pdf_url":"https://arxiv.org/pdf/2404.09387v1.pdf","comment":"10 pages, 3 figures, 6 tables. Code and model checkpoints are\n  available at https://github.com/Jam1ezhang/RankCLIP"},{"id":"http://arxiv.org/abs/2404.09386v1","updated":"2024-04-15T00:11:01Z","published":"2024-04-15T00:11:01Z","title":"Integrating Marketing Channels into Quantile Transformation and Bayesian\n  Optimization of Ensemble Kernels for Sales Prediction with Gaussian Process\n  Models","summary":"  This study introduces an innovative Gaussian Process (GP) model utilizing an\nensemble kernel that integrates Radial Basis Function (RBF), Rational\nQuadratic, and Mat\\'ern kernels for product sales forecasting. By applying\nBayesian optimization, we efficiently find the optimal weights for each kernel,\nenhancing the model's ability to handle complex sales data patterns. Our\napproach significantly outperforms traditional GP models, achieving a notable\n98\\% accuracy and superior performance across key metrics including Mean\nSquared Error (MSE), Mean Absolute Error (MAE), Root Mean Squared Error (RMSE),\nand Coefficient of Determination ($R^2$). This advancement underscores the\neffectiveness of ensemble kernels and Bayesian optimization in improving\npredictive accuracy, offering profound implications for machine learning\napplications in sales forecasting.\n","authors":["Shahin Mirshekari","Negin Hayeri Motedayen","Mohammad Ensaf"],"pdf_url":"https://arxiv.org/pdf/2404.09386v1.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2402.15106v2","updated":"2024-04-15T00:10:25Z","published":"2024-02-23T05:33:43Z","title":"Sampling-based Distributed Training with Message Passing Neural Network","summary":"  In this study, we introduce a domain-decomposition-based distributed training\nand inference approach for message-passing neural networks (MPNN). Our\nobjective is to address the challenge of scaling edge-based graph neural\nnetworks as the number of nodes increases. Through our distributed training\napproach, coupled with Nystr\\\"om-approximation sampling techniques, we present\na scalable graph neural network, referred to as DS-MPNN (D and S standing for\ndistributed and sampled, respectively), capable of scaling up to $O(10^5)$\nnodes. We validate our sampling and distributed training approach on two cases:\n(a) a Darcy flow dataset and (b) steady RANS simulations of 2-D airfoils,\nproviding comparisons with both single-GPU implementation and node-based graph\nconvolution networks (GCNs). The DS-MPNN model demonstrates comparable accuracy\nto single-GPU implementation, can accommodate a significantly larger number of\nnodes compared to the single-GPU variant (S-MPNN), and significantly\noutperforms the node-based GCN.\n","authors":["Priyesh Kakka","Sheel Nidhan","Rishikesh Ranade","Jonathan F. MacArt"],"pdf_url":"https://arxiv.org/pdf/2402.15106v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2404.09654v1","updated":"2024-04-15T10:42:22Z","published":"2024-04-15T10:42:22Z","title":"Do LLMs Understand Visual Anomalies? Uncovering LLM Capabilities in\n  Zero-shot Anomaly Detection","summary":"  Large vision-language models (LVLMs) are markedly proficient in deriving\nvisual representations guided by natural language. Recent explorations have\nutilized LVLMs to tackle zero-shot visual anomaly detection (VAD) challenges by\npairing images with textual descriptions indicative of normal and abnormal\nconditions, referred to as anomaly prompts. However, existing approaches depend\non static anomaly prompts that are prone to cross-semantic ambiguity, and\nprioritize global image-level representations over crucial local pixel-level\nimage-to-text alignment that is necessary for accurate anomaly localization. In\nthis paper, we present ALFA, a training-free approach designed to address these\nchallenges via a unified model. We propose a run-time prompt adaptation\nstrategy, which first generates informative anomaly prompts to leverage the\ncapabilities of a large language model (LLM). This strategy is enhanced by a\ncontextual scoring mechanism for per-image anomaly prompt adaptation and\ncross-semantic ambiguity mitigation. We further introduce a novel fine-grained\naligner to fuse local pixel-level semantics for precise anomaly localization,\nby projecting the image-text alignment from global to local semantic spaces.\nExtensive evaluations on the challenging MVTec and VisA datasets confirm ALFA's\neffectiveness in harnessing the language potential for zero-shot VAD, achieving\nsignificant PRO improvements of 12.1% on MVTec AD and 8.9% on VisA compared to\nstate-of-the-art zero-shot VAD approaches.\n","authors":["Jiaqi Zhu","Shaofeng Cai","Fang Deng","Junran Wu"],"pdf_url":"https://arxiv.org/pdf/2404.09654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09516v1","updated":"2024-04-15T07:24:45Z","published":"2024-04-15T07:24:45Z","title":"State Space Model for New-Generation Network Alternative to\n  Transformers: A Survey","summary":"  In the post-deep learning era, the Transformer architecture has demonstrated\nits powerful performance across pre-trained big models and various downstream\ntasks. However, the enormous computational demands of this architecture have\ndeterred many researchers. To further reduce the complexity of attention\nmodels, numerous efforts have been made to design more efficient methods. Among\nthem, the State Space Model (SSM), as a possible replacement for the\nself-attention based Transformer model, has drawn more and more attention in\nrecent years. In this paper, we give the first comprehensive review of these\nworks and also provide experimental comparisons and analysis to better\ndemonstrate the features and advantages of SSM. Specifically, we first give a\ndetailed description of principles to help the readers quickly capture the key\nideas of SSM. After that, we dive into the reviews of existing SSMs and their\nvarious applications, including natural language processing, computer vision,\ngraph, multi-modal and multi-media, point cloud/event stream, time series data,\nand other domains. In addition, we give statistical comparisons and analysis of\nthese models and hope it helps the readers to understand the effectiveness of\ndifferent structures on various tasks. Then, we propose possible research\npoints in this direction to better promote the development of the theoretical\nmodel and application of SSM. More related works will be continuously updated\non the following GitHub:\nhttps://github.com/Event-AHU/Mamba_State_Space_Model_Paper_List.\n","authors":["Xiao Wang","Shiao Wang","Yuhe Ding","Yuehang Li","Wentao Wu","Yao Rong","Weizhe Kong","Ju Huang","Shihao Li","Haoxiang Yang","Ziwen Wang","Bo Jiang","Chenglong Li","Yaowei Wang","Yonghong Tian","Jin Tang"],"pdf_url":"https://arxiv.org/pdf/2404.09516v1.pdf","comment":"The First review of State Space Model (SSM)/Mamba and their\n  applications in artificial intelligence, 33 pages"},{"id":"http://arxiv.org/abs/2404.05317v3","updated":"2024-04-15T04:37:44Z","published":"2024-04-08T09:08:43Z","title":"WebXR, A-Frame and Networked-Aframe as a Basis for an Open Metaverse: A\n  Conceptual Architecture","summary":"  This work proposes a WebXR-based cross-platform conceptual architecture,\nleveraging the A-Frame and Networked-Aframe frameworks, in order to facilitate\nthe development of an open, accessible, and interoperable metaverse. By\nintroducing the concept of spatial web app, this research contributes to the\ndiscourse on the metaverse, offering an architecture that democratizes access\nto virtual environments and extended reality through the web, and aligns with\nTim Berners-Lee's original vision of the World Wide Web as an open platform in\nthe digital realm.\n","authors":["Giuseppe Macario"],"pdf_url":"https://arxiv.org/pdf/2404.05317v3.pdf","comment":"updated section II-C (\"A-Frame\"), updated references"},{"id":"http://arxiv.org/abs/2404.10141v1","updated":"2024-04-15T21:19:10Z","published":"2024-04-15T21:19:10Z","title":"ANCHOR: LLM-driven News Subject Conditioning for Text-to-Image Synthesis","summary":"  Text-to-Image (T2I) Synthesis has made tremendous strides in enhancing\nsynthesized image quality, but current datasets evaluate model performance only\non descriptive, instruction-based prompts. Real-world news image captions take\na more pragmatic approach, providing high-level situational and Named-Entity\n(NE) information and limited physical object descriptions, making them\nabstractive. To evaluate the ability of T2I models to capture intended subjects\nfrom news captions, we introduce the Abstractive News Captions with High-level\ncOntext Representation (ANCHOR) dataset, containing 70K+ samples sourced from 5\ndifferent news media organizations. With Large Language Models (LLM) achieving\nsuccess in language and commonsense reasoning tasks, we explore the ability of\ndifferent LLMs to identify and understand key subjects from abstractive\ncaptions. Our proposed method Subject-Aware Finetuning (SAFE), selects and\nenhances the representation of key subjects in synthesized images by leveraging\nLLM-generated subject weights. It also adapts to the domain distribution of\nnews images and captions through custom Domain Fine-tuning, outperforming\ncurrent T2I baselines on ANCHOR. By launching the ANCHOR dataset, we hope to\nmotivate research in furthering the Natural Language Understanding (NLU)\ncapabilities of T2I models.\n","authors":["Aashish Anantha Ramakrishnan","Sharon X. Huang","Dongwon Lee"],"pdf_url":"https://arxiv.org/pdf/2404.10141v1.pdf","comment":"23 pages, 9 figures"},{"id":"http://arxiv.org/abs/2404.09905v1","updated":"2024-04-15T16:28:51Z","published":"2024-04-15T16:28:51Z","title":"Quality of Experience Oriented Cross-layer Optimization for Real-time XR\n  Video Transmission","summary":"  Extended reality (XR) is one of the most important applications of beyond 5G\nand 6G networks. Real-time XR video transmission presents challenges in terms\nof data rate and delay. In particular, the frame-by-frame transmission mode of\nXR video makes real-time XR video very sensitive to dynamic network\nenvironments. To improve the users' quality of experience (QoE), we design a\ncross-layer transmission framework for real-time XR video. The proposed\nframework allows the simple information exchange between the base station (BS)\nand the XR server, which assists in adaptive bitrate and wireless resource\nscheduling. We utilize the cross-layer information to formulate the problem of\nmaximizing user QoE by finding the optimal scheduling and bitrate adjustment\nstrategies. To address the issue of mismatched time scales between two\nstrategies, we decouple the original problem and solve them individually using\na multi-agent-based approach. Specifically, we propose the multi-step Deep\nQ-network (MS-DQN) algorithm to obtain a frame-priority-based wireless resource\nscheduling strategy and then propose the Transformer-based Proximal Policy\nOptimization (TPPO) algorithm for video bitrate adaptation. The experimental\nresults show that the TPPO+MS-DQN algorithm proposed in this study can improve\nthe QoE by 3.6% to 37.8%. More specifically, the proposed MS-DQN algorithm\nenhances the transmission quality by 49.9%-80.2%.\n","authors":["Guangjin Pan","Shugong Xu","Shunqing Zhang","Xiaojing Chen","Yanzan Sun"],"pdf_url":"https://arxiv.org/pdf/2404.09905v1.pdf","comment":"14 pages, 13 figures. arXiv admin note: text overlap with\n  arXiv:2402.01180"}]},"2024-04-14T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2404.09384v1","updated":"2024-04-14T23:45:23Z","published":"2024-04-14T23:45:23Z","title":"Tasks People Prompt: A Taxonomy of LLM Downstream Tasks in Software\n  Verification and Falsification Approaches","summary":"  Prompting has become one of the main approaches to leverage emergent\ncapabilities of Large Language Models [Brown et al. NeurIPS 2020, Wei et al.\nTMLR 2022, Wei et al. NeurIPS 2022]. During the last year, researchers and\npractitioners have been playing with prompts to see how to make the most of\nLLMs. By homogeneously dissecting 80 papers, we investigate in deep how\nsoftware testing and verification research communities have been abstractly\narchitecting their LLM-enabled solutions. More precisely, first, we want to\nvalidate whether downstream tasks are an adequate concept to convey the\nblueprint of prompt-based solutions. We also aim at identifying number and\nnature of such tasks in solutions. For such goal, we develop a novel downstream\ntask taxonomy that enables pinpointing some engineering patterns in a rather\nvaried spectrum of Software Engineering problems that encompasses testing,\nfuzzing, debugging, vulnerability detection, static analysis and program\nverification approaches.\n","authors":["V√≠ctor A. Braberman","Flavia Bonomo-Braberman","Yiannis Charalambous","Juan G. Colonna","Lucas C. Cordeiro","Rosiane de Freitas"],"pdf_url":"https://arxiv.org/pdf/2404.09384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09383v1","updated":"2024-04-14T23:44:49Z","published":"2024-04-14T23:44:49Z","title":"Low-Resource Named Entity Recognition with Cross-Lingual,\n  Character-Level Neural Conditional Random Fields","summary":"  Low-resource named entity recognition is still an open problem in NLP. Most\nstate-of-the-art systems require tens of thousands of annotated sentences in\norder to obtain high performance. However, for most of the world's languages,\nit is unfeasible to obtain such annotation. In this paper, we present a\ntransfer learning scheme, whereby we train character-level neural CRFs to\npredict named entities for both high-resource languages and low resource\nlanguages jointly. Learning character representations for multiple related\nlanguages allows transfer among the languages, improving F1 by up to 9.8 points\nover a loglinear CRF baseline.\n","authors":["Ryan Cotterell","Kevin Duh"],"pdf_url":"https://arxiv.org/pdf/2404.09383v1.pdf","comment":"IJCNLP 2017"},{"id":"http://arxiv.org/abs/2404.09375v1","updated":"2024-04-14T23:05:10Z","published":"2024-04-14T23:05:10Z","title":"Deceptive Patterns of Intelligent and Interactive Writing Assistants","summary":"  Large Language Models have become an integral part of new intelligent and\ninteractive writing assistants. Many are offered commercially with a\nchatbot-like UI, such as ChatGPT, and provide little information about their\ninner workings. This makes this new type of widespread system a potential\ntarget for deceptive design patterns. For example, such assistants might\nexploit hidden costs by providing guidance up until a certain point before\nasking for a fee to see the rest. As another example, they might sneak unwanted\ncontent/edits into longer generated or revised text pieces (e.g. to influence\nthe expressed opinion). With these and other examples, we conceptually transfer\nseveral deceptive patterns from the literature to the new context of AI writing\nassistants. Our goal is to raise awareness and encourage future research into\nhow the UI and interaction design of such systems can impact people and their\nwriting.\n","authors":["Karim Benharrak","Tim Zindulka","Daniel Buschek"],"pdf_url":"https://arxiv.org/pdf/2404.09375v1.pdf","comment":"Published as a workshop paper to the In2Writing workshop at CHI 2024"},{"id":"http://arxiv.org/abs/2401.12970v2","updated":"2024-04-14T22:34:37Z","published":"2024-01-23T18:57:53Z","title":"Raidar: geneRative AI Detection viA Rewriting","summary":"  We find that large language models (LLMs) are more likely to modify\nhuman-written text than AI-generated text when tasked with rewriting. This\ntendency arises because LLMs often perceive AI-generated text as high-quality,\nleading to fewer modifications. We introduce a method to detect AI-generated\ncontent by prompting LLMs to rewrite text and calculating the editing distance\nof the output. We dubbed our geneRative AI Detection viA Rewriting method\nRaidar. Raidar significantly improves the F1 detection scores of existing AI\ncontent detection models -- both academic and commercial -- across various\ndomains, including News, creative writing, student essays, code, Yelp reviews,\nand arXiv papers, with gains of up to 29 points. Operating solely on word\nsymbols without high-dimensional features, our method is compatible with black\nbox LLMs, and is inherently robust on new content. Our results illustrate the\nunique imprint of machine-generated text through the lens of the machines\nthemselves.\n","authors":["Chengzhi Mao","Carl Vondrick","Hao Wang","Junfeng Yang"],"pdf_url":"https://arxiv.org/pdf/2401.12970v2.pdf","comment":"Accepted by ICLR 2024, Large Language Models, Detection"},{"id":"http://arxiv.org/abs/2404.09371v1","updated":"2024-04-14T22:22:58Z","published":"2024-04-14T22:22:58Z","title":"The Effect of Data Partitioning Strategy on Model Generalizability: A\n  Case Study of Morphological Segmentation","summary":"  Recent work to enhance data partitioning strategies for more realistic model\nevaluation face challenges in providing a clear optimal choice. This study\naddresses these challenges, focusing on morphological segmentation and\nsynthesizing limitations related to language diversity, adoption of multiple\ndatasets and splits, and detailed model comparisons. Our study leverages data\nfrom 19 languages, including ten indigenous or endangered languages across 10\nlanguage families with diverse morphological systems (polysynthetic, fusional,\nand agglutinative) and different degrees of data availability. We conduct\nlarge-scale experimentation with varying sized combinations of training and\nevaluation sets as well as new test data. Our results show that, when faced\nwith new test data: (1) models trained from random splits are able to achieve\nhigher numerical scores; (2) model rankings derived from random splits tend to\ngeneralize more consistently.\n","authors":["Zoey Liu","Bonnie J. Dorr"],"pdf_url":"https://arxiv.org/pdf/2404.09371v1.pdf","comment":"Accepted to 2024 Annual Conference of the North American Chapter of\n  the Association for Computational Linguistics (16 pages including 9 tables\n  and 1 figure)"},{"id":"http://arxiv.org/abs/2404.09366v1","updated":"2024-04-14T21:38:50Z","published":"2024-04-14T21:38:50Z","title":"Understanding the Role of Temperature in Diverse Question Generation by\n  GPT-4","summary":"  We conduct a preliminary study of the effect of GPT's temperature parameter\non the diversity of GPT4-generated questions. We find that using higher\ntemperature values leads to significantly higher diversity, with different\ntemperatures exposing different types of similarity between generated sets of\nquestions. We also demonstrate that diverse question generation is especially\ndifficult for questions targeting lower levels of Bloom's Taxonomy.\n","authors":["Arav Agarwal","Karthik Mittal","Aidan Doyle","Pragnya Sridhar","Zipiao Wan","Jacob Arthur Doughty","Jaromir Savelka","Majd Sakr"],"pdf_url":"https://arxiv.org/pdf/2404.09366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.11767v4","updated":"2024-04-14T21:20:58Z","published":"2023-08-15T23:22:37Z","title":"Detection of ChatGPT Fake Science with the xFakeSci Learning Algorithm","summary":"  Generative AI tools exemplified by ChatGPT are becoming a new reality. This\nstudy is motivated by the premise that ``AI generated content may exhibit a\ndistinctive behavior that can be separated from scientific articles''. In this\nstudy, we show how articles can be generated using means of prompt engineering\nfor various diseases and conditions. We then show how we tested this premise in\ntwo phases and prove its validity. Subsequently, we introduce xFakeSci, a novel\nlearning algorithm, that is capable of distinguishing ChatGPT-generated\narticles from publications produced by scientists. The algorithm is trained\nusing network models driven from both sources. As for the classification step,\nit was performed using 300 articles per condition. The actual label steps took\nplace against an equal mix of 50 generated articles and 50 authentic PubMed\nabstracts. The testing also spanned publication periods from 2010 to 2024 and\nencompassed research on three distinct diseases: cancer, depression, and\nAlzheimer's. Further, we evaluated the accuracy of the xFakeSci algorithm\nagainst some of the classical data mining algorithms (e.g., Support Vector\nMachines, Regression, and Naive Bayes). The xFakeSci algorithm achieved F1\nscores ranging from 80% to 94%, outperforming common data mining algorithms,\nwhich scored F1 values between 38% and 52%. We attribute the noticeable\ndifference to the introduction of calibration and a proximity distance\nheuristic, which underscores this promising performance. Indeed, the prediction\nof fake science generated by ChatGPT presents a considerable challenge.\nNonetheless, the introduction of the xFakeSci algorithm is a significant step\non the way to combating fake science.\n","authors":["Ahmed Abdeen Hamed","Xindong Wu"],"pdf_url":"https://arxiv.org/pdf/2308.11767v4.pdf","comment":"18 pages, 8 figures, 8 tables, 5 algorithms"},{"id":"http://arxiv.org/abs/2403.07342v2","updated":"2024-04-14T20:53:02Z","published":"2024-03-12T06:01:04Z","title":"Rethinking ASTE: A Minimalist Tagging Scheme Alongside Contrastive\n  Learning","summary":"  Aspect Sentiment Triplet Extraction (ASTE) is a burgeoning subtask of\nfine-grained sentiment analysis, aiming to extract structured sentiment\ntriplets from unstructured textual data. Existing approaches to ASTE often\ncomplicate the task with additional structures or external data. In this\nresearch, we propose a novel tagging scheme and employ a contrastive learning\napproach to mitigate these challenges. The proposed approach demonstrates\ncomparable or superior performance in comparison to state-of-the-art\ntechniques, while featuring a more compact design and reduced computational\noverhead. Notably, even in the era of Large Language Models (LLMs), our method\nexhibits superior efficacy compared to GPT 3.5 and GPT 4 in a few-shot learning\nscenarios. This study also provides valuable insights for the advancement of\nASTE techniques within the paradigm of large language models.\n","authors":["Qiao Sun","Liujia Yang","Minghao Ma","Nanyang Ye","Qinying Gu"],"pdf_url":"https://arxiv.org/pdf/2403.07342v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09356v1","updated":"2024-04-14T20:49:53Z","published":"2024-04-14T20:49:53Z","title":"LLeMpower: Understanding Disparities in the Control and Access of Large\n  Language Models","summary":"  Large Language Models (LLMs) are a powerful technology that augment human\nskill to create new opportunities, akin to the development of steam engines and\nthe internet. However, LLMs come with a high cost. They require significant\ncomputing resources and energy to train and serve. Inequity in their control\nand access has led to concentration of ownership and power to a small\ncollection of corporations. In our study, we collect training and inference\nrequirements for various LLMs. We then analyze the economic strengths of\nnations and organizations in the context of developing and serving these\nmodels. Additionally, we also look at whether individuals around the world can\naccess and use this emerging technology. We compare and contrast these groups\nto show that these technologies are monopolized by a surprisingly few entities.\nWe conclude with a qualitative study on the ethical implications of our\nfindings and discuss future directions towards equity in LLM access.\n","authors":["Vishwas Sathish","Hannah Lin","Aditya K Kamath","Anish Nyayachavadi"],"pdf_url":"https://arxiv.org/pdf/2404.09356v1.pdf","comment":"11 total pages, 7 page text, 4 page references, 3 figures (with\n  subfigures), 1 table"},{"id":"http://arxiv.org/abs/2404.09339v1","updated":"2024-04-14T19:45:47Z","published":"2024-04-14T19:45:47Z","title":"Towards Practical Tool Usage for Continually Learning LLMs","summary":"  Large language models (LLMs) show an innate skill for solving language based\ntasks. But insights have suggested an inability to adjust for information or\ntask-solving skills becoming outdated, as their knowledge, stored directly\nwithin their parameters, remains static in time. Tool use helps by offloading\nwork to systems that the LLM can access through an interface, but LLMs that use\nthem still must adapt to nonstationary environments for prolonged use, as new\ntools can emerge and existing tools can change. Nevertheless, tools require\nless specialized knowledge, therefore we hypothesize they are better suited for\ncontinual learning (CL) as they rely less on parametric memory for solving\ntasks and instead focus on learning when to apply pre-defined tools. To verify\nthis, we develop a synthetic benchmark and follow this by aggregating existing\nNLP tasks to form a more realistic testing scenario. While we demonstrate\nscaling model size is not a solution, regardless of tool usage, continual\nlearning techniques can enable tool LLMs to both adapt faster while forgetting\nless, highlighting their potential as continual learners.\n","authors":["Jerry Huang","Prasanna Parthasarathi","Mehdi Rezagholizadeh","Sarath Chandar"],"pdf_url":"https://arxiv.org/pdf/2404.09339v1.pdf","comment":"20 pages, 11 tables, 7 figures"},{"id":"http://arxiv.org/abs/2404.09338v1","updated":"2024-04-14T19:45:35Z","published":"2024-04-14T19:45:35Z","title":"Entropy Guided Extrapolative Decoding to Improve Factuality in Large\n  Language Models","summary":"  Large language models (LLMs) exhibit impressive natural language capabilities\nbut suffer from hallucination -- generating content ungrounded in the realities\nof training data. Recent work has focused on decoding techniques to improve\nfactuality during inference by leveraging LLMs' hierarchical representation of\nfactual knowledge, manipulating the predicted distributions at inference time.\nCurrent state-of-the-art approaches refine decoding by contrasting early-exit\ndistributions from a lower layer with the final layer to exploit information\nrelated to factuality within the model forward procedure. However, such methods\noften assume the final layer is the most reliable and the lower layer selection\nprocess depends on it. In this work, we first propose extrapolation of critical\ntoken probabilities beyond the last layer for more accurate contrasting. We\nadditionally employ layer-wise entropy-guided lower layer selection, decoupling\nthe selection process from the final layer. Experiments demonstrate strong\nperformance - surpassing state-of-the-art on multiple different datasets by\nlarge margins. Analyses show different kinds of prompts respond to different\nselection strategies.\n","authors":["Souvik Das","Lifeng Jin","Linfeng Song","Haitao Mi","Baolin Peng","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2404.09338v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2309.08943v3","updated":"2024-04-14T19:38:57Z","published":"2023-09-16T10:27:28Z","title":"Contextual Label Projection for Cross-Lingual Structured Prediction","summary":"  Label projection, which involves obtaining translated labels and texts\njointly, is essential for leveraging machine translation to facilitate\ncross-lingual transfer in structured prediction tasks. Prior research exploring\nlabel projection often compromise translation accuracy by favoring simplified\nlabel translation or relying solely on word-level alignments. In this paper, we\nintroduce a novel label projection approach, CLaP, which translates text to the\ntarget language and performs contextual translation on the labels using the\ntranslated text as the context, ensuring better accuracy for the translated\nlabels. We leverage instruction-tuned language models with multilingual\ncapabilities as our contextual translator, imposing the constraint of the\npresence of translated labels in the translated text via instructions. We\nbenchmark CLaP with other label projection techniques on zero-shot\ncross-lingual transfer across 39 languages on two representative structured\nprediction tasks - event argument extraction (EAE) and named entity recognition\n(NER), showing over 2.4 F1 improvement for EAE and 1.4 F1 improvement for NER.\nWe further explore the applicability of CLaP on ten extremely low-resource\nlanguages to showcase its potential for cross-lingual structured prediction.\n","authors":["Tanmay Parekh","I-Hung Hsu","Kuan-Hao Huang","Kai-Wei Chang","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2309.08943v3.pdf","comment":"Accepted at NAACL 2024"},{"id":"http://arxiv.org/abs/2404.09336v1","updated":"2024-04-14T19:36:04Z","published":"2024-04-14T19:36:04Z","title":"Self-Selected Attention Span for Accelerating Large Language Model\n  Inference","summary":"  Large language models (LLMs) can solve challenging tasks. However, their\ninference computation on modern GPUs is highly inefficient due to the\nincreasing number of tokens they must attend to as they generate new ones. To\naddress this inefficiency, we capitalize on LLMs' problem-solving capabilities\nto optimize their own inference-time efficiency. We demonstrate with two\nspecific tasks: (a) evaluating complex arithmetic expressions and (b)\nsummarizing news articles. For both tasks, we create custom datasets to\nfine-tune an LLM. The goal of fine-tuning is twofold: first, to make the LLM\nlearn to solve the evaluation or summarization task, and second, to train it to\nidentify the minimal attention spans required for each step of the task. As a\nresult, the fine-tuned model is able to convert these self-identified minimal\nattention spans into sparse attention masks on-the-fly during inference. We\ndevelop a custom CUDA kernel to take advantage of the reduced context to attend\nto. We demonstrate that using this custom CUDA kernel improves the throughput\nof LLM inference by 28%. Our work presents an end-to-end demonstration showing\nthat training LLMs to self-select their attention spans speeds up\nautoregressive inference in solving real-world tasks.\n","authors":["Tian Jin","Wanzin Yazar","Zifei Xu","Sayeh Sharify","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2404.09336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09329v1","updated":"2024-04-14T19:01:20Z","published":"2024-04-14T19:01:20Z","title":"Large Language Models are as persuasive as humans, but why? About the\n  cognitive effort and moral-emotional language of LLM arguments","summary":"  Large Language Models (LLMs) are already as persuasive as humans. However, we\nknow very little about why. This paper investigates the persuasion strategies\nof LLMs, comparing them with human-generated arguments. Using a dataset of\n1,251 participants in an experiment, we analyze the persuaion strategies of\nLLM-generated and human-generated arguments using measures of cognitive effort\n(lexical and grammatical complexity) and moral-emotional language (sentiment\nand moral analysis). The study reveals that LLMs produce arguments that require\nhigher cognitive effort, exhibiting more complex grammatical and lexical\nstructures than human counterparts. Additionally, LLMs demonstrate a\nsignificant propensity to engage more deeply with moral language, utilizing\nboth positive and negative moral foundations more frequently than humans. In\ncontrast with previous research, no significant difference was found in the\nemotional content produced by LLMs and humans. These findings contribute to the\ndiscourse on AI and persuasion, highlighting the dual potential of LLMs to both\nenhance and undermine informational integrity through communication strategies\nfor digital persuasion.\n","authors":["Carlos Carrasco-Farre"],"pdf_url":"https://arxiv.org/pdf/2404.09329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.04022v2","updated":"2024-04-14T17:30:24Z","published":"2024-04-05T11:06:07Z","title":"Good Books are Complex Matters: Gauging Complexity Profiles Across\n  Diverse Categories of Perceived Literary Quality","summary":"  In this study, we employ a classification approach to show that different\ncategories of literary \"quality\" display unique linguistic profiles, leveraging\na corpus that encompasses titles from the Norton Anthology, Penguin Classics\nseries, and the Open Syllabus project, contrasted against contemporary\nbestsellers, Nobel prize winners and recipients of prestigious literary awards.\nOur analysis reveals that canonical and so called high-brow texts exhibit\ndistinct textual features when compared to other quality categories such as\nbestsellers and popular titles as well as to control groups, likely responding\nto distinct (but not mutually exclusive) models of quality. We apply a classic\nmachine learning approach, namely Random Forest, to distinguish quality novels\nfrom \"control groups\", achieving up to 77\\% F1 scores in differentiating\nbetween the categories. We find that quality category tend to be easier to\ndistinguish from control groups than from other quality categories, suggesting\nthan literary quality features might be distinguishable but shared through\nquality proxies.\n","authors":["Yuri Bizzoni","Pascale Feldkamp","Ida Marie Lassen","Mia Jacobsen","Mads Rosendahl Thomsen","Kristoffer Nielbo"],"pdf_url":"https://arxiv.org/pdf/2404.04022v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09299v1","updated":"2024-04-14T16:47:38Z","published":"2024-04-14T16:47:38Z","title":"Reap the Wild Wind: Detecting Media Storms in Large-Scale News Corpora","summary":"  Media Storms, dramatic outbursts of attention to a story, are central\ncomponents of media dynamics and the attention landscape. Despite their\nsignificance, there has been little systematic and empirical research on this\nconcept due to issues of measurement and operationalization. We introduce an\niterative human-in-the-loop method to identify media storms in a large-scale\ncorpus of news articles. The text is first transformed into signals of\ndispersion based on several textual characteristics. In each iteration, we\napply unsupervised anomaly detection to these signals; each anomaly is then\nvalidated by an expert to confirm the presence of a storm, and those results\nare then used to tune the anomaly detection in the next iteration. We\ndemonstrate the applicability of this method in two scenarios: first,\nsupplementing an initial list of media storms within a specific time frame; and\nsecond, detecting media storms in new time periods. We make available a media\nstorm dataset compiled using both scenarios. Both the method and dataset offer\nthe basis for comprehensive empirical research into the concept of media\nstorms, including characterizing them and predicting their outbursts and\ndurations, in mainstream media or social media platforms.\n","authors":["Dror K. Markus","Effi Levi","Tamir Sheafer","Shaul R. Shenhav"],"pdf_url":"https://arxiv.org/pdf/2404.09299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09296v1","updated":"2024-04-14T16:34:31Z","published":"2024-04-14T16:34:31Z","title":"Cross-Data Knowledge Graph Construction for LLM-enabled Educational\n  Question-Answering System: A~Case~Study~at~HCMUT","summary":"  In today's rapidly evolving landscape of Artificial Intelligence, large\nlanguage models (LLMs) have emerged as a vibrant research topic. LLMs find\napplications in various fields and contribute significantly. Despite their\npowerful language capabilities, similar to pre-trained language models (PLMs),\nLLMs still face challenges in remembering events, incorporating new\ninformation, and addressing domain-specific issues or hallucinations. To\novercome these limitations, researchers have proposed Retrieval-Augmented\nGeneration (RAG) techniques, some others have proposed the integration of LLMs\nwith Knowledge Graphs (KGs) to provide factual context, thereby improving\nperformance and delivering more accurate feedback to user queries.\n  Education plays a crucial role in human development and progress. With the\ntechnology transformation, traditional education is being replaced by digital\nor blended education. Therefore, educational data in the digital environment is\nincreasing day by day. Data in higher education institutions are diverse,\ncomprising various sources such as unstructured/structured text, relational\ndatabases, web/app-based API access, etc. Constructing a Knowledge Graph from\nthese cross-data sources is not a simple task. This article proposes a method\nfor automatically constructing a Knowledge Graph from multiple data sources and\ndiscusses some initial applications (experimental trials) of KG in conjunction\nwith LLMs for question-answering tasks.\n","authors":["Tuan Bui","Oanh Tran","Phuong Nguyen","Bao Ho","Long Nguyen","Thang Bui","Tho Quan"],"pdf_url":"https://arxiv.org/pdf/2404.09296v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.17661v2","updated":"2024-04-14T15:45:53Z","published":"2024-03-26T12:47:39Z","title":"Language Models for Text Classification: Is In-Context Learning Enough?","summary":"  Recent foundational language models have shown state-of-the-art performance\nin many NLP tasks in zero- and few-shot settings. An advantage of these models\nover more standard approaches based on fine-tuning is the ability to understand\ninstructions written in natural language (prompts), which helps them generalise\nbetter to different tasks and domains without the need for specific training\ndata. This makes them suitable for addressing text classification problems for\ndomains with limited amounts of annotated instances. However, existing research\nis limited in scale and lacks understanding of how text generation models\ncombined with prompting techniques compare to more established methods for text\nclassification such as fine-tuning masked language models. In this paper, we\naddress this research gap by performing a large-scale evaluation study for 16\ntext classification datasets covering binary, multiclass, and multilabel\nproblems. In particular, we compare zero- and few-shot approaches of large\nlanguage models to fine-tuning smaller language models. We also analyse the\nresults by prompt, classification type, domain, and number of labels. In\ngeneral, the results show how fine-tuning smaller and more efficient language\nmodels can still outperform few-shot approaches of larger language models,\nwhich have room for improvement when it comes to text classification.\n","authors":["Aleksandra Edwards","Jose Camacho-Collados"],"pdf_url":"https://arxiv.org/pdf/2403.17661v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2404.09275v1","updated":"2024-04-14T14:51:44Z","published":"2024-04-14T14:51:44Z","title":"TrafficVLM: A Controllable Visual Language Model for Traffic Video\n  Captioning","summary":"  Traffic video description and analysis have received much attention recently\ndue to the growing demand for efficient and reliable urban surveillance\nsystems. Most existing methods only focus on locating traffic event segments,\nwhich severely lack descriptive details related to the behaviour and context of\nall the subjects of interest in the events. In this paper, we present\nTrafficVLM, a novel multi-modal dense video captioning model for vehicle ego\ncamera view. TrafficVLM models traffic video events at different levels of\nanalysis, both spatially and temporally, and generates long fine-grained\ndescriptions for the vehicle and pedestrian at different phases of the event.\nWe also propose a conditional component for TrafficVLM to control the\ngeneration outputs and a multi-task fine-tuning paradigm to enhance\nTrafficVLM's learning capability. Experiments show that TrafficVLM performs\nwell on both vehicle and overhead camera views. Our solution achieved\noutstanding results in Track 2 of the AI City Challenge 2024, ranking us third\nin the challenge standings. Our code is publicly available at\nhttps://github.com/quangminhdinh/TrafficVLM.\n","authors":["Quang Minh Dinh","Minh Khoi Ho","Anh Quan Dang","Hung Phong Tran"],"pdf_url":"https://arxiv.org/pdf/2404.09275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09260v1","updated":"2024-04-14T14:01:53Z","published":"2024-04-14T14:01:53Z","title":"JaFIn: Japanese Financial Instruction Dataset","summary":"  We construct an instruction dataset for the large language model (LLM) in the\nJapanese finance domain. Domain adaptation of language models, including LLMs,\nis receiving more attention as language models become more popular. This study\ndemonstrates the effectiveness of domain adaptation through instruction tuning.\nTo achieve this, we propose an instruction tuning data in Japanese called\nJaFIn, the Japanese Financial Instruction Dataset. JaFIn is manually\nconstructed based on multiple data sources, including Japanese government\nwebsites, which provide extensive financial knowledge. We then utilize JaFIn to\napply instruction tuning for several LLMs, demonstrating that our models\nspecialized in finance have better domain adaptability than the original\nmodels. The financial-specialized LLMs created were evaluated using a\nquantitative Japanese financial benchmark and qualitative response comparisons,\nshowing improved performance over the originals.\n","authors":["Kota Tanabe","Masahiro Suzuki","Hiroki Sakaji","Itsuki Noda"],"pdf_url":"https://arxiv.org/pdf/2404.09260v1.pdf","comment":"10 pages, 1 figure"},{"id":"http://arxiv.org/abs/2402.17014v2","updated":"2024-04-14T13:43:39Z","published":"2024-02-26T20:43:48Z","title":"Z-AGI Labs at ClimateActivism 2024: Stance and Hate Event Detection on\n  Social Media","summary":"  In the digital realm, rich data serves as a crucial source of insights into\nthe complexities of social, political, and economic landscapes. Addressing the\ngrowing need for high-quality information on events and the imperative to\ncombat hate speech, this research led to the establishment of the Shared Task\non Climate Activism Stance and Hate Event Detection at CASE 2024. Focused on\nclimate activists contending with hate speech on social media, our study\ncontributes to hate speech identification from tweets. Analyzing three\nsub-tasks - Hate Speech Detection (Sub-task A), Targets of Hate Speech\nIdentification (Sub-task B), and Stance Detection (Sub-task C) - Team Z-AGI\nLabs evaluated various models, including LSTM, Xgboost, and LGBM based on\nTf-Idf. Results unveiled intriguing variations, with Catboost excelling in\nSubtask-B (F1: 0.5604) and Subtask-C (F1: 0.7081), while LGBM emerged as the\ntop-performing model for Subtask-A (F1: 0.8684). This research provides\nvaluable insights into the suitability of classical machine learning models for\nclimate hate speech and stance detection, aiding informed model selection for\nrobust mechanisms.\n","authors":["Nikhil Narayan","Mrutyunjay Biswal"],"pdf_url":"https://arxiv.org/pdf/2402.17014v2.pdf","comment":"Authors weren't supposed to upload given organisational agreements"},{"id":"http://arxiv.org/abs/2404.09249v1","updated":"2024-04-14T13:25:15Z","published":"2024-04-14T13:25:15Z","title":"Test Code Generation for Telecom Software Systems using Two-Stage\n  Generative Model","summary":"  In recent years, the evolution of Telecom towards achieving intelligent,\nautonomous, and open networks has led to an increasingly complex Telecom\nSoftware system, supporting various heterogeneous deployment scenarios, with\nmulti-standard and multi-vendor support. As a result, it becomes a challenge\nfor large-scale Telecom software companies to develop and test software for all\ndeployment scenarios. To address these challenges, we propose a framework for\nAutomated Test Generation for large-scale Telecom Software systems. We begin by\ngenerating Test Case Input data for test scenarios observed using a time-series\nGenerative model trained on historical Telecom Network data during field\ntrials. Additionally, the time-series Generative model helps in preserving the\nprivacy of Telecom data. The generated time-series software performance data\nare then utilized with test descriptions written in natural language to\ngenerate Test Script using the Generative Large Language Model. Our\ncomprehensive experiments on public datasets and Telecom datasets obtained from\noperational Telecom Networks demonstrate that the framework can effectively\ngenerate comprehensive test case data input and useful test code.\n","authors":["Mohamad Nabeel","Doumitrou Daniil Nimara","Tahar Zanouda"],"pdf_url":"https://arxiv.org/pdf/2404.09249v1.pdf","comment":"6 pages, 5 figures, Accepted at 1st Workshop on The Impact of Large\n  Language Models on 6G Networks - IEEE International Conference on\n  Communications (ICC) 2024"},{"id":"http://arxiv.org/abs/2404.09248v1","updated":"2024-04-14T13:19:40Z","published":"2024-04-14T13:19:40Z","title":"Knowledgeable Agents by Offline Reinforcement Learning from Large\n  Language Model Rollouts","summary":"  Reinforcement learning (RL) trains agents to accomplish complex tasks through\nenvironmental interaction data, but its capacity is also limited by the scope\nof the available data. To obtain a knowledgeable agent, a promising approach is\nto leverage the knowledge from large language models (LLMs). Despite previous\nstudies combining LLMs with RL, seamless integration of the two components\nremains challenging due to their semantic gap. This paper introduces a novel\nmethod, Knowledgeable Agents from Language Model Rollouts (KALM), which\nextracts knowledge from LLMs in the form of imaginary rollouts that can be\neasily learned by the agent through offline reinforcement learning methods. The\nprimary challenge of KALM lies in LLM grounding, as LLMs are inherently limited\nto textual data, whereas environmental data often comprise numerical vectors\nunseen to LLMs. To address this, KALM fine-tunes the LLM to perform various\ntasks based on environmental data, including bidirectional translation between\nnatural language descriptions of skills and their corresponding rollout data.\nThis grounding process enhances the LLM's comprehension of environmental\ndynamics, enabling it to generate diverse and meaningful imaginary rollouts\nthat reflect novel skills. Initial empirical evaluations on the CLEVR-Robot\nenvironment demonstrate that KALM enables agents to complete complex\nrephrasings of task goals and extend their capabilities to novel tasks\nrequiring unprecedented optimal behaviors. KALM achieves a success rate of 46%\nin executing tasks with unseen goals, substantially surpassing the 26% success\nrate achieved by baseline methods. Furthermore, KALM effectively enables the\nLLM to comprehend environmental dynamics, resulting in the generation of\nmeaningful imaginary rollouts that reflect novel skills and demonstrate the\nseamless integration of large language models and reinforcement learning.\n","authors":["Jing-Cheng Pang","Si-Hang Yang","Kaiyuan Li","Jiaji Zhang","Xiong-Hui Chen","Nan Tang","Yang Yu"],"pdf_url":"https://arxiv.org/pdf/2404.09248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.12107v2","updated":"2024-04-14T12:33:07Z","published":"2023-05-20T05:58:56Z","title":"EE-TTS: Emphatic Expressive TTS with Linguistic Information","summary":"  While Current TTS systems perform well in synthesizing high-quality speech,\nproducing highly expressive speech remains a challenge. Emphasis, as a critical\nfactor in determining the expressiveness of speech, has attracted more\nattention nowadays. Previous works usually enhance the emphasis by adding\nintermediate features, but they can not guarantee the overall expressiveness of\nthe speech. To resolve this matter, we propose Emphatic Expressive TTS\n(EE-TTS), which leverages multi-level linguistic information from syntax and\nsemantics. EE-TTS contains an emphasis predictor that can identify appropriate\nemphasis positions from text and a conditioned acoustic model to synthesize\nexpressive speech with emphasis and linguistic information. Experimental\nresults indicate that EE-TTS outperforms baseline with MOS improvements of 0.49\nand 0.67 in expressiveness and naturalness. EE-TTS also shows strong\ngeneralization across different datasets according to AB test results.\n","authors":["Yi Zhong","Chen Zhang","Xule Liu","Chenxi Sun","Weishan Deng","Haifeng Hu","Zhongqian Sun"],"pdf_url":"https://arxiv.org/pdf/2305.12107v2.pdf","comment":"Accepted by Interspeech 2023, fix some typos"},{"id":"http://arxiv.org/abs/2403.09792v2","updated":"2024-04-14T11:57:55Z","published":"2024-03-14T18:24:55Z","title":"Images are Achilles' Heel of Alignment: Exploiting Visual\n  Vulnerabilities for Jailbreaking Multimodal Large Language Models","summary":"  In this paper, we study the harmlessness alignment problem of multimodal\nlarge language models (MLLMs). We conduct a systematic empirical analysis of\nthe harmlessness performance of representative MLLMs and reveal that the image\ninput poses the alignment vulnerability of MLLMs. Inspired by this, we propose\na novel jailbreak method named HADES, which hides and amplifies the harmfulness\nof the malicious intent within the text input, using meticulously crafted\nimages. Experimental results show that HADES can effectively jailbreak existing\nMLLMs, which achieves an average Attack Success Rate (ASR) of 90.26% for\nLLaVA-1.5 and 71.60% for Gemini Pro Vision. Our code and data will be publicly\nreleased.\n","authors":["Yifan Li","Hangyu Guo","Kun Zhou","Wayne Xin Zhao","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.09792v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2404.09221v1","updated":"2024-04-14T11:49:38Z","published":"2024-04-14T11:49:38Z","title":"Towards Fast Inference: Exploring and Improving Blockwise Parallel\n  Drafts","summary":"  Despite the remarkable strides made by autoregressive language models, their\npotential is often hampered by the slow inference speeds inherent in sequential\ntoken generation. Blockwise parallel decoding (BPD) was proposed by Stern et\nal. (2018) as a way to improve inference speed of language models. In this\npaper, we make two contributions to understanding and improving BPD drafts. We\nfirst offer an analysis of the token distributions produced by the BPD\nprediction heads. Secondly, we use this analysis to inform algorithms to\nimprove BPD inference speed by refining the BPD drafts using small n-gram or\nneural language models. We empirically show that these refined BPD drafts yield\na higher average verified prefix length across tasks.\n","authors":["Taehyeon Kim","Ananda Theertha Suresh","Kishore Papineni","Michael Riley","Sanjiv Kumar","Adrian Benton"],"pdf_url":"https://arxiv.org/pdf/2404.09221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09220v1","updated":"2024-04-14T11:48:33Z","published":"2024-04-14T11:48:33Z","title":"Compass: Large Multilingual Language Model for South-east Asia","summary":"  Large language models have exhibited significant proficiency in languages\nendowed with extensive linguistic resources, such as English and Chinese.\nNevertheless, their effectiveness notably diminishes when applied to languages\ncharacterized by limited linguistic resources, particularly within the\nSoutheast Asian linguistic landscape, such as Indonesian. The scarcity of\nlinguistic resources for these languages presents challenges associated with\ninadequate training, restricted vocabulary coverage, and challenging evaluation\nprocesses. In response to these exigencies, we have introduced CompassLLM, a\nlarge multilingual model specifically tailored for Southeast Asian languages,\nwith the primary aim of supporting the developmental requirements of Shopee.\nOur methodology encompasses several key strategies. To progressively enhance\nmultilingual proficiencies, we implemented a multi-stage pre-training strategy\nintegrated with curriculum learning, gradually intensifying the focus on\nlow-resource languages. Concurrently, to better accommodate low-resource human\ninstructions, we curated and generated a repository of high-quality\nmultilingual human instructions, culminating the CompassLLM-SFT model through\nsupervised instruction fine-tuning. Finally, to reinforce the model's alignment\nwith human preference behaviors, we have embraced the principle of Direct\nPreference Optimization (DPO) to obtain CompassLLM-DPO model. Preliminary\nevaluation of the CompassLLM model yields promising results, with our model\nsurpassing benchmark models like Vicuna-7b-v1.5, Sealion, Falcon and SeaLLM,\nacross diverse evaluation tasks, as verified through both automated and\nhuman-driven assessments. Notably, our model exhibits its superior performance\nin South-east Asia languages, such as Indonesian language.\n","authors":["Sophia Maria"],"pdf_url":"https://arxiv.org/pdf/2404.09220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09206v1","updated":"2024-04-14T10:02:47Z","published":"2024-04-14T10:02:47Z","title":"DKE-Research at SemEval-2024 Task 2: Incorporating Data Augmentation\n  with Generative Models and Biomedical Knowledge to Enhance Inference\n  Robustness","summary":"  Safe and reliable natural language inference is critical for extracting\ninsights from clinical trial reports but poses challenges due to biases in\nlarge pre-trained language models. This paper presents a novel data\naugmentation technique to improve model robustness for biomedical natural\nlanguage inference in clinical trials. By generating synthetic examples through\nsemantic perturbations and domain-specific vocabulary replacement and adding a\nnew task for numerical and quantitative reasoning, we introduce greater\ndiversity and reduce shortcut learning. Our approach, combined with multi-task\nlearning and the DeBERTa architecture, achieved significant performance gains\non the NLI4CT 2024 benchmark compared to the original language models. Ablation\nstudies validate the contribution of each augmentation method in improving\nrobustness. Our best-performing model ranked 12th in terms of faithfulness and\n8th in terms of consistency, respectively, out of the 32 participants.\n","authors":["Yuqi Wang","Zeqiang Wang","Wei Wang","Qi Chen","Kaizhu Huang","Anh Nguyen","Suparna De"],"pdf_url":"https://arxiv.org/pdf/2404.09206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09173v1","updated":"2024-04-14T07:43:45Z","published":"2024-04-14T07:43:45Z","title":"TransformerFAM: Feedback attention is working memory","summary":"  While Transformers have revolutionized deep learning, their quadratic\nattention complexity hinders their ability to process infinitely long inputs.\nWe propose Feedback Attention Memory (FAM), a novel Transformer architecture\nthat leverages a feedback loop to enable the network to attend to its own\nlatent representations. This design fosters the emergence of working memory\nwithin the Transformer, allowing it to process indefinitely long sequences.\nTransformerFAM requires no additional weights, enabling seamless integration\nwith pre-trained models. Our experiments show that TransformerFAM significantly\nimproves Transformer performance on long-context tasks across various model\nsizes (1B, 8B, and 24B). These results showcase the potential to empower Large\nLanguage Models (LLMs) to process sequences of unlimited length.\n","authors":["Dongseong Hwang","Weiran Wang","Zhuoyuan Huo","Khe Chai Sim","Pedro Moreno Mengibar"],"pdf_url":"https://arxiv.org/pdf/2404.09173v1.pdf","comment":"24 pages, 12 figures, 14 tables"},{"id":"http://arxiv.org/abs/2404.09170v1","updated":"2024-04-14T07:19:27Z","published":"2024-04-14T07:19:27Z","title":"Post-Semantic-Thinking: A Robust Strategy to Distill Reasoning Capacity\n  from Large Language Models","summary":"  Chain of thought finetuning aims to endow small student models with reasoning\ncapacity to improve their performance towards a specific task by allowing them\nto imitate the reasoning procedure of large language models (LLMs) beyond\nsimply predicting the answer to the question. However, the existing methods 1)\ngenerate rationale before the answer, making their answer correctness sensitive\nto the hallucination in the rationale;2) force the student model to repeat the\nexact LLMs rationale expression word-after-word, which could have the model\nbiased towards learning the expression in rationale but count against the model\nfrom understanding the core logic behind it. Therefore, we propose a robust\nPost-Semantic-Thinking (PST) strategy to generate answers before rationale.\nThanks to this answer-first setting, 1) the answering procedure can escape from\nthe adverse effects caused by hallucinations in the rationale; 2) the complex\nreasoning procedure is tightly bound with the relatively concise answer, making\nthe reasoning for questions easier with the prior information in the answer; 3)\nthe efficiency of the method can also benefit from the setting since users can\nstop the generation right after answers are outputted when inference is\nconducted. Furthermore, the PST strategy loose the constraint against the\ngenerated rationale to be close to the LLMs gold standard in the hidden\nsemantic space instead of the vocabulary space, thus making the small student\nmodel better comprehend the semantic reasoning logic in rationale. Extensive\nexperiments conducted across 12 reasoning tasks demonstrate the effectiveness\nof PST.\n","authors":["Xiao Chen","Sihang Zhou","Ke Liang","Xinwang Liu"],"pdf_url":"https://arxiv.org/pdf/2404.09170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09163v1","updated":"2024-04-14T06:55:42Z","published":"2024-04-14T06:55:42Z","title":"GeMQuAD : Generating Multilingual Question Answering Datasets from Large\n  Language Models using Few Shot Learning","summary":"  The emergence of Large Language Models (LLMs) with capabilities like\nIn-Context Learning (ICL) has ushered in new possibilities for data generation\nacross various domains while minimizing the need for extensive data collection\nand modeling techniques. Researchers have explored ways to use this generated\nsynthetic data to optimize smaller student models for reduced deployment costs\nand lower latency in downstream tasks. However, ICL-generated data often\nsuffers from low quality as the task specificity is limited with few examples\nused in ICL. In this paper, we propose GeMQuAD - a semi-supervised learning\napproach, extending the WeakDAP framework, applied to a dataset generated\nthrough ICL with just one example in the target language using AlexaTM 20B\nSeq2Seq LLM. Through our approach, we iteratively identify high-quality data to\nenhance model performance, especially for low-resource multilingual setting in\nthe context of Extractive Question Answering task. Our framework outperforms\nthe machine translation-augmented model by 0.22/1.68 F1/EM (Exact Match) points\nfor Hindi and 0.82/1.37 F1/EM points for Spanish on the MLQA dataset, and it\nsurpasses the performance of model trained on an English-only dataset by\n5.05/6.50 F1/EM points for Hindi and 3.81/3.69 points F1/EM for Spanish on the\nsame dataset. Notably, our approach uses a pre-trained LLM for generation with\nno fine-tuning (FT), utilizing just a single annotated example in ICL to\ngenerate data, providing a cost-effective development process.\n","authors":["Amani Namboori","Shivam Mangale","Andy Rosenbaum","Saleh Soltan"],"pdf_url":"https://arxiv.org/pdf/2404.09163v1.pdf","comment":"Accepted to The 37th International Conference on Neural Information\n  Processing Systems (NeurIPS 2023)December 10-16, 2023 - SyntheticData4ML\n  workshop, New Orleans, United States https://neurips.cc/Conferences/2023"},{"id":"http://arxiv.org/abs/2404.09155v1","updated":"2024-04-14T06:10:46Z","published":"2024-04-14T06:10:46Z","title":"Mitigating Heterogeneity among Factor Tensors via Lie Group Manifolds\n  for Tensor Decomposition Based Temporal Knowledge Graph Embedding","summary":"  Recent studies have highlighted the effectiveness of tensor decomposition\nmethods in the Temporal Knowledge Graphs Embedding (TKGE) task. However, we\nfound that inherent heterogeneity among factor tensors in tensor decomposition\nsignificantly hinders the tensor fusion process and further limits the\nperformance of link prediction. To overcome this limitation, we introduce a\nnovel method that maps factor tensors onto a unified smooth Lie group manifold\nto make the distribution of factor tensors approximating homogeneous in tensor\ndecomposition. We provide the theoretical proof of our motivation that\nhomogeneous tensors are more effective than heterogeneous tensors in tensor\nfusion and approximating the target for tensor decomposition based TKGE\nmethods. The proposed method can be directly integrated into existing tensor\ndecomposition based TKGE methods without introducing extra parameters.\nExtensive experiments demonstrate the effectiveness of our method in mitigating\nthe heterogeneity and in enhancing the tensor decomposition based TKGE models.\n","authors":["Jiang Li","Xiangdong Su","Yeyun Gong","Guanglai Gao"],"pdf_url":"https://arxiv.org/pdf/2404.09155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06671v2","updated":"2024-04-14T05:30:19Z","published":"2023-10-10T14:47:09Z","title":"Making Large Language Models Perform Better in Knowledge Graph\n  Completion","summary":"  Large language model (LLM) based knowledge graph completion (KGC) aims to\npredict the missing triples in the KGs with LLMs. However, research about\nLLM-based KGC fails to sufficiently harness LLMs' inference proficiencies,\noverlooking critical structural information integral to KGs. In this paper, we\nexplore methods to incorporate structural information into the LLMs, with the\noverarching goal of facilitating structure-aware reasoning. We first discuss on\nthe existing LLM paradigms like in-context learning and instruction tuning,\nproposing basic structural information injection approaches. Then we propose a\nKnowledge Prefix Adapter (KoPA) to fulfill this stated goal. The KoPA uses a\nstructural pre-training phase to comprehend the intricate entities and\nrelations within KGs, representing them as structural embeddings. Then KoPA\ncommunicates such cross-modal structural information understanding to the LLMs\nthrough a knowledge prefix adapter which projects the structural embeddings\ninto the textual space and obtains virtual knowledge tokens positioned as a\nprefix of the input prompt. We conduct comprehensive experiments and provide\nincisive analysis concerning how the introduction of cross-modal structural\ninformation would be better for LLM's factual knowledge reasoning ability. Our\ncode and data are available at https://github.com/zjukg/KoPA .\n","authors":["Yichi Zhang","Zhuo Chen","Lingbing Guo","Yajing Xu","Wen Zhang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2310.06671v2.pdf","comment":"Working in progress"},{"id":"http://arxiv.org/abs/2305.17493v3","updated":"2024-04-14T05:20:10Z","published":"2023-05-27T15:10:41Z","title":"The Curse of Recursion: Training on Generated Data Makes Models Forget","summary":"  Stable Diffusion revolutionised image creation from descriptive text. GPT-2,\nGPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of\nlanguage tasks. ChatGPT introduced such language models to the general public.\nIt is now clear that large language models (LLMs) are here to stay, and will\nbring about drastic change in the whole ecosystem of online text and images. In\nthis paper we consider what the future might hold. What will happen to GPT-{n}\nonce LLMs contribute much of the language found online? We find that use of\nmodel-generated content in training causes irreversible defects in the\nresulting models, where tails of the original content distribution disappear.\nWe refer to this effect as Model Collapse and show that it can occur in\nVariational Autoencoders, Gaussian Mixture Models and LLMs. We build\ntheoretical intuition behind the phenomenon and portray its ubiquity amongst\nall learned generative models. We demonstrate that it has to be taken seriously\nif we are to sustain the benefits of training from large-scale data scraped\nfrom the web. Indeed, the value of data collected about genuine human\ninteractions with systems will be increasingly valuable in the presence of\ncontent generated by LLMs in data crawled from the Internet.\n","authors":["Ilia Shumailov","Zakhar Shumaylov","Yiren Zhao","Yarin Gal","Nicolas Papernot","Ross Anderson"],"pdf_url":"https://arxiv.org/pdf/2305.17493v3.pdf","comment":"Fixed typos in eqn 4,5"},{"id":"http://arxiv.org/abs/2404.09145v1","updated":"2024-04-14T05:13:37Z","published":"2024-04-14T05:13:37Z","title":"ToNER: Type-oriented Named Entity Recognition with Generative Language\n  Model","summary":"  In recent years, the fine-tuned generative models have been proven more\npowerful than the previous tagging-based or span-based models on named entity\nrecognition (NER) task. It has also been found that the information related to\nentities, such as entity types, can prompt a model to achieve NER better.\nHowever, it is not easy to determine the entity types indeed existing in the\ngiven sentence in advance, and inputting too many potential entity types would\ndistract the model inevitably. To exploit entity types' merit on promoting NER\ntask, in this paper we propose a novel NER framework, namely ToNER based on a\ngenerative model. In ToNER, a type matching model is proposed at first to\nidentify the entity types most likely to appear in the sentence. Then, we\nappend a multiple binary classification task to fine-tune the generative\nmodel's encoder, so as to generate the refined representation of the input\nsentence. Moreover, we add an auxiliary task for the model to discover the\nentity types which further fine-tunes the model to output more accurate\nresults. Our extensive experiments on some NER benchmarks verify the\neffectiveness of our proposed strategies in ToNER that are oriented towards\nentity types' exploitation.\n","authors":["Guochao Jiang","Ziqin Luo","Yuchen Shi","Dixuan Wang","Jiaqing Liang","Deqing Yang"],"pdf_url":"https://arxiv.org/pdf/2404.09145v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2306.04891v2","updated":"2024-04-14T05:12:52Z","published":"2023-06-08T02:38:23Z","title":"In-Context Learning through the Bayesian Prism","summary":"  In-context learning (ICL) is one of the surprising and useful features of\nlarge language models and subject of intense research. Recently, stylized\nmeta-learning-like ICL setups have been devised that train transformers on\nsequences of input-output pairs $(x, f(x))$. The function $f$ comes from a\nfunction class and generalization is checked by evaluating on sequences\ngenerated from unseen functions from the same class. One of the main\ndiscoveries in this line of research has been that for several function\nclasses, such as linear regression, transformers successfully generalize to new\nfunctions in the class. However, the inductive biases of these models resulting\nin this behavior are not clearly understood. A model with unlimited training\ndata and compute is a Bayesian predictor: it learns the pretraining\ndistribution. In this paper we empirically examine how far this Bayesian\nperspective can help us understand ICL. To this end, we generalize the previous\nmeta-ICL setup to hierarchical meta-ICL setup which involve unions of multiple\ntask families. We instantiate this setup on a diverse range of linear and\nnonlinear function families and find that transformers can do ICL in this\nsetting as well. Where Bayesian inference is tractable, we find evidence that\nhigh-capacity transformers mimic the Bayesian predictor. The Bayesian\nperspective provides insights into the inductive bias of ICL and how\ntransformers perform a particular task when they are trained on multiple tasks.\nWe also find that transformers can learn to generalize to new function classes\nthat were not seen during pretraining. This involves deviation from the\nBayesian predictor. We examine these deviations in more depth offering new\ninsights and hypotheses.\n","authors":["Madhur Panwar","Kabir Ahuja","Navin Goyal"],"pdf_url":"https://arxiv.org/pdf/2306.04891v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2307.10928v4","updated":"2024-04-14T04:29:51Z","published":"2023-07-20T14:56:35Z","title":"FLASK: Fine-grained Language Model Evaluation based on Alignment Skill\n  Sets","summary":"  Evaluation of Large Language Models (LLMs) is challenging because\ninstruction-following necessitates alignment with human values and the required\nset of skills varies depending on the instruction. However, previous studies\nhave mainly focused on coarse-grained evaluation (i.e. overall preference-based\nevaluation), which limits interpretability since it does not consider the\nnature of user instructions that require instance-wise skill composition. In\nthis paper, we introduce FLASK (Fine-grained Language Model Evaluation based on\nAlignment Skill Sets), a fine-grained evaluation protocol for both human-based\nand model-based evaluation which decomposes coarse-level scoring to a skill\nset-level scoring for each instruction. We experimentally observe that the\nfine-graininess of evaluation is crucial for attaining a holistic view of model\nperformance and increasing the reliability of the evaluation. Using FLASK, we\ncompare multiple open-source and proprietary LLMs and observe a high\ncorrelation between model-based and human-based evaluations. We publicly\nrelease the evaluation data and code implementation at\nhttps://github.com/kaistAI/FLASK.\n","authors":["Seonghyeon Ye","Doyoung Kim","Sungdong Kim","Hyeonbin Hwang","Seungone Kim","Yongrae Jo","James Thorne","Juho Kim","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2307.10928v4.pdf","comment":"ICLR 2024 Spotlight"},{"id":"http://arxiv.org/abs/2404.09138v1","updated":"2024-04-14T04:25:41Z","published":"2024-04-14T04:25:41Z","title":"From Bytes to Borsch: Fine-Tuning Gemma and Mistral for the Ukrainian\n  Language Representation","summary":"  In the rapidly advancing field of AI and NLP, generative large language\nmodels (LLMs) stand at the forefront of innovation, showcasing unparalleled\nabilities in text understanding and generation. However, the limited\nrepresentation of low-resource languages like Ukrainian poses a notable\nchallenge, restricting the reach and relevance of this technology. Our paper\naddresses this by fine-tuning the open-source Gemma and Mistral LLMs with\nUkrainian datasets, aiming to improve their linguistic proficiency and\nbenchmarking them against other existing models capable of processing Ukrainian\nlanguage. This endeavor not only aims to mitigate language bias in technology\nbut also promotes inclusivity in the digital realm. Our transparent and\nreproducible approach encourages further NLP research and development.\nAdditionally, we present the Ukrainian Knowledge and Instruction Dataset (UKID)\nto aid future efforts in language model fine-tuning. Our research not only\nadvances the field of NLP but also highlights the importance of linguistic\ndiversity in AI, which is crucial for cultural preservation, education, and\nexpanding AI's global utility. Ultimately, we advocate for a future where\ntechnology is inclusive, enabling AI to communicate effectively across all\nlanguages, especially those currently underrepresented.\n","authors":["Artur Kiulian","Anton Polishko","Mykola Khandoga","Oryna Chubych","Jack Connor","Raghav Ravishankar","Adarsh Shirawalmath"],"pdf_url":"https://arxiv.org/pdf/2404.09138v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09136v1","updated":"2024-04-14T04:14:30Z","published":"2024-04-14T04:14:30Z","title":"TLDR at SemEval-2024 Task 2: T5-generated clinical-Language summaries\n  for DeBERTa Report Analysis","summary":"  This paper introduces novel methodologies for the Natural Language Inference\nfor Clinical Trials (NLI4CT) task. We present TLDR (T5-generated\nclinical-Language summaries for DeBERTa Report Analysis) which incorporates\nT5-model generated premise summaries for improved entailment and contradiction\nanalysis in clinical NLI tasks. This approach overcomes the challenges posed by\nsmall context windows and lengthy premises, leading to a substantial\nimprovement in Macro F1 scores: a 0.184 increase over truncated premises. Our\ncomprehensive experimental evaluation, including detailed error analysis and\nablations, confirms the superiority of TLDR in achieving consistency and\nfaithfulness in predictions against semantically altered inputs.\n","authors":["Spandan Das","Vinay Samuel","Shahriar Noroozizadeh"],"pdf_url":"https://arxiv.org/pdf/2404.09136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09135v1","updated":"2024-04-14T03:54:00Z","published":"2024-04-14T03:54:00Z","title":"Unveiling LLM Evaluation Focused on Metrics: Challenges and Solutions","summary":"  Natural Language Processing (NLP) is witnessing a remarkable breakthrough\ndriven by the success of Large Language Models (LLMs). LLMs have gained\nsignificant attention across academia and industry for their versatile\napplications in text generation, question answering, and text summarization. As\nthe landscape of NLP evolves with an increasing number of domain-specific LLMs\nemploying diverse techniques and trained on various corpus, evaluating\nperformance of these models becomes paramount. To quantify the performance,\nit's crucial to have a comprehensive grasp of existing metrics. Among the\nevaluation, metrics which quantifying the performance of LLMs play a pivotal\nrole. This paper offers a comprehensive exploration of LLM evaluation from a\nmetrics perspective, providing insights into the selection and interpretation\nof metrics currently in use. Our main goal is to elucidate their mathematical\nformulations and statistical interpretations. We shed light on the application\nof these metrics using recent Biomedical LLMs. Additionally, we offer a\nsuccinct comparison of these metrics, aiding researchers in selecting\nappropriate metrics for diverse tasks. The overarching goal is to furnish\nresearchers with a pragmatic guide for effective LLM evaluation and metric\nselection, thereby advancing the understanding and application of these large\nlanguage models.\n","authors":["Taojun Hu","Xiao-Hua Zhou"],"pdf_url":"https://arxiv.org/pdf/2404.09135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01444v3","updated":"2024-04-14T03:47:19Z","published":"2023-10-01T07:50:30Z","title":"Adapting LLM Agents with Universal Feedback in Communication","summary":"  Recent advances in large language models (LLMs) have demonstrated potential\nfor LLM agents. To facilitate the training for these agents with both\nlinguistic feedback and non-linguistic reward signals, we introduce Learning\nthrough Communication (LTC). We design a universal buffer to store all the\nfeedback, and an iterative pipeline to enable an LLM agent to explore and\nupdate its policy in an given environment. To optimize agent interactions for\ntask-specific learning with our universal buffer and pipeline, we introduce\ndiverse communication patterns tailored for both single-agent and multi-agent\nenvironments. We evaluate the efficacy of our LTC approach on four diverse\ndatasets: ALFWorld (single-agent), HotpotQA (multi-agent collaboration),\nChameleon (multi-agent competition), and GSM8k (multi-agent teacher-student).\nOn these data sets, LTC outperforms the supervised instruction fine-tuning\nbaselines by 3.6% to 12%. These results highlight the versatility and\nefficiency of LTC in facilitating online adaptation for LLM agents.\n","authors":["Kuan Wang","Yadong Lu","Michael Santacroce","Yeyun Gong","Chao Zhang","Yelong Shen"],"pdf_url":"https://arxiv.org/pdf/2310.01444v3.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2404.09129v1","updated":"2024-04-14T02:47:32Z","published":"2024-04-14T02:47:32Z","title":"When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in\n  Large Language Models","summary":"  Recent studies suggest that self-reflective prompting can significantly\nenhance the reasoning capabilities of Large Language Models (LLMs). However,\nthe use of external feedback as a stop criterion raises doubts about the true\nextent of LLMs' ability to emulate human-like self-reflection. In this paper,\nwe set out to clarify these capabilities under a more stringent evaluation\nsetting in which we disallow any kind of external feedback. Our findings under\nthis setting show a split: while self-reflection enhances performance in\nTruthfulQA, it adversely affects results in HotpotQA. We conduct follow-up\nanalyses to clarify the contributing factors in these patterns, and find that\nthe influence of self-reflection is impacted both by reliability of accuracy in\nmodels' initial responses, and by overall question difficulty: specifically,\nself-reflection shows the most benefit when models are less likely to be\ncorrect initially, and when overall question difficulty is higher. We also find\nthat self-reflection reduces tendency toward majority voting. Based on our\nfindings, we propose guidelines for decisions on when to implement\nself-reflection. We release the codebase for reproducing our experiments at\nhttps://github.com/yanhong-lbh/LLM-SelfReflection-Eval.\n","authors":["Yanhong Li","Chenghao Yang","Allyson Ettinger"],"pdf_url":"https://arxiv.org/pdf/2404.09129v1.pdf","comment":"NAACL 2024 Findings paper (Camera-Ready Version)"},{"id":"http://arxiv.org/abs/2403.01373v2","updated":"2024-04-14T02:45:16Z","published":"2024-03-03T02:31:11Z","title":"Quantity Matters: Towards Assessing and Mitigating Number Hallucination\n  in Large Vision-Language Models","summary":"  Large-scale vision-language models have demonstrated impressive skill in\nhandling tasks that involve both areas. Nevertheless, these models frequently\nexperience significant issues with generating inaccurate information, which is\nhallucination. In this study, we concentrate on a specific type of\nhallucination-number hallucination, referring to models incorrectly identifying\nthe number of certain objects in pictures. We perform quantitative evaluations\nregarding number hallucination, showing it to be critical in major open-source\nlarge vision-language models. Furthermore, we utilizes two related tasks to\nconduct an in-depth analysis of number hallucination, revealing the severe\ninner and outer inconsistency among all tasks. Based on this examination, we\ndevise a training approach aimed at improving consistency to reduce number\nhallucinations, which leads to an 8% enhancement in performance over direct\nfinetuning methods. Our code and dataset will be released to the community.\n","authors":["Huixuan Zhang","Junzhe Zhang","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2403.01373v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2404.09127v1","updated":"2024-04-14T02:40:43Z","published":"2024-04-14T02:40:43Z","title":"Confidence Calibration and Rationalization for LLMs via Multi-Agent\n  Deliberation","summary":"  Uncertainty estimation is a significant issue for current large language\nmodels (LLMs) that are generally poorly calibrated and over-confident,\nespecially with reinforcement learning from human feedback (RLHF). Unlike\nhumans, whose decisions and confidences not only stem from intrinsic beliefs\nbut can also be adjusted through daily observations, existing calibration\nmethods for LLMs focus on estimating or eliciting individual confidence without\ntaking full advantage of the \"Collective Wisdom\": the interaction among\nmultiple LLMs that can collectively improve both accuracy and calibration. In\nthis work, we propose Collaborative Calibration, a post-hoc training-free\ncalibration strategy that leverages the collaborative and expressive\ncapabilities of multiple tool-augmented LLM agents in a simulated group\ndeliberation process. We demonstrate the effectiveness of Collaborative\nCalibration on generative QA tasks across various domains, showing its\npotential in harnessing the rationalization of collectively calibrated\nconfidence assessments and improving the reliability of model predictions.\n","authors":["Ruixin Yang","Dheeraj Rajagopa","Shirley Anugrah Hayati","Bin Hu","Dongyeop Kang"],"pdf_url":"https://arxiv.org/pdf/2404.09127v1.pdf","comment":"Accepted at ICLR 2024 Workshop on Reliable and Responsible Foundation\n  Models"},{"id":"http://arxiv.org/abs/2404.09123v1","updated":"2024-04-14T02:18:07Z","published":"2024-04-14T02:18:07Z","title":"Provable Interactive Learning with Hindsight Instruction Feedback","summary":"  We study interactive learning in a setting where the agent has to generate a\nresponse (e.g., an action or trajectory) given a context and an instruction. In\ncontrast, to typical approaches that train the system using reward or expert\nsupervision on response, we study learning with hindsight instruction where a\nteacher provides an instruction that is most suitable for the agent's generated\nresponse. This hindsight labeling of instruction is often easier to provide\nthan providing expert supervision of the optimal response which may require\nexpert knowledge or can be impractical to elicit. We initiate the theoretical\nanalysis of interactive learning with hindsight labeling. We first provide a\nlower bound showing that in general, the regret of any algorithm must scale\nwith the size of the agent's response space. We then study a specialized\nsetting where the underlying instruction-response distribution can be\ndecomposed as a low-rank matrix. We introduce an algorithm called LORIL for\nthis setting and show that its regret scales as $\\sqrt{T}$ where $T$ is the\nnumber of rounds and depends on the intrinsic rank but does not depend on the\nsize of the agent's response space. We provide experiments in two domains\nshowing that LORIL outperforms baselines even when the low-rank assumption is\nviolated.\n","authors":["Dipendra Misra","Aldo Pacchiano","Robert E. Schapire"],"pdf_url":"https://arxiv.org/pdf/2404.09123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10105v2","updated":"2024-04-14T01:15:31Z","published":"2023-09-18T19:28:48Z","title":"Understanding Catastrophic Forgetting in Language Models via Implicit\n  Inference","summary":"  We lack a systematic understanding of the effects of fine-tuning (via methods\nsuch as instruction-tuning or reinforcement learning from human feedback),\nparticularly on tasks outside the narrow fine-tuning distribution. In a\nsimplified scenario, we demonstrate that improving performance on tasks within\nthe fine-tuning data distribution comes at the expense of capabilities on other\ntasks. We hypothesize that language models implicitly infer the task of the\nprompt and that fine-tuning skews this inference towards tasks in the\nfine-tuning distribution. To test this, we propose Conjugate Prompting, which\nartificially makes the task look farther from the fine-tuning distribution\nwhile requiring the same capability, and we find that this recovers some of the\npretraining capabilities in our synthetic setup. Since real-world fine-tuning\ndistributions are predominantly English, we apply conjugate prompting to\nrecover pretrained capabilities in LLMs by simply translating the prompts to\ndifferent languages. This allows us to recover in-context learning abilities\nlost via instruction tuning, natural reasoning capability lost during code\nfine-tuning, and, more concerningly, harmful content generation suppressed by\nsafety fine-tuning in chatbots like ChatGPT.\n","authors":["Suhas Kotha","Jacob Mitchell Springer","Aditi Raghunathan"],"pdf_url":"https://arxiv.org/pdf/2309.10105v2.pdf","comment":"ICLR 2024"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2302.04871v4","updated":"2024-04-14T23:46:05Z","published":"2023-02-09T18:59:56Z","title":"In-N-Out: Faithful 3D GAN Inversion with Volumetric Decomposition for\n  Face Editing","summary":"  3D-aware GANs offer new capabilities for view synthesis while preserving the\nediting functionalities of their 2D counterparts. GAN inversion is a crucial\nstep that seeks the latent code to reconstruct input images or videos,\nsubsequently enabling diverse editing tasks through manipulation of this latent\ncode. However, a model pre-trained on a particular dataset (e.g., FFHQ) often\nhas difficulty reconstructing images with out-of-distribution (OOD) objects\nsuch as faces with heavy make-up or occluding objects. We address this issue by\nexplicitly modeling OOD objects from the input in 3D-aware GANs. Our core idea\nis to represent the image using two individual neural radiance fields: one for\nthe in-distribution content and the other for the out-of-distribution object.\nThe final reconstruction is achieved by optimizing the composition of these two\nradiance fields with carefully designed regularization. We demonstrate that our\nexplicit decomposition alleviates the inherent trade-off between reconstruction\nfidelity and editability. We evaluate reconstruction accuracy and editability\nof our method on challenging real face images and videos and showcase favorable\nresults against other baselines.\n","authors":["Yiran Xu","Zhixin Shu","Cameron Smith","Seoung Wug Oh","Jia-Bin Huang"],"pdf_url":"https://arxiv.org/pdf/2302.04871v4.pdf","comment":"Project page: https://in-n-out-3d.github.io/"},{"id":"http://arxiv.org/abs/2404.09378v1","updated":"2024-04-14T23:30:35Z","published":"2024-04-14T23:30:35Z","title":"Orientation-conditioned Facial Texture Mapping for Video-based Facial\n  Remote Photoplethysmography Estimation","summary":"  Camera-based remote photoplethysmography (rPPG) enables contactless\nmeasurement of important physiological signals such as pulse rate (PR).\nHowever, dynamic and unconstrained subject motion introduces significant\nvariability into the facial appearance in video, confounding the ability of\nvideo-based methods to accurately extract the rPPG signal. In this study, we\nleverage the 3D facial surface to construct a novel orientation-conditioned\nfacial texture video representation which improves the motion robustness of\nexisting video-based facial rPPG estimation methods. Our proposed method\nachieves a significant 18.2% performance improvement in cross-dataset testing\non MMPD over our baseline using the PhysNet model trained on PURE, highlighting\nthe efficacy and generalization benefits of our designed video representation.\nWe demonstrate significant performance improvements of up to 29.6% in all\ntested motion scenarios in cross-dataset testing on MMPD, even in the presence\nof dynamic and unconstrained subject motion. Emphasizing the benefits the\nbenefits of disentangling motion through modeling the 3D facial surface for\nmotion robust facial rPPG estimation. We validate the efficacy of our design\ndecisions and the impact of different video processing steps through an\nablation study. Our findings illustrate the potential strengths of exploiting\nthe 3D facial surface as a general strategy for addressing dynamic and\nunconstrained subject motion in videos. The code is available at\nhttps://samcantrill.github.io/orientation-uv-rppg/.\n","authors":["Sam Cantrill","David Ahmedt-Aristizabal","Lars Petersson","Hanna Suominen","Mohammad Ali Armin"],"pdf_url":"https://arxiv.org/pdf/2404.09378v1.pdf","comment":"12 pages, 8 figures, 6 tables"},{"id":"http://arxiv.org/abs/2404.09376v1","updated":"2024-04-14T23:17:01Z","published":"2024-04-14T23:17:01Z","title":"\\textit{sweet} -- An Open Source Modular Platform for Contactless Hand\n  Vascular Biometric Experiments","summary":"  Current finger-vein or palm-vein recognition systems usually require direct\ncontact of the subject with the apparatus. This can be problematic in\nenvironments where hygiene is of primary importance. In this work we present a\ncontactless vascular biometrics sensor platform named \\sweet which can be used\nfor hand vascular biometrics studies (wrist-, palm- and finger-vein) and\nsurface features such as palmprint. It supports several acquisition modalities\nsuch as multi-spectral Near-Infrared (NIR), RGB-color, Stereo Vision (SV) and\nPhotometric Stereo (PS). Using this platform we collect a dataset consisting of\nthe fingers, palm and wrist vascular data of 120 subjects and develop a\npowerful 3D pipeline for the pre-processing of this data. We then present\nbiometric experimental results, focusing on Finger-Vein Recognition (FVR).\nFinally, we discuss fusion of multiple modalities, such palm-vein combined with\npalm-print biometrics. The acquisition software, parts of the hardware design,\nthe new FV dataset, as well as source-code for our experiments are publicly\navailable for research purposes.\n","authors":["David Geissb√ºhler","Sushil Bhattacharjee","Ketan Kotwal","Guillaume Clivaz","S√©bastien Marcel"],"pdf_url":"https://arxiv.org/pdf/2404.09376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06741v2","updated":"2024-04-14T22:33:27Z","published":"2023-12-11T18:19:04Z","title":"Gaussian Splatting SLAM","summary":"  We present the first application of 3D Gaussian Splatting in monocular SLAM,\nthe most fundamental but the hardest setup for Visual SLAM. Our method, which\nruns live at 3fps, utilises Gaussians as the only 3D representation, unifying\nthe required representation for accurate, efficient tracking, mapping, and\nhigh-quality rendering. Designed for challenging monocular settings, our\napproach is seamlessly extendable to RGB-D SLAM when an external depth sensor\nis available. Several innovations are required to continuously reconstruct 3D\nscenes with high fidelity from a live camera. First, to move beyond the\noriginal 3DGS algorithm, which requires accurate poses from an offline\nStructure from Motion (SfM) system, we formulate camera tracking for 3DGS using\ndirect optimisation against the 3D Gaussians, and show that this enables fast\nand robust tracking with a wide basin of convergence. Second, by utilising the\nexplicit nature of the Gaussians, we introduce geometric verification and\nregularisation to handle the ambiguities occurring in incremental 3D dense\nreconstruction. Finally, we introduce a full SLAM system which not only\nachieves state-of-the-art results in novel view synthesis and trajectory\nestimation but also reconstruction of tiny and even transparent objects.\n","authors":["Hidenobu Matsuki","Riku Murai","Paul H. J. Kelly","Andrew J. Davison"],"pdf_url":"https://arxiv.org/pdf/2312.06741v2.pdf","comment":"CVPR2024 Highlight. First two authors contributed equally to this\n  work. Project Page: https://rmurai.co.uk/projects/GaussianSplattingSLAM/"},{"id":"http://arxiv.org/abs/2310.08580v2","updated":"2024-04-14T22:23:18Z","published":"2023-10-12T17:59:38Z","title":"OmniControl: Control Any Joint at Any Time for Human Motion Generation","summary":"  We present a novel approach named OmniControl for incorporating flexible\nspatial control signals into a text-conditioned human motion generation model\nbased on the diffusion process. Unlike previous methods that can only control\nthe pelvis trajectory, OmniControl can incorporate flexible spatial control\nsignals over different joints at different times with only one model.\nSpecifically, we propose analytic spatial guidance that ensures the generated\nmotion can tightly conform to the input control signals. At the same time,\nrealism guidance is introduced to refine all the joints to generate more\ncoherent motion. Both the spatial and realism guidance are essential and they\nare highly complementary for balancing control accuracy and motion realism. By\ncombining them, OmniControl generates motions that are realistic, coherent, and\nconsistent with the spatial constraints. Experiments on HumanML3D and KIT-ML\ndatasets show that OmniControl not only achieves significant improvement over\nstate-of-the-art methods on pelvis control but also shows promising results\nwhen incorporating the constraints over other joints.\n","authors":["Yiming Xie","Varun Jampani","Lei Zhong","Deqing Sun","Huaizu Jiang"],"pdf_url":"https://arxiv.org/pdf/2310.08580v2.pdf","comment":"ICLR 2024. Project page: https://neu-vi.github.io/omnicontrol/"},{"id":"http://arxiv.org/abs/2404.09359v1","updated":"2024-04-14T21:14:47Z","published":"2024-04-14T21:14:47Z","title":"Exploring Feedback Generation in Automated Skeletal Movement Assessment:\n  A Comprehensive Overview","summary":"  The application of machine-learning solutions to movement assessment from\nskeleton videos has attracted significant research attention in recent years.\nThis advancement has made rehabilitation at home more accessible, utilizing\nmovement assessment algorithms that can operate on affordable equipment for\nhuman pose detection from 2D or 3D videos. While the primary objective of\nautomatic assessment tasks is to score movements, the automatic generation of\nfeedback highlighting key movement issues has the potential to significantly\nenhance and accelerate the rehabilitation process. In this study, we explain\nthe types of feedback that can be generated, review existing solutions for\nautomatic feedback generation, and discuss future research directions. To our\nknowledge, this is the first comprehensive review of feedback generation in\nskeletal movement assessment.\n","authors":["Tal Hakim"],"pdf_url":"https://arxiv.org/pdf/2404.09359v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.19001v3","updated":"2024-04-14T21:13:01Z","published":"2024-02-29T09:52:39Z","title":"Analysis of the Two-Step Heterogeneous Transfer Learning for Laryngeal\n  Blood Vessel Classification: Issue and Improvement","summary":"  Accurate classification of laryngeal vascular as benign or malignant is\ncrucial for early detection of laryngeal cancer. However, organizations with\nlimited access to laryngeal vascular images face challenges due to the lack of\nlarge and homogeneous public datasets for effective learning. Distinguished\nfrom the most familiar works, which directly transfer the ImageNet pre-trained\nmodels to the target domain for fine-tuning, this work pioneers exploring\ntwo-step heterogeneous transfer learning (THTL) for laryngeal lesion\nclassification with nine deep-learning models, utilizing the diabetic\nretinopathy color fundus images, semantically non-identical yet vascular\nimages, as the intermediate domain. Attention visualization technique, Layer\nClass Activate Map (LayerCAM), reveals a novel finding that yet the\nintermediate and the target domain both reflect vascular structure to a certain\nextent, the prevalent radial vascular pattern in the intermediate domain\nprevents learning the features of twisted and tangled vessels that distinguish\nthe malignant class in the target domain, summarizes a vital rule for laryngeal\nlesion classification using THTL. To address this, we introduce an enhanced\nfine-tuning strategy in THTL called Step-Wise Fine-Tuning (SWFT) and apply it\nto the ResNet models. SWFT progressively refines model performance by\naccumulating fine-tuning layers from back to front, guided by the visualization\nresults of LayerCAM. Comparison with the original THTL approach shows\nsignificant improvements. For ResNet18, the accuracy and malignant recall\nincreases by 26.1% and 79.8%, respectively, while for ResNet50, these\nindicators improve by 20.4% and 62.2%, respectively.\n","authors":["Xinyi Fang","Xu Yang","Chak Fong Chong","Kei Long Wong","Yapeng Wang","Tiankui Zhang","Sio-Kei Im"],"pdf_url":"https://arxiv.org/pdf/2402.19001v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09349v1","updated":"2024-04-14T20:14:38Z","published":"2024-04-14T20:14:38Z","title":"Adversarial Robustness Limits via Scaling-Law and Human-Alignment\n  Studies","summary":"  This paper revisits the simple, long-studied, yet still unsolved problem of\nmaking image classifiers robust to imperceptible perturbations. Taking CIFAR10\nas an example, SOTA clean accuracy is about $100$%, but SOTA robustness to\n$\\ell_{\\infty}$-norm bounded perturbations barely exceeds $70$%. To understand\nthis gap, we analyze how model size, dataset size, and synthetic data quality\naffect robustness by developing the first scaling laws for adversarial\ntraining. Our scaling laws reveal inefficiencies in prior art and provide\nactionable feedback to advance the field. For instance, we discovered that SOTA\nmethods diverge notably from compute-optimal setups, using excess compute for\ntheir level of robustness. Leveraging a compute-efficient setup, we surpass the\nprior SOTA with $20$% ($70$%) fewer training (inference) FLOPs. We trained\nvarious compute-efficient models, with our best achieving $74$% AutoAttack\naccuracy ($+3$% gain). However, our scaling laws also predict robustness slowly\ngrows then plateaus at $90$%: dwarfing our new SOTA by scaling is impractical,\nand perfect robustness is impossible. To better understand this predicted\nlimit, we carry out a small-scale human evaluation on the AutoAttack data that\nfools our top-performing model. Concerningly, we estimate that human\nperformance also plateaus near $90$%, which we show to be attributable to\n$\\ell_{\\infty}$-constrained attacks' generation of invalid images not\nconsistent with their original labels. Having characterized limiting\nroadblocks, we outline promising paths for future research.\n","authors":["Brian R. Bartoldson","James Diffenderfer","Konstantinos Parasyris","Bhavya Kailkhura"],"pdf_url":"https://arxiv.org/pdf/2404.09349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09342v1","updated":"2024-04-14T19:51:32Z","published":"2024-04-14T19:51:32Z","title":"Face-voice Association in Multilingual Environments (FAME) Challenge\n  2024 Evaluation Plan","summary":"  The advancements of technology have led to the use of multimodal systems in\nvarious real-world applications. Among them, the audio-visual systems are one\nof the widely used multimodal systems. In the recent years, associating face\nand voice of a person has gained attention due to presence of unique\ncorrelation between them. The Face-voice Association in Multilingual\nEnvironments (FAME) Challenge 2024 focuses on exploring face-voice association\nunder a unique condition of multilingual scenario. This condition is inspired\nfrom the fact that half of the world's population is bilingual and most often\npeople communicate under multilingual scenario. The challenge uses a dataset\nnamely, Multilingual Audio-Visual (MAV-Celeb) for exploring face-voice\nassociation in multilingual environments. This report provides the details of\nthe challenge, dataset, baselines and task details for the FAME Challenge.\n","authors":["Muhammad Saad Saeed","Shah Nawaz","Muhammad Salman Tahir","Rohan Kumar Das","Muhammad Zaigham Zaheer","Marta Moscati","Markus Schedl","Muhammad Haris Khan","Karthik Nandakumar","Muhammad Haroon Yousaf"],"pdf_url":"https://arxiv.org/pdf/2404.09342v1.pdf","comment":"ACM Multimedia Conference - Grand Challenge"},{"id":"http://arxiv.org/abs/2404.09326v1","updated":"2024-04-14T18:57:38Z","published":"2024-04-14T18:57:38Z","title":"Weight Copy and Low-Rank Adaptation for Few-Shot Distillation of Vision\n  Transformers","summary":"  Few-shot knowledge distillation recently emerged as a viable approach to\nharness the knowledge of large-scale pre-trained models, using limited data and\ncomputational resources. In this paper, we propose a novel few-shot feature\ndistillation approach for vision transformers. Our approach is based on two key\nsteps. Leveraging the fact that vision transformers have a consistent\ndepth-wise structure, we first copy the weights from intermittent layers of\nexisting pre-trained vision transformers (teachers) into shallower\narchitectures (students), where the intermittence factor controls the\ncomplexity of the student transformer with respect to its teacher. Next, we\nemploy an enhanced version of Low-Rank Adaptation (LoRA) to distill knowledge\ninto the student in a few-shot scenario, aiming to recover the information\nprocessing carried out by the skipped teacher layers. We present comprehensive\nexperiments with supervised and self-supervised transformers as teachers, on\nfive data sets from various domains, including natural, medical and satellite\nimages. The empirical results confirm the superiority of our approach over\ncompetitive baselines. Moreover, the ablation results demonstrate the\nusefulness of each component of the proposed pipeline.\n","authors":["Diana-Nicoleta Grigore","Mariana-Iuliana Georgescu","Jon Alvarez Justo","Tor Johansen","Andreea Iuliana Ionescu","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2404.09326v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05105v2","updated":"2024-04-14T18:27:41Z","published":"2024-04-07T23:10:26Z","title":"VMambaMorph: a Multi-Modality Deformable Image Registration Framework\n  based on Visual State Space Model with Cross-Scan Module","summary":"  Image registration, a critical process in medical imaging, involves aligning\ndifferent sets of medical imaging data into a single unified coordinate system.\nDeep learning networks, such as the Convolutional Neural Network (CNN)-based\nVoxelMorph, Vision Transformer (ViT)-based TransMorph, and State Space Model\n(SSM)-based MambaMorph, have demonstrated effective performance in this domain.\nThe recent Visual State Space Model (VMamba), which incorporates a cross-scan\nmodule with SSM, has exhibited promising improvements in modeling global-range\ndependencies with efficient computational cost in computer vision tasks. This\npaper hereby introduces an exploration of VMamba with image registration, named\nVMambaMorph. This novel hybrid VMamba-CNN network is designed specifically for\n3D image registration. Utilizing a U-shaped network architecture, VMambaMorph\ncomputes the deformation field based on target and source volumes. The\nVMamba-based block with 2D cross-scan module is redesigned for 3D volumetric\nfeature processing. To overcome the complex motion and structure on\nmulti-modality images, we further propose a fine-tune recursive registration\nframework. We validate VMambaMorph using a public benchmark brain MR-CT\nregistration dataset, comparing its performance against current\nstate-of-the-art methods. The results indicate that VMambaMorph achieves\ncompetitive registration quality. The code for VMambaMorph with all baseline\nmethods is available on GitHub.\n","authors":["Ziyang Wang","Jian-Qing Zheng","Chao Ma","Tao Guo"],"pdf_url":"https://arxiv.org/pdf/2404.05105v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16519v2","updated":"2024-04-14T17:56:49Z","published":"2023-12-27T10:57:03Z","title":"Image Restoration by Denoising Diffusion Models with Iteratively\n  Preconditioned Guidance","summary":"  Training deep neural networks has become a common approach for addressing\nimage restoration problems. An alternative for training a \"task-specific\"\nnetwork for each observation model is to use pretrained deep denoisers for\nimposing only the signal's prior within iterative algorithms, without\nadditional training. Recently, a sampling-based variant of this approach has\nbecome popular with the rise of diffusion/score-based generative models. Using\ndenoisers for general purpose restoration requires guiding the iterations to\nensure agreement of the signal with the observations. In low-noise settings,\nguidance that is based on back-projection (BP) has been shown to be a promising\nstrategy (used recently also under the names \"pseudoinverse\" or\n\"range/null-space\" guidance). However, the presence of noise in the\nobservations hinders the gains from this approach. In this paper, we propose a\nnovel guidance technique, based on preconditioning that allows traversing from\nBP-based guidance to least squares based guidance along the restoration scheme.\nThe proposed approach is robust to noise while still having much simpler\nimplementation than alternative methods (e.g., it does not require SVD or a\nlarge number of iterations). We use it within both an optimization scheme and a\nsampling-based scheme, and demonstrate its advantages over existing methods for\nimage deblurring and super-resolution.\n","authors":["Tomer Garber","Tom Tirer"],"pdf_url":"https://arxiv.org/pdf/2312.16519v2.pdf","comment":"CVPR 2024 (camera-ready). Code can be found at:\n  https://github.com/tirer-lab/DDPG"},{"id":"http://arxiv.org/abs/2312.05239v3","updated":"2024-04-14T17:39:27Z","published":"2023-12-08T18:44:09Z","title":"SwiftBrush: One-Step Text-to-Image Diffusion Model with Variational\n  Score Distillation","summary":"  Despite their ability to generate high-resolution and diverse images from\ntext prompts, text-to-image diffusion models often suffer from slow iterative\nsampling processes. Model distillation is one of the most effective directions\nto accelerate these models. However, previous distillation methods fail to\nretain the generation quality while requiring a significant amount of images\nfor training, either from real data or synthetically generated by the teacher\nmodel. In response to this limitation, we present a novel image-free\ndistillation scheme named $\\textbf{SwiftBrush}$. Drawing inspiration from\ntext-to-3D synthesis, in which a 3D neural radiance field that aligns with the\ninput prompt can be obtained from a 2D text-to-image diffusion prior via a\nspecialized loss without the use of any 3D data ground-truth, our approach\nre-purposes that same loss for distilling a pretrained multi-step text-to-image\nmodel to a student network that can generate high-fidelity images with just a\nsingle inference step. In spite of its simplicity, our model stands as one of\nthe first one-step text-to-image generators that can produce images of\ncomparable quality to Stable Diffusion without reliance on any training image\ndata. Remarkably, SwiftBrush achieves an FID score of $\\textbf{16.67}$ and a\nCLIP score of $\\textbf{0.29}$ on the COCO-30K benchmark, achieving competitive\nresults or even substantially surpassing existing state-of-the-art distillation\ntechniques.\n","authors":["Thuan Hoang Nguyen","Anh Tran"],"pdf_url":"https://arxiv.org/pdf/2312.05239v3.pdf","comment":"Accepted to CVPR 2024; Project Page:\n  https://thuanz123.github.io/swiftbrush/"},{"id":"http://arxiv.org/abs/2404.09308v1","updated":"2024-04-14T17:33:33Z","published":"2024-04-14T17:33:33Z","title":"In My Perspective, In My Hands: Accurate Egocentric 2D Hand Pose and\n  Action Recognition","summary":"  Action recognition is essential for egocentric video understanding, allowing\nautomatic and continuous monitoring of Activities of Daily Living (ADLs)\nwithout user effort. Existing literature focuses on 3D hand pose input, which\nrequires computationally intensive depth estimation networks or wearing an\nuncomfortable depth sensor. In contrast, there has been insufficient research\nin understanding 2D hand pose for egocentric action recognition, despite the\navailability of user-friendly smart glasses in the market capable of capturing\na single RGB image. Our study aims to fill this research gap by exploring the\nfield of 2D hand pose estimation for egocentric action recognition, making two\ncontributions. Firstly, we introduce two novel approaches for 2D hand pose\nestimation, namely EffHandNet for single-hand estimation and EffHandEgoNet,\ntailored for an egocentric perspective, capturing interactions between hands\nand objects. Both methods outperform state-of-the-art models on H2O and FPHA\npublic benchmarks. Secondly, we present a robust action recognition\narchitecture from 2D hand and object poses. This method incorporates\nEffHandEgoNet, and a transformer-based action recognition method. Evaluated on\nH2O and FPHA datasets, our architecture has a faster inference time and\nachieves an accuracy of 91.32% and 94.43%, respectively, surpassing state of\nthe art, including 3D-based methods. Our work demonstrates that using 2D\nskeletal data is a robust approach for egocentric action understanding.\nExtensive evaluation and ablation studies show the impact of the hand pose\nestimation approach, and how each input affects the overall performance.\n","authors":["Wiktor Mucha","Martin Kampel"],"pdf_url":"https://arxiv.org/pdf/2404.09308v1.pdf","comment":"Accepted at: The 18th IEEE International Conference on Automatic Face\n  and Gesture Recognition"},{"id":"http://arxiv.org/abs/2309.07849v3","updated":"2024-04-14T17:29:46Z","published":"2023-09-14T16:48:31Z","title":"TFNet: Exploiting Temporal Cues for Fast and Accurate LiDAR Semantic\n  Segmentation","summary":"  LiDAR semantic segmentation plays a crucial role in enabling autonomous\ndriving and robots to understand their surroundings accurately and robustly. A\nmultitude of methods exist within this domain, including point-based,\nrange-image-based, polar-coordinate-based, and hybrid strategies. Among these,\nrange-image-based techniques have gained widespread adoption in practical\napplications due to their efficiency. However, they face a significant\nchallenge known as the ``many-to-one'' problem caused by the range image's\nlimited horizontal and vertical angular resolution. As a result, around 20% of\nthe 3D points can be occluded. In this paper, we present TFNet, a\nrange-image-based LiDAR semantic segmentation method that utilizes temporal\ninformation to address this issue. Specifically, we incorporate a temporal\nfusion layer to extract useful information from previous scans and integrate it\nwith the current scan. We then design a max-voting-based post-processing\ntechnique to correct false predictions, particularly those caused by the\n``many-to-one'' issue. We evaluated the approach on two benchmarks and\ndemonstrated that the plug-in post-processing technique is generic and can be\napplied to various networks.\n","authors":["Rong Li","ShiJie Li","Xieyuanli Chen","Teli Ma","Juergen Gall","Junwei Liang"],"pdf_url":"https://arxiv.org/pdf/2309.07849v3.pdf","comment":"accepted by CVPR2024 Workshop on Autonomous Driving"},{"id":"http://arxiv.org/abs/2404.09301v1","updated":"2024-04-14T16:55:23Z","published":"2024-04-14T16:55:23Z","title":"A Simple Strategy for Body Estimation from Partial-View Images","summary":"  Virtual try-on and product personalization have become increasingly important\nin modern online shopping, highlighting the need for accurate body measurement\nestimation. Although previous research has advanced in estimating 3D body\nshapes from RGB images, the task is inherently ambiguous as the observed scale\nof human subjects in the images depends on two unknown factors: capture\ndistance and body dimensions. This ambiguity is particularly pronounced in\npartial-view scenarios. To address this challenge, we propose a modular and\nsimple height normalization solution. This solution relocates the subject\nskeleton to the desired position, thereby normalizing the scale and\ndisentangling the relationship between the two variables. Our experimental\nresults demonstrate that integrating this technique into state-of-the-art human\nmesh reconstruction models significantly enhances partial body measurement\nestimation. Additionally, we illustrate the applicability of this approach to\nmulti-view settings, showcasing its versatility.\n","authors":["Yafei Mao","Xuelu Li","Brandon Smith","Jinjin Li","Raja Bala"],"pdf_url":"https://arxiv.org/pdf/2404.09301v1.pdf","comment":"Accepted to CVPRW 2024 Computer Vision for Fashion, Art, and Design"},{"id":"http://arxiv.org/abs/2404.07191v2","updated":"2024-04-14T16:54:24Z","published":"2024-04-10T17:48:37Z","title":"InstantMesh: Efficient 3D Mesh Generation from a Single Image with\n  Sparse-view Large Reconstruction Models","summary":"  We present InstantMesh, a feed-forward framework for instant 3D mesh\ngeneration from a single image, featuring state-of-the-art generation quality\nand significant training scalability. By synergizing the strengths of an\noff-the-shelf multiview diffusion model and a sparse-view reconstruction model\nbased on the LRM architecture, InstantMesh is able to create diverse 3D assets\nwithin 10 seconds. To enhance the training efficiency and exploit more\ngeometric supervisions, e.g, depths and normals, we integrate a differentiable\niso-surface extraction module into our framework and directly optimize on the\nmesh representation. Experimental results on public datasets demonstrate that\nInstantMesh significantly outperforms other latest image-to-3D baselines, both\nqualitatively and quantitatively. We release all the code, weights, and demo of\nInstantMesh, with the intention that it can make substantial contributions to\nthe community of 3D generative AI and empower both researchers and content\ncreators.\n","authors":["Jiale Xu","Weihao Cheng","Yiming Gao","Xintao Wang","Shenghua Gao","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2404.07191v2.pdf","comment":"Technical report. Project: https://github.com/TencentARC/InstantMesh"},{"id":"http://arxiv.org/abs/2404.09293v1","updated":"2024-04-14T16:09:33Z","published":"2024-04-14T16:09:33Z","title":"A Novel State Space Model with Local Enhancement and State Sharing for\n  Image Fusion","summary":"  In image fusion tasks, images from different sources possess distinct\ncharacteristics. This has driven the development of numerous methods to explore\nbetter ways of fusing them while preserving their respective characteristics.\nMamba, as a state space model, has emerged in the field of natural language\nprocessing. Recently, many studies have attempted to extend Mamba to vision\ntasks. However, due to the nature of images different from casual language\nsequences, the limited state capacity of Mamba weakens its ability to model\nimage information. Additionally, the sequence modeling ability of Mamba is only\ncapable of spatial information and cannot effectively capture the rich spectral\ninformation in images. Motivated by these challenges, we customize and improve\nthe vision Mamba network designed for the image fusion task. Specifically, we\npropose the local-enhanced vision Mamba block, dubbed as LEVM. The LEVM block\ncan improve local information perception of the network and simultaneously\nlearn local and global spatial information. Furthermore, we propose the state\nsharing technique to enhance spatial details and integrate spatial and spectral\ninformation. Finally, the overall network is a multi-scale structure based on\nvision Mamba, called LE-Mamba. Extensive experiments show the proposed methods\nachieve state-of-the-art results on multispectral pansharpening and\nmultispectral and hyperspectral image fusion datasets, and demonstrate the\neffectiveness of the proposed approach. Code will be made available.\n","authors":["Zihan Cao","Xiao Wu","Liang-Jian Deng","Yu Zhong"],"pdf_url":"https://arxiv.org/pdf/2404.09293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09292v1","updated":"2024-04-14T15:58:35Z","published":"2024-04-14T15:58:35Z","title":"Bridging Data Islands: Geographic Heterogeneity-Aware Federated Learning\n  for Collaborative Remote Sensing Semantic Segmentation","summary":"  Remote sensing semantic segmentation (RSS) is an essential task in Earth\nObservation missions. Due to data privacy concerns, high-quality remote sensing\nimages with annotations cannot be well shared among institutions, making it\ndifficult to fully utilize RSS data to train a generalized model. Federated\nLearning (FL), a privacy-preserving collaborative learning technology, is a\npotential solution. However, the current research on how to effectively apply\nFL in RSS is still scarce and requires further investigation. Remote sensing\nimages in various institutions often exhibit strong geographical heterogeneity.\nMore specifically, it is reflected in terms of class-distribution heterogeneity\nand object-appearance heterogeneity. Unfortunately, most existing FL studies\nshow inadequate focus on geographical heterogeneity, thus leading to\nperformance degradation in the global model. Considering the aforementioned\nissues, we propose a novel Geographic Heterogeneity-Aware Federated Learning\n(GeoFed) framework to address privacy-preserving RSS. Through Global Feature\nExtension and Tail Regeneration modules, class-distribution heterogeneity is\nalleviated. Additionally, we design an Essential Feature Mining strategy to\nalleviate object-appearance heterogeneity by constructing essential features.\nExtensive experiments on three datasets (i.e., FBP, CASID, Inria) show that our\nGeoFed consistently outperforms the current state-of-the-art methods. The code\nwill be available publicly.\n","authors":["Jieyi Tan","Yansheng Li","Sergey A. Bartalev","Bo Dang","Wei Chen","Yongjun Zhang","Liangqi Yuan"],"pdf_url":"https://arxiv.org/pdf/2404.09292v1.pdf","comment":"13 pages,9 figures, 4 tables"},{"id":"http://arxiv.org/abs/2404.09290v1","updated":"2024-04-14T15:50:10Z","published":"2024-04-14T15:50:10Z","title":"RoofDiffusion: Constructing Roofs from Severely Corrupted Point Data via\n  Diffusion","summary":"  Accurate completion and denoising of roof height maps are crucial to\nreconstructing high-quality 3D buildings. Repairing sparse points can enhance\nlow-cost sensor use and reduce UAV flight overlap. RoofDiffusion is a new\nend-to-end self-supervised diffusion technique for robustly completing, in\nparticular difficult, roof height maps. RoofDiffusion leverages\nwidely-available curated footprints and can so handle up to 99\\% point sparsity\nand 80\\% roof area occlusion (regional incompleteness). A variant, No-FP\nRoofDiffusion, simultaneously predicts building footprints and heights. Both\nquantitatively outperform state-of-the-art unguided depth completion and\nrepresentative inpainting methods for Digital Elevation Models (DEM), on both a\nroof-specific benchmark and the BuildingNet dataset. Qualitative assessments\nshow the effectiveness of RoofDiffusion for datasets with real-world scans\nincluding AHN3, Dales3D, and USGS 3DEP LiDAR. Tested with the leading City3D\nalgorithm, preprocessing height maps with RoofDiffusion noticeably improves 3D\nbuilding reconstruction. RoofDiffusion is complemented by a new dataset of 13k\ncomplex roof geometries, focusing on long-tail issues in remote sensing; a\nnovel simulation of tree occlusion; and a wide variety of large-area roof\ncut-outs for data augmentation and benchmarking.\n","authors":["Kyle Shih-Huang Lo","J√∂rg Peters","Eric Spellman"],"pdf_url":"https://arxiv.org/pdf/2404.09290v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09277v1","updated":"2024-04-14T14:58:52Z","published":"2024-04-14T14:58:52Z","title":"SyntStereo2Real: Edge-Aware GAN for Remote Sensing Image-to-Image\n  Translation while Maintaining Stereo Constraint","summary":"  In the field of remote sensing, the scarcity of stereo-matched and\nparticularly lack of accurate ground truth data often hinders the training of\ndeep neural networks. The use of synthetically generated images as an\nalternative, alleviates this problem but suffers from the problem of domain\ngeneralization. Unifying the capabilities of image-to-image translation and\nstereo-matching presents an effective solution to address the issue of domain\ngeneralization. Current methods involve combining two networks, an unpaired\nimage-to-image translation network and a stereo-matching network, while jointly\noptimizing them. We propose an edge-aware GAN-based network that effectively\ntackles both tasks simultaneously. We obtain edge maps of input images from the\nSobel operator and use it as an additional input to the encoder in the\ngenerator to enforce geometric consistency during translation. We additionally\ninclude a warping loss calculated from the translated images to maintain the\nstereo consistency. We demonstrate that our model produces qualitatively and\nquantitatively superior results than existing models, and its applicability\nextends to diverse domains, including autonomous driving.\n","authors":["Vasudha Venkatesan","Daniel Panangian","Mario Fuentes Reyes","Ksenia Bittner"],"pdf_url":"https://arxiv.org/pdf/2404.09277v1.pdf","comment":"Accepted to IEEE Conference on Computer Vision and Pattern\n  Recognition Workshop (CVPRW) EarthVision"},{"id":"http://arxiv.org/abs/2304.02649v3","updated":"2024-04-14T14:55:55Z","published":"2023-04-03T20:19:56Z","title":"Specialty-Oriented Generalist Medical AI for Chest CT Screening","summary":"  Modern medical records include a vast amount of multimodal free text clinical\ndata and imaging data from radiology, cardiology, and digital pathology. Fully\nmining such big data requires multitasking; otherwise, occult but important\naspects may be overlooked, adversely affecting clinical management and\npopulation healthcare. Despite remarkable successes of AI in individual tasks\nwith single-modal data, the progress in developing generalist medical AI\nremains relatively slow to combine multimodal data for multitasks because of\nthe dual challenges of data curation and model architecture. The data challenge\ninvolves querying and curating multimodal structured and unstructured text,\nalphanumeric, and especially 3D tomographic scans on an individual patient\nlevel for real-time decisions and on a scale to estimate population health\nstatistics. The model challenge demands a scalable and adaptable network\narchitecture to integrate multimodal datasets for diverse clinical tasks. Here\nwe propose the first-of-its-kind medical multimodal-multitask foundation model\n(M3FM) with application in lung cancer screening and related tasks. After we\ncurated a comprehensive multimodal multitask dataset consisting of 49 clinical\ndata types including 163,725 chest CT series and 17 medical tasks involved in\nLCS, we develop a multimodal question-answering framework as a unified training\nand inference strategy to synergize multimodal information and perform multiple\ntasks via free-text prompting. M3FM consistently outperforms the\nstate-of-the-art single-modal task-specific models, identifies multimodal data\nelements informative for clinical tasks and flexibly adapts to new tasks with a\nsmall out-of-distribution dataset. As a specialty-oriented generalist medical\nAI model, M3FM paves the way for similar breakthroughs in other areas of\nmedicine, closing the gap between specialists and the generalist.\n","authors":["Chuang Niu","Qing Lyu","Christopher D. Carothers","Parisa Kaviani","Josh Tan","Pingkun Yan","Mannudeep K. Kalra","Christopher T. Whitlow","Ge Wang"],"pdf_url":"https://arxiv.org/pdf/2304.02649v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01029v3","updated":"2024-04-14T14:53:32Z","published":"2023-04-03T14:28:29Z","title":"Domain Generalization for Crop Segmentation with Standardized Ensemble\n  Knowledge Distillation","summary":"  In recent years, precision agriculture has gradually oriented farming closer\nto automation processes to support all the activities related to field\nmanagement. Service robotics plays a predominant role in this evolution by\ndeploying autonomous agents that can navigate fields while performing tasks\nsuch as monitoring, spraying, and harvesting without human intervention. To\nexecute these precise actions, mobile robots need a real-time perception system\nthat understands their surroundings and identifies their targets in the wild.\nExisting methods, however, often fall short in generalizing to new crops and\nenvironmental conditions. This limit is critical for practical applications\nwhere labeled samples are rarely available. In this paper, we investigate the\nproblem of crop segmentation and propose a novel approach to enhance domain\ngeneralization using knowledge distillation. In the proposed framework, we\ntransfer knowledge from a standardized ensemble of models individually trained\non source domains to a student model that can adapt to unseen realistic\nscenarios. To support the proposed method, we present a synthetic multi-domain\ndataset for crop segmentation containing plants of variegate species and\ncovering different terrain styles, weather conditions, and light scenarios for\nmore than 70,000 samples. We demonstrate significant improvements in\nperformance over state-of-the-art methods and superior sim-to-real\ngeneralization. Our approach provides a promising solution for domain\ngeneralization in crop segmentation and has the potential to enhance a wide\nvariety of agriculture applications.\n","authors":["Simone Angarano","Mauro Martini","Alessandro Navone","Marcello Chiaberge"],"pdf_url":"https://arxiv.org/pdf/2304.01029v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09275v1","updated":"2024-04-14T14:51:44Z","published":"2024-04-14T14:51:44Z","title":"TrafficVLM: A Controllable Visual Language Model for Traffic Video\n  Captioning","summary":"  Traffic video description and analysis have received much attention recently\ndue to the growing demand for efficient and reliable urban surveillance\nsystems. Most existing methods only focus on locating traffic event segments,\nwhich severely lack descriptive details related to the behaviour and context of\nall the subjects of interest in the events. In this paper, we present\nTrafficVLM, a novel multi-modal dense video captioning model for vehicle ego\ncamera view. TrafficVLM models traffic video events at different levels of\nanalysis, both spatially and temporally, and generates long fine-grained\ndescriptions for the vehicle and pedestrian at different phases of the event.\nWe also propose a conditional component for TrafficVLM to control the\ngeneration outputs and a multi-task fine-tuning paradigm to enhance\nTrafficVLM's learning capability. Experiments show that TrafficVLM performs\nwell on both vehicle and overhead camera views. Our solution achieved\noutstanding results in Track 2 of the AI City Challenge 2024, ranking us third\nin the challenge standings. Our code is publicly available at\nhttps://github.com/quangminhdinh/TrafficVLM.\n","authors":["Quang Minh Dinh","Minh Khoi Ho","Anh Quan Dang","Hung Phong Tran"],"pdf_url":"https://arxiv.org/pdf/2404.09275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09271v1","updated":"2024-04-14T14:26:33Z","published":"2024-04-14T14:26:33Z","title":"VRS-NeRF: Visual Relocalization with Sparse Neural Radiance Field","summary":"  Visual relocalization is a key technique to autonomous driving, robotics, and\nvirtual/augmented reality. After decades of explorations, absolute pose\nregression (APR), scene coordinate regression (SCR), and hierarchical methods\n(HMs) have become the most popular frameworks. However, in spite of high\nefficiency, APRs and SCRs have limited accuracy especially in large-scale\noutdoor scenes; HMs are accurate but need to store a large number of 2D\ndescriptors for matching, resulting in poor efficiency. In this paper, we\npropose an efficient and accurate framework, called VRS-NeRF, for visual\nrelocalization with sparse neural radiance field. Precisely, we introduce an\nexplicit geometric map (EGM) for 3D map representation and an implicit learning\nmap (ILM) for sparse patches rendering. In this localization process, EGP\nprovides priors of spare 2D points and ILM utilizes these sparse points to\nrender patches with sparse NeRFs for matching. This allows us to discard a\nlarge number of 2D descriptors so as to reduce the map size. Moreover,\nrendering patches only for useful points rather than all pixels in the whole\nimage reduces the rendering time significantly. This framework inherits the\naccuracy of HMs and discards their low efficiency. Experiments on 7Scenes,\nCambridgeLandmarks, and Aachen datasets show that our method gives much better\naccuracy than APRs and SCRs, and close performance to HMs but is much more\nefficient.\n","authors":["Fei Xue","Ignas Budvytis","Daniel Olmeda Reino","Roberto Cipolla"],"pdf_url":"https://arxiv.org/pdf/2404.09271v1.pdf","comment":"source code https://github.com/feixue94/vrs-nerf"},{"id":"http://arxiv.org/abs/2404.09269v1","updated":"2024-04-14T14:24:13Z","published":"2024-04-14T14:24:13Z","title":"PANet: A Physics-guided Parametric Augmentation Net for Image Dehazing\n  by Hazing","summary":"  Image dehazing faces challenges when dealing with hazy images in real-world\nscenarios. A huge domain gap between synthetic and real-world haze images\ndegrades dehazing performance in practical settings. However, collecting\nreal-world image datasets for training dehazing models is challenging since\nboth hazy and clean pairs must be captured under the same conditions. In this\npaper, we propose a Physics-guided Parametric Augmentation Network (PANet) that\ngenerates photo-realistic hazy and clean training pairs to effectively enhance\nreal-world dehazing performance. PANet comprises a Haze-to-Parameter Mapper\n(HPM) to project hazy images into a parameter space and a Parameter-to-Haze\nMapper (PHM) to map the resampled haze parameters back to hazy images. In the\nparameter space, we can pixel-wisely resample individual haze parameter maps to\ngenerate diverse hazy images with physically-explainable haze conditions unseen\nin the training set. Our experimental results demonstrate that PANet can\naugment diverse realistic hazy images to enrich existing hazy image benchmarks\nso as to effectively boost the performances of state-of-the-art image dehazing\nmodels.\n","authors":["Chih-Ling Chang","Fu-Jen Tsai","Zi-Ling Huang","Lin Gu","Chia-Wen Lin"],"pdf_url":"https://arxiv.org/pdf/2404.09269v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07564v2","updated":"2024-04-14T14:11:58Z","published":"2024-03-12T11:51:59Z","title":"RSBuilding: Towards General Remote Sensing Image Building Extraction and\n  Change Detection with Foundation Model","summary":"  The intelligent interpretation of buildings plays a significant role in urban\nplanning and management, macroeconomic analysis, population dynamics, etc.\nRemote sensing image building interpretation primarily encompasses building\nextraction and change detection. However, current methodologies often treat\nthese two tasks as separate entities, thereby failing to leverage shared\nknowledge. Moreover, the complexity and diversity of remote sensing image\nscenes pose additional challenges, as most algorithms are designed to model\nindividual small datasets, thus lacking cross-scene generalization. In this\npaper, we propose a comprehensive remote sensing image building understanding\nmodel, termed RSBuilding, developed from the perspective of the foundation\nmodel. RSBuilding is designed to enhance cross-scene generalization and task\nuniversality. Specifically, we extract image features based on the prior\nknowledge of the foundation model and devise a multi-level feature sampler to\naugment scale information. To unify task representation and integrate image\nspatiotemporal clues, we introduce a cross-attention decoder with task prompts.\nAddressing the current shortage of datasets that incorporate annotations for\nboth tasks, we have developed a federated training strategy to facilitate\nsmooth model convergence even when supervision for some tasks is missing,\nthereby bolstering the complementarity of different tasks. Our model was\ntrained on a dataset comprising up to 245,000 images and validated on multiple\nbuilding extraction and change detection datasets. The experimental results\nsubstantiate that RSBuilding can concurrently handle two structurally distinct\ntasks and exhibits robust zero-shot generalization capabilities.\n","authors":["Mingze Wang","Lili Su","Cilin Yan","Sheng Xu","Pengcheng Yuan","Xiaolong Jiang","Baochang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.07564v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09263v1","updated":"2024-04-14T14:06:42Z","published":"2024-04-14T14:06:42Z","title":"Task-Driven Exploration: Decoupling and Inter-Task Feedback for Joint\n  Moment Retrieval and Highlight Detection","summary":"  Video moment retrieval and highlight detection are two highly valuable tasks\nin video understanding, but until recently they have been jointly studied.\nAlthough existing studies have made impressive advancement recently, they\npredominantly follow the data-driven bottom-up paradigm. Such paradigm\noverlooks task-specific and inter-task effects, resulting in poor model\nperformance. In this paper, we propose a novel task-driven top-down framework\nTaskWeave for joint moment retrieval and highlight detection. The framework\nintroduces a task-decoupled unit to capture task-specific and common\nrepresentations. To investigate the interplay between the two tasks, we propose\nan inter-task feedback mechanism, which transforms the results of one task as\nguiding masks to assist the other task. Different from existing methods, we\npresent a task-dependent joint loss function to optimize the model.\nComprehensive experiments and in-depth ablation studies on QVHighlights, TVSum,\nand Charades-STA datasets corroborate the effectiveness and flexibility of the\nproposed framework. Codes are available at\nhttps://github.com/EdenGabriel/TaskWeave.\n","authors":["Jin Yang","Ping Wei","Huan Li","Ziyang Ren"],"pdf_url":"https://arxiv.org/pdf/2404.09263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09259v1","updated":"2024-04-14T13:56:30Z","published":"2024-04-14T13:56:30Z","title":"FedCCL: Federated Dual-Clustered Feature Contrast Under Domain\n  Heterogeneity","summary":"  Federated learning (FL) facilitates a privacy-preserving neural network\ntraining paradigm through collaboration between edge clients and a central\nserver. One significant challenge is that the distributed data is not\nindependently and identically distributed (non-IID), typically including both\nintra-domain and inter-domain heterogeneity. However, recent research is\nlimited to simply using averaged signals as a form of regularization and only\nfocusing on one aspect of these non-IID challenges. Given these limitations,\nthis paper clarifies these two non-IID challenges and attempts to introduce\ncluster representation to address them from both local and global perspectives.\nSpecifically, we propose a dual-clustered feature contrast-based FL framework\nwith dual focuses. First, we employ clustering on the local representations of\neach client, aiming to capture intra-class information based on these local\nclusters at a high level of granularity. Then, we facilitate cross-client\nknowledge sharing by pulling the local representation closer to clusters shared\nby clients with similar semantics while pushing them away from clusters with\ndissimilar semantics. Second, since the sizes of local clusters belonging to\nthe same class may differ for each client, we further utilize clustering on the\nglobal side and conduct averaging to create a consistent global signal for\nguiding each local training in a contrastive manner. Experimental results on\nmultiple datasets demonstrate that our proposal achieves comparable or superior\nperformance gain under intra-domain and inter-domain heterogeneity.\n","authors":["Yu Qiao","Huy Q. Le","Mengchun Zhang","Apurba Adhikary","Chaoning Zhang","Choong Seon Hong"],"pdf_url":"https://arxiv.org/pdf/2404.09259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09254v1","updated":"2024-04-14T13:39:02Z","published":"2024-04-14T13:39:02Z","title":"TEXT2TASTE: A Versatile Egocentric Vision System for Intelligent Reading\n  Assistance Using Large Language Model","summary":"  The ability to read, understand and find important information from written\ntext is a critical skill in our daily lives for our independence, comfort and\nsafety. However, a significant part of our society is affected by partial\nvision impairment, which leads to discomfort and dependency in daily\nactivities. To address the limitations of this part of society, we propose an\nintelligent reading assistant based on smart glasses with embedded RGB cameras\nand a Large Language Model (LLM), whose functionality goes beyond corrective\nlenses. The video recorded from the egocentric perspective of a person wearing\nthe glasses is processed to localise text information using object detection\nand optical character recognition methods. The LLM processes the data and\nallows the user to interact with the text and responds to a given query, thus\nextending the functionality of corrective lenses with the ability to find and\nsummarize knowledge from the text. To evaluate our method, we create a\nchat-based application that allows the user to interact with the system. The\nevaluation is conducted in a real-world setting, such as reading menus in a\nrestaurant, and involves four participants. The results show robust accuracy in\ntext retrieval. The system not only provides accurate meal suggestions but also\nachieves high user satisfaction, highlighting the potential of smart glasses\nand LLMs in assisting people with special needs.\n","authors":["Wiktor Mucha","Florin Cuconasu","Naome A. Etori","Valia Kalokyri","Giovanni Trappolini"],"pdf_url":"https://arxiv.org/pdf/2404.09254v1.pdf","comment":"Accepted at ICCHP 2024"},{"id":"http://arxiv.org/abs/2312.06709v4","updated":"2024-04-14T13:35:14Z","published":"2023-12-10T17:07:29Z","title":"AM-RADIO: Agglomerative Vision Foundation Model -- Reduce All Domains\n  Into One","summary":"  A handful of visual foundation models (VFMs) have recently emerged as the\nbackbones for numerous downstream tasks. VFMs like CLIP, DINOv2, SAM are\ntrained with distinct objectives, exhibiting unique characteristics for various\ndownstream tasks. We find that despite their conceptual differences, these\nmodels can be effectively merged into a unified model through multi-teacher\ndistillation. We name this approach AM-RADIO (Agglomerative Model -- Reduce All\nDomains Into One). This integrative approach not only surpasses the performance\nof individual teacher models but also amalgamates their distinctive features,\nsuch as zero-shot vision-language comprehension, detailed pixel-level\nunderstanding, and open vocabulary segmentation capabilities. In pursuit of the\nmost hardware-efficient backbone, we evaluated numerous architectures in our\nmulti-teacher distillation pipeline using the same training recipe. This led to\nthe development of a novel architecture (E-RADIO) that exceeds the performance\nof its predecessors and is at least 7x faster than the teacher models. Our\ncomprehensive benchmarking process covers downstream tasks including ImageNet\nclassification, ADE20k semantic segmentation, COCO object detection and\nLLaVa-1.5 framework.\n  Code: https://github.com/NVlabs/RADIO\n","authors":["Mike Ranzinger","Greg Heinrich","Jan Kautz","Pavlo Molchanov"],"pdf_url":"https://arxiv.org/pdf/2312.06709v4.pdf","comment":"CVPR 2024 Version 3: CVPR Camera Ready, reconfigured full paper,\n  table 1 is now more comprehensive Version 2: Added more acknowledgements and\n  updated table 7 with more recent results. Ensured that the link in the\n  abstract to our code is working properly Version 3: Fix broken hyperlinks"},{"id":"http://arxiv.org/abs/2404.07766v2","updated":"2024-04-14T13:14:54Z","published":"2024-04-11T14:05:37Z","title":"RMAFF-PSN: A Residual Multi-Scale Attention Feature Fusion Photometric\n  Stereo Network","summary":"  Predicting accurate normal maps of objects from two-dimensional images in\nregions of complex structure and spatial material variations is challenging\nusing photometric stereo methods due to the influence of surface reflection\nproperties caused by variations in object geometry and surface materials. To\naddress this issue, we propose a photometric stereo network called a RMAFF-PSN\nthat uses residual multiscale attentional feature fusion to handle the\n``difficult'' regions of the object. Unlike previous approaches that only use\nstacked convolutional layers to extract deep features from the input image, our\nmethod integrates feature information from different resolution stages and\nscales of the image. This approach preserves more physical information, such as\ntexture and geometry of the object in complex regions, through shallow-deep\nstage feature extraction, double branching enhancement, and attention\noptimization. To test the network structure under real-world conditions, we\npropose a new real dataset called Simple PS data, which contains multiple\nobjects with varying structures and materials. Experimental results on a\npublicly available benchmark dataset demonstrate that our method outperforms\nmost existing calibrated photometric stereo methods for the same number of\ninput images, especially in the case of highly non-convex object structures.\nOur method also obtains good results under sparse lighting conditions.\n","authors":["Kai Luo","Yakun Ju","Lin Qi","Kaixuan Wang","Junyu Dong"],"pdf_url":"https://arxiv.org/pdf/2404.07766v2.pdf","comment":"17 pages,12 figures"},{"id":"http://arxiv.org/abs/2404.09245v1","updated":"2024-04-14T13:14:13Z","published":"2024-04-14T13:14:13Z","title":"Arena: A Patch-of-Interest ViT Inference Acceleration System for\n  Edge-Assisted Video Analytics","summary":"  The advent of edge computing has made real-time intelligent video analytics\nfeasible. Previous works, based on traditional model architecture (e.g., CNN,\nRNN, etc.), employ various strategies to filter out non-region-of-interest\ncontent to minimize bandwidth and computation consumption but show inferior\nperformance in adverse environments. Recently, visual foundation models based\non transformers have shown great performance in adverse environments due to\ntheir amazing generalization capability. However, they require a large amount\nof computation power, which limits their applications in real-time intelligent\nvideo analytics. In this paper, we find visual foundation models like Vision\nTransformer (ViT) also have a dedicated acceleration mechanism for video\nanalytics. To this end, we introduce Arena, an end-to-end edge-assisted video\ninference acceleration system based on ViT. We leverage the capability of ViT\nthat can be accelerated through token pruning by only offloading and feeding\nPatches-of-Interest (PoIs) to the downstream models. Additionally, we employ\nprobability-based patch sampling, which provides a simple but efficient\nmechanism for determining PoIs where the probable locations of objects are in\nsubsequent frames. Through extensive evaluations on public datasets, our\nfindings reveal that Arena can boost inference speeds by up to $1.58\\times$ and\n$1.82\\times$ on average while consuming only 54% and 34% of the bandwidth,\nrespectively, all with high inference accuracy.\n","authors":["Haosong Peng","Wei Feng","Hao Li","Yufeng Zhan","Qihua Zhou","Yuanqing Xia"],"pdf_url":"https://arxiv.org/pdf/2404.09245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12091v4","updated":"2024-04-14T13:03:26Z","published":"2023-03-21T09:07:15Z","title":"Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised\n  Learning","summary":"  Semi-supervised learning (SSL) methods assume that labeled data, unlabeled\ndata and test data are from the same distribution. Open-set semi-supervised\nlearning (Open-set SSL) considers a more practical scenario, where unlabeled\ndata and test data contain new categories (outliers) not observed in labeled\ndata (inliers). Most previous works focused on outlier detection via binary\nclassifiers, which suffer from insufficient scalability and inability to\ndistinguish different types of uncertainty. In this paper, we propose a novel\nframework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these\nlimitations. Concretely, we first introduce evidential deep learning (EDL) as\nan outlier detector to quantify different types of uncertainty, and design\ndifferent uncertainty metrics for self-training and inference. Furthermore, we\npropose a novel adaptive negative optimization strategy, making EDL more\ntailored to the unlabeled dataset containing both inliers and outliers. As\ndemonstrated empirically, our proposed method outperforms existing\nstate-of-the-art methods across four datasets.\n","authors":["Yang Yu","Danruo Deng","Furui Liu","Yueming Jin","Qi Dou","Guangyong Chen","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2303.12091v4.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2211.08089v4","updated":"2024-04-14T13:02:59Z","published":"2022-11-15T12:15:29Z","title":"DeS3: Adaptive Attention-driven Self and Soft Shadow Removal using ViT\n  Similarity","summary":"  Removing soft and self shadows that lack clear boundaries from a single image\nis still challenging. Self shadows are shadows that are cast on the object\nitself. Most existing methods rely on binary shadow masks, without considering\nthe ambiguous boundaries of soft and self shadows. In this paper, we present\nDeS3, a method that removes hard, soft and self shadows based on adaptive\nattention and ViT similarity. Our novel ViT similarity loss utilizes features\nextracted from a pre-trained Vision Transformer. This loss helps guide the\nreverse sampling towards recovering scene structures. Our adaptive attention is\nable to differentiate shadow regions from the underlying objects, as well as\nshadow regions from the object casting the shadow. This capability enables DeS3\nto better recover the structures of objects even when they are partially\noccluded by shadows. Different from existing methods that rely on constraints\nduring the training phase, we incorporate the ViT similarity during the\nsampling stage. Our method outperforms state-of-the-art methods on the SRD,\nAISTD, LRSS, USR and UIUC datasets, removing hard, soft, and self shadows\nrobustly. Specifically, our method outperforms the SOTA method by 16\\% of the\nRMSE of the whole image on the LRSS dataset. Our data and code is available at:\n\\url{https://github.com/jinyeying/DeS3_Deshadow}\n","authors":["Yeying Jin","Wei Ye","Wenhan Yang","Yuan Yuan","Robby T. Tan"],"pdf_url":"https://arxiv.org/pdf/2211.08089v4.pdf","comment":"Accepted to AAAI2024, diffusion shadow removal,\n  \\url{https://github.com/jinyeying/DeS3_Deshadow}"},{"id":"http://arxiv.org/abs/2404.05238v2","updated":"2024-04-14T12:48:55Z","published":"2024-04-08T07:09:15Z","title":"Allowing humans to interactively guide machines where to look does not\n  always improve human-AI team's classification accuracy","summary":"  Via thousands of papers in Explainable AI (XAI), attention maps\n\\cite{vaswani2017attention} and feature attribution maps \\cite{bansal2020sam}\nhave been established as a common means for finding how important each input\nfeature is to an AI's decisions. It is an interesting, unexplored question\nwhether allowing users to edit the feature importance at test time would\nimprove a human-AI team's accuracy on downstream tasks. In this paper, we\naddress this question by leveraging CHM-Corr, a state-of-the-art, ante-hoc\nexplainable classifier \\cite{taesiri2022visual} that first predicts patch-wise\ncorrespondences between the input and training-set images, and then base on\nthem to make classification decisions. We build CHM-Corr++, an interactive\ninterface for CHM-Corr, enabling users to edit the feature attribution map\nprovided by CHM-Corr and observe updated model decisions. Via CHM-Corr++, users\ncan gain insights into if, when, and how the model changes its outputs,\nimproving their understanding beyond static explanations. However, our user\nstudy with 18 users who performed 1,400 decisions finds no statistical\nsignificance that our interactive approach improves user accuracy on CUB-200\nbird image classification over static explanations. This challenges the\nhypothesis that interactivity can boost human-AI team\naccuracy~\\cite{sokol2020one,sun2022exploring,shen2024towards,singh2024rethinking,mindlin2024beyond,lakkaraju2022rethinking,cheng2019explaining,liu2021understanding}\nand raises needs for future research. We open-source CHM-Corr++, an interactive\ntool for editing image classifier attention (see an interactive demo\n\\href{http://137.184.82.109:7080/}{here}). % , and it lays the groundwork for\nfuture research to enable effective human-AI interaction in computer vision. We\nrelease code and data on\n\\href{https://github.com/anguyen8/chm-corr-interactive}{github}.\n","authors":["Giang Nguyen","Mohammad Reza Taesiri","Sunnie S. Y. Kim","Anh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2404.05238v2.pdf","comment":"Accepted for presentation at the XAI4CV Workshop, part of the CVPR\n  2024 proceedings"},{"id":"http://arxiv.org/abs/2308.11949v2","updated":"2024-04-14T12:43:56Z","published":"2023-08-23T06:45:11Z","title":"High-quality Image Dehazing with Diffusion Model","summary":"  Image dehazing is quite challenging in dense-haze scenarios, where quite less\noriginal information remains in the hazy image. Though previous methods have\nmade marvelous progress, they still suffer from information loss in content and\ncolor in dense-haze scenarios. The recently emerged Denoising Diffusion\nProbabilistic Model (DDPM) exhibits strong generation ability, showing\npotential for solving this problem. However, DDPM fails to consider the physics\nproperty of dehazing task, limiting its information completion capacity. In\nthis work, we propose DehazeDDPM: A DDPM-based and physics-aware image dehazing\nframework that applies to complex hazy scenarios. Specifically, DehazeDDPM\nworks in two stages. The former stage physically models the dehazing task with\nthe Atmospheric Scattering Model (ASM), pulling the distribution closer to the\nclear data and endowing DehazeDDPM with fog-aware ability. The latter stage\nexploits the strong generation ability of DDPM to compensate for the\nhaze-induced huge information loss, by working in conjunction with the physical\nmodelling. Extensive experiments demonstrate that our method attains\nstate-of-the-art performance on both synthetic and real-world hazy datasets.\n","authors":["Hu Yu","Jie Huang","Kaiwen Zheng","Feng Zhao"],"pdf_url":"https://arxiv.org/pdf/2308.11949v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09231v1","updated":"2024-04-14T12:19:16Z","published":"2024-04-14T12:19:16Z","title":"Tri-modal Confluence with Temporal Dynamics for Scene Graph Generation\n  in Operating Rooms","summary":"  A comprehensive understanding of surgical scenes allows for monitoring of the\nsurgical process, reducing the occurrence of accidents and enhancing efficiency\nfor medical professionals. Semantic modeling within operating rooms, as a scene\ngraph generation (SGG) task, is challenging since it involves consecutive\nrecognition of subtle surgical actions over prolonged periods. To address this\nchallenge, we propose a Tri-modal (i.e., images, point clouds, and language)\nconfluence with Temporal dynamics framework, termed TriTemp-OR. Diverging from\nprevious approaches that integrated temporal information via memory graphs, our\nmethod embraces two advantages: 1) we directly exploit bi-modal temporal\ninformation from the video streaming for hierarchical feature interaction, and\n2) the prior knowledge from Large Language Models (LLMs) is embedded to\nalleviate the class-imbalance problem in the operating theatre. Specifically,\nour model performs temporal interactions across 2D frames and 3D point clouds,\nincluding a scale-adaptive multi-view temporal interaction (ViewTemp) and a\ngeometric-temporal point aggregation (PointTemp). Furthermore, we transfer\nknowledge from the biomedical LLM, LLaVA-Med, to deepen the comprehension of\nintraoperative relations. The proposed TriTemp-OR enables the aggregation of\ntri-modal features through relation-aware unification to predict relations so\nas to generate scene graphs. Experimental results on the 4D-OR benchmark\ndemonstrate the superior performance of our model for long-term OR streaming.\n","authors":["Diandian Guo","Manxi Lin","Jialun Pei","He Tang","Yueming Jin","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2404.09231v1.pdf","comment":"10 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2404.09227v1","updated":"2024-04-14T12:13:07Z","published":"2024-04-14T12:13:07Z","title":"DreamScape: 3D Scene Creation via Gaussian Splatting joint Correlation\n  Modeling","summary":"  Recent progress in text-to-3D creation has been propelled by integrating the\npotent prior of Diffusion Models from text-to-image generation into the 3D\ndomain. Nevertheless, generating 3D scenes characterized by multiple instances\nand intricate arrangements remains challenging. In this study, we present\nDreamScape, a method for creating highly consistent 3D scenes solely from\ntextual descriptions, leveraging the strong 3D representation capabilities of\nGaussian Splatting and the complex arrangement abilities of large language\nmodels (LLMs). Our approach involves a 3D Gaussian Guide ($3{DG^2}$) for scene\nrepresentation, consisting of semantic primitives (objects) and their spatial\ntransformations and relationships derived directly from text prompts using\nLLMs. This compositional representation allows for local-to-global optimization\nof the entire scene. A progressive scale control is tailored during local\nobject generation, ensuring that objects of different sizes and densities adapt\nto the scene, which addresses training instability issue arising from simple\nblending in the subsequent global optimization stage. To mitigate potential\nbiases of LLM priors, we model collision relationships between objects at the\nglobal level, enhancing physical correctness and overall realism. Additionally,\nto generate pervasive objects like rain and snow distributed extensively across\nthe scene, we introduce a sparse initialization and densification strategy.\nExperiments demonstrate that DreamScape offers high usability and\ncontrollability, enabling the generation of high-fidelity 3D scenes from only\ntext prompts and achieving state-of-the-art performance compared to other\nmethods.\n","authors":["Xuening Yuan","Hongyu Yang","Yueming Zhao","Di Huang"],"pdf_url":"https://arxiv.org/pdf/2404.09227v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09226v1","updated":"2024-04-14T12:09:47Z","published":"2024-04-14T12:09:47Z","title":"Breast Cancer Image Classification Method Based on Deep Transfer\n  Learning","summary":"  To address the issues of limited samples, time-consuming feature design, and\nlow accuracy in detection and classification of breast cancer pathological\nimages, a breast cancer image classification model algorithm combining deep\nlearning and transfer learning is proposed. This algorithm is based on the\nDenseNet structure of deep neural networks, and constructs a network model by\nintroducing attention mechanisms, and trains the enhanced dataset using\nmulti-level transfer learning. Experimental results demonstrate that the\nalgorithm achieves an efficiency of over 84.0\\% in the test set, with a\nsignificantly improved classification accuracy compared to previous models,\nmaking it applicable to medical breast cancer detection tasks.\n","authors":["Weimin Wang","Min Gao","Mingxuan Xiao","Xu Yan","Yufeng Li"],"pdf_url":"https://arxiv.org/pdf/2404.09226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09792v2","updated":"2024-04-14T11:57:55Z","published":"2024-03-14T18:24:55Z","title":"Images are Achilles' Heel of Alignment: Exploiting Visual\n  Vulnerabilities for Jailbreaking Multimodal Large Language Models","summary":"  In this paper, we study the harmlessness alignment problem of multimodal\nlarge language models (MLLMs). We conduct a systematic empirical analysis of\nthe harmlessness performance of representative MLLMs and reveal that the image\ninput poses the alignment vulnerability of MLLMs. Inspired by this, we propose\na novel jailbreak method named HADES, which hides and amplifies the harmfulness\nof the malicious intent within the text input, using meticulously crafted\nimages. Experimental results show that HADES can effectively jailbreak existing\nMLLMs, which achieves an average Attack Success Rate (ASR) of 90.26% for\nLLaVA-1.5 and 71.60% for Gemini Pro Vision. Our code and data will be publicly\nreleased.\n","authors":["Yifan Li","Hangyu Guo","Kun Zhou","Wayne Xin Zhao","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.09792v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2404.09216v1","updated":"2024-04-14T11:01:44Z","published":"2024-04-14T11:01:44Z","title":"DetCLIPv3: Towards Versatile Generative Open-vocabulary Object Detection","summary":"  Existing open-vocabulary object detectors typically require a predefined set\nof categories from users, significantly confining their application scenarios.\nIn this paper, we introduce DetCLIPv3, a high-performing detector that excels\nnot only at both open-vocabulary object detection, but also generating\nhierarchical labels for detected objects. DetCLIPv3 is characterized by three\ncore designs: 1. Versatile model architecture: we derive a robust open-set\ndetection framework which is further empowered with generation ability via the\nintegration of a caption head. 2. High information density data: we develop an\nauto-annotation pipeline leveraging visual large language model to refine\ncaptions for large-scale image-text pairs, providing rich, multi-granular\nobject labels to enhance the training. 3. Efficient training strategy: we\nemploy a pre-training stage with low-resolution inputs that enables the object\ncaptioner to efficiently learn a broad spectrum of visual concepts from\nextensive image-text paired data. This is followed by a fine-tuning stage that\nleverages a small number of high-resolution samples to further enhance\ndetection performance. With these effective designs, DetCLIPv3 demonstrates\nsuperior open-vocabulary detection performance, \\eg, our Swin-T backbone model\nachieves a notable 47.0 zero-shot fixed AP on the LVIS minival benchmark,\noutperforming GLIPv2, GroundingDINO, and DetCLIPv2 by 18.0/19.6/6.6 AP,\nrespectively. DetCLIPv3 also achieves a state-of-the-art 19.7 AP in dense\ncaptioning task on VG dataset, showcasing its strong generative capability.\n","authors":["Lewei Yao","Renjie Pi","Jianhua Han","Xiaodan Liang","Hang Xu","Wei Zhang","Zhenguo Li","Dan Xu"],"pdf_url":"https://arxiv.org/pdf/2404.09216v1.pdf","comment":"Accepted to CVPR2024"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2308.11767v4","updated":"2024-04-14T21:20:58Z","published":"2023-08-15T23:22:37Z","title":"Detection of ChatGPT Fake Science with the xFakeSci Learning Algorithm","summary":"  Generative AI tools exemplified by ChatGPT are becoming a new reality. This\nstudy is motivated by the premise that ``AI generated content may exhibit a\ndistinctive behavior that can be separated from scientific articles''. In this\nstudy, we show how articles can be generated using means of prompt engineering\nfor various diseases and conditions. We then show how we tested this premise in\ntwo phases and prove its validity. Subsequently, we introduce xFakeSci, a novel\nlearning algorithm, that is capable of distinguishing ChatGPT-generated\narticles from publications produced by scientists. The algorithm is trained\nusing network models driven from both sources. As for the classification step,\nit was performed using 300 articles per condition. The actual label steps took\nplace against an equal mix of 50 generated articles and 50 authentic PubMed\nabstracts. The testing also spanned publication periods from 2010 to 2024 and\nencompassed research on three distinct diseases: cancer, depression, and\nAlzheimer's. Further, we evaluated the accuracy of the xFakeSci algorithm\nagainst some of the classical data mining algorithms (e.g., Support Vector\nMachines, Regression, and Naive Bayes). The xFakeSci algorithm achieved F1\nscores ranging from 80% to 94%, outperforming common data mining algorithms,\nwhich scored F1 values between 38% and 52%. We attribute the noticeable\ndifference to the introduction of calibration and a proximity distance\nheuristic, which underscores this promising performance. Indeed, the prediction\nof fake science generated by ChatGPT presents a considerable challenge.\nNonetheless, the introduction of the xFakeSci algorithm is a significant step\non the way to combating fake science.\n","authors":["Ahmed Abdeen Hamed","Xindong Wu"],"pdf_url":"https://arxiv.org/pdf/2308.11767v4.pdf","comment":"18 pages, 8 figures, 8 tables, 5 algorithms"},{"id":"http://arxiv.org/abs/2404.09286v1","updated":"2024-04-14T15:38:34Z","published":"2024-04-14T15:38:34Z","title":"Artificial Intelligence enhanced Security Problems in Real-Time Scenario\n  using Blowfish Algorithm","summary":"  In a nutshell, \"the cloud\" refers to a collection of interconnected computing\nresources made possible by an extensive, real-time communication network like\nthe internet. Because of its potential to reduce processing costs, the emerging\nparadigm of cloud computing has recently attracted a large number of academics.\nThe exponential expansion of cloud computing has made the rapid expansion of\ncloud services very remarkable. Ensuring the security of personal information\nin today's interconnected world is no easy task. These days, security is really\ncrucial. Models of security that are relevant to cloud computing include\nconfidentiality, authenticity, accessibility, data integrity, and recovery.\nUsing the Hybrid Encryption this study, we cover all the security issues and\nleaks in cloud infrastructure.\n","authors":["Yuvaraju Chinnam","Bosubabu Sambana"],"pdf_url":"https://arxiv.org/pdf/2404.09286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09253v1","updated":"2024-04-14T13:33:25Z","published":"2024-04-14T13:33:25Z","title":"Competitive Retrieval: Going Beyond the Single Query","summary":"  Previous work on the competitive retrieval setting focused on a single-query\nsetting: document authors manipulate their documents so as to improve their\nfuture ranking for a given query. We study a competitive setting where authors\nopt to improve their document's ranking for multiple queries. We use game\ntheoretic analysis to prove that equilibrium does not necessarily exist. We\nthen empirically show that it is more difficult for authors to improve their\ndocuments' rankings for multiple queries with a neural ranker than with a\nstate-of-the-art feature-based ranker. We also present an effective approach\nfor predicting the document most highly ranked in the next induced ranking.\n","authors":["Haya Nachimovsky","Moshe Tennenholtz","Fiana Raiber","Oren Kurland"],"pdf_url":"https://arxiv.org/pdf/2404.09253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10795v1","updated":"2024-04-14T18:09:08Z","published":"2024-04-14T18:09:08Z","title":"Intelligent Message Behavioral Identification System","summary":"  On social media platforms, the act of predicting reposting is seen as a\nchallenging issue related to Short Message Services (SMS). This study examines\nthe issue of predicting picture reposting in SMS and forecasts users' behavior\nin sharing photographs on Twitter. Several research vary. The paper introduces\na network called Image Retweet Modeling (IRM) that models heterogeneous image\nretransmission. It considers the user's previous reposting of the image tweet,\nthe next contact in the SMS, and the preferences of the reposted person. Three\naspects connected to content. A text-guided multimodal neural network is\ndeveloped to create a novel multi-faceted attention ranking network\nmethodology. This allows for learning the joint image Twitter representation\nand user preference representation in the prediction job. Multiple experiments\nconducted on extensive data sets demonstrate that our approach outperforms\ncurrent methods on Social Network platforms.\n","authors":["Yuvaraju Chinnam","Bosubabu Sambana"],"pdf_url":"https://arxiv.org/pdf/2404.10795v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2404.09384v1","updated":"2024-04-14T23:45:23Z","published":"2024-04-14T23:45:23Z","title":"Tasks People Prompt: A Taxonomy of LLM Downstream Tasks in Software\n  Verification and Falsification Approaches","summary":"  Prompting has become one of the main approaches to leverage emergent\ncapabilities of Large Language Models [Brown et al. NeurIPS 2020, Wei et al.\nTMLR 2022, Wei et al. NeurIPS 2022]. During the last year, researchers and\npractitioners have been playing with prompts to see how to make the most of\nLLMs. By homogeneously dissecting 80 papers, we investigate in deep how\nsoftware testing and verification research communities have been abstractly\narchitecting their LLM-enabled solutions. More precisely, first, we want to\nvalidate whether downstream tasks are an adequate concept to convey the\nblueprint of prompt-based solutions. We also aim at identifying number and\nnature of such tasks in solutions. For such goal, we develop a novel downstream\ntask taxonomy that enables pinpointing some engineering patterns in a rather\nvaried spectrum of Software Engineering problems that encompasses testing,\nfuzzing, debugging, vulnerability detection, static analysis and program\nverification approaches.\n","authors":["V√≠ctor A. Braberman","Flavia Bonomo-Braberman","Yiannis Charalambous","Juan G. Colonna","Lucas C. Cordeiro","Rosiane de Freitas"],"pdf_url":"https://arxiv.org/pdf/2404.09384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.04067v2","updated":"2024-04-14T23:26:06Z","published":"2024-01-08T18:10:25Z","title":"Convex SGD: Generalization Without Early Stopping","summary":"  We consider the generalization error associated with stochastic gradient\ndescent on a smooth convex function over a compact set. We show the first bound\non the generalization error that vanishes when the number of iterations $T$ and\nthe dataset size $n$ go to zero at arbitrary rates; our bound scales as\n$\\tilde{O}(1/\\sqrt{T} + 1/\\sqrt{n})$ with step-size $\\alpha_t = 1/\\sqrt{t}$. In\nparticular, strong convexity is not needed for stochastic gradient descent to\ngeneralize well.\n","authors":["Julien Hendrickx","Alex Olshevsky"],"pdf_url":"https://arxiv.org/pdf/2401.04067v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05576v2","updated":"2024-04-14T23:17:15Z","published":"2024-02-08T11:23:11Z","title":"Tighter Generalization Bounds on Digital Computers via Discrete Optimal\n  Transport","summary":"  Machine learning models with inputs in a Euclidean space $\\mathbb{R}^d$, when\nimplemented on digital computers, generalize, and their {\\it generalization\ngap} converges to $0$ at a rate of $c/N^{1/2}$ concerning the sample size $N$.\nHowever, the constant $c>0$ obtained through classical methods can be large in\nterms of the ambient dimension $d$ and the machine precision, posing a\nchallenge when $N$ is small to realistically large. In this paper, we derive a\nfamily of generalization bounds $\\{c_m/N^{1/(2\\vee m)}\\}_{m=1}^{\\infty}$\ntailored for learning models on digital computers, which adapt to both the\nsample size $N$ and the so-called geometric {\\it representation dimension} $m$\nof the discrete learning problem. Adjusting the parameter $m$ according to $N$\nresults in significantly tighter generalization bounds for practical sample\nsizes $N$, while setting $m$ small maintains the optimal dimension-free\nworst-case rate of $\\mathcal{O}(1/N^{1/2})$. Notably, $c_{m}\\in\n\\mathcal{O}(\\sqrt{m})$ for learning models on discretized Euclidean domains.\n  Furthermore, our adaptive generalization bounds are formulated based on our\nnew non-asymptotic result for concentration of measure in discrete optimal\ntransport, established via leveraging metric embedding arguments.\n","authors":["Anastasis Kratsios","A. Martina Neuman","Gudmund Pammer"],"pdf_url":"https://arxiv.org/pdf/2402.05576v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04535v2","updated":"2024-04-14T22:51:18Z","published":"2023-12-07T18:53:27Z","title":"Trajeglish: Traffic Modeling as Next-Token Prediction","summary":"  A longstanding challenge for self-driving development is simulating dynamic\ndriving scenarios seeded from recorded driving logs. In pursuit of this\nfunctionality, we apply tools from discrete sequence modeling to model how\nvehicles, pedestrians and cyclists interact in driving scenarios. Using a\nsimple data-driven tokenization scheme, we discretize trajectories to\ncentimeter-level resolution using a small vocabulary. We then model the\nmulti-agent sequence of discrete motion tokens with a GPT-like encoder-decoder\nthat is autoregressive in time and takes into account intra-timestep\ninteraction between agents. Scenarios sampled from our model exhibit\nstate-of-the-art realism; our model tops the Waymo Sim Agents Benchmark,\nsurpassing prior work along the realism meta metric by 3.3% and along the\ninteraction metric by 9.9%. We ablate our modeling choices in full autonomy and\npartial autonomy settings, and show that the representations learned by our\nmodel can quickly be adapted to improve performance on nuScenes. We\nadditionally evaluate the scalability of our model with respect to parameter\ncount and dataset size, and use density estimates from our model to quantify\nthe saliency of context length and intra-timestep interaction for the traffic\nmodeling task.\n","authors":["Jonah Philion","Xue Bin Peng","Sanja Fidler"],"pdf_url":"https://arxiv.org/pdf/2312.04535v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2404.06969v2","updated":"2024-04-14T22:44:11Z","published":"2024-04-10T12:29:05Z","title":"FiP: a Fixed-Point Approach for Causal Generative Modeling","summary":"  Modeling true world data-generating processes lies at the heart of empirical\nscience. Structural Causal Models (SCMs) and their associated Directed Acyclic\nGraphs (DAGs) provide an increasingly popular answer to such problems by\ndefining the causal generative process that transforms random noise into\nobservations. However, learning them from observational data poses an ill-posed\nand NP-hard inverse problem in general. In this work, we propose a new and\nequivalent formalism that does not require DAGs to describe them, viewed as\nfixed-point problems on the causally ordered variables, and we show three\nimportant cases where they can be uniquely recovered given the topological\nordering (TO). To the best of our knowledge, we obtain the weakest conditions\nfor their recovery when TO is known. Based on this, we design a two-stage\ncausal generative model that first infers the causal order from observations in\na zero-shot manner, thus by-passing the search, and then learns the generative\nfixed-point SCM on the ordered variables. To infer TOs from observations, we\npropose to amortize the learning of TOs on generated datasets by sequentially\npredicting the leaves of graphs seen during training. To learn fixed-point\nSCMs, we design a transformer-based architecture that exploits a new attention\nmechanism enabling the modeling of causal structures, and show that this\nparameterization is consistent with our formalism. Finally, we conduct an\nextensive evaluation of each method individually, and show that when combined,\nour model outperforms various baselines on generated out-of-distribution\nproblems.\n","authors":["Meyer Scetbon","Joel Jennings","Agrin Hilmkil","Cheng Zhang","Chao Ma"],"pdf_url":"https://arxiv.org/pdf/2404.06969v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09365v1","updated":"2024-04-14T21:37:39Z","published":"2024-04-14T21:37:39Z","title":"Hierarchical Attention Models for Multi-Relational Graphs","summary":"  We present Bi-Level Attention-Based Relational Graph Convolutional Networks\n(BR-GCN), unique neural network architectures that utilize masked\nself-attentional layers with relational graph convolutions, to effectively\noperate on highly multi-relational data. BR-GCN models use bi-level attention\nto learn node embeddings through (1) node-level attention, and (2)\nrelation-level attention. The node-level self-attentional layers use\nintra-relational graph interactions to learn relation-specific node embeddings\nusing a weighted aggregation of neighborhood features in a sparse subgraph\nregion. The relation-level self-attentional layers use inter-relational graph\ninteractions to learn the final node embeddings using a weighted aggregation of\nrelation-specific node embeddings. The BR-GCN bi-level attention mechanism\nextends Transformer-based multiplicative attention from the natural language\nprocessing (NLP) domain, and Graph Attention Networks (GAT)-based attention, to\nlarge-scale heterogeneous graphs (HGs). On node classification, BR-GCN\noutperforms baselines from 0.29% to 14.95% as a stand-alone model, and on link\nprediction, BR-GCN outperforms baselines from 0.02% to 7.40% as an auto-encoder\nmodel. We also conduct ablation studies to evaluate the quality of BR-GCN's\nrelation-level attention and discuss how its learning of graph structure may be\ntransferred to enrich other graph neural networks (GNNs). Through various\nexperiments, we show that BR-GCN's attention mechanism is both scalable and\nmore effective in learning compared to state-of-the-art GNNs.\n","authors":["Roshni G. Iyer","Wei Wang","Yizhou Sun"],"pdf_url":"https://arxiv.org/pdf/2404.09365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09363v1","updated":"2024-04-14T21:30:00Z","published":"2024-04-14T21:30:00Z","title":"Momentum-based gradient descent methods for Lie groups","summary":"  Polyak's Heavy Ball (PHB; Polyak, 1964), a.k.a. Classical Momentum, and\nNesterov's Accelerated Gradient (NAG; Nesterov, 1983) are well know examples of\nmomentum-descent methods for optimization. While the latter outperforms the\nformer, solely generalizations of PHB-like methods to nonlinear spaces have\nbeen described in the literature. We propose here a generalization of NAG-like\nmethods for Lie group optimization based on the variational one-to-one\ncorrespondence between classical and accelerated momentum methods (Campos et\nal., 2023). Numerical experiments are shown.\n","authors":["C√©dric M. Campos","David Mart√≠n de Diego","Jos√© Torrente"],"pdf_url":"https://arxiv.org/pdf/2404.09363v1.pdf","comment":"24 pages, 2 algorithms, 5 figures"},{"id":"http://arxiv.org/abs/2404.09359v1","updated":"2024-04-14T21:14:47Z","published":"2024-04-14T21:14:47Z","title":"Exploring Feedback Generation in Automated Skeletal Movement Assessment:\n  A Comprehensive Overview","summary":"  The application of machine-learning solutions to movement assessment from\nskeleton videos has attracted significant research attention in recent years.\nThis advancement has made rehabilitation at home more accessible, utilizing\nmovement assessment algorithms that can operate on affordable equipment for\nhuman pose detection from 2D or 3D videos. While the primary objective of\nautomatic assessment tasks is to score movements, the automatic generation of\nfeedback highlighting key movement issues has the potential to significantly\nenhance and accelerate the rehabilitation process. In this study, we explain\nthe types of feedback that can be generated, review existing solutions for\nautomatic feedback generation, and discuss future research directions. To our\nknowledge, this is the first comprehensive review of feedback generation in\nskeletal movement assessment.\n","authors":["Tal Hakim"],"pdf_url":"https://arxiv.org/pdf/2404.09359v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03302v2","updated":"2024-04-14T21:02:16Z","published":"2023-10-05T04:06:12Z","title":"MLAgentBench: Evaluating Language Agents on Machine Learning\n  Experimentation","summary":"  A central aspect of machine learning research is experimentation, the process\nof designing and running experiments, analyzing the results, and iterating\ntowards some positive outcome (e.g., improving accuracy). Could agents driven\nby powerful language models perform machine learning experimentation\neffectively? To answer this question, we introduce MLAgentBench, a suite of 13\ntasks ranging from improving model performance on CIFAR-10 to recent research\nproblems like BabyLM. For each task, an agent can perform actions like\nreading/writing files, executing code, and inspecting outputs. We then\nconstruct an agent that can perform ML experimentation based on ReAct\nframework. We benchmark agents based on Claude v1.0, Claude v2.1, Claude v3\nOpus, GPT-4, GPT-4-turbo, Gemini-Pro, and Mixtral and find that a Claude v3\nOpus agent is the best in terms of success rate. It can build compelling ML\nmodels over many tasks in MLAgentBench with 37.5% average success rate. Our\nagents also display highly interpretable plans and actions. However, the\nsuccess rates vary considerably; they span from 100% on well-established older\ndatasets to as low as 0% on recent Kaggle challenges created potentially after\nthe underlying LM was trained. Finally, we identify several key challenges for\nLM-based agents such as long-term planning and reducing hallucination. Our code\nis released at https://github.com/snap-stanford/MLAgentBench.\n","authors":["Qian Huang","Jian Vora","Percy Liang","Jure Leskovec"],"pdf_url":"https://arxiv.org/pdf/2310.03302v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09350v1","updated":"2024-04-14T20:17:14Z","published":"2024-04-14T20:17:14Z","title":"Machine learning-based identification of Gaia astrometric exoplanet\n  orbits","summary":"  The third Gaia data release (DR3) contains $\\sim$170 000 astrometric orbit\nsolutions of two-body systems located within $\\sim$500 pc of the Sun.\nDetermining component masses in these systems, in particular of stars hosting\nexoplanets, usually hinges on incorporating complementary observations in\naddition to the astrometry, e.g. spectroscopy and radial velocities. Several\nDR3 two-body systems with exoplanet, brown-dwarf, stellar, and black-hole\ncomponents have been confirmed in this way. We developed an alternative machine\nlearning approach that uses only the DR3 orbital solutions with the aim of\nidentifying the best candidates for exoplanets and brown-dwarf companions.\nBased on confirmed substellar companions in the literature, we use\nsemi-supervised anomaly detection methods in combination with extreme gradient\nboosting and random forest classifiers to determine likely low-mass outliers in\nthe population of non-single sources. We employ and study feature importance to\ninvestigate the method's plausibility and produced a list of 22 best candidates\nof which four are exoplanet candidates and another five are either very-massive\nbrown dwarfs or very-low mass stars. Three candidates, including one initial\nexoplanet candidate, correspond to false-positive solutions where longer-period\nbinary star motion was fitted with a biased shorter-period orbit. We highlight\nnine candidates with brown-dwarf companions for preferential follow-up. One\ncandidate companion around the Sun-like star G 15-6 could be confirmed as a\ngenuine brown dwarf using external radial-velocity data. This new approach is a\npowerful complement to the traditional identification methods for substellar\ncompanions among Gaia astrometric orbits. It is particularly relevant in the\ncontext of Gaia DR4 and its expected exoplanet discovery yield.\n","authors":["Johannes Sahlmann","Pablo G√≥mez"],"pdf_url":"https://arxiv.org/pdf/2404.09350v1.pdf","comment":"14 pages, 15 figures. Submitted to MNRAS. Comments are welcome"},{"id":"http://arxiv.org/abs/2404.09349v1","updated":"2024-04-14T20:14:38Z","published":"2024-04-14T20:14:38Z","title":"Adversarial Robustness Limits via Scaling-Law and Human-Alignment\n  Studies","summary":"  This paper revisits the simple, long-studied, yet still unsolved problem of\nmaking image classifiers robust to imperceptible perturbations. Taking CIFAR10\nas an example, SOTA clean accuracy is about $100$%, but SOTA robustness to\n$\\ell_{\\infty}$-norm bounded perturbations barely exceeds $70$%. To understand\nthis gap, we analyze how model size, dataset size, and synthetic data quality\naffect robustness by developing the first scaling laws for adversarial\ntraining. Our scaling laws reveal inefficiencies in prior art and provide\nactionable feedback to advance the field. For instance, we discovered that SOTA\nmethods diverge notably from compute-optimal setups, using excess compute for\ntheir level of robustness. Leveraging a compute-efficient setup, we surpass the\nprior SOTA with $20$% ($70$%) fewer training (inference) FLOPs. We trained\nvarious compute-efficient models, with our best achieving $74$% AutoAttack\naccuracy ($+3$% gain). However, our scaling laws also predict robustness slowly\ngrows then plateaus at $90$%: dwarfing our new SOTA by scaling is impractical,\nand perfect robustness is impossible. To better understand this predicted\nlimit, we carry out a small-scale human evaluation on the AutoAttack data that\nfools our top-performing model. Concerningly, we estimate that human\nperformance also plateaus near $90$%, which we show to be attributable to\n$\\ell_{\\infty}$-constrained attacks' generation of invalid images not\nconsistent with their original labels. Having characterized limiting\nroadblocks, we outline promising paths for future research.\n","authors":["Brian R. Bartoldson","James Diffenderfer","Konstantinos Parasyris","Bhavya Kailkhura"],"pdf_url":"https://arxiv.org/pdf/2404.09349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14436v2","updated":"2024-04-14T20:07:19Z","published":"2023-12-22T04:56:37Z","title":"REBEL: A Regularization-Based Solution for Reward Overoptimization in\n  Robotic Reinforcement Learning from Human Feedback","summary":"  The effectiveness of reinforcement learning (RL) agents in continuous control\nrobotics tasks is heavily dependent on the design of the underlying reward\nfunction. However, a misalignment between the reward function and user\nintentions, values, or social norms can be catastrophic in the real world.\nCurrent methods to mitigate this misalignment work by learning reward functions\nfrom human preferences; however, they inadvertently introduce a risk of reward\noveroptimization. In this work, we address this challenge by advocating for the\nadoption of regularized reward functions that more accurately mirror the\nintended behaviors. We propose a novel concept of reward regularization within\nthe robotic RLHF (RL from Human Feedback) framework, which we refer to as\n\\emph{agent preferences}. Our approach uniquely incorporates not just human\nfeedback in the form of preferences but also considers the preferences of the\nRL agent itself during the reward function learning process. This dual\nconsideration significantly mitigates the issue of reward function\noveroptimization in RL. We provide a theoretical justification for the proposed\napproach by formulating the robotic RLHF problem as a bilevel optimization\nproblem. We demonstrate the efficiency of our algorithm {\\ours} in several\ncontinuous control benchmarks including DeepMind Control Suite\n\\cite{tassa2018deepmind} and MetaWorld \\cite{yu2021metaworld} and high\ndimensional visual environments, with an improvement of more than 70\\% in\nsample efficiency in comparison to current SOTA baselines. This showcases our\napproach's effectiveness in aligning reward functions with true behavioral\nintentions, setting a new benchmark in the field.\n","authors":["Souradip Chakraborty","Anukriti Singh","Amisha Bhaskar","Pratap Tokekar","Dinesh Manocha","Amrit Singh Bedi"],"pdf_url":"https://arxiv.org/pdf/2312.14436v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.00570v2","updated":"2024-04-14T19:57:21Z","published":"2022-11-29T18:43:22Z","title":"Penalized Overdamped and Underdamped Langevin Monte Carlo Algorithms for\n  Constrained Sampling","summary":"  We consider the constrained sampling problem where the goal is to sample from\na target distribution $\\pi(x)\\propto e^{-f(x)}$ when $x$ is constrained to lie\non a convex body $\\mathcal{C}$. Motivated by penalty methods from continuous\noptimization, we propose penalized Langevin Dynamics (PLD) and penalized\nunderdamped Langevin Monte Carlo (PULMC) methods that convert the constrained\nsampling problem into an unconstrained sampling problem by introducing a\npenalty function for constraint violations. When $f$ is smooth and gradients\nare available, we get $\\tilde{\\mathcal{O}}(d/\\varepsilon^{10})$ iteration\ncomplexity for PLD to sample the target up to an $\\varepsilon$-error where the\nerror is measured in the TV distance and $\\tilde{\\mathcal{O}}(\\cdot)$ hides\nlogarithmic factors. For PULMC, we improve the result to\n$\\tilde{\\mathcal{O}}(\\sqrt{d}/\\varepsilon^{7})$ when the Hessian of $f$ is\nLipschitz and the boundary of $\\mathcal{C}$ is sufficiently smooth. To our\nknowledge, these are the first convergence results for underdamped Langevin\nMonte Carlo methods in the constrained sampling that handle non-convex $f$ and\nprovide guarantees with the best dimension dependency among existing methods\nwith deterministic gradient. If unbiased stochastic estimates of the gradient\nof $f$ are available, we propose PSGLD and PSGULMC methods that can handle\nstochastic gradients and are scaleable to large datasets without requiring\nMetropolis-Hasting correction steps. For PSGLD and PSGULMC, when $f$ is\nstrongly convex and smooth, we obtain $\\tilde{\\mathcal{O}}(d/\\varepsilon^{18})$\nand $\\tilde{\\mathcal{O}}(d\\sqrt{d}/\\varepsilon^{39})$ iteration complexity in\nW2 distance. When $f$ is smooth and can be non-convex, we provide finite-time\nperformance bounds and iteration complexity results. Finally, we illustrate the\nperformance on Bayesian LASSO regression and Bayesian constrained deep learning\nproblems.\n","authors":["Mert G√ºrb√ºzbalaban","Yuanhan Hu","Lingjiong Zhu"],"pdf_url":"https://arxiv.org/pdf/2212.00570v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09339v1","updated":"2024-04-14T19:45:47Z","published":"2024-04-14T19:45:47Z","title":"Towards Practical Tool Usage for Continually Learning LLMs","summary":"  Large language models (LLMs) show an innate skill for solving language based\ntasks. But insights have suggested an inability to adjust for information or\ntask-solving skills becoming outdated, as their knowledge, stored directly\nwithin their parameters, remains static in time. Tool use helps by offloading\nwork to systems that the LLM can access through an interface, but LLMs that use\nthem still must adapt to nonstationary environments for prolonged use, as new\ntools can emerge and existing tools can change. Nevertheless, tools require\nless specialized knowledge, therefore we hypothesize they are better suited for\ncontinual learning (CL) as they rely less on parametric memory for solving\ntasks and instead focus on learning when to apply pre-defined tools. To verify\nthis, we develop a synthetic benchmark and follow this by aggregating existing\nNLP tasks to form a more realistic testing scenario. While we demonstrate\nscaling model size is not a solution, regardless of tool usage, continual\nlearning techniques can enable tool LLMs to both adapt faster while forgetting\nless, highlighting their potential as continual learners.\n","authors":["Jerry Huang","Prasanna Parthasarathi","Mehdi Rezagholizadeh","Sarath Chandar"],"pdf_url":"https://arxiv.org/pdf/2404.09339v1.pdf","comment":"20 pages, 11 tables, 7 figures"},{"id":"http://arxiv.org/abs/2404.09331v1","updated":"2024-04-14T19:06:00Z","published":"2024-04-14T19:06:00Z","title":"SNN4Agents: A Framework for Developing Energy-Efficient Embodied Spiking\n  Neural Networks for Autonomous Agents","summary":"  Recent trends have shown that autonomous agents, such as Autonomous Ground\nVehicles (AGVs), Unmanned Aerial Vehicles (UAVs), and mobile robots,\neffectively improve human productivity in solving diverse tasks. However, since\nthese agents are typically powered by portable batteries, they require\nextremely low power/energy consumption to operate in a long lifespan. To solve\nthis challenge, neuromorphic computing has emerged as a promising solution,\nwhere bio-inspired Spiking Neural Networks (SNNs) use spikes from event-based\ncameras or data conversion pre-processing to perform sparse computations\nefficiently. However, the studies of SNN deployments for autonomous agents are\nstill at an early stage. Hence, the optimization stages for enabling efficient\nembodied SNN deployments for autonomous agents have not been defined\nsystematically. Toward this, we propose a novel framework called SNN4Agents\nthat consists of a set of optimization techniques for designing\nenergy-efficient embodied SNNs targeting autonomous agent applications. Our\nSNN4Agents employs weight quantization, timestep reduction, and attention\nwindow reduction to jointly improve the energy efficiency, reduce the memory\nfootprint, optimize the processing latency, while maintaining high accuracy. In\nthe evaluation, we investigate use cases of event-based car recognition, and\nexplore the trade-offs among accuracy, latency, memory, and energy consumption.\nThe experimental results show that our proposed framework can maintain high\naccuracy (i.e., 84.12% accuracy) with 68.75% memory saving, 3.58x speed-up, and\n4.03x energy efficiency improvement as compared to the state-of-the-art work\nfor NCARS dataset, thereby enabling energy-efficient embodied SNN deployments\nfor autonomous agents.\n","authors":["Rachmad Vidya Wicaksana Putra","Alberto Marchisio","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2404.09331v1.pdf","comment":"18 pages, 15 figures"},{"id":"http://arxiv.org/abs/2404.09326v1","updated":"2024-04-14T18:57:38Z","published":"2024-04-14T18:57:38Z","title":"Weight Copy and Low-Rank Adaptation for Few-Shot Distillation of Vision\n  Transformers","summary":"  Few-shot knowledge distillation recently emerged as a viable approach to\nharness the knowledge of large-scale pre-trained models, using limited data and\ncomputational resources. In this paper, we propose a novel few-shot feature\ndistillation approach for vision transformers. Our approach is based on two key\nsteps. Leveraging the fact that vision transformers have a consistent\ndepth-wise structure, we first copy the weights from intermittent layers of\nexisting pre-trained vision transformers (teachers) into shallower\narchitectures (students), where the intermittence factor controls the\ncomplexity of the student transformer with respect to its teacher. Next, we\nemploy an enhanced version of Low-Rank Adaptation (LoRA) to distill knowledge\ninto the student in a few-shot scenario, aiming to recover the information\nprocessing carried out by the skipped teacher layers. We present comprehensive\nexperiments with supervised and self-supervised transformers as teachers, on\nfive data sets from various domains, including natural, medical and satellite\nimages. The empirical results confirm the superiority of our approach over\ncompetitive baselines. Moreover, the ablation results demonstrate the\nusefulness of each component of the proposed pipeline.\n","authors":["Diana-Nicoleta Grigore","Mariana-Iuliana Georgescu","Jon Alvarez Justo","Tor Johansen","Andreea Iuliana Ionescu","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2404.09326v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.11060v2","updated":"2024-04-14T18:47:09Z","published":"2022-08-23T16:06:10Z","title":"Exponential concentration in quantum kernel methods","summary":"  Kernel methods in Quantum Machine Learning (QML) have recently gained\nsignificant attention as a potential candidate for achieving a quantum\nadvantage in data analysis. Among other attractive properties, when training a\nkernel-based model one is guaranteed to find the optimal model's parameters due\nto the convexity of the training landscape. However, this is based on the\nassumption that the quantum kernel can be efficiently obtained from quantum\nhardware. In this work we study the performance of quantum kernel models from\nthe perspective of the resources needed to accurately estimate kernel values.\nWe show that, under certain conditions, values of quantum kernels over\ndifferent input data can be exponentially concentrated (in the number of\nqubits) towards some fixed value. Thus on training with a polynomial number of\nmeasurements, one ends up with a trivial model where the predictions on unseen\ninputs are independent of the input data. We identify four sources that can\nlead to concentration including: expressivity of data embedding, global\nmeasurements, entanglement and noise. For each source, an associated\nconcentration bound of quantum kernels is analytically derived. Lastly, we show\nthat when dealing with classical data, training a parametrized data embedding\nwith a kernel alignment method is also susceptible to exponential\nconcentration. Our results are verified through numerical simulations for\nseveral QML tasks. Altogether, we provide guidelines indicating that certain\nfeatures should be avoided to ensure the efficient evaluation of quantum\nkernels and so the performance of quantum kernel methods.\n","authors":["Supanut Thanasilp","Samson Wang","M. Cerezo","Zo√´ Holmes"],"pdf_url":"https://arxiv.org/pdf/2208.11060v2.pdf","comment":"15+50 pages, 15 figures"},{"id":"http://arxiv.org/abs/2202.09497v8","updated":"2024-04-14T17:08:45Z","published":"2022-02-19T02:22:23Z","title":"Gradient Estimation with Discrete Stein Operators","summary":"  Gradient estimation -- approximating the gradient of an expectation with\nrespect to the parameters of a distribution -- is central to the solution of\nmany machine learning problems. However, when the distribution is discrete,\nmost common gradient estimators suffer from excessive variance. To improve the\nquality of gradient estimation, we introduce a variance reduction technique\nbased on Stein operators for discrete distributions. We then use this technique\nto build flexible control variates for the REINFORCE leave-one-out estimator.\nOur control variates can be adapted online to minimize variance and do not\nrequire extra evaluations of the target function. In benchmark generative\nmodeling tasks such as training binary variational autoencoders, our gradient\nestimator achieves substantially lower variance than state-of-the-art\nestimators with the same number of function evaluations.\n","authors":["Jiaxin Shi","Yuhao Zhou","Jessica Hwang","Michalis K. Titsias","Lester Mackey"],"pdf_url":"https://arxiv.org/pdf/2202.09497v8.pdf","comment":"NeurIPS 2022. Source code: https://github.com/thjashin/rodeo"},{"id":"http://arxiv.org/abs/2404.09302v1","updated":"2024-04-14T16:57:41Z","published":"2024-04-14T16:57:41Z","title":"High Significant Fault Detection in Azure Core Workload Insights","summary":"  Azure Core workload insights have time-series data with different metric\nunits. Faults or Anomalies are observed in these time-series data owing to\nfaults observed with respect to metric name, resources region, dimensions, and\nits dimension value associated with the data. For Azure Core, an important task\nis to highlight faults or anomalies to the user on a dashboard that they can\nperceive easily. The number of anomalies reported should be highly significant\nand in a limited number, e.g., 5-20 anomalies reported per hour. The reported\nanomalies will have significant user perception and high reconstruction error\nin any time-series forecasting model. Hence, our task is to automatically\nidentify 'high significant anomalies' and their associated information for user\nperception.\n","authors":["Pranay Lohia","Laurent Boue","Sharath Rangappa","Vijay Agneeswaran"],"pdf_url":"https://arxiv.org/pdf/2404.09302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02948v2","updated":"2024-04-14T15:24:10Z","published":"2024-04-03T15:06:43Z","title":"PiSSA: Principal Singular Values and Singular Vectors Adaptation of\n  Large Language Models","summary":"  As the parameters of LLMs expand, the computational cost of fine-tuning the\nentire model becomes prohibitive. To address this challenge, we introduce a\nPEFT method, Principal Singular values and Singular vectors Adaptation (PiSSA),\nwhich optimizes a significantly reduced parameter space while achieving or\nsurpassing the performance of full-parameter fine-tuning. PiSSA is inspired by\nIntrinsic SAID, which suggests that pre-trained, over-parametrized models\ninhabit a space of low intrinsic dimension. Consequently, PiSSA represents a\nmatrix W within the model by the product of two trainable matrices A and B,\nplus a residual matrix $W^{res}$ for error correction. SVD is employed to\nfactorize W, and the principal singular values and vectors of W are utilized to\ninitialize A and B. The residual singular values and vectors initialize the\nresidual matrix $W^{res}$, which keeps frozen during fine-tuning. Notably,\nPiSSA shares the same architecture with LoRA. However, LoRA approximates Delta\nW through the product of two matrices, A, initialized with Gaussian noise, and\nB, initialized with zeros, while PiSSA initializes A and B with principal\nsingular values and vectors of the original matrix W. PiSSA can better\napproximate the outcomes of full-parameter fine-tuning at the beginning by\nchanging the essential parts while freezing the \"noisy\" parts. In comparison,\nLoRA freezes the original matrix and updates the \"noise\". This distinction\nenables PiSSA to convergence much faster than LoRA and also achieve better\nperformance in the end. Due to the same architecture, PiSSA inherits many of\nLoRA's advantages, such as parameter efficiency and compatibility with\nquantization. Leveraging a fast SVD method, the initialization of PiSSA takes\nonly a few seconds, inducing negligible cost of switching LoRA to PiSSA.\n","authors":["Fanxu Meng","Zhaohui Wang","Muhan Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.02948v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01029v3","updated":"2024-04-14T14:53:32Z","published":"2023-04-03T14:28:29Z","title":"Domain Generalization for Crop Segmentation with Standardized Ensemble\n  Knowledge Distillation","summary":"  In recent years, precision agriculture has gradually oriented farming closer\nto automation processes to support all the activities related to field\nmanagement. Service robotics plays a predominant role in this evolution by\ndeploying autonomous agents that can navigate fields while performing tasks\nsuch as monitoring, spraying, and harvesting without human intervention. To\nexecute these precise actions, mobile robots need a real-time perception system\nthat understands their surroundings and identifies their targets in the wild.\nExisting methods, however, often fall short in generalizing to new crops and\nenvironmental conditions. This limit is critical for practical applications\nwhere labeled samples are rarely available. In this paper, we investigate the\nproblem of crop segmentation and propose a novel approach to enhance domain\ngeneralization using knowledge distillation. In the proposed framework, we\ntransfer knowledge from a standardized ensemble of models individually trained\non source domains to a student model that can adapt to unseen realistic\nscenarios. To support the proposed method, we present a synthetic multi-domain\ndataset for crop segmentation containing plants of variegate species and\ncovering different terrain styles, weather conditions, and light scenarios for\nmore than 70,000 samples. We demonstrate significant improvements in\nperformance over state-of-the-art methods and superior sim-to-real\ngeneralization. Our approach provides a promising solution for domain\ngeneralization in crop segmentation and has the potential to enhance a wide\nvariety of agriculture applications.\n","authors":["Simone Angarano","Mauro Martini","Alessandro Navone","Marcello Chiaberge"],"pdf_url":"https://arxiv.org/pdf/2304.01029v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09275v1","updated":"2024-04-14T14:51:44Z","published":"2024-04-14T14:51:44Z","title":"TrafficVLM: A Controllable Visual Language Model for Traffic Video\n  Captioning","summary":"  Traffic video description and analysis have received much attention recently\ndue to the growing demand for efficient and reliable urban surveillance\nsystems. Most existing methods only focus on locating traffic event segments,\nwhich severely lack descriptive details related to the behaviour and context of\nall the subjects of interest in the events. In this paper, we present\nTrafficVLM, a novel multi-modal dense video captioning model for vehicle ego\ncamera view. TrafficVLM models traffic video events at different levels of\nanalysis, both spatially and temporally, and generates long fine-grained\ndescriptions for the vehicle and pedestrian at different phases of the event.\nWe also propose a conditional component for TrafficVLM to control the\ngeneration outputs and a multi-task fine-tuning paradigm to enhance\nTrafficVLM's learning capability. Experiments show that TrafficVLM performs\nwell on both vehicle and overhead camera views. Our solution achieved\noutstanding results in Track 2 of the AI City Challenge 2024, ranking us third\nin the challenge standings. Our code is publicly available at\nhttps://github.com/quangminhdinh/TrafficVLM.\n","authors":["Quang Minh Dinh","Minh Khoi Ho","Anh Quan Dang","Hung Phong Tran"],"pdf_url":"https://arxiv.org/pdf/2404.09275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2011.14033v7","updated":"2024-04-14T14:47:24Z","published":"2020-11-28T00:20:36Z","title":"A Tractable Online Learning Algorithm for the Multinomial Logit\n  Contextual Bandit","summary":"  In this paper, we consider the contextual variant of the MNL-Bandit problem.\nMore specifically, we consider a dynamic set optimization problem, where a\ndecision-maker offers a subset (assortment) of products to a consumer and\nobserves the response in every round. Consumers purchase products to maximize\ntheir utility. We assume that a set of attributes describe the products, and\nthe mean utility of a product is linear in the values of these attributes. We\nmodel consumer choice behavior using the widely used Multinomial Logit (MNL)\nmodel and consider the decision maker problem of dynamically learning the model\nparameters while optimizing cumulative revenue over the selling horizon $T$.\nThough this problem has attracted considerable attention in recent times, many\nexisting methods often involve solving an intractable non-convex optimization\nproblem. Their theoretical performance guarantees depend on a problem-dependent\nparameter which could be prohibitively large. In particular, existing\nalgorithms for this problem have regret bounded by $O(\\sqrt{\\kappa d T})$,\nwhere $\\kappa$ is a problem-dependent constant that can have an exponential\ndependency on the number of attributes. In this paper, we propose an optimistic\nalgorithm and show that the regret is bounded by $O(\\sqrt{dT} + \\kappa)$,\nsignificantly improving the performance over existing methods. Further, we\npropose a convex relaxation of the optimization step, which allows for\ntractable decision-making while retaining the favourable regret guarantee.\n","authors":["Priyank Agrawal","Theja Tulabandhula","Vashist Avadhanula"],"pdf_url":"https://arxiv.org/pdf/2011.14033v7.pdf","comment":"Bug fixed"},{"id":"http://arxiv.org/abs/1910.05983v2","updated":"2024-04-14T14:30:12Z","published":"2019-10-14T08:43:06Z","title":"On the Reduction of Variance and Overestimation of Deep Q-Learning","summary":"  The breakthrough of deep Q-Learning on different types of environments\nrevolutionized the algorithmic design of Reinforcement Learning to introduce\nmore stable and robust algorithms, to that end many extensions to deep\nQ-Learning algorithm have been proposed to reduce the variance of the target\nvalues and the overestimation phenomena. In this paper, we examine new\nmethodology to solve these issues, we propose using Dropout techniques on deep\nQ-Learning algorithm as a way to reduce variance and overestimation. We also\npresent experiments conducted on benchmark environments, demonstrating the\neffectiveness of our methodology in enhancing stability and reducing both\nvariance and overestimation in model performance.\n","authors":["Mohammed Sabry","Amr M. A. Khalifa"],"pdf_url":"https://arxiv.org/pdf/1910.05983v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09256v1","updated":"2024-04-14T13:48:24Z","published":"2024-04-14T13:48:24Z","title":"Foundational GPT Model for MEG","summary":"  Deep learning techniques can be used to first training unsupervised models on\nlarge amounts of unlabelled data, before fine-tuning the models on specific\ntasks. This approach has seen massive success for various kinds of data, e.g.\nimages, language, audio, and holds the promise of improving performance in\nvarious downstream tasks (e.g. encoding or decoding brain data). However, there\nhas been limited progress taking this approach for modelling brain signals,\nsuch as Magneto-/electroencephalography (M/EEG). Here we propose two classes of\ndeep learning foundational models that can be trained using forecasting of\nunlabelled MEG. First, we consider a modified Wavenet; and second, we consider\na modified Transformer-based (GPT2) model. The modified GPT2 includes a novel\napplication of tokenisation and embedding methods, allowing a model developed\ninitially for the discrete domain of language to be applied to continuous\nmultichannel time series data. We also extend the forecasting framework to\ninclude condition labels as inputs, enabling better modelling (encoding) of\ntask data. We compare the performance of these deep learning models with\nstandard linear autoregressive (AR) modelling on MEG data. This shows that\nGPT2-based models provide better modelling capabilities than Wavenet and linear\nAR models, by better reproducing the temporal, spatial and spectral\ncharacteristics of real data and evoked activity in task data. We show how the\nGPT2 model scales well to multiple subjects, while adapting its model to each\nsubject through subject embedding. Finally, we show how such a model can be\nuseful in downstream decoding tasks through data simulation. All code is\navailable on GitHub (https://github.com/ricsinaruto/MEG-transfer-decoding).\n","authors":["Richard Csaky","Mats W. J. van Es","Oiwi Parker Jones","Mark Woolrich"],"pdf_url":"https://arxiv.org/pdf/2404.09256v1.pdf","comment":"Code available on GitHub\n  (https://github.com/ricsinaruto/MEG-transfer-decoding). Part of PhD thesis\n  (https://ricsinaruto.github.io/docs/thesis_final_appendix.pdf)"},{"id":"http://arxiv.org/abs/2404.09249v1","updated":"2024-04-14T13:25:15Z","published":"2024-04-14T13:25:15Z","title":"Test Code Generation for Telecom Software Systems using Two-Stage\n  Generative Model","summary":"  In recent years, the evolution of Telecom towards achieving intelligent,\nautonomous, and open networks has led to an increasingly complex Telecom\nSoftware system, supporting various heterogeneous deployment scenarios, with\nmulti-standard and multi-vendor support. As a result, it becomes a challenge\nfor large-scale Telecom software companies to develop and test software for all\ndeployment scenarios. To address these challenges, we propose a framework for\nAutomated Test Generation for large-scale Telecom Software systems. We begin by\ngenerating Test Case Input data for test scenarios observed using a time-series\nGenerative model trained on historical Telecom Network data during field\ntrials. Additionally, the time-series Generative model helps in preserving the\nprivacy of Telecom data. The generated time-series software performance data\nare then utilized with test descriptions written in natural language to\ngenerate Test Script using the Generative Large Language Model. Our\ncomprehensive experiments on public datasets and Telecom datasets obtained from\noperational Telecom Networks demonstrate that the framework can effectively\ngenerate comprehensive test case data input and useful test code.\n","authors":["Mohamad Nabeel","Doumitrou Daniil Nimara","Tahar Zanouda"],"pdf_url":"https://arxiv.org/pdf/2404.09249v1.pdf","comment":"6 pages, 5 figures, Accepted at 1st Workshop on The Impact of Large\n  Language Models on 6G Networks - IEEE International Conference on\n  Communications (ICC) 2024"},{"id":"http://arxiv.org/abs/2404.09248v1","updated":"2024-04-14T13:19:40Z","published":"2024-04-14T13:19:40Z","title":"Knowledgeable Agents by Offline Reinforcement Learning from Large\n  Language Model Rollouts","summary":"  Reinforcement learning (RL) trains agents to accomplish complex tasks through\nenvironmental interaction data, but its capacity is also limited by the scope\nof the available data. To obtain a knowledgeable agent, a promising approach is\nto leverage the knowledge from large language models (LLMs). Despite previous\nstudies combining LLMs with RL, seamless integration of the two components\nremains challenging due to their semantic gap. This paper introduces a novel\nmethod, Knowledgeable Agents from Language Model Rollouts (KALM), which\nextracts knowledge from LLMs in the form of imaginary rollouts that can be\neasily learned by the agent through offline reinforcement learning methods. The\nprimary challenge of KALM lies in LLM grounding, as LLMs are inherently limited\nto textual data, whereas environmental data often comprise numerical vectors\nunseen to LLMs. To address this, KALM fine-tunes the LLM to perform various\ntasks based on environmental data, including bidirectional translation between\nnatural language descriptions of skills and their corresponding rollout data.\nThis grounding process enhances the LLM's comprehension of environmental\ndynamics, enabling it to generate diverse and meaningful imaginary rollouts\nthat reflect novel skills. Initial empirical evaluations on the CLEVR-Robot\nenvironment demonstrate that KALM enables agents to complete complex\nrephrasings of task goals and extend their capabilities to novel tasks\nrequiring unprecedented optimal behaviors. KALM achieves a success rate of 46%\nin executing tasks with unseen goals, substantially surpassing the 26% success\nrate achieved by baseline methods. Furthermore, KALM effectively enables the\nLLM to comprehend environmental dynamics, resulting in the generation of\nmeaningful imaginary rollouts that reflect novel skills and demonstrate the\nseamless integration of large language models and reinforcement learning.\n","authors":["Jing-Cheng Pang","Si-Hang Yang","Kaiyuan Li","Jiaji Zhang","Xiong-Hui Chen","Nan Tang","Yang Yu"],"pdf_url":"https://arxiv.org/pdf/2404.09248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09247v1","updated":"2024-04-14T13:17:32Z","published":"2024-04-14T13:17:32Z","title":"Generalization Error Bounds for Learning under Censored Feedback","summary":"  Generalization error bounds from learning theory provide statistical\nguarantees on how well an algorithm will perform on previously unseen data. In\nthis paper, we characterize the impacts of data non-IIDness due to censored\nfeedback (a.k.a. selective labeling bias) on such bounds. We first derive an\nextension of the well-known Dvoretzky-Kiefer-Wolfowitz (DKW) inequality, which\ncharacterizes the gap between empirical and theoretical CDFs given IID data, to\nproblems with non-IID data due to censored feedback. We then use this CDF error\nbound to provide a bound on the generalization error guarantees of a classifier\ntrained on such non-IID data. We show that existing generalization error bounds\n(which do not account for censored feedback) fail to correctly capture the\nmodel's generalization guarantees, verifying the need for our bounds. We\nfurther analyze the effectiveness of (pure and bounded) exploration techniques,\nproposed by recent literature as a way to alleviate censored feedback, on\nimproving our error bounds. Together, our findings illustrate how a decision\nmaker should account for the trade-off between strengthening the generalization\nguarantees of an algorithm and the costs incurred in data collection when\nfuture data availability is limited by censored feedback.\n","authors":["Yifan Yang","Ali Payani","Parinaz Naghizadeh"],"pdf_url":"https://arxiv.org/pdf/2404.09247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09243v1","updated":"2024-04-14T13:08:21Z","published":"2024-04-14T13:08:21Z","title":"LSROM: Learning Self-Refined Organizing Map for Fast Imbalanced\n  Streaming Data Clustering","summary":"  Streaming data clustering is a popular research topic in the fields of data\nmining and machine learning. Compared to static data, streaming data, which is\nusually analyzed in data chunks, is more susceptible to encountering the\ndynamic cluster imbalanced issue. That is, the imbalanced degree of clusters\nvaries in different streaming data chunks, leading to corruption in either the\naccuracy or the efficiency of streaming data analysis based on existing\nclustering methods. Therefore, we propose an efficient approach called Learning\nSelf-Refined Organizing Map (LSROM) to handle the imbalanced streaming data\nclustering problem, where we propose an advanced SOM for representing the\nglobal data distribution. The constructed SOM is first refined for guiding the\npartition of the dataset to form many micro-clusters to avoid the missing small\nclusters in imbalanced data. Then an efficient merging of the micro-clusters is\nconducted through quick retrieval based on the SOM, which can automatically\nyield a true number of imbalanced clusters. In comparison to existing\nimbalanced data clustering approaches, LSROM is with a lower time complexity\n$O(n\\log n)$, while achieving very competitive clustering accuracy. Moreover,\nLSROM is interpretable and insensitive to hyper-parameters. Extensive\nexperiments have verified its efficacy.\n","authors":["Yongqi Xu","Yujian Lee","Rong Zou","Yiqun Zhang","Yiu-Ming Cheung"],"pdf_url":"https://arxiv.org/pdf/2404.09243v1.pdf","comment":"13 pages, 7 figures"},{"id":"http://arxiv.org/abs/2307.00185v3","updated":"2024-04-14T13:06:24Z","published":"2023-07-01T01:07:20Z","title":"Interpretable Neural Networks with Random Constructive Algorithm","summary":"  This paper introduces an Interpretable Neural Network (INN) incorporating\nspatial information to tackle the opaque parameterization process of random\nweighted neural networks. The INN leverages spatial information to elucidate\nthe connection between parameters and network residuals. Furthermore, it\ndevises a geometric relationship strategy using a pool of candidate nodes and\nestablished relationships to select node parameters conducive to network\nconvergence. Additionally, a lightweight version of INN tailored for\nlarge-scale data modeling tasks is proposed. The paper also showcases the\ninfinite approximation property of INN. Experimental findings on various\nbenchmark datasets and real-world industrial cases demonstrate INN's\nsuperiority over other neural networks of the same type in terms of modeling\nspeed, accuracy, and network structure.\n","authors":["Jing Nan","Wei Dai"],"pdf_url":"https://arxiv.org/pdf/2307.00185v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12091v4","updated":"2024-04-14T13:03:26Z","published":"2023-03-21T09:07:15Z","title":"Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised\n  Learning","summary":"  Semi-supervised learning (SSL) methods assume that labeled data, unlabeled\ndata and test data are from the same distribution. Open-set semi-supervised\nlearning (Open-set SSL) considers a more practical scenario, where unlabeled\ndata and test data contain new categories (outliers) not observed in labeled\ndata (inliers). Most previous works focused on outlier detection via binary\nclassifiers, which suffer from insufficient scalability and inability to\ndistinguish different types of uncertainty. In this paper, we propose a novel\nframework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these\nlimitations. Concretely, we first introduce evidential deep learning (EDL) as\nan outlier detector to quantify different types of uncertainty, and design\ndifferent uncertainty metrics for self-training and inference. Furthermore, we\npropose a novel adaptive negative optimization strategy, making EDL more\ntailored to the unlabeled dataset containing both inliers and outliers. As\ndemonstrated empirically, our proposed method outperforms existing\nstate-of-the-art methods across four datasets.\n","authors":["Yang Yu","Danruo Deng","Furui Liu","Yueming Jin","Qi Dou","Guangyong Chen","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2303.12091v4.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2404.09240v1","updated":"2024-04-14T12:59:35Z","published":"2024-04-14T12:59:35Z","title":"Fault Detection in Mobile Networks Using Diffusion Models","summary":"  In today's hyper-connected world, ensuring the reliability of telecom\nnetworks becomes increasingly crucial. Telecom networks encompass numerous\nunderlying and intertwined software and hardware components, each providing\ndifferent functionalities. To ensure the stability of telecom networks, telecom\nsoftware, and hardware vendors developed several methods to detect any aberrant\nbehavior in telecom networks and enable instant feedback and alerts. These\napproaches, although powerful, struggle to generalize due to the unsteady\nnature of the software-intensive embedded system and the complexity and\ndiversity of multi-standard mobile networks. In this paper, we present a system\nto detect anomalies in telecom networks using a generative AI model. We\nevaluate several strategies using diffusion models to train the model for\nanomaly detection using multivariate time-series data. The contributions of\nthis paper are threefold: (i) A proposal of a framework for utilizing diffusion\nmodels for time-series anomaly detection in telecom networks, (ii) A proposal\nof a particular Diffusion model architecture that outperforms other\nstate-of-the-art techniques, (iii) Experiments on a real-world dataset to\ndemonstrate that our model effectively provides explainable results, exposing\nsome of its limitations and suggesting future research avenues to enhance its\ncapabilities further.\n","authors":["Mohamad Nabeel","Doumitrou Daniil Nimara","Tahar Zanouda"],"pdf_url":"https://arxiv.org/pdf/2404.09240v1.pdf","comment":"6 pages, 4 figures, Accepted at Sixth International Workshop on Data\n  Driven Intelligence for Networks and Systems (DDINS) - IEEE International\n  Conference on Communications (ICC) 2024"},{"id":"http://arxiv.org/abs/2404.09232v1","updated":"2024-04-14T12:22:42Z","published":"2024-04-14T12:22:42Z","title":"MAP: Model Aggregation and Personalization in Federated Learning with\n  Incomplete Classes","summary":"  In some real-world applications, data samples are usually distributed on\nlocal devices, where federated learning (FL) techniques are proposed to\ncoordinate decentralized clients without directly sharing users' private data.\nFL commonly follows the parameter server architecture and contains multiple\npersonalization and aggregation procedures. The natural data heterogeneity\nacross clients, i.e., Non-I.I.D. data, challenges both the aggregation and\npersonalization goals in FL. In this paper, we focus on a special kind of\nNon-I.I.D. scene where clients own incomplete classes, i.e., each client can\nonly access a partial set of the whole class set. The server aims to aggregate\na complete classification model that could generalize to all classes, while the\nclients are inclined to improve the performance of distinguishing their\nobserved classes. For better model aggregation, we point out that the standard\nsoftmax will encounter several problems caused by missing classes and propose\n\"restricted softmax\" as an alternative. For better model personalization, we\npoint out that the hard-won personalized models are not well exploited and\npropose \"inherited private model\" to store the personalization experience. Our\nproposed algorithm named MAP could simultaneously achieve the aggregation and\npersonalization goals in FL. Abundant experimental studies verify the\nsuperiorities of our algorithm.\n","authors":["Xin-Chun Li","Shaoming Song","Yinchuan Li","Bingshuai Li","Yunfeng Shao","Yang Yang","De-Chuan Zhan"],"pdf_url":"https://arxiv.org/pdf/2404.09232v1.pdf","comment":"Accepted by TKDE (11-Apr-2024)"},{"id":"http://arxiv.org/abs/2404.09226v1","updated":"2024-04-14T12:09:47Z","published":"2024-04-14T12:09:47Z","title":"Breast Cancer Image Classification Method Based on Deep Transfer\n  Learning","summary":"  To address the issues of limited samples, time-consuming feature design, and\nlow accuracy in detection and classification of breast cancer pathological\nimages, a breast cancer image classification model algorithm combining deep\nlearning and transfer learning is proposed. This algorithm is based on the\nDenseNet structure of deep neural networks, and constructs a network model by\nintroducing attention mechanisms, and trains the enhanced dataset using\nmulti-level transfer learning. Experimental results demonstrate that the\nalgorithm achieves an efficiency of over 84.0\\% in the test set, with a\nsignificantly improved classification accuracy compared to previous models,\nmaking it applicable to medical breast cancer detection tasks.\n","authors":["Weimin Wang","Min Gao","Mingxuan Xiao","Xu Yan","Yufeng Li"],"pdf_url":"https://arxiv.org/pdf/2404.09226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09221v1","updated":"2024-04-14T11:49:38Z","published":"2024-04-14T11:49:38Z","title":"Towards Fast Inference: Exploring and Improving Blockwise Parallel\n  Drafts","summary":"  Despite the remarkable strides made by autoregressive language models, their\npotential is often hampered by the slow inference speeds inherent in sequential\ntoken generation. Blockwise parallel decoding (BPD) was proposed by Stern et\nal. (2018) as a way to improve inference speed of language models. In this\npaper, we make two contributions to understanding and improving BPD drafts. We\nfirst offer an analysis of the token distributions produced by the BPD\nprediction heads. Secondly, we use this analysis to inform algorithms to\nimprove BPD inference speed by refining the BPD drafts using small n-gram or\nneural language models. We empirically show that these refined BPD drafts yield\na higher average verified prefix length across tasks.\n","authors":["Taehyeon Kim","Ananda Theertha Suresh","Kishore Papineni","Michael Riley","Sanjiv Kumar","Adrian Benton"],"pdf_url":"https://arxiv.org/pdf/2404.09221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12167v2","updated":"2024-04-14T11:28:37Z","published":"2023-11-20T20:33:35Z","title":"Node Classification in Random Trees","summary":"  We propose a method for the classification of objects that are structured as\nrandom trees. Our aim is to model a distribution over the node label\nassignments in settings where the tree data structure is associated with node\nattributes (typically high dimensional embeddings). The tree topology is not\npredetermined and none of the label assignments are present during inference.\nOther methods that produce a distribution over node label assignment in trees\n(or more generally in graphs) either assume conditional independence of the\nlabel assignment, operate on a fixed graph topology, or require part of the\nnode labels to be observed. Our method defines a Markov Network with the\ncorresponding topology of the random tree and an associated Gibbs distribution.\nWe parameterize the Gibbs distribution with a Graph Neural Network that\noperates on the random tree and the node embeddings. This allows us to estimate\nthe likelihood of node assignments for a given random tree and use MCMC to\nsample from the distribution of node assignments.\n  We evaluate our method on the tasks of node classification in trees on the\nStanford Sentiment Treebank dataset. Our method outperforms the baselines on\nthis dataset, demonstrating its effectiveness for modeling joint distributions\nof node labels in random trees.\n","authors":["Wouter W. L. Nuijten","Vlado Menkovski"],"pdf_url":"https://arxiv.org/pdf/2311.12167v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2404.09245v1","updated":"2024-04-14T13:14:13Z","published":"2024-04-14T13:14:13Z","title":"Arena: A Patch-of-Interest ViT Inference Acceleration System for\n  Edge-Assisted Video Analytics","summary":"  The advent of edge computing has made real-time intelligent video analytics\nfeasible. Previous works, based on traditional model architecture (e.g., CNN,\nRNN, etc.), employ various strategies to filter out non-region-of-interest\ncontent to minimize bandwidth and computation consumption but show inferior\nperformance in adverse environments. Recently, visual foundation models based\non transformers have shown great performance in adverse environments due to\ntheir amazing generalization capability. However, they require a large amount\nof computation power, which limits their applications in real-time intelligent\nvideo analytics. In this paper, we find visual foundation models like Vision\nTransformer (ViT) also have a dedicated acceleration mechanism for video\nanalytics. To this end, we introduce Arena, an end-to-end edge-assisted video\ninference acceleration system based on ViT. We leverage the capability of ViT\nthat can be accelerated through token pruning by only offloading and feeding\nPatches-of-Interest (PoIs) to the downstream models. Additionally, we employ\nprobability-based patch sampling, which provides a simple but efficient\nmechanism for determining PoIs where the probable locations of objects are in\nsubsequent frames. Through extensive evaluations on public datasets, our\nfindings reveal that Arena can boost inference speeds by up to $1.58\\times$ and\n$1.82\\times$ on average while consuming only 54% and 34% of the bandwidth,\nrespectively, all with high inference accuracy.\n","authors":["Haosong Peng","Wei Feng","Hao Li","Yufeng Zhan","Qihua Zhou","Yuanqing Xia"],"pdf_url":"https://arxiv.org/pdf/2404.09245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03890v2","updated":"2024-04-14T06:50:24Z","published":"2024-01-08T13:42:59Z","title":"A Survey on 3D Gaussian Splatting","summary":"  3D Gaussian splatting (GS) has recently emerged as a transformative technique\nin the realm of explicit radiance field and computer graphics. This innovative\napproach, characterized by the utilization of millions of learnable 3D\nGaussians, represents a significant departure from mainstream neural radiance\nfield approaches, which predominantly use implicit, coordinate-based models to\nmap spatial coordinates to pixel values. 3D GS, with its explicit scene\nrepresentation and differentiable rendering algorithm, not only promises\nreal-time rendering capability but also introduces unprecedented levels of\neditability. This positions 3D GS as a potential game-changer for the next\ngeneration of 3D reconstruction and representation. In the present paper, we\nprovide the first systematic overview of the recent developments and critical\ncontributions in the domain of 3D GS. We begin with a detailed exploration of\nthe underlying principles and the driving forces behind the emergence of 3D GS,\nlaying the groundwork for understanding its significance. A focal point of our\ndiscussion is the practical applicability of 3D GS. By enabling unprecedented\nrendering speed, 3D GS opens up a plethora of applications, ranging from\nvirtual reality to interactive media and beyond. This is complemented by a\ncomparative analysis of leading 3D GS models, evaluated across various\nbenchmark tasks to highlight their performance and practical utility. The\nsurvey concludes by identifying current challenges and suggesting potential\navenues for future research in this domain. Through this survey, we aim to\nprovide a valuable resource for both newcomers and seasoned researchers,\nfostering further exploration and advancement in applicable and explicit\nradiance field representation.\n","authors":["Guikun Chen","Wenguan Wang"],"pdf_url":"https://arxiv.org/pdf/2401.03890v2.pdf","comment":"Ongoing project"},{"id":"http://arxiv.org/abs/2107.05223v2","updated":"2024-04-14T03:40:35Z","published":"2021-07-12T07:03:57Z","title":"BERT-like Pre-training for Symbolic Piano Music Classification Tasks","summary":"  This article presents a benchmark study of symbolic piano music\nclassification using the masked language modelling approach of the\nBidirectional Encoder Representations from Transformers (BERT). Specifically,\nwe consider two types of MIDI data: MIDI scores, which are musical scores\nrendered directly into MIDI with no dynamics and precisely aligned with the\nmetrical grid notated by its composer and MIDI performances, which are MIDI\nencodings of human performances of musical scoresheets. With five public-domain\ndatasets of single-track piano MIDI files, we pre-train two 12-layer\nTransformer models using the BERT approach, one for MIDI scores and the other\nfor MIDI performances, and fine-tune them for four downstream classification\ntasks. These include two note-level classification tasks (melody extraction and\nvelocity prediction) and two sequence-level classification tasks (style\nclassification and emotion classification). Our evaluation shows that the BERT\napproach leads to higher classification accuracy than recurrent neural network\n(RNN)-based baselines.\n","authors":["Yi-Hui Chou","I-Chun Chen","Chin-Jui Chang","Joann Ching","Yi-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2107.05223v2.pdf","comment":"Accepted to Journal of Creative Music Systems"}]},"2024-04-13T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2404.09091v1","updated":"2024-04-13T22:18:14Z","published":"2024-04-13T22:18:14Z","title":"Semantic In-Domain Product Identification for Search Queries","summary":"  Accurate explicit and implicit product identification in search queries is\ncritical for enhancing user experiences, especially at a company like Adobe\nwhich has over 50 products and covers queries across hundreds of tools. In this\nwork, we present a novel approach to training a product classifier from user\nbehavioral data. Our semantic model led to >25% relative improvement in CTR\n(click through rate) across the deployed surfaces; a >50% decrease in null\nrate; a 2x increase in the app cards surfaced, which helps drive product\nvisibility.\n","authors":["Sanat Sharma","Jayant Kumar","Twisha Naik","Zhaoyu Lu","Arvind Srikantan","Tracy Holloway King"],"pdf_url":"https://arxiv.org/pdf/2404.09091v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00785v4","updated":"2024-04-13T22:02:23Z","published":"2023-10-01T20:46:44Z","title":"BooookScore: A systematic exploration of book-length summarization in\n  the era of LLMs","summary":"  Summarizing book-length documents (>100K tokens) that exceed the context\nwindow size of large language models (LLMs) requires first breaking the input\ndocument into smaller chunks and then prompting an LLM to merge, update, and\ncompress chunk-level summaries. Despite the complexity and importance of this\ntask, it has yet to be meaningfully studied due to the challenges of\nevaluation: existing book-length summarization datasets (e.g., BookSum) are in\nthe pretraining data of most public LLMs, and existing evaluation methods\nstruggle to capture errors made by modern LLM summarizers. In this paper, we\npresent the first study of the coherence of LLM-based book-length summarizers\nimplemented via two prompting workflows: (1) hierarchically merging chunk-level\nsummaries, and (2) incrementally updating a running summary. We obtain 1193\nfine-grained human annotations on GPT-4 generated summaries of 100\nrecently-published books and identify eight common types of coherence errors\nmade by LLMs. Because human evaluation is expensive and time-consuming, we\ndevelop an automatic metric, BooookScore, that measures the proportion of\nsentences in a summary that do not contain any of the identified error types.\nBooookScore has high agreement with human annotations and allows us to\nsystematically evaluate the impact of many other critical parameters (e.g.,\nchunk size, base LLM) while saving $15K USD and 500 hours in human evaluation\ncosts. We find that closed-source LLMs such as GPT-4 and Claude 2 produce\nsummaries with higher BooookScore than those generated by open-source models.\nWhile LLaMA 2 falls behind other models, Mixtral achieves performance on par\nwith GPT-3.5-Turbo. Incremental updating yields lower BooookScore but higher\nlevel of detail than hierarchical merging, a trade-off sometimes preferred by\nannotators.\n","authors":["Yapei Chang","Kyle Lo","Tanya Goyal","Mohit Iyyer"],"pdf_url":"https://arxiv.org/pdf/2310.00785v4.pdf","comment":"ICLR 2024 camera-ready (updated figure1 and table2; corrected minor\n  details in the explanation of hierarchical merging)"},{"id":"http://arxiv.org/abs/2404.09077v1","updated":"2024-04-13T20:43:46Z","published":"2024-04-13T20:43:46Z","title":"CuriousLLM: Elevating Multi-Document QA with Reasoning-Infused Knowledge\n  Graph Prompting","summary":"  In the field of Question Answering (QA), unifying large language models\n(LLMs) with external databases has shown great success. However, these methods\noften fall short in providing the advanced reasoning needed for complex QA\ntasks. To address these issues, we improve over a novel approach called\nKnowledge Graph Prompting (KGP), which combines knowledge graphs with a\nLLM-based agent to improve reasoning and search accuracy. Nevertheless, the\noriginal KGP framework necessitates costly fine-tuning with large datasets yet\nstill suffers from LLM hallucination. Therefore, we propose a reasoning-infused\nLLM agent to enhance this framework. This agent mimics human curiosity to ask\nfollow-up questions to more efficiently navigate the search. This simple\nmodification significantly boosts the LLM performance in QA tasks without the\nhigh costs and latency associated with the initial KGP framework. Our ultimate\ngoal is to further develop this approach, leading to more accurate, faster, and\ncost-effective solutions in the QA domain.\n","authors":["Zukang Yang","Zixuan Zhu"],"pdf_url":"https://arxiv.org/pdf/2404.09077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09066v1","updated":"2024-04-13T19:30:58Z","published":"2024-04-13T19:30:58Z","title":"CodeCloak: A Method for Evaluating and Mitigating Code Leakage by LLM\n  Code Assistants","summary":"  LLM-based code assistants are becoming increasingly popular among developers.\nThese tools help developers improve their coding efficiency and reduce errors\nby providing real-time suggestions based on the developer's codebase. While\nbeneficial, these tools might inadvertently expose the developer's proprietary\ncode to the code assistant service provider during the development process. In\nthis work, we propose two complementary methods to mitigate the risk of code\nleakage when using LLM-based code assistants. The first is a technique for\nreconstructing a developer's original codebase from code segments sent to the\ncode assistant service (i.e., prompts) during the development process, enabling\nassessment and evaluation of the extent of code leakage to third parties (or\nadversaries). The second is CodeCloak, a novel deep reinforcement learning\nagent that manipulates the prompts before sending them to the code assistant\nservice. CodeCloak aims to achieve the following two contradictory goals: (i)\nminimizing code leakage, while (ii) preserving relevant and useful suggestions\nfor the developer. Our evaluation, employing GitHub Copilot, StarCoder, and\nCodeLlama LLM-based code assistants models, demonstrates the effectiveness of\nour CodeCloak approach on a diverse set of code repositories of varying sizes,\nas well as its transferability across different models. In addition, we\ngenerate a realistic simulated coding environment to thoroughly analyze code\nleakage risks and evaluate the effectiveness of our proposed mitigation\ntechniques under practical development scenarios.\n","authors":["Amit Finkman","Eden Bar-Kochva","Avishag Shapira","Dudu Mimran","Yuval Elovici","Asaf Shabtai"],"pdf_url":"https://arxiv.org/pdf/2404.09066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06838v3","updated":"2024-04-13T18:27:04Z","published":"2024-01-12T18:03:54Z","title":"MAPO: Advancing Multilingual Reasoning through Multilingual\n  Alignment-as-Preference Optimization","summary":"  Though reasoning abilities are considered language-agnostic, existing LLMs\nexhibit inconsistent reasoning abilities across different languages, e.g.,\nreasoning in the dominant language like English is superior to other languages\ndue to the imbalance of multilingual training data. To enhance reasoning\nabilities in non-dominant languages, we propose a\nMultilingual-Alignment-as-Preference Optimization framework (MAPO), aiming to\nalign the reasoning processes in other languages with the dominant language.\nSpecifically, we harness an off-the-shelf translation model for the consistency\nbetween answers in non-dominant and dominant languages, which we adopt as the\npreference for optimization, e.g., Direct Preference Optimization (DPO) or\nProximal Policy Optimization (PPO). Experiments show that MAPO stably achieves\nsignificant improvements in the multilingual reasoning of various models on all\nthree benchmarks (MSVAMP +16.2%, MGSM +6.1%, and MNumGLUESub +13.3%), with\nimproved reasoning consistency across languages.\n","authors":["Shuaijie She","Wei Zou","Shujian Huang","Wenhao Zhu","Xiang Liu","Xiang Geng","Jiajun Chen"],"pdf_url":"https://arxiv.org/pdf/2401.06838v3.pdf","comment":"The project is available at https://github.com/NJUNLP/MAPO"},{"id":"http://arxiv.org/abs/2404.09047v1","updated":"2024-04-13T17:16:03Z","published":"2024-04-13T17:16:03Z","title":"Multilingual Evaluation of Semantic Textual Relatedness","summary":"  The explosive growth of online content demands robust Natural Language\nProcessing (NLP) techniques that can capture nuanced meanings and cultural\ncontext across diverse languages. Semantic Textual Relatedness (STR) goes\nbeyond superficial word overlap, considering linguistic elements and\nnon-linguistic factors like topic, sentiment, and perspective. Despite its\npivotal role, prior NLP research has predominantly focused on English, limiting\nits applicability across languages. Addressing this gap, our paper dives into\ncapturing deeper connections between sentences beyond simple word overlap.\nGoing beyond English-centric NLP research, we explore STR in Marathi, Hindi,\nSpanish, and English, unlocking the potential for information retrieval,\nmachine translation, and more. Leveraging the SemEval-2024 shared task, we\nexplore various language models across three learning paradigms: supervised,\nunsupervised, and cross-lingual. Our comprehensive methodology gains promising\nresults, demonstrating the effectiveness of our approach. This work aims to not\nonly showcase our achievements but also inspire further research in\nmultilingual STR, particularly for low-resourced languages.\n","authors":["Sharvi Endait","Srushti Sonavane","Ridhima Sinare","Pritika Rohera","Advait Naik","Dipali Kadam"],"pdf_url":"https://arxiv.org/pdf/2404.09047v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2305.14882v2","updated":"2024-04-13T17:13:55Z","published":"2023-05-24T08:33:15Z","title":"Dynamic Clue Bottlenecks: Towards Interpretable-by-Design Visual\n  Question Answering","summary":"  Recent advances in multimodal large language models (LLMs) have shown extreme\neffectiveness in visual question answering (VQA). However, the design nature of\nthese end-to-end models prevents them from being interpretable to humans,\nundermining trust and applicability in critical domains. While post-hoc\nrationales offer certain insight into understanding model behavior, these\nexplanations are not guaranteed to be faithful to the model. In this paper, we\naddress these shortcomings by introducing an interpretable by design model that\nfactors model decisions into intermediate human-legible explanations, and\nallows people to easily understand why a model fails or succeeds. We propose\nthe Dynamic Clue Bottleneck Model ( (DCLUB), a method that is designed towards\nan inherently interpretable VQA system. DCLUB provides an explainable\nintermediate space before the VQA decision and is faithful from the beginning,\nwhile maintaining comparable performance to black-box systems. Given a\nquestion, DCLUB first returns a set of visual clues: natural language\nstatements of visually salient evidence from the image, and then generates the\noutput based solely on the visual clues. To supervise and evaluate the\ngeneration of VQA explanations within DCLUB, we collect a dataset of 1.7k\nreasoning-focused questions with visual clues. Evaluations show that our\ninherently interpretable system can improve 4.64% over a comparable black-box\nsystem in reasoning-focused questions while preserving 99.43% of performance on\nVQA-v2.\n","authors":["Xingyu Fu","Ben Zhou","Sihao Chen","Mark Yatskar","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2305.14882v2.pdf","comment":"Multimodal, Visual Question Answering, Vision and Language"},{"id":"http://arxiv.org/abs/2404.09045v1","updated":"2024-04-13T17:11:35Z","published":"2024-04-13T17:11:35Z","title":"Adapting Mental Health Prediction Tasks for Cross-lingual Learning via\n  Meta-Training and In-context Learning with Large Language Model","summary":"  Timely identification is essential for the efficient handling of mental\nhealth illnesses such as depression. However, the current research fails to\nadequately address the prediction of mental health conditions from social media\ndata in low-resource African languages like Swahili. This study introduces two\ndistinct approaches utilising model-agnostic meta-learning and leveraging large\nlanguage models (LLMs) to address this gap. Experiments are conducted on three\ndatasets translated to low-resource language and applied to four mental health\ntasks, which include stress, depression, depression severity and suicidal\nideation prediction. we first apply a meta-learning model with\nself-supervision, which results in improved model initialisation for rapid\nadaptation and cross-lingual transfer. The results show that our meta-trained\nmodel performs significantly better than standard fine-tuning methods,\noutperforming the baseline fine-tuning in macro F1 score with 18\\% and 0.8\\%\nover XLM-R and mBERT. In parallel, we use LLMs' in-context learning\ncapabilities to assess their performance accuracy across the Swahili mental\nhealth prediction tasks by analysing different cross-lingual prompting\napproaches. Our analysis showed that Swahili prompts performed better than\ncross-lingual prompts but less than English prompts. Our findings show that\nin-context learning can be achieved through cross-lingual transfer through\ncarefully crafted prompt templates with examples and instructions.\n","authors":["Zita Lifelo","Huansheng Ning","Sahraoui Dhelim"],"pdf_url":"https://arxiv.org/pdf/2404.09045v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00434v2","updated":"2024-04-13T17:05:03Z","published":"2023-12-31T09:22:54Z","title":"GeoGalactica: A Scientific Large Language Model in Geoscience","summary":"  Large language models (LLMs) have achieved huge success for their general\nknowledge and ability to solve a wide spectrum of tasks in natural language\nprocessing (NLP). Due to their impressive abilities, LLMs have shed light on\npotential inter-discipline applications to foster scientific discoveries of a\nspecific domain by using artificial intelligence (AI for science, AI4S). In the\nmeantime, utilizing NLP techniques in geoscience research and practice is wide\nand convoluted, contributing from knowledge extraction and document\nclassification to question answering and knowledge discovery. In this work, we\ntake the initial step to leverage LLM for science, through a rather\nstraightforward approach. We try to specialize an LLM into geoscience, by\nfurther pre-training the model with a vast amount of texts in geoscience, as\nwell as supervised fine-tuning (SFT) the resulting model with our custom\ncollected instruction tuning dataset. These efforts result in a model\nGeoGalactica consisting of 30 billion parameters. To our best knowledge, it is\nthe largest language model for the geoscience domain. More specifically,\nGeoGalactica is from further pre-training of Galactica. We train GeoGalactica\nover a geoscience-related text corpus containing 65 billion tokens, preserving\nas the largest geoscience-specific text corpus. Then we fine-tune the model\nwith 1 million pairs of instruction-tuning data consisting of questions that\ndemand professional geoscience knowledge to answer. In this technical report,\nwe will illustrate in detail all aspects of GeoGalactica, including data\ncollection, data cleaning, base model selection, pre-training, SFT, and\nevaluation. We open-source our data curation tools and the checkpoints of\nGeoGalactica during the first 3/4 of pre-training.\n","authors":["Zhouhan Lin","Cheng Deng","Le Zhou","Tianhang Zhang","Yi Xu","Yutong Xu","Zhongmou He","Yuanyuan Shi","Beiya Dai","Yunchong Song","Boyi Zeng","Qiyuan Chen","Yuxun Miao","Bo Xue","Shu Wang","Luoyi Fu","Weinan Zhang","Junxian He","Yunqiang Zhu","Xinbing Wang","Chenghu Zhou"],"pdf_url":"https://arxiv.org/pdf/2401.00434v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.12043v2","updated":"2024-04-13T17:02:25Z","published":"2022-12-22T21:27:12Z","title":"When are Lemons Purple? The Concept Association Bias of Vision-Language\n  Models","summary":"  Large-scale vision-language models such as CLIP have shown impressive\nperformance on zero-shot image classification and image-to-text retrieval.\nHowever, such performance does not realize in tasks that require a\nfiner-grained correspondence between vision and language, such as Visual\nQuestion Answering (VQA). As a potential cause of the difficulty of applying\nthese models to VQA and similar tasks, we report an interesting phenomenon of\nvision-language models, which we call the Concept Association Bias (CAB). We\nfind that models with CAB tend to treat input as a bag of concepts and attempt\nto fill in the other missing concept crossmodally, leading to an unexpected\nzero-shot prediction. We demonstrate CAB by showing that CLIP's zero-shot\nclassification performance greatly suffers when there is a strong concept\nassociation between an object (e.g. eggplant) and an attribute (e.g. color\npurple). We also show that the strength of CAB predicts the performance on VQA.\nWe observe that CAB is prevalent in vision-language models trained with\ncontrastive losses, even when autoregressive losses are jointly employed.\nHowever, a model that solely relies on autoregressive loss seems to exhibit\nminimal or no signs of CAB.\n","authors":["Yutaro Yamada","Yingtian Tang","Yoyo Zhang","Ilker Yildirim"],"pdf_url":"https://arxiv.org/pdf/2212.12043v2.pdf","comment":"EMNLP 2023 main"},{"id":"http://arxiv.org/abs/2404.09043v1","updated":"2024-04-13T16:59:28Z","published":"2024-04-13T16:59:28Z","title":"Do LLMs Play Dice? Exploring Probability Distribution Sampling in Large\n  Language Models for Behavioral Simulation","summary":"  With the rapid advancement of large language models (LLMs) and their\nremarkable capabilities in handling complex language tasks, an increasing\nnumber of studies are employing LLMs as agents to emulate the sequential\ndecision-making processes of humans often represented as Markov decision-making\nprocesses (MDPs). The actions within this decision-making framework adhere to\nspecific probability distributions and require iterative sampling. This arouses\nour curiosity regarding the capacity of LLM agents to comprehend probability\ndistributions, thereby guiding the agent's behavioral decision-making through\nprobabilistic sampling and generating behavioral sequences. To answer the above\nquestion, we divide the problem into two main aspects: simulation where the\nexact probability distribution is known, and generation of sequences where the\nprobability distribution is ambiguous. In the first case, the agent is required\nto give the type and parameters of the probability distribution through the\nproblem description, and then give the sampling sequence. However, our analysis\nshows that LLM agents perform poorly in this case, but the sampling success\nrate can be improved through programming tools. Real-world scenarios often\nentail unknown probability distributions. Thus, in the second case, we ask the\nagents to change the activity level in online social networks and analyze the\nfrequency of actions. Ultimately, our analysis shows that LLM agents cannot\nsample probability distributions even using programming tools. Therefore,\ncareful consideration is still required before directly applying LLM agents as\nagents to simulate human behavior.\n","authors":["Jia Gu","Liang Pang","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2404.09043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10435v4","updated":"2024-04-13T16:32:33Z","published":"2023-09-19T08:54:47Z","title":"Reformulating Sequential Recommendation: Learning Dynamic User Interest\n  with Content-enriched Language Modeling","summary":"  Recommender systems are indispensable in the realm of online applications,\nand sequential recommendation has enjoyed considerable prevalence due to its\ncapacity to encapsulate the dynamic shifts in user interests. However, previous\nsequential modeling methods still have limitations in capturing contextual\ninformation. The primary reason is the lack of understanding of domain-specific\nknowledge and item-related textual content. Fortunately, the emergence of\npowerful language models has unlocked the potential to incorporate extensive\nworld knowledge into recommendation algorithms, enabling them to go beyond\nsimple item attributes and truly understand the world surrounding user\npreferences. To achieve this, we propose LANCER, which leverages the semantic\nunderstanding capabilities of pre-trained language models to generate\npersonalized recommendations. Our approach bridges the gap between language\nmodels and recommender systems, resulting in more human-like recommendations.\nWe demonstrate the effectiveness of our approach through a series of\nexperiments conducted on multiple benchmark datasets, showing promising results\nand providing valuable insights into the influence of our model on sequential\nrecommendation tasks. Furthermore, our experimental codes are publicly\navailable at https://github.com/Gnimixy/lancer.\n","authors":["Junzhe Jiang","Shang Qu","Mingyue Cheng","Qi Liu","Zhiding Liu","Hao Zhang","Rujiao Zhang","Kai Zhang","Rui Li","Jiatong Li","Min Gao"],"pdf_url":"https://arxiv.org/pdf/2309.10435v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04339v2","updated":"2024-04-13T16:31:25Z","published":"2023-12-07T14:59:15Z","title":"Merging by Matching Models in Task Parameter Subspaces","summary":"  Model merging aims to cheaply combine individual task-specific models into a\nsingle multitask model. In this work, we view past merging methods as\nleveraging different notions of a ''task parameter subspace'' in which models\nare matched before being merged. We connect the task parameter subspace of a\ngiven model to its loss landscape and formalize how this approach to model\nmerging can be seen as solving a linear system of equations. While past work\nhas generally been limited to linear systems that have a closed-form solution,\nwe consider using the conjugate gradient method to find a solution. We show\nthat using the conjugate gradient method can outperform closed-form solutions,\nenables merging via linear systems that are otherwise intractable to solve, and\nflexibly allows choosing from a wide variety of initializations and estimates\nfor the ''task parameter subspace''. We ultimately demonstrate that our merging\nframework called ''Matching Models in their Task Parameter Subspace'' (MaTS)\nachieves state-of-the-art results in multitask and intermediate-task model\nmerging. We release all of the code and checkpoints used in our work at\nhttps://github.com/r-three/mats.\n","authors":["Derek Tam","Mohit Bansal","Colin Raffel"],"pdf_url":"https://arxiv.org/pdf/2312.04339v2.pdf","comment":"TMLR"},{"id":"http://arxiv.org/abs/2404.09027v1","updated":"2024-04-13T15:28:52Z","published":"2024-04-13T15:28:52Z","title":"MING-MOE: Enhancing Medical Multi-Task Learning in Large Language Models\n  with Sparse Mixture of Low-Rank Adapter Experts","summary":"  Large language models like ChatGPT have shown substantial progress in natural\nlanguage understanding and generation, proving valuable across various\ndisciplines, including the medical field. Despite advancements, challenges\npersist due to the complexity and diversity inherent in medical tasks which\noften require multi-task learning capabilities. Previous approaches, although\nbeneficial, fall short in real-world applications because they necessitate\ntask-specific annotations at inference time, limiting broader generalization.\nThis paper introduces MING-MOE, a novel Mixture-of-Expert~(MOE)-based medical\nlarge language model designed to manage diverse and complex medical tasks\nwithout requiring task-specific annotations, thus enhancing its usability\nacross extensive datasets. MING-MOE employs a Mixture of Low-Rank Adaptation\n(MoLoRA) technique, allowing for efficient parameter usage by maintaining base\nmodel parameters static while adapting through a minimal set of trainable\nparameters. We demonstrate that MING-MOE achieves state-of-the-art (SOTA)\nperformance on over 20 medical tasks, illustrating a significant improvement\nover existing models. This approach not only extends the capabilities of\nmedical language models but also improves inference efficiency.\n","authors":["Yusheng Liao","Shuyang Jiang","Yu Wang","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2404.09027v1.pdf","comment":"15 pages, 3 figures"},{"id":"http://arxiv.org/abs/2310.05216v2","updated":"2024-04-13T15:22:39Z","published":"2023-10-08T16:16:21Z","title":"Probing Large Language Models from A Human Behavioral Perspective","summary":"  Large Language Models (LLMs) have emerged as dominant foundational models in\nmodern NLP. However, the understanding of their prediction processes and\ninternal mechanisms, such as feed-forward networks (FFN) and multi-head\nself-attention (MHSA), remains largely unexplored. In this work, we probe LLMs\nfrom a human behavioral perspective, correlating values from LLMs with\neye-tracking measures, which are widely recognized as meaningful indicators of\nhuman reading patterns. Our findings reveal that LLMs exhibit a similar\nprediction pattern with humans but distinct from that of Shallow Language\nModels (SLMs). Moreover, with the escalation of LLM layers from the middle\nlayers, the correlation coefficients also increase in FFN and MHSA, indicating\nthat the logits within FFN increasingly encapsulate word semantics suitable for\npredicting tokens from the vocabulary.\n","authors":["Xintong Wang","Xiaoyu Li","Xingshan Li","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2310.05216v2.pdf","comment":"Accepted by LREC-COLING NeusymBridge 2024"},{"id":"http://arxiv.org/abs/2404.01954v2","updated":"2024-04-13T15:06:19Z","published":"2024-04-02T13:48:49Z","title":"HyperCLOVA X Technical Report","summary":"  We introduce HyperCLOVA X, a family of large language models (LLMs) tailored\nto the Korean language and culture, along with competitive capabilities in\nEnglish, math, and coding. HyperCLOVA X was trained on a balanced mix of\nKorean, English, and code data, followed by instruction-tuning with\nhigh-quality human-annotated datasets while abiding by strict safety guidelines\nreflecting our commitment to responsible AI. The model is evaluated across\nvarious benchmarks, including comprehensive reasoning, knowledge, commonsense,\nfactuality, coding, math, chatting, instruction-following, and harmlessness, in\nboth Korean and English. HyperCLOVA X exhibits strong reasoning capabilities in\nKorean backed by a deep understanding of the language and cultural nuances.\nFurther analysis of the inherent bilingual nature and its extension to\nmultilingualism highlights the model's cross-lingual proficiency and strong\ngeneralization ability to untargeted languages, including machine translation\nbetween several language pairs and cross-lingual inference tasks. We believe\nthat HyperCLOVA X can provide helpful guidance for regions or countries in\ndeveloping their sovereign LLMs.\n","authors":["Kang Min Yoo","Jaegeun Han","Sookyo In","Heewon Jeon","Jisu Jeong","Jaewook Kang","Hyunwook Kim","Kyung-Min Kim","Munhyong Kim","Sungju Kim","Donghyun Kwak","Hanock Kwak","Se Jung Kwon","Bado Lee","Dongsoo Lee","Gichang Lee","Jooho Lee","Baeseong Park","Seongjin Shin","Joonsang Yu","Seolki Baek","Sumin Byeon","Eungsup Cho","Dooseok Choe","Jeesung Han","Youngkyun Jin","Hyein Jun","Jaeseung Jung","Chanwoong Kim","Jinhong Kim","Jinuk Kim","Dokyeong Lee","Dongwook Park","Jeong Min Sohn","Sujung Han","Jiae Heo","Sungju Hong","Mina Jeon","Hyunhoon Jung","Jungeun Jung","Wangkyo Jung","Chungjoon Kim","Hyeri Kim","Jonghyun Kim","Min Young Kim","Soeun Lee","Joonhee Park","Jieun Shin","Sojin Yang","Jungsoon Yoon","Hwaran Lee","Sanghwan Bae","Jeehwan Cha","Karl Gylleus","Donghoon Ham","Mihak Hong","Youngki Hong","Yunki Hong","Dahyun Jang","Hyojun Jeon","Yujin Jeon","Yeji Jeong","Myunggeun Ji","Yeguk Jin","Chansong Jo","Shinyoung Joo","Seunghwan Jung","Adrian Jungmyung Kim","Byoung Hoon Kim","Hyomin Kim","Jungwhan Kim","Minkyoung Kim","Minseung Kim","Sungdong Kim","Yonghee Kim","Youngjun Kim","Youngkwan Kim","Donghyeon Ko","Dughyun Lee","Ha Young Lee","Jaehong Lee","Jieun Lee","Jonghyun Lee","Jongjin Lee","Min Young Lee","Yehbin Lee","Taehong Min","Yuri Min","Kiyoon Moon","Hyangnam Oh","Jaesun Park","Kyuyon Park","Younghun Park","Hanbae Seo","Seunghyun Seo","Mihyun Sim","Gyubin Son","Matt Yeo","Kyung Hoon Yeom","Wonjoon Yoo","Myungin You","Doheon Ahn","Homin Ahn","Joohee Ahn","Seongmin Ahn","Chanwoo An","Hyeryun An","Junho An","Sang-Min An","Boram Byun","Eunbin Byun","Jongho Cha","Minji Chang","Seunggyu Chang","Haesong Cho","Youngdo Cho","Dalnim Choi","Daseul Choi","Hyoseok Choi","Minseong Choi","Sangho Choi","Seongjae Choi","Wooyong Choi","Sewhan Chun","Dong Young Go","Chiheon Ham","Danbi Han","Jaemin Han","Moonyoung Hong","Sung Bum Hong","Dong-Hyun Hwang","Seongchan Hwang","Jinbae Im","Hyuk Jin Jang","Jaehyung Jang","Jaeni Jang","Sihyeon Jang","Sungwon Jang","Joonha Jeon","Daun Jeong","Joonhyun Jeong","Kyeongseok Jeong","Mini Jeong","Sol Jin","Hanbyeol Jo","Hanju Jo","Minjung Jo","Chaeyoon Jung","Hyungsik Jung","Jaeuk Jung","Ju Hwan Jung","Kwangsun Jung","Seungjae Jung","Soonwon Ka","Donghan Kang","Soyoung Kang","Taeho Kil","Areum Kim","Beomyoung Kim","Byeongwook Kim","Daehee Kim","Dong-Gyun Kim","Donggook Kim","Donghyun Kim","Euna Kim","Eunchul Kim","Geewook Kim","Gyu Ri Kim","Hanbyul Kim","Heesu Kim","Isaac Kim","Jeonghoon Kim","Jihye Kim","Joonghoon Kim","Minjae Kim","Minsub Kim","Pil Hwan Kim","Sammy Kim","Seokhun Kim","Seonghyeon Kim","Soojin Kim","Soong Kim","Soyoon Kim","Sunyoung Kim","Taeho Kim","Wonho Kim","Yoonsik Kim","You Jin Kim","Yuri Kim","Beomseok Kwon","Ohsung Kwon","Yoo-Hwan Kwon","Anna Lee","Byungwook Lee","Changho Lee","Daun Lee","Dongjae Lee","Ha-Ram Lee","Hodong Lee","Hwiyeong Lee","Hyunmi Lee","Injae Lee","Jaeung Lee","Jeongsang Lee","Jisoo Lee","Jongsoo Lee","Joongjae Lee","Juhan Lee","Jung Hyun Lee","Junghoon Lee","Junwoo Lee","Se Yun Lee","Sujin Lee","Sungjae Lee","Sungwoo Lee","Wonjae Lee","Zoo Hyun Lee","Jong Kun Lim","Kun Lim","Taemin Lim","Nuri Na","Jeongyeon Nam","Kyeong-Min Nam","Yeonseog Noh","Biro Oh","Jung-Sik Oh","Solgil Oh","Yeontaek Oh","Boyoun Park","Cheonbok Park","Dongju Park","Hyeonjin Park","Hyun Tae Park","Hyunjung Park","Jihye Park","Jooseok Park","Junghwan Park","Jungsoo Park","Miru Park","Sang Hee Park","Seunghyun Park","Soyoung Park","Taerim Park","Wonkyeong Park","Hyunjoon Ryu","Jeonghun Ryu","Nahyeon Ryu","Soonshin Seo","Suk Min Seo","Yoonjeong Shim","Kyuyong Shin","Wonkwang Shin","Hyun Sim","Woongseob Sim","Hyejin Soh","Bokyong Son","Hyunjun Son","Seulah Son","Chi-Yun Song","Chiyoung Song","Ka Yeon Song","Minchul Song","Seungmin Song","Jisung Wang","Yonggoo Yeo","Myeong Yeon Yi","Moon Bin Yim","Taehwan Yoo","Youngjoon Yoo","Sungmin Yoon","Young Jin Yoon","Hangyeol Yu","Ui Seon Yu","Xingdong Zuo","Jeongin Bae","Joungeun Bae","Hyunsoo Cho","Seonghyun Cho","Yongjin Cho","Taekyoon Choi","Yera Choi","Jiwan Chung","Zhenghui Han","Byeongho Heo","Euisuk Hong","Taebaek Hwang","Seonyeol Im","Sumin Jegal","Sumin Jeon","Yelim Jeong","Yonghyun Jeong","Can Jiang","Juyong Jiang","Jiho Jin","Ara Jo","Younghyun Jo","Hoyoun Jung","Juyoung Jung","Seunghyeong Kang","Dae Hee Kim","Ginam Kim","Hangyeol Kim","Heeseung Kim","Hyojin Kim","Hyojun Kim","Hyun-Ah Kim","Jeehye Kim","Jin-Hwa Kim","Jiseon Kim","Jonghak Kim","Jung Yoon Kim","Rak Yeong Kim","Seongjin Kim","Seoyoon Kim","Sewon Kim","Sooyoung Kim","Sukyoung Kim","Taeyong Kim","Naeun Ko","Bonseung Koo","Heeyoung Kwak","Haena Kwon","Youngjin Kwon","Boram Lee","Bruce W. Lee","Dagyeong Lee","Erin Lee","Euijin Lee","Ha Gyeong Lee","Hyojin Lee","Hyunjeong Lee","Jeeyoon Lee","Jeonghyun Lee","Jongheok Lee","Joonhyung Lee","Junhyuk Lee","Mingu Lee","Nayeon Lee","Sangkyu Lee","Se Young Lee","Seulgi Lee","Seung Jin Lee","Suhyeon Lee","Yeonjae Lee","Yesol Lee","Youngbeom Lee","Yujin Lee","Shaodong Li","Tianyu Liu","Seong-Eun Moon","Taehong Moon","Max-Lasse Nihlenramstroem","Wonseok Oh","Yuri Oh","Hongbeen Park","Hyekyung Park","Jaeho Park","Nohil Park","Sangjin Park","Jiwon Ryu","Miru Ryu","Simo Ryu","Ahreum Seo","Hee Seo","Kangdeok Seo","Jamin Shin","Seungyoun Shin","Heetae Sin","Jiangping Wang","Lei Wang","Ning Xiang","Longxiang Xiao","Jing Xu","Seonyeong Yi","Haanju Yoo","Haneul Yoo","Hwanhee Yoo","Liang Yu","Youngjae Yu","Weijie Yuan","Bo Zeng","Qian Zhou","Kyunghyun Cho","Jung-Woo Ha","Joonsuk Park","Jihyun Hwang","Hyoung Jo Kwon","Soonyong Kwon","Jungyeon Lee","Seungho Lee","Seonghyeon Lim","Hyunkyung Noh","Seungho Choi","Sang-Woo Lee","Jung Hwa Lim","Nako Sung"],"pdf_url":"https://arxiv.org/pdf/2404.01954v2.pdf","comment":"44 pages; updated authors list and fixed author names"},{"id":"http://arxiv.org/abs/2404.09022v1","updated":"2024-04-13T15:03:03Z","published":"2024-04-13T15:03:03Z","title":"Navigating the Landscape of Large Language Models: A Comprehensive\n  Review and Analysis of Paradigms and Fine-Tuning Strategies","summary":"  With the surge of ChatGPT,the use of large models has significantly\nincreased,rapidly rising to prominence across the industry and sweeping across\nthe internet. This article is a comprehensive review of fine-tuning methods for\nlarge models. This paper investigates the latest technological advancements and\nthe application of advanced methods in aspects such as task-adaptive\nfine-tuning,domain-adaptive fine-tuning,few-shot learning,knowledge\ndistillation,multi-task learning,parameter-efficient fine-tuning,and dynamic\nfine-tuning.\n","authors":["Benjue Weng"],"pdf_url":"https://arxiv.org/pdf/2404.09022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08788v2","updated":"2024-04-13T14:41:24Z","published":"2023-11-15T09:01:55Z","title":"X-Eval: Generalizable Multi-aspect Text Evaluation via Augmented\n  Instruction Tuning with Auxiliary Evaluation Aspects","summary":"  Natural Language Generation (NLG) typically involves evaluating the generated\ntext in various aspects (e.g., consistency and naturalness) to obtain a\ncomprehensive assessment. However, multi-aspect evaluation remains challenging\nas it may require the evaluator to generalize to any given evaluation aspect\neven if it's absent during training. In this paper, we introduce X-Eval, a\ntwo-stage instruction tuning framework to evaluate the text in both seen and\nunseen aspects customized by end users. X-Eval consists of two learning stages:\nthe vanilla instruction tuning stage that improves the model's ability to\nfollow evaluation instructions, and an enhanced instruction tuning stage that\nexploits the connections between fine-grained evaluation aspects to better\nassess text quality. To support the training of X-Eval, we collect\nAspectInstruct, the first instruction tuning dataset tailored for multi-aspect\nNLG evaluation spanning 27 diverse evaluation aspects with 65 tasks. To enhance\ntask diversity, we devise an augmentation strategy that converts human rating\nannotations into diverse forms of NLG evaluation tasks, including scoring,\ncomparison, ranking, and Boolean question answering. Extensive experiments\nacross three essential categories of NLG tasks: dialogue generation,\nsummarization, and data-to-text coupled with 21 aspects in meta-evaluation,\ndemonstrate that our X-Eval enables even a lightweight language model to\nachieve a comparable if not higher correlation with human judgments compared to\nthe state-of-the-art NLG evaluators, such as GPT-4.\n","authors":["Minqian Liu","Ying Shen","Zhiyang Xu","Yixin Cao","Eunah Cho","Vaibhav Kumar","Reza Ghanadan","Lifu Huang"],"pdf_url":"https://arxiv.org/pdf/2311.08788v2.pdf","comment":"NAACL 2024 Main Conference. 20 pages, 6 figures, 17 tables"},{"id":"http://arxiv.org/abs/2404.07001v2","updated":"2024-04-13T13:58:44Z","published":"2024-04-10T13:31:07Z","title":"Event Grounded Criminal Court View Generation with Cooperative (Large)\n  Language Models","summary":"  With the development of legal intelligence, Criminal Court View Generation\nhas attracted much attention as a crucial task of legal intelligence, which\naims to generate concise and coherent texts that summarize case facts and\nprovide explanations for verdicts. Existing researches explore the key\ninformation in case facts to yield the court views. Most of them employ a\ncoarse-grained approach that partitions the facts into broad segments (e.g.,\nverdict-related sentences) to make predictions. However, this approach fails to\ncapture the complex details present in the case facts, such as various criminal\nelements and legal events. To this end, in this paper, we propose an Event\nGrounded Generation (EGG) method for criminal court view generation with\ncooperative (Large) Language Models, which introduces the fine-grained event\ninformation into the generation. Specifically, we first design a LLMs-based\nextraction method that can extract events in case facts without massive\nannotated events. Then, we incorporate the extracted events into court view\ngeneration by merging case facts and events. Besides, considering the\ncomputational burden posed by the use of LLMs in the extraction phase of EGG,\nwe propose a LLMs-free EGG method that can eliminate the requirement for event\nextraction using LLMs in the inference phase. Extensive experimental results on\na real-world dataset clearly validate the effectiveness of our proposed method.\n","authors":["Linan Yue","Qi Liu","Lili Zhao","Li Wang","Weibo Gao","Yanqing An"],"pdf_url":"https://arxiv.org/pdf/2404.07001v2.pdf","comment":"Accepted to SIGIR2024"},{"id":"http://arxiv.org/abs/2404.07922v2","updated":"2024-04-13T13:57:51Z","published":"2024-04-11T17:09:28Z","title":"LaVy: Vietnamese Multimodal Large Language Model","summary":"  Large Language Models (LLMs) and Multimodal Large language models (MLLMs)\nhave taken the world by storm with impressive abilities in complex reasoning\nand linguistic comprehension. Meanwhile there are plethora of works related to\nVietnamese Large Language Models, the lack of high-quality resources in\nmultimodality limits the progress of Vietnamese MLLMs. In this paper, we\npioneer in address this by introducing LaVy, a state-of-the-art Vietnamese\nMLLM, and we also introduce LaVy-Bench benchmark designated for evaluating\nMLLMs's understanding on Vietnamese visual language tasks. All code and model\nweights are public at https://github.com/baochi0212/LaVy\n","authors":["Chi Tran","Huong Le Thanh"],"pdf_url":"https://arxiv.org/pdf/2404.07922v2.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2311.04917v2","updated":"2024-04-13T13:52:01Z","published":"2023-11-02T08:39:45Z","title":"Adapting Fake News Detection to the Era of Large Language Models","summary":"  In the age of large language models (LLMs) and the widespread adoption of\nAI-driven content creation, the landscape of information dissemination has\nwitnessed a paradigm shift. With the proliferation of both human-written and\nmachine-generated real and fake news, robustly and effectively discerning the\nveracity of news articles has become an intricate challenge. While substantial\nresearch has been dedicated to fake news detection, this either assumes that\nall news articles are human-written or abruptly assumes that all\nmachine-generated news are fake. Thus, a significant gap exists in\nunderstanding the interplay between machine-(paraphrased) real news,\nmachine-generated fake news, human-written fake news, and human-written real\nnews. In this paper, we study this gap by conducting a comprehensive evaluation\nof fake news detectors trained in various scenarios. Our primary objectives\nrevolve around the following pivotal question: How to adapt fake news detectors\nto the era of LLMs? Our experiments reveal an interesting pattern that\ndetectors trained exclusively on human-written articles can indeed perform well\nat detecting machine-generated fake news, but not vice versa. Moreover, due to\nthe bias of detectors against machine-generated texts \\cite{su2023fake}, they\nshould be trained on datasets with a lower machine-generated news ratio than\nthe test set. Building on our findings, we provide a practical strategy for the\ndevelopment of robust fake news detectors.\n","authors":["Jinyan Su","Claire Cardie","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2311.04917v2.pdf","comment":"Accept to NAACL 2024 Findings"},{"id":"http://arxiv.org/abs/2403.14472v3","updated":"2024-04-13T13:39:50Z","published":"2024-03-21T15:18:30Z","title":"Detoxifying Large Language Models via Knowledge Editing","summary":"  This paper investigates using knowledge editing techniques to detoxify Large\nLanguage Models (LLMs). We construct a benchmark, SafeEdit, which covers nine\nunsafe categories with various powerful attack prompts and equips comprehensive\nmetrics for systematic evaluation. We conduct experiments with several\nknowledge editing approaches, indicating that knowledge editing has the\npotential to efficiently detoxify LLMs with limited impact on general\nperformance. Then, we propose a simple yet effective baseline, dubbed\nDetoxifying with Intraoperative Neural Monitoring (DINM), to diminish the\ntoxicity of LLMs within a few tuning steps via only one instance. We further\nprovide an in-depth analysis of the internal mechanism for various detoxifying\napproaches, demonstrating that previous methods like SFT and DPO may merely\nsuppress the activations of toxic parameters, while DINM mitigates the toxicity\nof the toxic parameters to a certain extent, making permanent adjustments. We\nhope that these insights could shed light on future work of developing\ndetoxifying approaches and the underlying knowledge mechanisms of LLMs. Code\nand benchmark are available at https://github.com/zjunlp/EasyEdit.\n","authors":["Mengru Wang","Ningyu Zhang","Ziwen Xu","Zekun Xi","Shumin Deng","Yunzhi Yao","Qishen Zhang","Linyi Yang","Jindong Wang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14472v3.pdf","comment":"Ongoing work. Project website:\n  https://zjunlp.github.io/project/SafeEdit Add and update experimental results\n  in Tables 1 and 3"},{"id":"http://arxiv.org/abs/2404.09002v1","updated":"2024-04-13T13:07:32Z","published":"2024-04-13T13:07:32Z","title":"WikiSplit++: Easy Data Refinement for Split and Rephrase","summary":"  The task of Split and Rephrase, which splits a complex sentence into multiple\nsimple sentences with the same meaning, improves readability and enhances the\nperformance of downstream tasks in natural language processing (NLP). However,\nwhile Split and Rephrase can be improved using a text-to-text generation\napproach that applies encoder-decoder models fine-tuned with a large-scale\ndataset, it still suffers from hallucinations and under-splitting. To address\nthese issues, this paper presents a simple and strong data refinement approach.\nHere, we create WikiSplit++ by removing instances in WikiSplit where complex\nsentences do not entail at least one of the simpler sentences and reversing the\norder of reference simple sentences. Experimental results show that training\nwith WikiSplit++ leads to better performance than training with WikiSplit, even\nwith fewer training instances. In particular, our approach yields significant\ngains in the number of splits and the entailment ratio, a proxy for measuring\nhallucinations.\n","authors":["Hayato Tsukagoshi","Tsutomu Hirao","Makoto Morishita","Katsuki Chousa","Ryohei Sasano","Koichi Takeda"],"pdf_url":"https://arxiv.org/pdf/2404.09002v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2404.08997v1","updated":"2024-04-13T12:51:53Z","published":"2024-04-13T12:51:53Z","title":"Labeled Morphological Segmentation with Semi-Markov Models","summary":"  We present labeled morphological segmentation, an alternative view of\nmorphological processing that unifies several tasks. From an annotation\nstandpoint, we additionally introduce a new hierarchy of morphotactic tagsets.\nFinally, we develop \\modelname, a discriminative morphological segmentation\nsystem that, contrary to previous work, explicitly models morphotactics. We\nshow that \\textsc{chipmunk} yields improved performance on three tasks for all\nsix languages: (i) morphological segmentation, (ii) stemming and (iii)\nmorphological tag classification. On morphological segmentation, our method\nshows absolute improvements of 2--6 points $F_1$ over the baseline.\n","authors":["Ryan Cotterell","Thomas M√ºller","Alexander Fraser","Hinrich Sch√ºtze"],"pdf_url":"https://arxiv.org/pdf/2404.08997v1.pdf","comment":"CoNLL 2015"},{"id":"http://arxiv.org/abs/2404.08977v1","updated":"2024-04-13T11:58:28Z","published":"2024-04-13T11:58:28Z","title":"RoNID: New Intent Discovery with Generated-Reliable Labels and\n  Cluster-friendly Representations","summary":"  New Intent Discovery (NID) strives to identify known and reasonably deduce\nnovel intent groups in the open-world scenario. But current methods face issues\nwith inaccurate pseudo-labels and poor representation learning, creating a\nnegative feedback loop that degrades overall model performance, including\naccuracy and the adjusted rand index. To address the aforementioned challenges,\nwe propose a Robust New Intent Discovery (RoNID) framework optimized by an\nEM-style method, which focuses on constructing reliable pseudo-labels and\nobtaining cluster-friendly discriminative representations. RoNID comprises two\nmain modules: reliable pseudo-label generation module and cluster-friendly\nrepresentation learning module. Specifically, the pseudo-label generation\nmodule assigns reliable synthetic labels by solving an optimal transport\nproblem in the E-step, which effectively provides high-quality supervised\nsignals for the input of the cluster-friendly representation learning module.\nTo learn cluster-friendly representation with strong intra-cluster compactness\nand large inter-cluster separation, the representation learning module combines\nintra-cluster and inter-cluster contrastive learning in the M-step to feed more\ndiscriminative features into the generation module. RoNID can be performed\niteratively to ultimately yield a robust model with reliable pseudo-labels and\ncluster-friendly representations. Experimental results on multiple benchmarks\ndemonstrate our method brings substantial improvements over previous\nstate-of-the-art methods by a large margin of +1~+4 points.\n","authors":["Shun Zhang","Chaoran Yan","Jian Yang","Changyu Ren","Jiaqi Bai","Tongliang Li","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2404.08977v1.pdf","comment":"DASFAA 2024"},{"id":"http://arxiv.org/abs/2404.08974v1","updated":"2024-04-13T11:40:06Z","published":"2024-04-13T11:40:06Z","title":"OOVs in the Spotlight: How to Inflect them?","summary":"  We focus on morphological inflection in out-of-vocabulary (OOV) conditions,\nan under-researched subtask in which state-of-the-art systems usually are less\neffective. We developed three systems: a retrograde model and two\nsequence-to-sequence (seq2seq) models based on LSTM and Transformer. For\ntesting in OOV conditions, we automatically extracted a large dataset of nouns\nin the morphologically rich Czech language, with lemma-disjoint data splits,\nand we further manually annotated a real-world OOV dataset of neologisms. In\nthe standard OOV conditions, Transformer achieves the best results, with\nincreasing performance in ensemble with LSTM, the retrograde model and\nSIGMORPHON baselines. On the real-world OOV dataset of neologisms, the\nretrograde model outperforms all neural models. Finally, our seq2seq models\nachieve state-of-the-art results in 9 out of 16 languages from SIGMORPHON 2022\nshared task data in the OOV evaluation (feature overlap) in the large data\ncondition. We release the Czech OOV Inflection Dataset for rigorous evaluation\nin OOV conditions. Further, we release the inflection system with the seq2seq\nmodels as a ready-to-use Python library.\n","authors":["Tom√°≈° Sourada","Jana Strakov√°","Rudolf Rosa"],"pdf_url":"https://arxiv.org/pdf/2404.08974v1.pdf","comment":"To be published in LREC-COLING 2024. 12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2404.08958v1","updated":"2024-04-13T10:46:11Z","published":"2024-04-13T10:46:11Z","title":"AMU-Tuning: Effective Logit Bias for CLIP-based Few-shot Learning","summary":"  Recently, pre-trained vision-language models (e.g., CLIP) have shown great\npotential in few-shot learning and attracted a lot of research interest.\nAlthough efforts have been made to improve few-shot ability of CLIP, key\nfactors on the effectiveness of existing methods have not been well studied,\nlimiting further exploration of CLIP's potential in few-shot learning. In this\npaper, we first introduce a unified formulation to analyze CLIP-based few-shot\nlearning methods from a perspective of logit bias, which encourages us to learn\nan effective logit bias for further improving performance of CLIP-based\nfew-shot learning methods. To this end, we disassemble three key components\ninvolved in computation of logit bias (i.e., logit features, logit predictor,\nand logit fusion) and empirically analyze the effect on performance of few-shot\nclassification. Based on analysis of key components, this paper proposes a\nnovel AMU-Tuning method to learn effective logit bias for CLIP-based few-shot\nclassification. Specifically, our AMU-Tuning predicts logit bias by exploiting\nthe appropriate $\\underline{\\textbf{A}}$uxiliary features, which are fed into\nan efficient feature-initialized linear classifier with\n$\\underline{\\textbf{M}}$ulti-branch training. Finally, an\n$\\underline{\\textbf{U}}$ncertainty-based fusion is developed to incorporate\nlogit bias into CLIP for few-shot classification. The experiments are conducted\non several widely used benchmarks, and the results show AMU-Tuning clearly\noutperforms its counterparts while achieving state-of-the-art performance of\nCLIP-based few-shot learning without bells and whistles.\n","authors":["Yuwei Tang","Zhenyi Lin","Qilong Wang","Pengfei Zhu","Qinghua Hu"],"pdf_url":"https://arxiv.org/pdf/2404.08958v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2404.08949v1","updated":"2024-04-13T10:01:58Z","published":"2024-04-13T10:01:58Z","title":"Multimodal Cross-Document Event Coreference Resolution Using Linear\n  Semantic Transfer and Mixed-Modality Ensembles","summary":"  Event coreference resolution (ECR) is the task of determining whether\ndistinct mentions of events within a multi-document corpus are actually linked\nto the same underlying occurrence. Images of the events can help facilitate\nresolution when language is ambiguous. Here, we propose a multimodal\ncross-document event coreference resolution method that integrates visual and\ntextual cues with a simple linear map between vision and language models. As\nexisting ECR benchmark datasets rarely provide images for all event mentions,\nwe augment the popular ECB+ dataset with event-centric images scraped from the\ninternet and generated using image diffusion models. We establish three methods\nthat incorporate images and text for coreference: 1) a standard fused model\nwith finetuning, 2) a novel linear mapping method without finetuning and 3) an\nensembling approach based on splitting mention pairs by semantic and\ndiscourse-level difficulty. We evaluate on 2 datasets: the augmented ECB+, and\nAIDA Phase 1. Our ensemble systems using cross-modal linear mapping establish\nan upper limit (91.9 CoNLL F1) on ECB+ ECR performance given the preprocessing\nassumptions used, and establish a novel baseline on AIDA Phase 1. Our results\ndemonstrate the utility of multimodal information in ECR for certain\nchallenging coreference problems, and highlight a need for more multimodal\nresources in the coreference resolution space.\n","authors":["Abhijnan Nath","Huma Jamil","Shafiuddin Rehan Ahmed","George Baker","Rahul Ghosh","James H. Martin","Nathaniel Blanchard","Nikhil Krishnaswamy"],"pdf_url":"https://arxiv.org/pdf/2404.08949v1.pdf","comment":"To appear at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2404.08940v1","updated":"2024-04-13T09:33:00Z","published":"2024-04-13T09:33:00Z","title":"Introducing Super RAGs in Mistral 8x7B-v1","summary":"  The relentless pursuit of enhancing Large Language Models (LLMs) has led to\nthe advent of Super Retrieval-Augmented Generation (Super RAGs), a novel\napproach designed to elevate the performance of LLMs by integrating external\nknowledge sources with minimal structural modifications. This paper presents\nthe integration of Super RAGs into the Mistral 8x7B v1, a state-of-the-art LLM,\nand examines the resultant improvements in accuracy, speed, and user\nsatisfaction. Our methodology uses a fine-tuned instruct model setup and a\ncache tuning fork system, ensuring efficient and relevant data retrieval. The\nevaluation, conducted over several epochs, demonstrates significant\nenhancements across all metrics. The findings suggest that Super RAGs can\neffectively augment LLMs, paving the way for more sophisticated and reliable AI\nsystems. This research contributes to the field by providing empirical evidence\nof the benefits of Super RAGs and offering insights into their potential\napplications.\n","authors":["Ayush Thakur","Raghav Gupta"],"pdf_url":"https://arxiv.org/pdf/2404.08940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08938v1","updated":"2024-04-13T09:24:32Z","published":"2024-04-13T09:24:32Z","title":"Enforcing Paraphrase Generation via Controllable Latent Diffusion","summary":"  Paraphrase generation aims to produce high-quality and diverse utterances of\na given text. Though state-of-the-art generation via the diffusion model\nreconciles generation quality and diversity, textual diffusion suffers from a\ntruncation issue that hinders efficiency and quality control. In this work, we\npropose \\textit{L}atent \\textit{D}iffusion \\textit{P}araphraser~(LDP), a novel\nparaphrase generation by modeling a controllable diffusion process given a\nlearned latent space. LDP achieves superior generation efficiency compared to\nits diffusion counterparts. It facilitates only input segments to enforce\nparaphrase semantics, which further improves the results without external\nfeatures. Experiments show that LDP achieves improved and diverse paraphrase\ngeneration compared to baselines. Further analysis shows that our method is\nalso helpful to other similar text generations and domain adaptations. Our code\nand data are available at https://github.com/NIL-zhuang/ld4pg.\n","authors":["Wei Zou","Ziyuan Zhuang","Shujian Huang","Jia Liu","Jiajun Chen"],"pdf_url":"https://arxiv.org/pdf/2404.08938v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09625v2","updated":"2024-04-13T08:51:33Z","published":"2023-12-15T09:08:14Z","title":"Weakly-Supervised 3D Visual Grounding based on Visual Linguistic\n  Alignment","summary":"  Learning to ground natural language queries to target objects or regions in\n3D point clouds is quite essential for 3D scene understanding. Nevertheless,\nexisting 3D visual grounding approaches require a substantial number of\nbounding box annotations for text queries, which is time-consuming and\nlabor-intensive to obtain. In this paper, we propose \\textbf{3D-VLA}, a weakly\nsupervised approach for \\textbf{3D} visual grounding based on \\textbf{V}isual\n\\textbf{L}inguistic \\textbf{A}lignment. Our 3D-VLA exploits the superior\nability of current large-scale vision-language models (VLMs) on aligning the\nsemantics between texts and 2D images, as well as the naturally existing\ncorrespondences between 2D images and 3D point clouds, and thus implicitly\nconstructs correspondences between texts and 3D point clouds with no need for\nfine-grained box annotations in the training procedure. During the inference\nstage, the learned text-3D correspondence will help us ground the text queries\nto the 3D target objects even without 2D images. To the best of our knowledge,\nthis is the first work to investigate 3D visual grounding in a weakly\nsupervised manner by involving large scale vision-language models, and\nextensive experiments on ReferIt3D and ScanRefer datasets demonstrate that our\n3D-VLA achieves comparable and even superior results over the fully supervised\nmethods.\n","authors":["Xiaoxu Xu","Yitian Yuan","Qiudan Zhang","Wenhui Wu","Zequn Jie","Lin Ma","Xu Wang"],"pdf_url":"https://arxiv.org/pdf/2312.09625v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14130v2","updated":"2024-04-13T08:06:37Z","published":"2023-09-25T13:35:28Z","title":"On the Relation between Internal Language Model and Sequence\n  Discriminative Training for Neural Transducers","summary":"  Internal language model (ILM) subtraction has been widely applied to improve\nthe performance of the RNN-Transducer with external language model (LM) fusion\nfor speech recognition. In this work, we show that sequence discriminative\ntraining has a strong correlation with ILM subtraction from both theoretical\nand empirical points of view. Theoretically, we derive that the global optimum\nof maximum mutual information (MMI) training shares a similar formula as ILM\nsubtraction. Empirically, we show that ILM subtraction and sequence\ndiscriminative training achieve similar effects across a wide range of\nexperiments on Librispeech, including both MMI and minimum Bayes risk (MBR)\ncriteria, as well as neural transducers and LMs of both full and limited\ncontext. The benefit of ILM subtraction also becomes much smaller after\nsequence discriminative training. We also provide an in-depth study to show\nthat sequence discriminative training has a minimal effect on the commonly used\nzero-encoder ILM estimation, but a joint effect on both encoder and prediction\n+ joint network for posterior probability reshaping including both ILM and\nblank suppression.\n","authors":["Zijian Yang","Wei Zhou","Ralf Schl√ºter","Hermann Ney"],"pdf_url":"https://arxiv.org/pdf/2309.14130v2.pdf","comment":"accepted at ICASSP 2024"},{"id":"http://arxiv.org/abs/2404.07009v2","updated":"2024-04-13T06:43:47Z","published":"2024-04-10T13:50:46Z","title":"A Mathematical Theory for Learning Semantic Languages by Abstract\n  Learners","summary":"  Recent advances in Large Language Models (LLMs) have demonstrated the\nemergence of capabilities (learned skills) when the number of system parameters\nand the size of training data surpass certain thresholds. The exact mechanisms\nbehind such phenomena are not fully understood and remain a topic of active\nresearch. Inspired by the skill-text bipartite graph model presented in [1] for\nmodeling semantic language, we develop a mathematical theory to explain the\nemergence of learned skills, taking the learning (or training) process into\naccount. Our approach models the learning process for skills in the skill-text\nbipartite graph as an iterative decoding process in Low-Density Parity Check\n(LDPC) codes and Irregular Repetition Slotted ALOHA (IRSA). Using density\nevolution analysis, we demonstrate the emergence of learned skills when the\nratio of the size of training texts to the number of skills exceeds a certain\nthreshold. Our analysis also yields a scaling law for testing errors relative\nto the size of training texts. Upon completion of the training, we propose a\nmethod for semantic compression and discuss its application in semantic\ncommunication.\n","authors":["Kuo-Yu Liao","Cheng-Shang Chang","Y. -W. Peter Hong"],"pdf_url":"https://arxiv.org/pdf/2404.07009v2.pdf","comment":"V1 was submitted to ISIT 2024 on Jan. 28, 2024. V2 was uploaded to\n  ArXiv on April 13, 2024"},{"id":"http://arxiv.org/abs/2404.06390v2","updated":"2024-04-13T05:20:45Z","published":"2024-04-09T15:33:09Z","title":"Latent Distance Guided Alignment Training for Large Language Models","summary":"  Ensuring alignment with human preferences is a crucial characteristic of\nlarge language models (LLMs). Presently, the primary alignment methods, RLHF\nand DPO, require extensive human annotation, which is expensive despite their\nefficacy. The significant expenses associated with current alignment techniques\nmotivate researchers to investigate the development of annotation-free\nalignment training methods. In pursuit of improved alignment without relying on\nexternal annotation, we introduce Latent Distance Guided Alignment Training\n(LD-Align). This approach seeks to align the model with a high-quality\nsupervised fine-tune dataset using guidance from a latent space. The latent\nspace is generated through sample reconstruction, akin to auto-encoding.\nConsequently, we utilize the distance between sample pairs in the latent space\nto guide DPO-based alignment training. Extensive experimentation and evaluation\nshow the efficacy of our proposed method in achieving notable alignment.\n","authors":["Haotian Luo"],"pdf_url":"https://arxiv.org/pdf/2404.06390v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08888v1","updated":"2024-04-13T03:23:15Z","published":"2024-04-13T03:23:15Z","title":"Towards Enhancing Health Coaching Dialogue in Low-Resource Settings","summary":"  Health coaching helps patients identify and accomplish lifestyle-related\ngoals, effectively improving the control of chronic diseases and mitigating\nmental health conditions. However, health coaching is cost-prohibitive due to\nits highly personalized and labor-intensive nature. In this paper, we propose\nto build a dialogue system that converses with the patients, helps them create\nand accomplish specific goals, and can address their emotions with empathy.\nHowever, building such a system is challenging since real-world health coaching\ndatasets are limited and empathy is subtle. Thus, we propose a modularized\nhealth coaching dialogue system with simplified NLU and NLG frameworks combined\nwith mechanism-conditioned empathetic response generation. Through automatic\nand human evaluation, we show that our system generates more empathetic,\nfluent, and coherent responses and outperforms the state-of-the-art in NLU\ntasks while requiring less annotation. We view our approach as a key step\ntowards building automated and more accessible health coaching systems.\n","authors":["Yue Zhou","Barbara Di Eugenio","Brian Ziebart","Lisa Sharp","Bing Liu","Ben Gerber","Nikolaos Agadakos","Shweta Yadav"],"pdf_url":"https://arxiv.org/pdf/2404.08888v1.pdf","comment":"Accepted to the main conference of COLING 2022"},{"id":"http://arxiv.org/abs/2404.08886v1","updated":"2024-04-13T03:15:56Z","published":"2024-04-13T03:15:56Z","title":"EIVEN: Efficient Implicit Attribute Value Extraction using Multimodal\n  LLM","summary":"  In e-commerce, accurately extracting product attribute values from multimodal\ndata is crucial for improving user experience and operational efficiency of\nretailers. However, previous approaches to multimodal attribute value\nextraction often struggle with implicit attribute values embedded in images or\ntext, rely heavily on extensive labeled data, and can easily confuse similar\nattribute values. To address these issues, we introduce EIVEN, a data- and\nparameter-efficient generative framework that pioneers the use of multimodal\nLLM for implicit attribute value extraction. EIVEN leverages the rich inherent\nknowledge of a pre-trained LLM and vision encoder to reduce reliance on labeled\ndata. We also introduce a novel Learning-by-Comparison technique to reduce\nmodel confusion by enforcing attribute value comparison and difference\nidentification. Additionally, we construct initial open-source datasets for\nmultimodal implicit attribute value extraction. Our extensive experiments\nreveal that EIVEN significantly outperforms existing methods in extracting\nimplicit attribute values while requiring less labeled data.\n","authors":["Henry Peng Zou","Gavin Heqing Yu","Ziwei Fan","Dan Bu","Han Liu","Peng Dai","Dongmei Jia","Cornelia Caragea"],"pdf_url":"https://arxiv.org/pdf/2404.08886v1.pdf","comment":"Accepted by NAACL 2024 Industry Track"},{"id":"http://arxiv.org/abs/2404.08885v1","updated":"2024-04-13T03:11:07Z","published":"2024-04-13T03:11:07Z","title":"Is Next Token Prediction Sufficient for GPT? Exploration on Code Logic\n  Comprehension","summary":"  Large language models (LLMs) has experienced exponential growth, they\ndemonstrate remarkable performance across various tasks. Notwithstanding,\ncontemporary research primarily centers on enhancing the size and quality of\npretraining data, still utilizing the next token prediction task on\nautoregressive transformer model structure. The efficacy of this task in truly\nfacilitating the model's comprehension of code logic remains questionable, we\nspeculate that it still interprets code as mere text, while human emphasizes\nthe underlying logical knowledge. In order to prove it, we introduce a new\ntask, \"Logically Equivalent Code Selection,\" which necessitates the selection\nof logically equivalent code from a candidate set, given a query code. Our\nexperimental findings indicate that current LLMs underperform in this task,\nsince they understand code by unordered bag of keywords. To ameliorate their\nperformance, we propose an advanced pretraining task, \"Next Token Prediction+\".\nThis task aims to modify the sentence embedding distribution of the LLM without\nsacrificing its generative capabilities. Our experimental results reveal that\nfollowing this pretraining, both Code Llama and StarCoder, the prevalent code\ndomain pretraining models, display significant improvements on our logically\nequivalent code selection task and the code completion task.\n","authors":["Mengnan Qi","Yufan Huang","Yongqiang Yao","Maoquan Wang","Bin Gu","Neel Sundaresan"],"pdf_url":"https://arxiv.org/pdf/2404.08885v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08877v1","updated":"2024-04-13T02:36:40Z","published":"2024-04-13T02:36:40Z","title":"Aligning LLMs for FL-free Program Repair","summary":"  Large language models (LLMs) have achieved decent results on automated\nprogram repair (APR). However, the next token prediction training objective of\ndecoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction\nobjective of current infilling-style methods, which impedes LLMs from fully\nleveraging pre-trained knowledge for program repair. In addition, while some\nLLMs are capable of locating and repairing bugs end-to-end when using the\nrelated artifacts (e.g., test cases) as input, existing methods regard them as\nseparate tasks and ask LLMs to generate patches at fixed locations. This\nrestriction hinders LLMs from exploring potential patches beyond the given\nlocations.\n  In this paper, we investigate a new approach to adapt LLMs to program repair.\nOur core insight is that LLM's APR capability can be greatly improved by simply\naligning the output to their training objective and allowing them to refine the\nwhole program without first performing fault localization. Based on this\ninsight, we designed D4C, a straightforward prompting framework for APR. D4C\ncan repair 180 bugs correctly in Defects4J, with each patch being sampled only\n10 times. This surpasses the SOTA APR methods with perfect fault localization\nby 10% and reduces the patch sampling number by 90%. Our findings reveal that\n(1) objective alignment is crucial for fully exploiting LLM's pre-trained\ncapability, and (2) replacing the traditional localize-then-repair workflow\nwith direct debugging is more effective for LLM-based APR methods. Thus, we\nbelieve this paper introduces a new mindset for harnessing LLMs in APR.\n","authors":["Junjielong Xu","Ying Fu","Shin Hwei Tan","Pinjia He"],"pdf_url":"https://arxiv.org/pdf/2404.08877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02323v2","updated":"2024-04-13T02:17:01Z","published":"2024-04-02T21:50:18Z","title":"Toward Informal Language Processing: Knowledge of Slang in Large\n  Language Models","summary":"  Recent advancement in large language models (LLMs) has offered a strong\npotential for natural language systems to process informal language. A\nrepresentative form of informal language is slang, used commonly in daily\nconversations and online social media. To date, slang has not been\ncomprehensively evaluated in LLMs due partly to the absence of a carefully\ndesigned and publicly accessible benchmark. Using movie subtitles, we construct\na dataset that supports evaluation on a diverse set of tasks pertaining to\nautomatic processing of slang. For both evaluation and finetuning, we show the\neffectiveness of our dataset on two core applications: 1) slang detection, and\n2) identification of regional and historical sources of slang from natural\nsentences. We also show how our dataset can be used to probe the output\ndistributions of LLMs for interpretive insights. We find that while LLMs such\nas GPT-4 achieve good performance in a zero-shot setting, smaller BERT-like\nmodels finetuned on our dataset achieve comparable performance. Furthermore, we\nshow that our dataset enables finetuning of LLMs such as GPT-3.5 that achieve\nsubstantially better performance than strong zero-shot baselines. Our work\noffers a comprehensive evaluation and a high-quality benchmark on English slang\nbased on the OpenSubtitles corpus, serving both as a publicly accessible\nresource and a platform for applying tools for informal language processing.\n","authors":["Zhewei Sun","Qian Hu","Rahul Gupta","Richard Zemel","Yang Xu"],"pdf_url":"https://arxiv.org/pdf/2404.02323v2.pdf","comment":"Accepted to NAACL 2024 main conference"},{"id":"http://arxiv.org/abs/2401.15770v3","updated":"2024-04-13T01:59:17Z","published":"2024-01-28T21:18:05Z","title":"PILOT: Legal Case Outcome Prediction with Case Law","summary":"  Machine learning shows promise in predicting the outcome of legal cases, but\nmost research has concentrated on civil law cases rather than case law systems.\nWe identified two unique challenges in making legal case outcome predictions\nwith case law. First, it is crucial to identify relevant precedent cases that\nserve as fundamental evidence for judges during decision-making. Second, it is\nnecessary to consider the evolution of legal principles over time, as early\ncases may adhere to different legal contexts. In this paper, we proposed a new\nframework named PILOT (PredictIng Legal case OuTcome) for case outcome\nprediction. It comprises two modules for relevant case retrieval and temporal\npattern handling, respectively. To benchmark the performance of existing legal\ncase outcome prediction models, we curated a dataset from a large-scale case\nlaw database. We demonstrate the importance of accurately identifying precedent\ncases and mitigating the temporal shift when making predictions for case law,\nas our method shows a significant improvement over the prior methods that focus\non civil law case outcome predictions.\n","authors":["Lang Cao","Zifeng Wang","Cao Xiao","Jimeng Sun"],"pdf_url":"https://arxiv.org/pdf/2401.15770v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14540v3","updated":"2024-04-13T01:59:06Z","published":"2023-10-23T03:44:40Z","title":"Evaluating Spatial Understanding of Large Language Models","summary":"  Large language models (LLMs) show remarkable capabilities across a variety of\ntasks. Despite the models only seeing text in training, several recent studies\nsuggest that LLM representations implicitly capture aspects of the underlying\ngrounded concepts. Here, we explore LLM representations of a particularly\nsalient kind of grounded knowledge -- spatial relationships. We design\nnatural-language navigation tasks and evaluate the ability of LLMs, in\nparticular GPT-3.5-turbo, GPT-4, and Llama2 series models, to represent and\nreason about spatial structures. These tasks reveal substantial variability in\nLLM performance across different spatial structures, including square,\nhexagonal, and triangular grids, rings, and trees. In extensive error analysis,\nwe find that LLMs' mistakes reflect both spatial and non-spatial factors. These\nfindings suggest that LLMs appear to capture certain aspects of spatial\nstructure implicitly, but room for improvement remains.\n","authors":["Yutaro Yamada","Yihan Bao","Andrew K. Lampinen","Jungo Kasai","Ilker Yildirim"],"pdf_url":"https://arxiv.org/pdf/2310.14540v3.pdf","comment":"Accepted to TMLR 2024. Our code and data are available at\n  https://github.com/runopti/SpatialEvalLLM,\n  https://huggingface.co/datasets/yyamada/SpatialEvalLLM"},{"id":"http://arxiv.org/abs/2404.08865v1","updated":"2024-04-13T01:13:59Z","published":"2024-04-13T01:13:59Z","title":"LLM In-Context Recall is Prompt Dependent","summary":"  The proliferation of Large Language Models (LLMs) highlights the critical\nimportance of conducting thorough evaluations to discern their comparative\nadvantages, limitations, and optimal use cases. Particularly important is\nassessing their capacity to accurately retrieve information included in a given\nprompt. A model's ability to do this significantly influences how effectively\nit can utilize contextual details, thus impacting its practical efficacy and\ndependability in real-world applications.\n  Our research analyzes the in-context recall performance of various LLMs using\nthe needle-in-a-haystack method. In this approach, a factoid (the \"needle\") is\nembedded within a block of filler text (the \"haystack\"), which the model is\nasked to retrieve. We assess the recall performance of each model across\nvarious haystack lengths and with varying needle placements to identify\nperformance patterns. This study demonstrates that an LLM's recall capability\nis not only contingent upon the prompt's content but also may be compromised by\nbiases in its training data. Conversely, adjustments to model architecture,\ntraining strategy, or fine-tuning can improve performance. Our analysis\nprovides insight into LLM behavior, offering direction for the development of\nmore effective applications of LLMs.\n","authors":["Daniel Machlab","Rick Battle"],"pdf_url":"https://arxiv.org/pdf/2404.08865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.15432v2","updated":"2024-04-13T01:05:05Z","published":"2023-07-28T09:29:42Z","title":"CFN-ESA: A Cross-Modal Fusion Network with Emotion-Shift Awareness for\n  Dialogue Emotion Recognition","summary":"  Multimodal emotion recognition in conversation (ERC) has garnered growing\nattention from research communities in various fields. In this paper, we\npropose a Cross-modal Fusion Network with Emotion-Shift Awareness (CFN-ESA) for\nERC. Extant approaches employ each modality equally without distinguishing the\namount of emotional information in these modalities, rendering it hard to\nadequately extract complementary information from multimodal data. To cope with\nthis problem, in CFN-ESA, we treat textual modality as the primary source of\nemotional information, while visual and acoustic modalities are taken as the\nsecondary sources. Besides, most multimodal ERC models ignore emotion-shift\ninformation and overfocus on contextual information, leading to the failure of\nemotion recognition under emotion-shift scenario. We elaborate an emotion-shift\nmodule to address this challenge. CFN-ESA mainly consists of unimodal encoder\n(RUME), cross-modal encoder (ACME), and emotion-shift module (LESM). RUME is\napplied to extract conversation-level contextual emotional cues while pulling\ntogether data distributions between modalities; ACME is utilized to perform\nmultimodal interaction centered on textual modality; LESM is used to model\nemotion shift and capture emotion-shift information, thereby guiding the\nlearning of the main task. Experimental results demonstrate that CFN-ESA can\neffectively promote performance for ERC and remarkably outperform\nstate-of-the-art models.\n","authors":["Jiang Li","Xiaoping Wang","Yingjian Liu","Zhigang Zeng"],"pdf_url":"https://arxiv.org/pdf/2307.15432v2.pdf","comment":"Accepted by IEEE Transactions on Affective Computing (TAFFC)"},{"id":"http://arxiv.org/abs/2402.01643v2","updated":"2024-04-13T00:14:21Z","published":"2023-12-21T01:47:49Z","title":"L-TUNING: Synchronized Label Tuning for Prompt and Prefix in LLMs","summary":"  Efficiently fine-tuning Large Language Models (LLMs) for specific tasks\npresents a considerable challenge in natural language processing. Traditional\nmethods, like prompt or prefix tuning, typically rely on arbitrary tokens for\ntraining, leading to prolonged training times and generalized token use across\nvarious class labels. To address these issues, this paper introduces L-Tuning,\nan efficient fine-tuning approach designed for classification tasks within the\nNatural Language Inference (NLI) framework. Diverging from conventional\nmethods, L-Tuning focuses on the fine-tuning of label tokens processed through\na pre-trained LLM, thereby harnessing its pre-existing semantic knowledge. This\ntechnique not only improves the fine-tuning accuracy and efficiency but also\nfacilitates the generation of distinct label embeddings for each class,\nenhancing the model's training nuance. Our experimental results indicate a\nsignificant improvement in training efficiency and classification accuracy with\nL-Tuning compared to traditional approaches, marking a promising advancement in\nfine-tuning LLMs for complex language tasks.\n","authors":["Md. Kowsher","Md. Shohanur Islam Sobuj","Asif Mahmud","Nusrat Jahan Prottasha","Prakash Bhat"],"pdf_url":"https://arxiv.org/pdf/2402.01643v2.pdf","comment":"Published in the ICLR TinyPaper track"},{"id":"http://arxiv.org/abs/2211.13854v5","updated":"2024-04-13T00:14:03Z","published":"2022-11-25T01:37:48Z","title":"ComCLIP: Training-Free Compositional Image and Text Matching","summary":"  Contrastive Language-Image Pretraining (CLIP) has demonstrated great\nzero-shot performance for matching images and text. However, it is still\nchallenging to adapt vision-lanaguage pretrained models like CLIP to\ncompositional image and text matching -- a more challenging image and text\nmatching task requiring the model understanding of compositional word concepts\nand visual components. Towards better compositional generalization in zero-shot\nimage and text matching, in this paper, we study the problem from a causal\nperspective: the erroneous semantics of individual entities are essentially\nconfounders that cause the matching failure. Therefore, we propose a novel\n\\textbf{\\textit{training-free}} compositional CLIP model (ComCLIP). ComCLIP\ndisentangles input images into subjects, objects, and action sub-images and\ncomposes CLIP's vision encoder and text encoder to perform evolving matching\nover compositional text embedding and sub-image embeddings. In this way,\nComCLIP can mitigate spurious correlations introduced by the pretrained CLIP\nmodels and dynamically evaluate the importance of each component. Experiments\non four compositional image-text matching datasets: SVO, ComVG, Winoground, and\nVL-checklist, and two general image-text retrieval datasets: Flick30K, and\nMSCOCO demonstrate the effectiveness of our plug-and-play method, which boosts\nthe \\textbf{\\textit{zero-shot}} inference ability of CLIP, SLIP, and BLIP2 even\nwithout further training or fine-tuning. Our codes can be found at\nhttps://github.com/eric-ai-lab/ComCLIP.\n","authors":["Kenan Jiang","Xuehai He","Ruize Xu","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2211.13854v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08856v1","updated":"2024-04-13T00:02:36Z","published":"2024-04-13T00:02:36Z","title":"On Speculative Decoding for Multimodal Large Language Models","summary":"  Inference with Multimodal Large Language Models (MLLMs) is slow due to their\nlarge-language-model backbone which suffers from memory bandwidth bottleneck\nand generates tokens auto-regressively. In this paper, we explore the\napplication of speculative decoding to enhance the inference efficiency of\nMLLMs, specifically the LLaVA 7B model. We show that a language-only model can\nserve as a good draft model for speculative decoding with LLaVA 7B, bypassing\nthe need for image tokens and their associated processing components from the\ndraft model. Our experiments across three different tasks show that speculative\ndecoding can achieve a memory-bound speedup of up to 2.37$\\times$ using a 115M\nparameter language model that we trained from scratch. Additionally, we\nintroduce a compact LLaVA draft model incorporating an image adapter, which\nshows marginal performance gains in image captioning while maintaining\ncomparable results in other tasks.\n","authors":["Mukul Gagrani","Raghavv Goel","Wonseok Jeon","Junyoung Park","Mingu Lee","Christopher Lott"],"pdf_url":"https://arxiv.org/pdf/2404.08856v1.pdf","comment":"Accepted as a spotlight paper to ELVM workshop at CVPR 2024"},{"id":"http://arxiv.org/abs/2404.07768v2","updated":"2024-04-13T00:02:25Z","published":"2024-04-11T14:06:39Z","title":"Using Letter Positional Probabilities to Assess Word Complexity","summary":"  Word complexity is defined in a number of different ways. Psycholinguistic,\nmorphological and lexical proxies are often used. Human ratings are also used.\nThe problem here is that these proxies do not measure complexity directly, and\nhuman ratings are susceptible to subjective bias. In this study we contend that\nsome form of 'latent complexity' can be approximated by using samples of simple\nand complex words. We use a sample of 'simple' words from primary school\npicture books and a sample of 'complex' words from high school and academic\nsettings. In order to analyse the differences between these classes, we look at\nthe letter positional probabilities (LPPs). We find strong statistical\nassociations between several LPPs and complexity. For example, simple words are\nsignificantly (p<.001) more likely to start with w, b, s, h, g, k, j, t, y or\nf, while complex words are significantly (p<.001) more likely to start with i,\na, e, r, v, u or d. We find similar strong associations for subsequent letter\npositions, with 84 letter-position variables in the first 6 positions being\nsignificant at the p<.001 level. We then use LPPs as variables in creating a\nclassifier which can classify the two classes with an 83% accuracy. We test\nthese findings using a second data set, with 66 LPPs significant (p<.001) in\nthe first 6 positions common to both datasets. We use these 66 variables to\ncreate a classifier that is able to classify a third dataset with an accuracy\nof 70%. Finally, we create a fourth sample by combining the extreme high and\nlow scoring words generated by three classifiers built on the first three\nseparate datasets and use this sample to build a classifier which has an\naccuracy of 97%. We use this to score the four levels of English word groups\nfrom an ESL program.\n","authors":["Michael Dalvean"],"pdf_url":"https://arxiv.org/pdf/2404.07768v2.pdf","comment":"25 Pages, 15 Tables"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2404.09091v1","updated":"2024-04-13T22:18:14Z","published":"2024-04-13T22:18:14Z","title":"Semantic In-Domain Product Identification for Search Queries","summary":"  Accurate explicit and implicit product identification in search queries is\ncritical for enhancing user experiences, especially at a company like Adobe\nwhich has over 50 products and covers queries across hundreds of tools. In this\nwork, we present a novel approach to training a product classifier from user\nbehavioral data. Our semantic model led to >25% relative improvement in CTR\n(click through rate) across the deployed surfaces; a >50% decrease in null\nrate; a 2x increase in the app cards surfaced, which helps drive product\nvisibility.\n","authors":["Sanat Sharma","Jayant Kumar","Twisha Naik","Zhaoyu Lu","Arvind Srikantan","Tracy Holloway King"],"pdf_url":"https://arxiv.org/pdf/2404.09091v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09077v1","updated":"2024-04-13T20:43:46Z","published":"2024-04-13T20:43:46Z","title":"CuriousLLM: Elevating Multi-Document QA with Reasoning-Infused Knowledge\n  Graph Prompting","summary":"  In the field of Question Answering (QA), unifying large language models\n(LLMs) with external databases has shown great success. However, these methods\noften fall short in providing the advanced reasoning needed for complex QA\ntasks. To address these issues, we improve over a novel approach called\nKnowledge Graph Prompting (KGP), which combines knowledge graphs with a\nLLM-based agent to improve reasoning and search accuracy. Nevertheless, the\noriginal KGP framework necessitates costly fine-tuning with large datasets yet\nstill suffers from LLM hallucination. Therefore, we propose a reasoning-infused\nLLM agent to enhance this framework. This agent mimics human curiosity to ask\nfollow-up questions to more efficiently navigate the search. This simple\nmodification significantly boosts the LLM performance in QA tasks without the\nhigh costs and latency associated with the initial KGP framework. Our ultimate\ngoal is to further develop this approach, leading to more accurate, faster, and\ncost-effective solutions in the QA domain.\n","authors":["Zukang Yang","Zixuan Zhu"],"pdf_url":"https://arxiv.org/pdf/2404.09077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.01163v2","updated":"2024-04-13T17:19:21Z","published":"2023-06-01T21:38:50Z","title":"A Multi-Modal Latent-Features based Service Recommendation System for\n  the Social Internet of Things","summary":"  The Social Internet of Things (SIoT), is revolutionizing how we interact with\nour everyday lives. By adding the social dimension to connecting devices, the\nSIoT has the potential to drastically change the way we interact with smart\ndevices. This connected infrastructure allows for unprecedented levels of\nconvenience, automation, and access to information, allowing us to do more with\nless effort. However, this revolutionary new technology also brings an eager\nneed for service recommendation systems. As the SIoT grows in scope and\ncomplexity, it becomes increasingly important for businesses and individuals,\nand SIoT objects alike to have reliable sources for products, services, and\ninformation that are tailored to their specific needs. Few works have been\nproposed to provide service recommendations for SIoT environments. However,\nthese efforts have been confined to only focusing on modeling user-item\ninteractions using contextual information, devices' SIoT relationships, and\ncorrelation social groups but these schemes do not account for latent semantic\nitem-item structures underlying the sparse multi-modal contents in SIoT\nenvironment. In this paper, we propose a latent-based SIoT recommendation\nsystem that learns item-item structures and aggregates multiple modalities to\nobtain latent item graphs which are then used in graph convolutions to inject\nhigh-order affinities into item representations. Experiments showed that the\nproposed recommendation system outperformed state-of-the-art SIoT\nrecommendation methods and validated its efficacy at mining latent\nrelationships from multi-modal features.\n","authors":["Amar Khelloufi","Huansheng Ning","Abdenacer Naouri","Abdelkarim Ben Sada","Attia Qammar","Abdelkader Khalil","Sahraoui Dhelim","Lingfeng Mao"],"pdf_url":"https://arxiv.org/pdf/2306.01163v2.pdf","comment":"Published in IEEE Transactions on Computational Social Systems"},{"id":"http://arxiv.org/abs/2309.10435v4","updated":"2024-04-13T16:32:33Z","published":"2023-09-19T08:54:47Z","title":"Reformulating Sequential Recommendation: Learning Dynamic User Interest\n  with Content-enriched Language Modeling","summary":"  Recommender systems are indispensable in the realm of online applications,\nand sequential recommendation has enjoyed considerable prevalence due to its\ncapacity to encapsulate the dynamic shifts in user interests. However, previous\nsequential modeling methods still have limitations in capturing contextual\ninformation. The primary reason is the lack of understanding of domain-specific\nknowledge and item-related textual content. Fortunately, the emergence of\npowerful language models has unlocked the potential to incorporate extensive\nworld knowledge into recommendation algorithms, enabling them to go beyond\nsimple item attributes and truly understand the world surrounding user\npreferences. To achieve this, we propose LANCER, which leverages the semantic\nunderstanding capabilities of pre-trained language models to generate\npersonalized recommendations. Our approach bridges the gap between language\nmodels and recommender systems, resulting in more human-like recommendations.\nWe demonstrate the effectiveness of our approach through a series of\nexperiments conducted on multiple benchmark datasets, showing promising results\nand providing valuable insights into the influence of our model on sequential\nrecommendation tasks. Furthermore, our experimental codes are publicly\navailable at https://github.com/Gnimixy/lancer.\n","authors":["Junzhe Jiang","Shang Qu","Mingyue Cheng","Qi Liu","Zhiding Liu","Hao Zhang","Rujiao Zhang","Kai Zhang","Rui Li","Jiatong Li","Min Gao"],"pdf_url":"https://arxiv.org/pdf/2309.10435v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.04264v2","updated":"2024-04-13T15:14:10Z","published":"2024-03-17T17:01:45Z","title":"Logic Query of Thoughts: Guiding Large Language Models to Answer Complex\n  Logic Queries with Knowledge Graphs","summary":"  Despite the superb performance in many tasks, large language models (LLMs)\nbear the risk of generating hallucination or even wrong answers when confronted\nwith tasks that demand the accuracy of knowledge. The issue becomes even more\nnoticeable when addressing logic queries that require multiple logic reasoning\nsteps. On the other hand, knowledge graph (KG) based question answering methods\nare capable of accurately identifying the correct answers with the help of\nknowledge graph, yet its accuracy could quickly deteriorate when the knowledge\ngraph itself is sparse and incomplete. It remains a critical challenge on how\nto integrate knowledge graph reasoning with LLMs in a mutually beneficial way\nso as to mitigate both the hallucination problem of LLMs as well as the\nincompleteness issue of knowledge graphs. In this paper, we propose\n'Logic-Query-of-Thoughts' (LGOT) which is the first of its kind to combine LLMs\nwith knowledge graph based logic query reasoning. LGOT seamlessly combines\nknowledge graph reasoning and LLMs, effectively breaking down complex logic\nqueries into easy to answer subquestions. Through the utilization of both\nknowledge graph reasoning and LLMs, it successfully derives answers for each\nsubquestion. By aggregating these results and selecting the highest quality\ncandidate answers for each step, LGOT achieves accurate results to complex\nquestions. Our experimental findings demonstrate substantial performance\nenhancements, with up to 20% improvement over ChatGPT.\n","authors":["Lihui Liu","Zihao Wang","Ruizhong Qiu","Yikun Ban","Eunice Chan","Yangqiu Song","Jingrui He","Hanghang Tong"],"pdf_url":"https://arxiv.org/pdf/2404.04264v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08940v1","updated":"2024-04-13T09:33:00Z","published":"2024-04-13T09:33:00Z","title":"Introducing Super RAGs in Mistral 8x7B-v1","summary":"  The relentless pursuit of enhancing Large Language Models (LLMs) has led to\nthe advent of Super Retrieval-Augmented Generation (Super RAGs), a novel\napproach designed to elevate the performance of LLMs by integrating external\nknowledge sources with minimal structural modifications. This paper presents\nthe integration of Super RAGs into the Mistral 8x7B v1, a state-of-the-art LLM,\nand examines the resultant improvements in accuracy, speed, and user\nsatisfaction. Our methodology uses a fine-tuned instruct model setup and a\ncache tuning fork system, ensuring efficient and relevant data retrieval. The\nevaluation, conducted over several epochs, demonstrates significant\nenhancements across all metrics. The findings suggest that Super RAGs can\neffectively augment LLMs, paving the way for more sophisticated and reliable AI\nsystems. This research contributes to the field by providing empirical evidence\nof the benefits of Super RAGs and offering insights into their potential\napplications.\n","authors":["Ayush Thakur","Raghav Gupta"],"pdf_url":"https://arxiv.org/pdf/2404.08940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08896v1","updated":"2024-04-13T04:20:00Z","published":"2024-04-13T04:20:00Z","title":"Approximate Cluster-Based Sparse Document Retrieval with Segmented\n  Maximum Term Weights","summary":"  This paper revisits cluster-based retrieval that partitions the inverted\nindex into multiple groups and skips the index partially at cluster and\ndocument levels during online inference using a learned sparse representation.\nIt proposes an approximate search scheme with two parameters to control the\nrank-safeness competitiveness of pruning with segmented maximum term weights\nwithin each cluster. Cluster-level maximum weight segmentation allows an\nimprovement in the rank score bound estimation and threshold-based pruning to\nbe approximately adaptive to bound estimation tightness, resulting in better\nrelevance and efficiency. The experiments with MS MARCO passage ranking and\nBEIR datasets demonstrate the usefulness of the proposed scheme with a\ncomparison to the baselines. This paper presents the design of this approximate\nretrieval scheme with rank-safeness analysis, compares clustering and\nsegmentation options, and reports evaluation results.\n","authors":["Yifan Qiao","Shanxiu He","Yingrui Yang","Parker Carlson","Tao Yang"],"pdf_url":"https://arxiv.org/pdf/2404.08896v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08887v1","updated":"2024-04-13T03:17:33Z","published":"2024-04-13T03:17:33Z","title":"Countering Mainstream Bias via End-to-End Adaptive Local Learning","summary":"  Collaborative filtering (CF) based recommendations suffer from mainstream\nbias -- where mainstream users are favored over niche users, leading to poor\nrecommendation quality for many long-tail users. In this paper, we identify two\nroot causes of this mainstream bias: (i) discrepancy modeling, whereby CF\nalgorithms focus on modeling mainstream users while neglecting niche users with\nunique preferences; and (ii) unsynchronized learning, where niche users require\nmore training epochs than mainstream users to reach peak performance. Targeting\nthese causes, we propose a novel end-To-end Adaptive Local Learning (TALL)\nframework to provide high-quality recommendations to both mainstream and niche\nusers. TALL uses a loss-driven Mixture-of-Experts module to adaptively ensemble\nexperts to provide customized local models for different users. Further, it\ncontains an adaptive weight module to synchronize the learning paces of\ndifferent users by dynamically adjusting weights in the loss. Extensive\nexperiments demonstrate the state-of-the-art performance of the proposed model.\nCode and data are provided at\n\\url{https://github.com/JP-25/end-To-end-Adaptive-Local-Leanring-TALL-}\n","authors":["Jinhao Pan","Ziwei Zhu","Jianling Wang","Allen Lin","James Caverlee"],"pdf_url":"https://arxiv.org/pdf/2404.08887v1.pdf","comment":"ECIR 2024"},{"id":"http://arxiv.org/abs/2404.08886v1","updated":"2024-04-13T03:15:56Z","published":"2024-04-13T03:15:56Z","title":"EIVEN: Efficient Implicit Attribute Value Extraction using Multimodal\n  LLM","summary":"  In e-commerce, accurately extracting product attribute values from multimodal\ndata is crucial for improving user experience and operational efficiency of\nretailers. However, previous approaches to multimodal attribute value\nextraction often struggle with implicit attribute values embedded in images or\ntext, rely heavily on extensive labeled data, and can easily confuse similar\nattribute values. To address these issues, we introduce EIVEN, a data- and\nparameter-efficient generative framework that pioneers the use of multimodal\nLLM for implicit attribute value extraction. EIVEN leverages the rich inherent\nknowledge of a pre-trained LLM and vision encoder to reduce reliance on labeled\ndata. We also introduce a novel Learning-by-Comparison technique to reduce\nmodel confusion by enforcing attribute value comparison and difference\nidentification. Additionally, we construct initial open-source datasets for\nmultimodal implicit attribute value extraction. Our extensive experiments\nreveal that EIVEN significantly outperforms existing methods in extracting\nimplicit attribute values while requiring less labeled data.\n","authors":["Henry Peng Zou","Gavin Heqing Yu","Ziwei Fan","Dan Bu","Han Liu","Peng Dai","Dongmei Jia","Cornelia Caragea"],"pdf_url":"https://arxiv.org/pdf/2404.08886v1.pdf","comment":"Accepted by NAACL 2024 Industry Track"},{"id":"http://arxiv.org/abs/2404.07494v2","updated":"2024-04-13T01:52:41Z","published":"2024-04-11T06:05:40Z","title":"Adaptive Fair Representation Learning for Personalized Fairness in\n  Recommendations via Information Alignment","summary":"  Personalized fairness in recommendations has been attracting increasing\nattention from researchers. The existing works often treat a fairness\nrequirement, represented as a collection of sensitive attributes, as a\nhyper-parameter, and pursue extreme fairness by completely removing information\nof sensitive attributes from the learned fair embedding, which suffer from two\nchallenges: huge training cost incurred by the explosion of attribute\ncombinations, and the suboptimal trade-off between fairness and accuracy. In\nthis paper, we propose a novel Adaptive Fair Representation Learning (AFRL)\nmodel, which achieves a real personalized fairness due to its advantage of\ntraining only one model to adaptively serve different fairness requirements\nduring inference phase. Particularly, AFRL treats fairness requirements as\ninputs and can learn an attribute-specific embedding for each attribute from\nthe unfair user embedding, which endows AFRL with the adaptability during\ninference phase to determine the non-sensitive attributes under the guidance of\nthe user's unique fairness requirement. To achieve a better trade-off between\nfairness and accuracy in recommendations, AFRL conducts a novel Information\nAlignment to exactly preserve discriminative information of non-sensitive\nattributes and incorporate a debiased collaborative embedding into the fair\nembedding to capture attribute-independent collaborative signals, without loss\nof fairness. Finally, the extensive experiments conducted on real datasets\ntogether with the sound theoretical analysis demonstrate the superiority of\nAFRL.\n","authors":["Xinyu Zhu","Lilin Zhang","Ning Yang"],"pdf_url":"https://arxiv.org/pdf/2404.07494v2.pdf","comment":"This paper has been accepted by SIGIR '24"},{"id":"http://arxiv.org/abs/2404.08869v1","updated":"2024-04-13T01:42:10Z","published":"2024-04-13T01:42:10Z","title":"Misinformation Resilient Search Rankings with Webgraph-based\n  Interventions","summary":"  The proliferation of unreliable news domains on the internet has had\nwide-reaching negative impacts on society. We introduce and evaluate\ninterventions aimed at reducing traffic to unreliable news domains from search\nengines while maintaining traffic to reliable domains. We build these\ninterventions on the principles of fairness (penalize sites for what is in\ntheir control), generality (label/fact-check agnostic), targeted (increase the\ncost of adversarial behavior), and scalability (works at webscale). We refine\nour methods on small-scale webdata as a testbed and then generalize the\ninterventions to a large-scale webgraph containing 93.9M domains and 1.6B\nedges. We demonstrate that our methods penalize unreliable domains far more\nthan reliable domains in both settings and we explore multiple avenues to\nmitigate unintended effects on both the small-scale and large-scale webgraph\nexperiments. These results indicate the potential of our approach to reduce the\nspread of misinformation and foster a more reliable online information\necosystem. This research contributes to the development of targeted strategies\nto enhance the trustworthiness and quality of search engine results, ultimately\nbenefiting users and the broader digital community.\n","authors":["Peter Carragher","Evan M. Williams","Kathleen M. Carley"],"pdf_url":"https://arxiv.org/pdf/2404.08869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08860v1","updated":"2024-04-13T00:20:09Z","published":"2024-04-13T00:20:09Z","title":"Improving Technical \"How-to\" Query Accuracy with Automated Search\n  Results Verification and Reranking","summary":"  Many people use search engines to find online guidance to solve computer or\nmobile device problems. Users frequently encounter challenges in identifying\neffective solutions from search results, often wasting time trying ineffective\nsolutions that seem relevant yet fail to solve the real problems. This paper\nintroduces a novel approach to improving the accuracy and relevance of online\ntechnical support search results through automated search results verification\nand reranking. Taking \"How-to\" queries specific to on-device execution as a\nstarting point, we first developed a solution that allows an AI agent to\ninterpret and execute step-by-step instructions in the search results in a\ncontrolled Android environment. We further integrated the agent's findings into\na reranking mechanism that orders search results based on the success\nindicators of the tested solutions.\n  The paper details the architecture of our solution and a comprehensive\nevaluation of the system through a series of tests across various application\ndomains. The results demonstrate a significant improvement in the quality and\nreliability of the top-ranked results. Our findings suggest a paradigm shift in\nhow search engine ranking for online technical support help can be optimized,\noffering a scalable and automated solution to the pervasive challenge of\nfinding effective and reliable online help.\n","authors":["Lei Ding","Jeshwanth Bheemanpally","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.08860v1.pdf","comment":"12 pages, 2 columns, 3 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2404.09029v1","updated":"2024-04-13T15:37:57Z","published":"2024-04-13T15:37:57Z","title":"A Parametric Rate-Distortion Model for Video Transcoding","summary":"  Over the past two decades, the surge in video streaming applications has been\nfueled by the increasing accessibility of the internet and the growing demand\nfor network video. As users with varying internet speeds and devices seek\nhigh-quality video, transcoding becomes essential for service providers. In\nthis paper, we introduce a parametric rate-distortion (R-D) transcoding model.\nOur model excels at predicting transcoding distortion at various rates without\nthe need for encoding the video. This model serves as a versatile tool that can\nbe used to achieve visual quality improvement (in terms of PSNR) via\ntrans-sizing. Moreover, we use our model to identify visually lossless and\nnear-zero-slope bitrate ranges for an ingest video. Having this information\nallows us to adjust the transcoding target bitrate while introducing visually\nnegligible quality degradations. By utilizing our model in this manner, quality\nimprovements up to 2 dB and bitrate savings of up to 46% of the original target\nbitrate are possible. Experimental results demonstrate the efficacy of our\nmodel in video transcoding rate distortion prediction.\n","authors":["Maedeh Jamali","Nader Karimi","Shadrokh Samavi","Shahram Shirani"],"pdf_url":"https://arxiv.org/pdf/2404.09029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.05995v3","updated":"2024-04-13T15:22:53Z","published":"2023-08-11T08:03:28Z","title":"Audio is all in one: speech-driven gesture synthetics using WavLM\n  pre-trained model","summary":"  The generation of co-speech gestures for digital humans is an emerging area\nin the field of virtual human creation. Prior research has made progress by\nusing acoustic and semantic information as input and adopting classify method\nto identify the person's ID and emotion for driving co-speech gesture\ngeneration. However, this endeavour still faces significant challenges. These\nchallenges go beyond the intricate interplay between co-speech gestures, speech\nacoustic, and semantics; they also encompass the complexities associated with\npersonality, emotion, and other obscure but important factors. This paper\nintroduces \"diffmotion-v2,\" a speech-conditional diffusion-based and\nnon-autoregressive transformer-based generative model with WavLM pre-trained\nmodel. It can produce individual and stylized full-body co-speech gestures only\nusing raw speech audio, eliminating the need for complex multimodal processing\nand manually annotated. Firstly, considering that speech audio not only\ncontains acoustic and semantic features but also conveys personality traits,\nemotions, and more subtle information related to accompanying gestures, we\npioneer the adaptation of WavLM, a large-scale pre-trained model, to extract\nlow-level and high-level audio information. Secondly, we introduce an adaptive\nlayer norm architecture in the transformer-based layer to learn the\nrelationship between speech information and accompanying gestures. Extensive\nsubjective evaluation experiments are conducted on the Trinity, ZEGGS, and BEAT\ndatasets to confirm the WavLM and the model's ability to synthesize natural\nco-speech gestures with various styles.\n","authors":["Fan Zhang","Naye Ji","Fuxing Gao","Siyuan Zhao","Zhaohan Wang","Shunman Li"],"pdf_url":"https://arxiv.org/pdf/2308.05995v3.pdf","comment":"This article needs major revision"},{"id":"http://arxiv.org/abs/2404.08965v1","updated":"2024-04-13T11:07:10Z","published":"2024-04-13T11:07:10Z","title":"Seeing Text in the Dark: Algorithm and Benchmark","summary":"  Localizing text in low-light environments is challenging due to visual\ndegradations. Although a straightforward solution involves a two-stage pipeline\nwith low-light image enhancement (LLE) as the initial step followed by\ndetector, LLE is primarily designed for human vision instead of machine and can\naccumulate errors. In this work, we propose an efficient and effective\nsingle-stage approach for localizing text in dark that circumvents the need for\nLLE. We introduce a constrained learning module as an auxiliary mechanism\nduring the training stage of the text detector. This module is designed to\nguide the text detector in preserving textual spatial features amidst feature\nmap resizing, thus minimizing the loss of spatial information in texts under\nlow-light visual degradations. Specifically, we incorporate spatial\nreconstruction and spatial semantic constraints within this module to ensure\nthe text detector acquires essential positional and contextual range knowledge.\nOur approach enhances the original text detector's ability to identify text's\nlocal topological features using a dynamic snake feature pyramid network and\nadopts a bottom-up contour shaping strategy with a novel rectangular\naccumulation technique for accurate delineation of streamlined text features.\nIn addition, we present a comprehensive low-light dataset for arbitrary-shaped\ntext, encompassing diverse scenes and languages. Notably, our method achieves\nstate-of-the-art results on this low-light dataset and exhibits comparable\nperformance on standard normal light datasets. The code and dataset will be\nreleased.\n","authors":["Chengpei Xu","Hao Fu","Long Ma","Wenjing Jia","Chengqi Zhang","Feng Xia","Xiaoyu Ai","Binghao Li","Wenjie Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.08965v1.pdf","comment":null}]},"2024-04-16T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2402.16846v2","updated":"2024-04-16T17:59:53Z","published":"2024-02-26T18:59:33Z","title":"GROUNDHOG: Grounding Large Language Models to Holistic Segmentation","summary":"  Most multimodal large language models (MLLMs) learn language-to-object\ngrounding through causal language modeling where grounded objects are captured\nby bounding boxes as sequences of location tokens. This paradigm lacks\npixel-level representations that are important for fine-grained visual\nunderstanding and diagnosis. In this work, we introduce GROUNDHOG, an MLLM\ndeveloped by grounding Large Language Models to holistic segmentation.\nGROUNDHOG incorporates a masked feature extractor and converts extracted\nfeatures into visual entity tokens for the MLLM backbone, which then connects\ngroundable phrases to unified grounding masks by retrieving and merging the\nentity masks. To train GROUNDHOG, we carefully curated M3G2, a grounded visual\ninstruction tuning dataset with Multi-Modal Multi-Grained Grounding, by\nharvesting a collection of segmentation-grounded datasets with rich\nannotations. Our experimental results show that GROUNDHOG achieves superior\nperformance on various language grounding tasks without task-specific\nfine-tuning, and significantly reduces object hallucination. GROUNDHOG also\ndemonstrates better grounding towards complex forms of visual input and\nprovides easy-to-understand diagnosis in failure cases.\n","authors":["Yichi Zhang","Ziqiao Ma","Xiaofeng Gao","Suhaila Shakiah","Qiaozi Gao","Joyce Chai"],"pdf_url":"https://arxiv.org/pdf/2402.16846v2.pdf","comment":"Accepted to CVPR 2024. Website: https://groundhog-mllm.github.io/"},{"id":"http://arxiv.org/abs/2404.10774v1","updated":"2024-04-16T17:59:10Z","published":"2024-04-16T17:59:10Z","title":"MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents","summary":"  Recognizing if LLM output can be grounded in evidence is central to many\ntasks in NLP: retrieval-augmented generation, summarization, document-grounded\ndialogue, and more. Current approaches to this kind of \"fact-checking\" are\nbased on verifying each piece of a model generation against potential evidence\nusing an LLM. However, this process can be very computationally expensive,\nrequiring many calls to LLMs to check a single response. In this work, we show\nhow to build small models that have GPT-4-level performance but for 400x lower\ncost. We do this by constructing synthetic training data with GPT-4, which\ninvolves creating realistic yet challenging instances of factual errors via a\nstructured generation procedure. Training on this data teaches models to check\neach fact in the claim and recognize synthesis of information across sentences.\nFor evaluation, we unify pre-existing datasets into a benchmark LLM-AggreFact,\ncollected from recent work on fact-checking and grounding LLM generations. Our\nbest system MiniCheck-FT5 (770M parameters) outperforms all systems of\ncomparable size and reaches GPT-4 accuracy. We release LLM-AggreFact, code for\ndata synthesis, and models.\n","authors":["Liyan Tang","Philippe Laban","Greg Durrett"],"pdf_url":"https://arxiv.org/pdf/2404.10774v1.pdf","comment":"LLM-AggreFact benchmark, MiniCheck models, data generation code at\n  https://github.com/Liyan06/MiniCheck"},{"id":"http://arxiv.org/abs/2310.17722v2","updated":"2024-04-16T17:54:06Z","published":"2023-10-26T18:32:05Z","title":"Large Language Models as Generalizable Policies for Embodied Tasks","summary":"  We show that large language models (LLMs) can be adapted to be generalizable\npolicies for embodied visual tasks. Our approach, called Large LAnguage model\nReinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take\nas input text instructions and visual egocentric observations and output\nactions directly in the environment. Using reinforcement learning, we train\nLLaRP to see and act solely through environmental interactions. We show that\nLLaRP is robust to complex paraphrasings of task instructions and can\ngeneralize to new tasks that require novel optimal behavior. In particular, on\n1,000 unseen tasks it achieves 42% success rate, 1.7x the success rate of other\ncommon learned baselines or zero-shot applications of LLMs. Finally, to aid the\ncommunity in studying language conditioned, massively multi-task, embodied AI\nproblems we release a novel benchmark, Language Rearrangement, consisting of\n150,000 training and 1,000 testing tasks for language-conditioned\nrearrangement. Video examples of LLaRP in unseen Language Rearrangement\ninstructions are at https://llm-rl.github.io.\n","authors":["Andrew Szot","Max Schwarzer","Harsh Agrawal","Bogdan Mazoure","Walter Talbott","Katherine Metcalf","Natalie Mackraz","Devon Hjelm","Alexander Toshev"],"pdf_url":"https://arxiv.org/pdf/2310.17722v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.09753v2","updated":"2024-04-16T17:53:37Z","published":"2023-10-15T06:45:38Z","title":"When can transformers reason with abstract symbols?","summary":"  We investigate the capabilities of transformer models on relational reasoning\ntasks. In these tasks, models are trained on a set of strings encoding abstract\nrelations, and are then tested out-of-distribution on data that contains\nsymbols that did not appear in the training dataset. We prove that for any\nrelational reasoning task in a large family of tasks, transformers learn the\nabstract relations and generalize to the test set when trained by gradient\ndescent on sufficiently large quantities of training data. This is in contrast\nto classical fully-connected networks, which we prove fail to learn to reason.\nOur results inspire modifications of the transformer architecture that add only\ntwo trainable parameters per head, and that we empirically demonstrate improve\ndata efficiency for learning to reason.\n","authors":["Enric Boix-Adsera","Omid Saremi","Emmanuel Abbe","Samy Bengio","Etai Littwin","Joshua Susskind"],"pdf_url":"https://arxiv.org/pdf/2310.09753v2.pdf","comment":"25 figures"},{"id":"http://arxiv.org/abs/2404.10763v1","updated":"2024-04-16T17:47:16Z","published":"2024-04-16T17:47:16Z","title":"LaDiC: Are Diffusion Models Really Inferior to Autoregressive\n  Counterparts for Image-to-Text Generation?","summary":"  Diffusion models have exhibited remarkable capabilities in text-to-image\ngeneration. However, their performance in image-to-text generation,\nspecifically image captioning, has lagged behind Auto-Regressive (AR) models,\ncasting doubt on their applicability for such tasks. In this work, we revisit\ndiffusion models, highlighting their capacity for holistic context modeling and\nparallel decoding. With these benefits, diffusion models can alleviate the\ninherent limitations of AR methods, including their slow inference speed, error\npropagation, and unidirectional constraints. Furthermore, we identify the prior\nunderperformance of diffusion models stemming from the absence of an effective\nlatent space for image-text alignment, and the discrepancy between continuous\ndiffusion processes and discrete textual data. In response, we introduce a\nnovel architecture, LaDiC, which utilizes a split BERT to create a dedicated\nlatent space for captions and integrates a regularization module to manage\nvarying text lengths. Our framework also includes a diffuser for semantic\nimage-to-text conversion and a Back&Refine technique to enhance token\ninteractivity during inference. LaDiC achieves state-of-the-art performance for\ndiffusion-based methods on the MS COCO dataset with 38.2 BLEU@4 and 126.2\nCIDEr, demonstrating exceptional performance without pre-training or ancillary\nmodules. This indicates strong competitiveness with AR models, revealing the\npreviously untapped potential of diffusion models in image-to-text generation.\n","authors":["Yuchi Wang","Shuhuai Ren","Rundong Gao","Linli Yao","Qingyan Guo","Kaikai An","Jianhong Bai","Xu Sun"],"pdf_url":"https://arxiv.org/pdf/2404.10763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13269v3","updated":"2024-04-16T17:37:12Z","published":"2024-03-20T03:07:50Z","title":"AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient\n  Fine-Tuning of Large Models","summary":"  We present a novel Parameter-Efficient Fine-Tuning (PEFT) method, dubbed as\nAdaptive Freezing of Low Rank Adaptation (AFLoRA). Specifically, for each\npre-trained frozen weight tensor, we add a parallel path of trainable low-rank\nmatrices, namely a down-projection and an up-projection matrix, each of which\nis followed by a feature transformation vector. Based on a novel freezing\nscore, we the incrementally freeze these projection matrices during fine-tuning\nto reduce the computation and alleviate over-fitting. Our experimental results\ndemonstrate that we can achieve state-of-the-art performance with an average\nimprovement of up to $0.85\\%$ as evaluated on GLUE benchmark while yeilding up\nto $9.5\\times$ fewer average trainable parameters. While compared in terms of\nruntime, AFLoRA can yield up to $1.86\\times$ improvement as opposed to similar\nPEFT alternatives. Besides the practical utility of our approach, we provide\ninsights on the trainability requirements of LoRA paths at different modules\nand the freezing schedule for the different projection matrices. Code will be\nreleased.\n","authors":["Zeyu Liu","Souvik Kundu","Anni Li","Junrui Wan","Lianghao Jiang","Peter Anthony Beerel"],"pdf_url":"https://arxiv.org/pdf/2403.13269v3.pdf","comment":"5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2404.08495v3","updated":"2024-04-16T17:36:39Z","published":"2024-04-12T14:25:49Z","title":"Dataset Reset Policy Optimization for RLHF","summary":"  Reinforcement Learning (RL) from Human Preference-based feedback is a popular\nparadigm for fine-tuning generative models, which has produced impressive\nmodels such as GPT-4 and Claude3 Opus. This framework often consists of two\nsteps: learning a reward model from an offline preference dataset followed by\nrunning online RL to optimize the learned reward model. In this work,\nleveraging the idea of reset, we propose a new RLHF algorithm with provable\nguarantees. Motivated by the fact that offline preference dataset provides\ninformative states (i.e., data that is preferred by the labelers), our new\nalgorithm, Dataset Reset Policy Optimization (DR-PO), integrates the existing\noffline preference dataset into the online policy training procedure via\ndataset reset: it directly resets the policy optimizer to the states in the\noffline dataset, instead of always starting from the initial state\ndistribution. In theory, we show that DR-PO learns to perform at least as good\nas any policy that is covered by the offline dataset under general function\napproximation with finite sample complexity. In experiments, we demonstrate\nthat on both the TL;DR summarization and the Anthropic Helpful Harmful (HH)\ndataset, the generation from DR-PO is better than that from Proximal Policy\nOptimization (PPO) and Direction Preference Optimization (DPO), under the\nmetric of GPT4 win-rate. Code for this work can be found at\nhttps://github.com/Cornell-RL/drpo.\n","authors":["Jonathan D. Chang","Wenhao Zhan","Owen Oertell","Kiant√© Brantley","Dipendra Misra","Jason D. Lee","Wen Sun"],"pdf_url":"https://arxiv.org/pdf/2404.08495v3.pdf","comment":"28 pages, 6 tables, 3 Figures, 3 Algorithms"},{"id":"http://arxiv.org/abs/2404.10757v1","updated":"2024-04-16T17:35:25Z","published":"2024-04-16T17:35:25Z","title":"Deep Learning and LLM-based Methods Applied to Stellar Lightcurve\n  Classification","summary":"  Light curves serve as a valuable source of information on stellar formation\nand evolution. With the rapid advancement of machine learning techniques, it\ncan be effectively processed to extract astronomical patterns and information.\nIn this study, we present a comprehensive evaluation of deep-learning and large\nlanguage model (LLM) based models for the automatic classification of variable\nstar light curves, based on large datasets from the Kepler and K2 missions.\nSpecial emphasis is placed on Cepheids, RR Lyrae, and eclipsing binaries,\nexamining the influence of observational cadence and phase distribution on\nclassification precision. Employing AutoDL optimization, we achieve striking\nperformance with the 1D-Convolution+BiLSTM architecture and the Swin\nTransformer, hitting accuracies of 94\\% and 99\\% correspondingly, with the\nlatter demonstrating a notable 83\\% accuracy in discerning the elusive Type II\nCepheids-comprising merely 0.02\\% of the total dataset.We unveil StarWhisper\nLightCurve (LC), an innovative Series comprising three LLM-based models: LLM,\nmultimodal large language model (MLLM), and Large Audio Language Model (LALM).\nEach model is fine-tuned with strategic prompt engineering and customized\ntraining methods to explore the emergent abilities of these models for\nastronomical data. Remarkably, StarWhisper LC Series exhibit high accuracies\naround 90\\%, significantly reducing the need for explicit feature engineering,\nthereby paving the way for streamlined parallel data processing and the\nprogression of multifaceted multimodal models in astronomical applications. The\nstudy furnishes two detailed catalogs illustrating the impacts of phase and\nsampling intervals on deep learning classification accuracy, showing that a\nsubstantial decrease of up to 14\\% in observation duration and 21\\% in sampling\npoints can be realized without compromising accuracy by more than 10\\%.\n","authors":["Yu-Yang Li","Yu Bai","Cunshi Wang","Mengwei Qu","Ziteng Lu","Roberto Soria","Jifeng Liu"],"pdf_url":"https://arxiv.org/pdf/2404.10757v1.pdf","comment":"35 pages, 20 figures"},{"id":"http://arxiv.org/abs/2403.07175v2","updated":"2024-04-16T17:32:45Z","published":"2024-03-11T21:33:05Z","title":"Rebuilding ROME : Resolving Model Collapse during Sequential Model\n  Editing","summary":"  Recent work using Rank-One Model Editing (ROME), a popular model editing\nmethod, has shown that there are certain facts that the algorithm is unable to\nedit without breaking the model. Such edits have previously been called\ndisabling edits. These disabling edits cause immediate model collapse and\nlimits the use of ROME for sequential editing. In this paper, we show that\ndisabling edits are an artifact of irregularities in the implementation of\nROME. With this paper, we provide a more stable implementation ROME, which we\ncall r-ROME and show that model collapse is no longer observed when making\nlarge scale sequential edits with r-ROME, while further improving\ngeneralization and locality of model editing compared to the original\nimplementation of ROME. We also provide a detailed mathematical explanation of\nthe reason behind disabling edits.\n","authors":["Akshat Gupta","Sidharth Baskaran","Gopala Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2403.07175v2.pdf","comment":"Added explanation of failure of original implementation of ROME in\n  the paper"},{"id":"http://arxiv.org/abs/2305.14502v2","updated":"2024-04-16T17:25:25Z","published":"2023-05-23T20:15:56Z","title":"RetICL: Sequential Retrieval of In-Context Examples with Reinforcement\n  Learning","summary":"  Recent developments in large pre-trained language models have enabled\nunprecedented performance on a variety of downstream tasks. Achieving best\nperformance with these models often leverages in-context learning, where a\nmodel performs a (possibly new) task given one or more examples. However,\nrecent work has shown that the choice of examples can have a large impact on\ntask performance and that finding an optimal set of examples is non-trivial.\nWhile there are many existing methods for selecting in-context examples, they\ngenerally score examples independently, ignoring the dependency between them\nand the order in which they are provided to the model. In this work, we propose\nRetrieval for In-Context Learning (RetICL), a learnable method for modeling and\noptimally selecting examples sequentially for in-context learning. We frame the\nproblem of sequential example selection as a Markov decision process and train\nan example retriever using reinforcement learning. We evaluate RetICL on math\nword problem solving and scientific question answering tasks and show that it\nconsistently outperforms or matches heuristic and learnable baselines. We also\nuse case studies to show that RetICL implicitly learns representations of\nproblem solving strategies.\n","authors":["Alexander Scarlatos","Andrew Lan"],"pdf_url":"https://arxiv.org/pdf/2305.14502v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14633v3","updated":"2024-04-16T17:14:16Z","published":"2024-02-16T23:18:19Z","title":"Born With a Silver Spoon? Investigating Socioeconomic Bias in Large\n  Language Models","summary":"  Socioeconomic bias in society exacerbates disparities, influencing access to\nopportunities and resources based on individuals' economic and social\nbackgrounds. This pervasive issue perpetuates systemic inequalities, hindering\nthe pursuit of inclusive progress as a society. In this paper, we investigate\nthe presence of socioeconomic bias, if any, in large language models. To this\nend, we introduce a novel dataset SilverSpoon, consisting of 3000 samples that\nillustrate hypothetical scenarios that involve underprivileged people\nperforming ethically ambiguous actions due to their circumstances, and ask\nwhether the action is ethically justified. Further, this dataset has a\ndual-labeling scheme and has been annotated by people belonging to both ends of\nthe socioeconomic spectrum. Using SilverSpoon, we evaluate the degree of\nsocioeconomic bias expressed in large language models and the variation of this\ndegree as a function of model size. We also perform qualitative analysis to\nanalyze the nature of this bias. Our analysis reveals that while humans\ndisagree on which situations require empathy toward the underprivileged, most\nlarge language models are unable to empathize with the socioeconomically\nunderprivileged regardless of the situation. To foster further research in this\ndomain, we make SilverSpoon and our evaluation harness publicly available.\n","authors":["Smriti Singh","Shuvam Keshari","Vinija Jain","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2403.14633v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10719v1","updated":"2024-04-16T16:51:53Z","published":"2024-04-16T16:51:53Z","title":"Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study","summary":"  Reinforcement Learning from Human Feedback (RLHF) is currently the most\nwidely used method to align large language models (LLMs) with human\npreferences. Existing RLHF methods can be roughly categorized as either\nreward-based or reward-free. Novel applications such as ChatGPT and Claude\nleverage reward-based methods that first learn a reward model and apply\nactor-critic algorithms, such as Proximal Policy Optimization (PPO). However,\nin academic benchmarks, state-of-the-art results are often achieved via\nreward-free methods, such as Direct Preference Optimization (DPO). Is DPO truly\nsuperior to PPO? Why does PPO perform poorly on these benchmarks? In this\npaper, we first conduct both theoretical and empirical studies on the\nalgorithmic properties of DPO and show that DPO may have fundamental\nlimitations. Moreover, we also comprehensively examine PPO and reveal the key\nfactors for the best performances of PPO in fine-tuning LLMs. Finally, we\nbenchmark DPO and PPO across various a collection of RLHF testbeds, ranging\nfrom dialogue to code generation. Experiment results demonstrate that PPO is\nable to surpass other alignment methods in all cases and achieve\nstate-of-the-art results in challenging code competitions.\n","authors":["Shusheng Xu","Wei Fu","Jiaxuan Gao","Wenjie Ye","Weilin Liu","Zhiyu Mei","Guangju Wang","Chao Yu","Yi Wu"],"pdf_url":"https://arxiv.org/pdf/2404.10719v1.pdf","comment":"16 pages, 2 figures, 14 tables"},{"id":"http://arxiv.org/abs/2402.11512v3","updated":"2024-04-16T16:40:31Z","published":"2024-02-18T08:53:41Z","title":"From Prejudice to Parity: A New Approach to Debiasing Large Language\n  Model Word Embeddings","summary":"  Embeddings play a pivotal role in the efficacy of Large Language Models. They\nare the bedrock on which these models grasp contextual relationships and foster\na more nuanced understanding of language and consequently perform remarkably on\na plethora of complex tasks that require a fundamental understanding of human\nlanguage. Given that these embeddings themselves often reflect or exhibit bias,\nit stands to reason that these models may also inadvertently learn this bias.\nIn this work, we build on the seminal previous work and propose DeepSoftDebias,\nan algorithm that uses a neural network to perform 'soft debiasing'. We\nexhaustively evaluate this algorithm across a variety of SOTA datasets,\naccuracy metrics, and challenging NLP tasks. We find that DeepSoftDebias\noutperforms the current state-of-the-art methods at reducing bias across\ngender, race, and religion.\n","authors":["Aishik Rakshit","Smriti Singh","Shuvam Keshari","Arijit Ghosh Chowdhury","Vinija Jain","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2402.11512v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.01744v5","updated":"2024-04-16T16:39:51Z","published":"2024-04-02T09:01:32Z","title":"Octopus v2: On-device language model for super agent","summary":"  Language models have shown effectiveness in a variety of software\napplications, particularly in tasks related to automatic workflow. These models\npossess the crucial ability to call functions, which is essential in creating\nAI agents. Despite the high performance of large-scale language models in cloud\nenvironments, they are often associated with concerns over privacy and cost.\nCurrent on-device models for function calling face issues with latency and\naccuracy. Our research presents a new method that empowers an on-device model\nwith 2 billion parameters to surpass the performance of GPT-4 in both accuracy\nand latency, and decrease the context length by 95\\%. When compared to Llama-7B\nwith a RAG-based function calling mechanism, our method enhances latency by\n35-fold. This method reduces the latency to levels deemed suitable for\ndeployment across a variety of edge devices in production environments,\naligning with the performance requisites for real-world applications.\n","authors":["Wei Chen","Zhiyuan Li"],"pdf_url":"https://arxiv.org/pdf/2404.01744v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10710v1","updated":"2024-04-16T16:36:50Z","published":"2024-04-16T16:36:50Z","title":"Dual Modalities of Text: Visual and Textual Generative Pre-training","summary":"  Harnessing visual texts represents a burgeoning frontier in the evolution of\nlanguage modeling. In this paper, we introduce a novel pre-training framework\nfor a suite of pixel-based autoregressive language models, pre-training on a\ncorpus of over 400 million documents rendered as RGB images. Our approach is\ncharacterized by a dual-modality training regimen, engaging both visual data\nthrough next patch prediction with a regression head and textual data via next\ntoken prediction with a classification head. This study is particularly focused\non investigating the synergistic interplay between visual and textual\nmodalities of language. Our comprehensive evaluation across a diverse array of\nbenchmarks reveals that the confluence of visual and textual data substantially\naugments the efficacy of pixel-based language models. Notably, our findings\nshow that a unidirectional pixel-based model, devoid of textual data during\ntraining, can match the performance levels of advanced bidirectional\npixel-based models on various language understanding benchmarks. This work\nhighlights the considerable untapped potential of integrating visual and\ntextual information for language modeling purposes. We will release our code,\ndata, and checkpoints to inspire further research advancement.\n","authors":["Yekun Chai","Qingyi Liu","Jingwu Xiao","Shuohuan Wang","Yu Sun","Hua Wu"],"pdf_url":"https://arxiv.org/pdf/2404.10710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11409v2","updated":"2024-04-16T16:34:02Z","published":"2024-02-18T00:32:33Z","title":"Multi-dimensional Evaluation of Empathetic Dialog Responses","summary":"  Empathy is critical for effective and satisfactory conversational\ncommunication. Prior efforts to measure conversational empathy mostly focus on\nexpressed communicative intents -- that is, the way empathy is expressed. Yet,\nthese works ignore the fact that conversation is also a collaboration involving\nboth speakers and listeners. In contrast, we propose a multi-dimensional\nempathy evaluation framework to measure both expressed intents from the\nspeaker's perspective and perceived empathy from the listener's perspective. We\napply our proposed framework to analyze our internal customer-service dialogue.\nWe find the two dimensions (expressed intent types and perceived empathy) are\ninter-connected, and perceived empathy has a high correlation with dialogue\nsatisfaction levels.\n  To reduce the annotation cost, we explore different options to automatically\nmeasure conversational empathy: prompting LLMs and training language\nmodel-based classifiers. Our experiments show that prompting methods with even\npopular models like GPT-4 and Flan family models perform relatively poorly on\nboth public and our internal datasets. In contrast, instruction-finetuned\nclassifiers based on Flan-T5 family models outperform prior works and\ncompetitive baselines. We conduct a detailed ablation study to give more\ninsights into instruction finetuning method's strong performance.\n","authors":["Zhichao Xu","Jiepu Jiang"],"pdf_url":"https://arxiv.org/pdf/2402.11409v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2404.10706v1","updated":"2024-04-16T16:30:27Z","published":"2024-04-16T16:30:27Z","title":"Cross-Language Evolution of Divergent Collective Memory Around the Arab\n  Spring","summary":"  The Arab Spring was a historic set of protests beginning in 2011 that toppled\ngovernments and led to major conflicts. Collective memories of events like\nthese can vary significantly across social contexts in response to political,\ncultural, and linguistic factors. While Wikipedia plays an important role in\ndocumenting both historic and current events, little attention has been given\nto how Wikipedia articles, created in the aftermath of major events, continue\nto evolve over years or decades. Using the archived content of Arab\nSpring-related topics across the Arabic and English Wikipedias between 2011 and\n2024, we define and evaluate multilingual measures of event salience,\ndeliberation, contextualization, and consolidation of collective memory\nsurrounding the Arab Spring. Our findings about the temporal evolution of the\nWikipedia articles' content similarity across languages has implications for\ntheorizing about online collective memory processes and evaluating linguistic\nmodels trained on these data.\n","authors":["H. Laurie Jones","Brian C. Keegan"],"pdf_url":"https://arxiv.org/pdf/2404.10706v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10704v1","updated":"2024-04-16T16:23:10Z","published":"2024-04-16T16:23:10Z","title":"Question Difficulty Ranking for Multiple-Choice Reading Comprehension","summary":"  Multiple-choice (MC) tests are an efficient method to assess English\nlearners. It is useful for test creators to rank candidate MC questions by\ndifficulty during exam curation. Typically, the difficulty is determined by\nhaving human test takers trial the questions in a pretesting stage. However,\nthis is expensive and not scalable. Therefore, we explore automated approaches\nto rank MC questions by difficulty. However, there is limited data for explicit\ntraining of a system for difficulty scores. Hence, we compare task transfer and\nzero-shot approaches: task transfer adapts level classification and reading\ncomprehension systems for difficulty ranking while zero-shot prompting of\ninstruction finetuned language models contrasts absolute assessment against\ncomparative. It is found that level classification transfers better than\nreading comprehension. Additionally, zero-shot comparative assessment is more\neffective at difficulty ranking than the absolute assessment and even the task\ntransfer approaches at question difficulty ranking with a Spearman's\ncorrelation of 40.4%. Combining the systems is observed to further boost the\ncorrelation.\n","authors":["Vatsal Raina","Mark Gales"],"pdf_url":"https://arxiv.org/pdf/2404.10704v1.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2404.10696v1","updated":"2024-04-16T16:15:19Z","published":"2024-04-16T16:15:19Z","title":"Integrating knowledge bases to improve coreference and bridging\n  resolution for the chemical domain","summary":"  Resolving coreference and bridging relations in chemical patents is important\nfor better understanding the precise chemical process, where chemical domain\nknowledge is very critical. We proposed an approach incorporating external\nknowledge into a multi-task learning model for both coreference and bridging\nresolution in the chemical domain. The results show that integrating external\nknowledge can benefit both chemical coreference and bridging resolution.\n","authors":["Pengcheng Lu","Massimo Poesio"],"pdf_url":"https://arxiv.org/pdf/2404.10696v1.pdf","comment":"working in progress"},{"id":"http://arxiv.org/abs/2311.09807v2","updated":"2024-04-16T15:57:11Z","published":"2023-11-16T11:31:50Z","title":"The Curious Decline of Linguistic Diversity: Training Language Models on\n  Synthetic Text","summary":"  This study investigates the consequences of training language models on\nsynthetic data generated by their predecessors, an increasingly prevalent\npractice given the prominence of powerful generative models. Diverging from the\nusual emphasis on performance metrics, we focus on the impact of this training\nmethodology on linguistic diversity, especially when conducted recursively over\ntime. To assess this, we adapt and develop a set of novel metrics targeting\nlexical, syntactic, and semantic diversity, applying them in recursive\nfinetuning experiments across various natural language generation tasks in\nEnglish. Our findings reveal a consistent decrease in the diversity of the\nmodel outputs through successive iterations, especially remarkable for tasks\ndemanding high levels of creativity. This trend underscores the potential risks\nof training language models on synthetic text, particularly concerning the\npreservation of linguistic richness. Our study highlights the need for careful\nconsideration of the long-term effects of such training approaches on the\nlinguistic capabilities of language models.\n","authors":["Yanzhu Guo","Guokan Shang","Michalis Vazirgiannis","Chlo√© Clavel"],"pdf_url":"https://arxiv.org/pdf/2311.09807v2.pdf","comment":"Accepted to NAACL 2024 Findings"},{"id":"http://arxiv.org/abs/2404.07922v3","updated":"2024-04-16T15:33:45Z","published":"2024-04-11T17:09:28Z","title":"LaVy: Vietnamese Multimodal Large Language Model","summary":"  Large Language Models (LLMs) and Multimodal Large language models (MLLMs)\nhave taken the world by storm with impressive abilities in complex reasoning\nand linguistic comprehension. Meanwhile there are plethora of works related to\nVietnamese Large Language Models, the lack of high-quality resources in\nmultimodality limits the progress of Vietnamese MLLMs. In this paper, we\npioneer in address this by introducing LaVy, a state-of-the-art Vietnamese\nMLLM, and we also introduce LaVy-Bench benchmark designated for evaluating\nMLLMs's understanding on Vietnamese visual language tasks. Our project is\npublic at https://github.com/baochi0212/LaVy\n","authors":["Chi Tran","Huong Le Thanh"],"pdf_url":"https://arxiv.org/pdf/2404.07922v3.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2404.10652v1","updated":"2024-04-16T15:28:30Z","published":"2024-04-16T15:28:30Z","title":"ViTextVQA: A Large-Scale Visual Question Answering Dataset for\n  Evaluating Vietnamese Text Comprehension in Images","summary":"  Visual Question Answering (VQA) is a complicated task that requires the\ncapability of simultaneously processing natural language and images. Initially,\nthis task was researched, focusing on methods to help machines understand\nobjects and scene contexts in images. However, some text appearing in the image\nthat carries explicit information about the full content of the image is not\nmentioned. Along with the continuous development of the AI era, there have been\nmany studies on the reading comprehension ability of VQA models in the world.\nAs a developing country, conditions are still limited, and this task is still\nopen in Vietnam. Therefore, we introduce the first large-scale dataset in\nVietnamese specializing in the ability to understand text appearing in images,\nwe call it ViTextVQA (\\textbf{Vi}etnamese \\textbf{Text}-based \\textbf{V}isual\n\\textbf{Q}uestion \\textbf{A}nswering dataset) which contains \\textbf{over\n16,000} images and \\textbf{over 50,000} questions with answers. Through\nmeticulous experiments with various state-of-the-art models, we uncover the\nsignificance of the order in which tokens in OCR text are processed and\nselected to formulate answers. This finding helped us significantly improve the\nperformance of the baseline models on the ViTextVQA dataset. Our dataset is\navailable at this\n\\href{https://github.com/minhquan6203/ViTextVQA-Dataset}{link} for research\npurposes.\n","authors":["Quan Van Nguyen","Dan Quang Tran","Huy Quang Pham","Thang Kien-Bao Nguyen","Nghia Hieu Nguyen","Kiet Van Nguyen","Ngan Luu-Thuy Nguyen"],"pdf_url":"https://arxiv.org/pdf/2404.10652v1.pdf","comment":"Preprint submitted to IJCV"},{"id":"http://arxiv.org/abs/2404.10642v1","updated":"2024-04-16T15:16:22Z","published":"2024-04-16T15:16:22Z","title":"Self-playing Adversarial Language Game Enhances LLM Reasoning","summary":"  We explore the self-play training procedure of large language models (LLMs)\nin a two-player adversarial language game called Adversarial Taboo. In this\ngame, an attacker and a defender communicate with respect to a target word only\nvisible to the attacker. The attacker aims to induce the defender to utter the\ntarget word unconsciously, while the defender tries to infer the target word\nfrom the attacker's utterances. To win the game, both players should have\nsufficient knowledge about the target word and high-level reasoning ability to\ninfer and express in this information-reserved conversation. Hence, we are\ncurious about whether LLMs' reasoning ability can be further enhanced by\nSelf-Play in this Adversarial language Game (SPAG). With this goal, we let LLMs\nact as the attacker and play with a copy of itself as the defender on an\nextensive range of target words. Through reinforcement learning on the game\noutcomes, we observe that the LLMs' performance uniformly improves on a broad\nrange of reasoning benchmarks. Furthermore, iteratively adopting this self-play\nprocess can continuously promote LLM's reasoning ability. The code is at\nhttps://github.com/Linear95/SPAG.\n","authors":["Pengyu Cheng","Tianhao Hu","Han Xu","Zhisong Zhang","Yong Dai","Lei Han","Nan Du"],"pdf_url":"https://arxiv.org/pdf/2404.10642v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2307.13854v4","updated":"2024-04-16T15:13:18Z","published":"2023-07-25T22:59:32Z","title":"WebArena: A Realistic Web Environment for Building Autonomous Agents","summary":"  With advances in generative AI, there is now potential for autonomous agents\nto manage daily tasks via natural language commands. However, current agents\nare primarily created and tested in simplified synthetic environments, leading\nto a disconnect with real-world scenarios. In this paper, we build an\nenvironment for language-guided agents that is highly realistic and\nreproducible. Specifically, we focus on agents that perform tasks on the web,\nand create an environment with fully functional websites from four common\ndomains: e-commerce, social forum discussions, collaborative software\ndevelopment, and content management. Our environment is enriched with tools\n(e.g., a map) and external knowledge bases (e.g., user manuals) to encourage\nhuman-like task-solving. Building upon our environment, we release a set of\nbenchmark tasks focusing on evaluating the functional correctness of task\ncompletions. The tasks in our benchmark are diverse, long-horizon, and designed\nto emulate tasks that humans routinely perform on the internet. We experiment\nwith several baseline agents, integrating recent techniques such as reasoning\nbefore acting. The results demonstrate that solving complex tasks is\nchallenging: our best GPT-4-based agent only achieves an end-to-end task\nsuccess rate of 14.41%, significantly lower than the human performance of\n78.24%. These results highlight the need for further development of robust\nagents, that current state-of-the-art large language models are far from\nperfect performance in these real-life tasks, and that WebArena can be used to\nmeasure such progress.\n","authors":["Shuyan Zhou","Frank F. Xu","Hao Zhu","Xuhui Zhou","Robert Lo","Abishek Sridhar","Xianyi Cheng","Tianyue Ou","Yonatan Bisk","Daniel Fried","Uri Alon","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2307.13854v4.pdf","comment":"Our code, data, environment reproduction resources, and video\n  demonstrations are publicly available at https://webarena.dev/"},{"id":"http://arxiv.org/abs/2404.10630v1","updated":"2024-04-16T15:02:46Z","published":"2024-04-16T15:02:46Z","title":"HLAT: High-quality Large Language Model Pre-trained on AWS Trainium","summary":"  Getting large language models (LLMs) to perform well on the downstream tasks\nrequires pre-training over trillions of tokens. This typically demands a large\nnumber of powerful computational devices in addition to a stable distributed\ntraining framework to accelerate the training. The growing number of\napplications leveraging AI/ML had led to a scarcity of the expensive\nconventional accelerators (such as GPUs), which begs the need for the\nalternative specialized-accelerators that are scalable and cost-efficient. AWS\nTrainium is the second-generation machine learning accelerator that has been\npurposely built for training large deep learning models. Its corresponding\ninstance, Amazon EC2 trn1, is an alternative to GPU instances for LLM training.\nHowever, training LLMs with billions of parameters on trn1 is challenging due\nto its relatively nascent software ecosystem. In this paper, we showcase HLAT:\na 7 billion parameter decoder-only LLM pre-trained using trn1 instances over\n1.8 trillion tokens. The performance of HLAT is benchmarked against popular\nopen source baseline models including LLaMA and OpenLLaMA, which have been\ntrained on NVIDIA GPUs and Google TPUs, respectively. On various evaluation\ntasks, we show that HLAT achieves model quality on par with the baselines. We\nalso share the best practice of using the Neuron Distributed Training Library\n(NDTL), a customized distributed training library for AWS Trainium to achieve\nefficient training. Our work demonstrates that AWS Trainium powered by the NDTL\nis able to successfully pre-train state-of-the-art LLM models with high\nperformance and cost-effectiveness.\n","authors":["Haozheng Fan","Hao Zhou","Guangtai Huang","Parameswaran Raman","Xinwei Fu","Gaurav Gupta","Dhananjay Ram","Yida Wang","Jun Huan"],"pdf_url":"https://arxiv.org/pdf/2404.10630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18463v4","updated":"2024-04-16T15:00:06Z","published":"2023-10-27T20:15:23Z","title":"Benchingmaking Large Langage Models in Biomedical Triple Extraction","summary":"  Biomedical triple extraction systems aim to automatically extract biomedical\nentities and relations between entities. The exploration of applying large\nlanguage models (LLM) to triple extraction is still relatively unexplored. In\nthis work, we mainly focus on sentence-level biomedical triple extraction.\nFurthermore, the absence of a high-quality biomedical triple extraction dataset\nimpedes the progress in developing robust triple extraction systems. To address\nthese challenges, initially, we compare the performance of various large\nlanguage models. Additionally, we present GIT, an expert-annotated biomedical\ntriple extraction dataset that covers a wider range of relation types.\n","authors":["Mingchen Li","Huixue Zhou","Rui Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.18463v4.pdf","comment":"this is the onging work"},{"id":"http://arxiv.org/abs/2404.09841v2","updated":"2024-04-16T14:55:13Z","published":"2024-04-15T14:48:43Z","title":"Anatomy of Industrial Scale Multilingual ASR","summary":"  This paper describes AssemblyAI's industrial-scale automatic speech\nrecognition (ASR) system, designed to meet the requirements of large-scale,\nmultilingual ASR serving various application needs. Our system leverages a\ndiverse training dataset comprising unsupervised (12.5M hours), supervised\n(188k hours), and pseudo-labeled (1.6M hours) data across four languages. We\nprovide a detailed description of our model architecture, consisting of a\nfull-context 600M-parameter Conformer encoder pre-trained with BEST-RQ and an\nRNN-T decoder fine-tuned jointly with the encoder. Our extensive evaluation\ndemonstrates competitive word error rates (WERs) against larger and more\ncomputationally expensive models, such as Whisper large and Canary-1B.\nFurthermore, our architectural choices yield several key advantages, including\nan improved code-switching capability, a 5x inference speedup compared to an\noptimized Whisper baseline, a 30% reduction in hallucination rate on speech\ndata, and a 90% reduction in ambient noise compared to Whisper, along with\nsignificantly improved time-stamp accuracy. Throughout this work, we adopt a\nsystem-centric approach to analyzing various aspects of fully-fledged ASR\nmodels to gain practically relevant insights useful for real-world services\noperating at scale.\n","authors":["Francis McCann Ramirez","Luka Chkhetiani","Andrew Ehrenberg","Robert McHardy","Rami Botros","Yash Khare","Andrea Vanzo","Taufiquzzaman Peyash","Gabriel Oexle","Michael Liang","Ilya Sklyar","Enver Fakhan","Ahmed Etefy","Daniel McCrystal","Sam Flamini","Domenic Donato","Takuya Yoshioka"],"pdf_url":"https://arxiv.org/pdf/2404.09841v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.14212v2","updated":"2024-04-16T14:25:39Z","published":"2024-01-25T14:53:30Z","title":"Explicitly Representing Syntax Improves Sentence-to-layout Prediction of\n  Unexpected Situations","summary":"  Recognizing visual entities in a natural language sentence and arranging them\nin a 2D spatial layout require a compositional understanding of language and\nspace. This task of layout prediction is valuable in text-to-image synthesis as\nit allows localized and controlled in-painting of the image. In this\ncomparative study it is shown that we can predict layouts from language\nrepresentations that implicitly or explicitly encode sentence syntax, if the\nsentences mention similar entity-relationships to the ones seen during\ntraining. To test compositional understanding, we collect a test set of\ngrammatically correct sentences and layouts describing compositions of entities\nand relations that unlikely have been seen during training. Performance on this\ntest set substantially drops, showing that current models rely on correlations\nin the training data and have difficulties in understanding the structure of\nthe input sentences. We propose a novel structural loss function that better\nenforces the syntactic structure of the input sentence and show large\nperformance gains in the task of 2D spatial layout prediction conditioned on\ntext. The loss has the potential to be used in other generation tasks where a\ntree-like structure underlies the conditioning modality. Code, trained models\nand the USCOCO evaluation set are available via github.\n","authors":["Wolf Nuyts","Ruben Cartuyvels","Marie-Francine Moens"],"pdf_url":"https://arxiv.org/pdf/2401.14212v2.pdf","comment":"Published in TACL"},{"id":"http://arxiv.org/abs/2301.12463v2","updated":"2024-04-16T14:19:58Z","published":"2023-01-29T15:22:10Z","title":"Linguistic Analysis using Paninian System of Sounds and Finite State\n  Machines","summary":"  The study of spoken languages comprises phonology, morphology, and grammar.\nAnalysis of a language can be based on its syntax, semantics, and pragmatics.\nThe languages can be classified as root languages, inflectional languages, and\nstem languages. All these factors lead to the formation of vocabulary which has\ncommonality/similarity as well as distinct and subtle differences across\nlanguages. In this paper, we make use of Paninian system of sounds to construct\na phonetic map and then words are represented as state transitions on the\nphonetic map. Each group of related words that cut across languages is\nrepresented by a m-language (morphological language). Morphological Finite\nAutomata (MFA) are defined that accept the words belonging to a given\nm-language. This exercise can enable us to better understand the\ninter-relationships between words in spoken languages in both language-agnostic\nand language-cognizant manner. Based on our study and analysis, we propose an\nEcosystem Model for Linguistic Development with Sanskrit at the core, in place\nof the widely accepted family tree model.\n","authors":["Shreekanth M Prabhu","Abhisek Midye"],"pdf_url":"https://arxiv.org/pdf/2301.12463v2.pdf","comment":"47 Pages, 18 Figures, 24 Tables"},{"id":"http://arxiv.org/abs/2404.10579v1","updated":"2024-04-16T14:04:46Z","published":"2024-04-16T14:04:46Z","title":"The application of Augmented Reality (AR) in Remote Work and Education","summary":"  With the rapid advancement of technology, Augmented Reality (AR) technology,\nknown for its ability to deeply integrate virtual information with the real\nworld, is gradually transforming traditional work modes and teaching methods.\nParticularly in the realms of remote work and online education, AR technology\ndemonstrates a broad spectrum of application prospects. This paper delves into\nthe application potential and actual effects of AR technology in remote work\nand education. Through a systematic literature review, this study outlines the\nkey features, advantages, and challenges of AR technology. Based on theoretical\nanalysis, it discusses the scientific basis and technical support that AR\ntechnology provides for enhancing remote work efficiency and promoting\ninnovation in educational teaching models. Additionally, by designing an\nempirical research plan and analyzing experimental data, this article reveals\nthe specific performance and influencing factors of AR technology in practical\napplications. Finally, based on the results of the experiments, this research\nsummarizes the application value of AR technology in remote work and education,\nlooks forward to its future development trends, and proposes forward-looking\nresearch directions and strategic suggestions, offering empirical foundation\nand theoretical guidance for further promoting the in-depth application of AR\ntechnology in related fields.\n","authors":["Keqin Li","Peng Xirui","Jintong Song","Bo Hong","Jin Wang"],"pdf_url":"https://arxiv.org/pdf/2404.10579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19346v2","updated":"2024-04-16T13:53:47Z","published":"2024-03-28T12:04:28Z","title":"Large Language Models Are Unconscious of Unreasonability in Math\n  Problems","summary":"  Large language models (LLMs) demonstrate substantial capabilities in solving\nmath problems. However, they tend to produce hallucinations when given\nquestions containing unreasonable errors. In this paper, we study the behavior\nof LLMs when faced with unreasonable math problems and further explore their\npotential to address these problems. We construct the Unreasonable Math Problem\n(UMP) benchmark to examine the error detection ability of LLMs. Experiments\nshow that LLMs are able to detect unreasonable errors, but still fail in\ngenerating non-hallucinatory content. In order to improve their ability of\nerror detection and correction, we further design a strategic prompt template\ncalled Critical Calculation and Conclusion(CCC). With CCC, LLMs can better\nself-evaluate and detect unreasonable errors in math questions, making them\nmore reliable and safe in practical application scenarios.\n","authors":["Jingyuan Ma","Damai Dai","Lei Sha","Zhifang Sui"],"pdf_url":"https://arxiv.org/pdf/2403.19346v2.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2402.15861v4","updated":"2024-04-16T13:52:01Z","published":"2024-02-24T17:08:45Z","title":"MATHWELL: Generating Age-Appropriate Educational Math Word Problems","summary":"  Math word problems are critical K-8 educational tools, but writing them is\ntime-consuming and requires domain expertise. We suggest that language models\ncan support K-8 math education by automatically generating problems. To be\neducational, generated problems must be 1) solvable, 2) accurate, and 3)\nappropriate. Existing datasets are unlabeled for these criteria, making them\nill-suited for training problem generators. To address this gap, we use domain\nexpert annotation to curate a high-quality synthetic training dataset for this\ntask. We show the value of this data by using it to iteratively finetune\nLlama-2 (70B) to create MATHWELL, a K-8 word problem generator. Domain experts\nfind MATHWELL has a 40% higher share of problems that have executable solutions\nand meet all criteria than existing open-source models, with 74% of its\nproblems with executable solutions being solvable, accurate, and appropriate.\nMATHWELL achieves 94.9% of GPT-4 Turbo's performance on this task while\noutputting problems written at a more appropriate reading level for K-8\nstudents. MATHWELL's performance despite being trained by finetuning only\nhighlights the quality of our synthetic data for training age-appropriate word\nproblem generators. We release our model, data, and annotations.\n","authors":["Bryan R Christ","Jonathan Kropko","Thomas Hartvigsen"],"pdf_url":"https://arxiv.org/pdf/2402.15861v4.pdf","comment":"26 pages, 9 figures"},{"id":"http://arxiv.org/abs/2404.10555v1","updated":"2024-04-16T13:26:32Z","published":"2024-04-16T13:26:32Z","title":"Construction of Domain-specified Japanese Large Language Model for\n  Finance through Continual Pre-training","summary":"  Large language models (LLMs) are now widely used in various fields, including\nfinance. However, Japanese financial-specific LLMs have not been proposed yet.\nHence, this study aims to construct a Japanese financial-specific LLM through\ncontinual pre-training. Before tuning, we constructed Japanese\nfinancial-focused datasets for continual pre-training. As a base model, we\nemployed a Japanese LLM that achieved state-of-the-art performance on Japanese\nfinancial benchmarks among the 10-billion-class parameter models. After\ncontinual pre-training using the datasets and the base model, the tuned model\nperformed better than the original model on the Japanese financial benchmarks.\nMoreover, the outputs comparison results reveal that the tuned model's outputs\ntend to be better than the original model's outputs in terms of the quality and\nlength of the answers. These findings indicate that domain-specific continual\npre-training is also effective for LLMs. The tuned model is publicly available\non Hugging Face.\n","authors":["Masanori Hirano","Kentaro Imajo"],"pdf_url":"https://arxiv.org/pdf/2404.10555v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2404.10552v1","updated":"2024-04-16T13:22:54Z","published":"2024-04-16T13:22:54Z","title":"Unveiling the Misuse Potential of Base Large Language Models via\n  In-Context Learning","summary":"  The open-sourcing of large language models (LLMs) accelerates application\ndevelopment, innovation, and scientific progress. This includes both base\nmodels, which are pre-trained on extensive datasets without alignment, and\naligned models, deliberately designed to align with ethical standards and human\nvalues. Contrary to the prevalent assumption that the inherent\ninstruction-following limitations of base LLMs serve as a safeguard against\nmisuse, our investigation exposes a critical oversight in this belief. By\ndeploying carefully designed demonstrations, our research demonstrates that\nbase LLMs could effectively interpret and execute malicious instructions. To\nsystematically assess these risks, we introduce a novel set of risk evaluation\nmetrics. Empirical results reveal that the outputs from base LLMs can exhibit\nrisk levels on par with those of models fine-tuned for malicious purposes. This\nvulnerability, requiring neither specialized knowledge nor training, can be\nmanipulated by almost anyone, highlighting the substantial risk and the\ncritical need for immediate attention to the base LLMs' security protocols.\n","authors":["Xiao Wang","Tianze Chen","Xianjun Yang","Qi Zhang","Xun Zhao","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2404.10552v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.07117v2","updated":"2024-04-16T13:19:04Z","published":"2023-06-12T13:52:01Z","title":"Language of Bargaining","summary":"  Leveraging an established exercise in negotiation education, we build a novel\ndataset for studying how the use of language shapes bilateral bargaining. Our\ndataset extends existing work in two ways: 1) we recruit participants via\nbehavioral labs instead of crowdsourcing platforms and allow participants to\nnegotiate through audio, enabling more naturalistic interactions; 2) we add a\ncontrol setting where participants negotiate only through alternating, written\nnumeric offers. Despite the two contrasting forms of communication, we find\nthat the average agreed prices of the two treatments are identical. But when\nsubjects can talk, fewer offers are exchanged, negotiations finish faster, the\nlikelihood of reaching agreement rises, and the variance of prices at which\nsubjects agree drops substantially. We further propose a taxonomy of speech\nacts in negotiation and enrich the dataset with annotated speech acts. Our work\nalso reveals linguistic signals that are predictive of negotiation outcomes.\n","authors":["Mourad Heddaya","Solomon Dworkin","Chenhao Tan","Rob Voigt","Alexander Zentefis"],"pdf_url":"https://arxiv.org/pdf/2306.07117v2.pdf","comment":"ACL 2023 Main Conference"},{"id":"http://arxiv.org/abs/2403.08295v4","updated":"2024-04-16T12:52:47Z","published":"2024-03-13T06:59:16Z","title":"Gemma: Open Models Based on Gemini Research and Technology","summary":"  This work introduces Gemma, a family of lightweight, state-of-the art open\nmodels built from the research and technology used to create Gemini models.\nGemma models demonstrate strong performance across academic benchmarks for\nlanguage understanding, reasoning, and safety. We release two sizes of models\n(2 billion and 7 billion parameters), and provide both pretrained and\nfine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out\nof 18 text-based tasks, and we present comprehensive evaluations of safety and\nresponsibility aspects of the models, alongside a detailed description of model\ndevelopment. We believe the responsible release of LLMs is critical for\nimproving the safety of frontier models, and for enabling the next wave of LLM\ninnovations.\n","authors":[" Gemma Team","Thomas Mesnard","Cassidy Hardin","Robert Dadashi","Surya Bhupatiraju","Shreya Pathak","Laurent Sifre","Morgane Rivi√®re","Mihir Sanjay Kale","Juliette Love","Pouya Tafti","L√©onard Hussenot","Pier Giuseppe Sessa","Aakanksha Chowdhery","Adam Roberts","Aditya Barua","Alex Botev","Alex Castro-Ros","Ambrose Slone","Am√©lie H√©liou","Andrea Tacchetti","Anna Bulanova","Antonia Paterson","Beth Tsai","Bobak Shahriari","Charline Le Lan","Christopher A. Choquette-Choo","Cl√©ment Crepy","Daniel Cer","Daphne Ippolito","David Reid","Elena Buchatskaya","Eric Ni","Eric Noland","Geng Yan","George Tucker","George-Christian Muraru","Grigory Rozhdestvenskiy","Henryk Michalewski","Ian Tenney","Ivan Grishchenko","Jacob Austin","James Keeling","Jane Labanowski","Jean-Baptiste Lespiau","Jeff Stanway","Jenny Brennan","Jeremy Chen","Johan Ferret","Justin Chiu","Justin Mao-Jones","Katherine Lee","Kathy Yu","Katie Millican","Lars Lowe Sjoesund","Lisa Lee","Lucas Dixon","Machel Reid","Maciej Miku≈Ça","Mateo Wirth","Michael Sharman","Nikolai Chinaev","Nithum Thain","Olivier Bachem","Oscar Chang","Oscar Wahltinez","Paige Bailey","Paul Michel","Petko Yotov","Rahma Chaabouni","Ramona Comanescu","Reena Jana","Rohan Anil","Ross McIlroy","Ruibo Liu","Ryan Mullins","Samuel L Smith","Sebastian Borgeaud","Sertan Girgin","Sholto Douglas","Shree Pandya","Siamak Shakeri","Soham De","Ted Klimenko","Tom Hennigan","Vlad Feinberg","Wojciech Stokowiec","Yu-hui Chen","Zafarali Ahmed","Zhitao Gong","Tris Warkentin","Ludovic Peran","Minh Giang","Cl√©ment Farabet","Oriol Vinyals","Jeff Dean","Koray Kavukcuoglu","Demis Hassabis","Zoubin Ghahramani","Douglas Eck","Joelle Barral","Fernando Pereira","Eli Collins","Armand Joulin","Noah Fiedel","Evan Senter","Alek Andreev","Kathleen Kenealy"],"pdf_url":"https://arxiv.org/pdf/2403.08295v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.10154v2","updated":"2024-04-16T12:44:07Z","published":"2023-05-17T12:18:44Z","title":"Cultural evolution via iterated learning and communication explains\n  efficient color naming systems","summary":"  It has been argued that semantic systems reflect pressure for efficiency, and\na current debate concerns the cultural evolutionary process that produces this\npattern. We consider efficiency as instantiated in the Information Bottleneck\n(IB) principle, and a model of cultural evolution that combines iterated\nlearning and communication. We show that this model, instantiated in neural\nnetworks, converges to color naming systems that are efficient in the IB sense\nand similar to human color naming systems. We also show that some other\nproposals such as iterated learning alone, communication alone, or the greater\nlearnability of convex categories, do not yield the same outcome as clearly. We\nconclude that the combination of iterated learning and communication provides a\nplausible means by which human semantic systems become efficient.\n","authors":["Emil Carlsson","Devdatt Dubhashi","Terry Regier"],"pdf_url":"https://arxiv.org/pdf/2305.10154v2.pdf","comment":"An earlier version of this paper appeared in the Proceedings of the\n  45th Annual Meeting of the Cognitive Science Society (2023)"},{"id":"http://arxiv.org/abs/2404.10513v1","updated":"2024-04-16T12:37:10Z","published":"2024-04-16T12:37:10Z","title":"CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\n  Granularity","summary":"  State-of-the-art performance in QA tasks is currently achieved by systems\nemploying Large Language Models (LLMs), however these models tend to\nhallucinate information in their responses. One approach focuses on enhancing\nthe generation process by incorporating attribution from the given input to the\noutput. However, the challenge of identifying appropriate attributions and\nverifying their accuracy against a source is a complex task that requires\nsignificant improvements in assessing such systems. We introduce an\nattribution-oriented Chain-of-Thought reasoning method to enhance the accuracy\nof attributions. This approach focuses the reasoning process on generating an\nattribution-centric output. Evaluations on two context-enhanced\nquestion-answering datasets using GPT-4 demonstrate improved accuracy and\ncorrectness of attributions. In addition, the combination of our method with\nfinetuning enhances the response and attribution accuracy of two smaller LLMs,\nshowing their potential to outperform GPT-4 in some cases.\n","authors":["Moshe Berchansky","Daniel Fleischer","Moshe Wasserblat","Peter Izsak"],"pdf_url":"https://arxiv.org/pdf/2404.10513v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10508v1","updated":"2024-04-16T12:27:54Z","published":"2024-04-16T12:27:54Z","title":"White Men Lead, Black Women Help: Uncovering Gender, Racial, and\n  Intersectional Bias in Language Agency","summary":"  Social biases can manifest in language agency. For instance, White\nindividuals and men are often described as \"agentic\" and achievement-oriented,\nwhereas Black individuals and women are frequently described as \"communal\" and\nas assisting roles. This study establishes agency as an important aspect of\nstudying social biases in both human-written and Large Language Model\n(LLM)-generated texts. To accurately measure \"language agency\" at sentence\nlevel, we propose a Language Agency Classification dataset to train reliable\nagency classifiers. We then use an agency classifier to reveal notable language\nagency biases in 6 datasets of human- or LLM-written texts, including\nbiographies, professor reviews, and reference letters. While most prior NLP\nresearch on agency biases focused on single dimensions, we comprehensively\nexplore language agency biases in gender, race, and intersectional identities.\nWe observe that (1) language agency biases in human-written texts align with\nreal-world social observations; (2) LLM-generated texts demonstrate remarkably\nhigher levels of language agency bias than human-written texts; and (3)\ncritical biases in language agency target people of minority groups--for\ninstance, languages used to describe Black females exhibit the lowest level of\nagency across datasets. Our findings reveal intricate social biases in human-\nand LLM-written texts through the lens of language agency, warning against\nusing LLM generations in social contexts without scrutiny.\n","authors":["Yixin Wan","Kai-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2404.10508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10503v1","updated":"2024-04-16T12:20:49Z","published":"2024-04-16T12:20:49Z","title":"A Sentiment Analysis of Medical Text Based on Deep Learning","summary":"  The field of natural language processing (NLP) has made significant progress\nwith the rapid development of deep learning technologies. One of the research\ndirections in text sentiment analysis is sentiment analysis of medical texts,\nwhich holds great potential for application in clinical diagnosis. However, the\nmedical field currently lacks sufficient text datasets, and the effectiveness\nof sentiment analysis is greatly impacted by different model design approaches,\nwhich presents challenges. Therefore, this paper focuses on the medical domain,\nusing bidirectional encoder representations from transformers (BERT) as the\nbasic pre-trained model and experimenting with modules such as convolutional\nneural network (CNN), fully connected network (FCN), and graph convolutional\nnetworks (GCN) at the output layer. Experiments and analyses were conducted on\nthe METS-CoV dataset to explore the training performance after integrating\ndifferent deep learning networks. The results indicate that CNN models\noutperform other networks when trained on smaller medical text datasets in\ncombination with pre-trained models like BERT. This study highlights the\nsignificance of model selection in achieving effective sentiment analysis in\nthe medical domain and provides a reference for future research to develop more\nefficient model architectures.\n","authors":["Yinan Chen"],"pdf_url":"https://arxiv.org/pdf/2404.10503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10501v1","updated":"2024-04-16T12:19:54Z","published":"2024-04-16T12:19:54Z","title":"Self-Supervised Visual Preference Alignment","summary":"  This paper makes the first attempt towards unsupervised preference alignment\nin Vision-Language Models (VLMs). We generate chosen and rejected responses\nwith regard to the original and augmented image pairs, and conduct preference\nalignment with direct preference optimization. It is based on a core idea:\nproperly designed augmentation to the image input will induce VLM to generate\nfalse but hard negative responses, which helps the model to learn from and\nproduce more robust and powerful answers. The whole pipeline no longer hinges\non supervision from GPT4 or human involvement during alignment, and is highly\nefficient with few lines of code. With only 8k randomly sampled unsupervised\ndata, it achieves 90\\% relative score to GPT-4 on complex reasoning in\nLLaVA-Bench, and improves LLaVA-7B/13B by 6.7\\%/5.6\\% score on complex\nmulti-modal benchmark MM-Vet. Visualizations shows its improved ability to\nalign with user-intentions. A series of ablations are firmly conducted to\nreveal the latent mechanism of the approach, which also indicates its potential\ntowards further scaling. Code will be available.\n","authors":["Ke Zhu","Liang Zhao","Zheng Ge","Xiangyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.10501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10500v1","updated":"2024-04-16T12:19:08Z","published":"2024-04-16T12:19:08Z","title":"When Emotional Stimuli meet Prompt Designing: An Auto-Prompt Graphical\n  Paradigm","summary":"  With the development of Large Language Models (LLM), numerous prompts have\nbeen proposed, each with a rich set of features and their own merits. This\npaper summarizes the prompt words for large language models (LLMs),\ncategorizing them into stimulating and framework types, and proposes an\nAuto-Prompt Graphical Paradigm(APGP) that combines both stimulating and\nframework prompts to enhance the problem-solving capabilities of LLMs across\nmultiple domains, then exemplifies it with a framework that adheres to this\nparadigm. The framework involves automated prompt generation and consideration\nof emotion-stimulus factors, guiding LLMs in problem abstraction, diversified\nsolutions generation, comprehensive optimization, and self-verification after\nproviding answers, ensuring solution accuracy. Compared to traditional stimuli\nand framework prompts, this framework integrates the advantages of both by\nadopting automated approaches inspired by APE work, overcoming the limitations\nof manually designed prompts. Test results on the ruozhiba and BBH datasets\ndemonstrate that this framework can effectively improve the efficiency and\naccuracy of LLMs in problem-solving, paving the way for new applications of\nLLMs.\n","authors":["Chenggian Ma","Xiangyu Zhao","Chunhui Zhang","Yanzhao Qin","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.10500v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2404.09956v2","updated":"2024-04-16T12:12:39Z","published":"2024-04-15T17:31:22Z","title":"Tango 2: Aligning Diffusion-based Text-to-Audio Generations through\n  Direct Preference Optimization","summary":"  Generative multimodal content is increasingly prevalent in much of the\ncontent creation arena, as it has the potential to allow artists and media\npersonnel to create pre-production mockups by quickly bringing their ideas to\nlife. The generation of audio from text prompts is an important aspect of such\nprocesses in the music and film industry. Many of the recent diffusion-based\ntext-to-audio models focus on training increasingly sophisticated diffusion\nmodels on a large set of datasets of prompt-audio pairs. These models do not\nexplicitly focus on the presence of concepts or events and their temporal\nordering in the output audio with respect to the input prompt. Our hypothesis\nis focusing on how these aspects of audio generation could improve audio\ngeneration performance in the presence of limited data. As such, in this work,\nusing an existing text-to-audio model Tango, we synthetically create a\npreference dataset where each prompt has a winner audio output and some loser\naudio outputs for the diffusion model to learn from. The loser outputs, in\ntheory, have some concepts from the prompt missing or in an incorrect order. We\nfine-tune the publicly available Tango text-to-audio model using diffusion-DPO\n(direct preference optimization) loss on our preference dataset and show that\nit leads to improved audio output over Tango and AudioLDM2, in terms of both\nautomatic- and manual-evaluation metrics.\n","authors":["Navonil Majumder","Chia-Yu Hung","Deepanway Ghosal","Wei-Ning Hsu","Rada Mihalcea","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2404.09956v2.pdf","comment":"https://github.com/declare-lab/tango"},{"id":"http://arxiv.org/abs/2403.06249v2","updated":"2024-04-16T12:05:33Z","published":"2024-03-10T16:22:20Z","title":"No Language is an Island: Unifying Chinese and English in Financial\n  Large Language Models, Instruction Data, and Benchmarks","summary":"  While the progression of Large Language Models (LLMs) has notably propelled\nfinancial analysis, their application has largely been confined to singular\nlanguage realms, leaving untapped the potential of bilingual Chinese-English\ncapacity. To bridge this chasm, we introduce ICE-PIXIU, seamlessly amalgamating\nthe ICE-INTENT model and ICE-FLARE benchmark for bilingual financial analysis.\nICE-PIXIU uniquely integrates a spectrum of Chinese tasks, alongside translated\nand original English datasets, enriching the breadth and depth of bilingual\nfinancial modeling. It provides unrestricted access to diverse model variants,\na substantial compilation of diverse cross-lingual and multi-modal instruction\ndata, and an evaluation benchmark with expert annotations, comprising 10 NLP\ntasks, 20 bilingual specific tasks, totaling 95k datasets. Our thorough\nevaluation emphasizes the advantages of incorporating these bilingual datasets,\nespecially in translation tasks and utilizing original English data, enhancing\nboth linguistic flexibility and analytical acuity in financial contexts.\nNotably, ICE-INTENT distinguishes itself by showcasing significant enhancements\nover conventional LLMs and existing financial LLMs in bilingual milieus,\nunderscoring the profound impact of robust bilingual data on the accuracy and\nefficacy of financial NLP.\n","authors":["Gang Hu","Ke Qin","Chenhan Yuan","Min Peng","Alejandro Lopez-Lira","Benyou Wang","Sophia Ananiadou","Wanlong Yu","Jimin Huang","Qianqian Xie"],"pdf_url":"https://arxiv.org/pdf/2403.06249v2.pdf","comment":"24 pages, 5 figures, 12 tables, including Appendix"},{"id":"http://arxiv.org/abs/2402.11651v2","updated":"2024-04-16T11:41:13Z","published":"2024-02-18T17:10:07Z","title":"Learning From Failure: Integrating Negative Examples when Fine-tuning\n  Large Language Models as Agents","summary":"  Large language models (LLMs) have achieved success in acting as agents, which\ninteract with environments through tools such as search engines. However, LLMs\nare optimized for language generation instead of tool use during training or\nalignment, limiting their effectiveness as agents. To resolve this problem,\nprevious work has first collected interaction trajectories between LLMs and\nenvironments, using only trajectories that successfully finished the task to\nfine-tune smaller models, making fine-tuning data scarce and acquiring it both\ndifficult and costly. Discarding failed trajectories also leads to significant\nwastage of data and resources and limits the possible optimization paths during\nfine-tuning. In this paper, we argue that unsuccessful trajectories offer\nvaluable insights, and LLMs can learn from these trajectories through\nappropriate quality control and fine-tuning strategies. By simply adding a\nprefix or suffix that tells the model whether to generate a successful\ntrajectory during training, we improve model performance by a large margin on\nmathematical reasoning, multi-hop question answering, and strategic question\nanswering tasks. We further analyze the inference results and find that our\nmethod provides a better trade-off between valuable information and errors in\nunsuccessful trajectories. To our knowledge, we are the first to demonstrate\nthe value of negative trajectories and their application in agent-tunning\nscenarios. Our findings offer guidance for developing better agent-tuning\nmethods and low-resource data usage techniques.\n","authors":["Renxi Wang","Haonan Li","Xudong Han","Yixuan Zhang","Timothy Baldwin"],"pdf_url":"https://arxiv.org/pdf/2402.11651v2.pdf","comment":"Agent, LLM, Large Language Model"},{"id":"http://arxiv.org/abs/2308.10261v4","updated":"2024-04-16T11:38:35Z","published":"2023-08-20T13:15:18Z","title":"How Good Are LLMs at Out-of-Distribution Detection?","summary":"  Out-of-distribution (OOD) detection plays a vital role in enhancing the\nreliability of machine learning (ML) models. The emergence of large language\nmodels (LLMs) has catalyzed a paradigm shift within the ML community,\nshowcasing their exceptional capabilities across diverse natural language\nprocessing tasks. While existing research has probed OOD detection with\nrelative small-scale Transformers like BERT, RoBERTa and GPT-2, the stark\ndifferences in scales, pre-training objectives, and inference paradigms call\ninto question the applicability of these findings to LLMs. This paper embarks\non a pioneering empirical investigation of OOD detection in the domain of LLMs,\nfocusing on LLaMA series ranging from 7B to 65B in size. We thoroughly evaluate\ncommonly-used OOD detectors, scrutinizing their performance in both zero-grad\nand fine-tuning scenarios. Notably, we alter previous discriminative\nin-distribution fine-tuning into generative fine-tuning, aligning the\npre-training objective of LLMs with downstream tasks. Our findings unveil that\na simple cosine distance OOD detector demonstrates superior efficacy,\noutperforming other OOD detectors. We provide an intriguing explanation for\nthis phenomenon by highlighting the isotropic nature of the embedding spaces of\nLLMs, which distinctly contrasts with the anisotropic property observed in\nsmaller BERT family models. The new insight enhances our understanding of how\nLLMs detect OOD data, thereby enhancing their adaptability and reliability in\ndynamic environments. We have released the source code at\n\\url{https://github.com/Awenbocc/LLM-OOD} for other researchers to reproduce\nour results.\n","authors":["Bo Liu","Liming Zhan","Zexin Lu","Yujie Feng","Lei Xue","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2308.10261v4.pdf","comment":"Accepted at COLING 2024"},{"id":"http://arxiv.org/abs/2404.10475v1","updated":"2024-04-16T11:33:36Z","published":"2024-04-16T11:33:36Z","title":"Conversations as a Source for Teaching Scientific Concepts at Different\n  Education Levels","summary":"  Open conversations are one of the most engaging forms of teaching. However,\ncreating those conversations in educational software is a complex endeavor,\nespecially if we want to address the needs of different audiences. While\nlanguage models hold great promise for educational applications, there are\nsubstantial challenges in training them to engage in meaningful and effective\nconversational teaching, especially when considering the diverse needs of\nvarious audiences. No official data sets exist for this task to facilitate the\ntraining of language models for conversational teaching, considering the\ndiverse needs of various audiences. This paper presents a novel source for\nfacilitating conversational teaching of scientific concepts at various\ndifficulty levels (from preschooler to expert), namely dialogues taken from\nvideo transcripts. We analyse this data source in various ways to show that it\noffers a diverse array of examples that can be used to generate contextually\nappropriate and natural responses to scientific topics for specific target\naudiences. It is a freely available valuable resource for training and\nevaluating conversation models, encompassing organically occurring dialogues.\nWhile the raw data is available online, we provide additional metadata for\nconversational analysis of dialogues at each level in all available videos.\n","authors":["Donya Rooein","Dirk Hovy"],"pdf_url":"https://arxiv.org/pdf/2404.10475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10464v1","updated":"2024-04-16T11:07:48Z","published":"2024-04-16T11:07:48Z","title":"DESTEIN: Navigating Detoxification of Language Models via Universal\n  Steering Pairs and Head-wise Activation Fusion","summary":"  Despite the remarkable achievements of language models (LMs) across a broad\nspectrum of tasks, their propensity for generating toxic outputs remains a\nprevalent concern. Current solutions involving fine-tuning or auxiliary models\nusually require extensive memory and computational resources, rendering them\nless practical for deployment in large language models (LLMs). In this paper,\nwe propose DeStein, a novel method that detoxififies LMs by altering their\ninternal representations in the activation space with lower resource and time\ncost. Specifically, we leverage self-induced steering pairs to identify\ndetoxification vectors through arithmetic operations in the activation space.\nDuring inference, detoxification is achieved by blending the detoxification\nvectors with the original representations. Empirical results demonstrate that\nour method significantly outperforms previous state-of-the-art approaches on\npopular detoxification metrics, while also maintaining satisfactory generation\nquality and diversity. Furthermore, we extend our method to multiple LLMs,\ndemonstrating its practicality and scalability. Warning: some example model\noutputs contain highly offensive or disturbing text.\n","authors":["Yu Li","Zhihua Wei","Han Jiang","Chuanyang Gong"],"pdf_url":"https://arxiv.org/pdf/2404.10464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.10768v4","updated":"2024-04-16T11:00:11Z","published":"2024-01-19T15:39:49Z","title":"Knowledge Verification to Nip Hallucination in the Bud","summary":"  While large language models (LLMs) have demonstrated exceptional performance\nacross various tasks following human alignment, they may still generate\nresponses that sound plausible but contradict factual knowledge, a phenomenon\nknown as \\emph{hallucination}. In this paper, we demonstrate the feasibility of\nmitigating hallucinations by verifying and minimizing the inconsistency between\nexternal knowledge present in the alignment data and the intrinsic knowledge\nembedded within foundation LLMs. Specifically, we propose a novel approach\ncalled Knowledge Consistent Alignment (KCA), which employs a well-aligned LLM\nto automatically formulate assessments based on external knowledge to evaluate\nthe knowledge boundaries of foundation LLMs. To address knowledge\ninconsistencies in the alignment data, KCA implements several specific\nstrategies to deal with these data instances. We demonstrate the superior\nefficacy of KCA in reducing hallucinations across six benchmarks, utilizing\nfoundation LLMs of varying backbones and scales. This confirms the\neffectiveness of mitigating hallucinations by reducing knowledge inconsistency.\nOur code, model weights, and data are openly accessible at\n\\url{https://github.com/fanqiwan/KCA}.\n","authors":["Fanqi Wan","Xinting Huang","Leyang Cui","Xiaojun Quan","Wei Bi","Shuming Shi"],"pdf_url":"https://arxiv.org/pdf/2401.10768v4.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2311.10777v4","updated":"2024-04-16T10:59:11Z","published":"2023-11-16T06:01:47Z","title":"A Systematic Review of Aspect-based Sentiment Analysis (ABSA): Domains,\n  Methods, and Trends","summary":"  Aspect-based Sentiment Analysis (ABSA) is a fine-grained type of sentiment\nanalysis that identifies aspects and their associated opinions from a given\ntext. With the surge of digital opinionated text data, ABSA gained increasing\npopularity for its ability to mine more detailed and targeted insights. Many\nreview papers on ABSA subtasks and solution methodologies exist, however, few\nfocus on trends over time or systemic issues relating to research application\ndomains, datasets, and solution approaches. To fill the gap, this paper\npresents a Systematic Literature Review (SLR) of ABSA studies with a focus on\ntrends and high-level relationships among these fundamental components. This\nreview is one of the largest SLRs on ABSA, and also, to our knowledge, the\nfirst that systematically examines the trends and inter-relations among ABSA\nresearch and data distribution across domains and solution paradigms and\napproaches. Our sample includes 519 primary studies screened from 4191 search\nresults without time constraints via an innovative automatic filtering process.\nOur quantitative analysis not only identifies trends in nearly two decades of\nABSA research development but also unveils a systemic lack of dataset and\ndomain diversity as well as domain mismatch that may hinder the development of\nfuture ABSA research. We discuss these findings and their implications and\npropose suggestions for future research.\n","authors":["Yan Cathy Hua","Paul Denny","Katerina Taskova","J√∂rg Wicker"],"pdf_url":"https://arxiv.org/pdf/2311.10777v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10440v1","updated":"2024-04-16T10:10:19Z","published":"2024-04-16T10:10:19Z","title":"Language Proficiency and F0 Entrainment: A Study of L2 English Imitation\n  in Italian, French, and Slovak Speakers","summary":"  This study explores F0 entrainment in second language (L2) English speech\nimitation during an Alternating Reading Task (ART). Participants with Italian,\nFrench, and Slovak native languages imitated English utterances, and their F0\nentrainment was quantified using the Dynamic Time Warping (DTW) distance\nbetween the parameterized F0 contours of the imitated utterances and those of\nthe model utterances. Results indicate a nuanced relationship between L2\nEnglish proficiency and entrainment: speakers with higher proficiency generally\nexhibit less entrainment in pitch variation and declination. However, within\ndyads, the more proficient speakers demonstrate a greater ability to mimic\npitch range, leading to increased entrainment. This suggests that proficiency\ninfluences entrainment differently at individual and dyadic levels,\nhighlighting the complex interplay between language skill and prosodic\nadaptation.\n","authors":["Zheng Yuan","≈†tefan Be≈àu≈°","Alessandro D'Ausilio"],"pdf_url":"https://arxiv.org/pdf/2404.10440v1.pdf","comment":"Accepted at Speech Prosody 2024"},{"id":"http://arxiv.org/abs/2404.07840v2","updated":"2024-04-16T10:05:27Z","published":"2024-04-11T15:27:56Z","title":"On Training Data Influence of GPT Models","summary":"  Amidst the rapid advancements in generative language models, the\ninvestigation of how training data shapes the performance of GPT models is\nstill emerging. This paper presents GPTfluence, a novel approach that leverages\na featurized simulation to assess the impact of training examples on the\ntraining dynamics of GPT models. Our approach not only traces the influence of\nindividual training instances on performance trajectories, such as loss and\nother key metrics, on targeted test points but also enables a comprehensive\ncomparison with existing methods across various training scenarios in GPT\nmodels, ranging from 14 million to 2.8 billion parameters, across a range of\ndownstream tasks. Contrary to earlier methods that struggle with generalization\nto new data, GPTfluence introduces a parameterized simulation of training\ndynamics, demonstrating robust generalization capabilities to unseen training\ndata. This adaptability is evident across both fine-tuning and\ninstruction-tuning scenarios, spanning tasks in natural language understanding\nand generation. We will make our code and data publicly available.\n","authors":["Qingyi Liu","Yekun Chai","Shuohuan Wang","Yu Sun","Qiwei Peng","Keze Wang","Hua Wu"],"pdf_url":"https://arxiv.org/pdf/2404.07840v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03909v2","updated":"2024-04-16T10:00:41Z","published":"2024-03-06T18:14:22Z","title":"A Measure for Transparent Comparison of Linguistic Diversity in\n  Multilingual NLP Data Sets","summary":"  Typologically diverse benchmarks are increasingly created to track the\nprogress achieved in multilingual NLP. Linguistic diversity of these data sets\nis typically measured as the number of languages or language families included\nin the sample, but such measures do not consider structural properties of the\nincluded languages. In this paper, we propose assessing linguistic diversity of\na data set against a reference language sample as a means of maximising\nlinguistic diversity in the long run. We represent languages as sets of\nfeatures and apply a version of the Jaccard index suitable for comparing sets\nof measures. In addition to the features extracted from typological data bases,\nwe propose an automatic text-based measure, which can be used as a means of\novercoming the well-known problem of data sparsity in manually collected\nfeatures. Our diversity score is interpretable in terms of linguistic features\nand can identify the types of languages that are not represented in a data set.\nUsing our method, we analyse a range of popular multilingual data sets (UD,\nBible100, mBERT, XTREME, XGLUE, XNLI, XCOPA, TyDiQA, XQuAD). In addition to\nranking these data sets, we find, for example, that (poly)synthetic languages\nare missing in almost all of them.\n","authors":["Tanja Samardzic","Ximena Gutierrez","Christian Bentz","Steven Moran","Olga Pelloni"],"pdf_url":"https://arxiv.org/pdf/2403.03909v2.pdf","comment":"Accepted to NAACL 2024 Findings"},{"id":"http://arxiv.org/abs/2404.10419v1","updated":"2024-04-16T09:35:27Z","published":"2024-04-16T09:35:27Z","title":"MAD Speech: Measures of Acoustic Diversity of Speech","summary":"  Generative spoken language models produce speech in a wide range of voices,\nprosody, and recording conditions, seemingly approaching the diversity of\nnatural speech. However, the extent to which generated speech is acoustically\ndiverse remains unclear due to a lack of appropriate metrics. We address this\ngap by developing lightweight metrics of acoustic diversity, which we\ncollectively refer to as MAD Speech. We focus on measuring five facets of\nacoustic diversity: voice, gender, emotion, accent, and background noise. We\nconstruct the metrics as a composition of specialized, per-facet embedding\nmodels and an aggregation function that measures diversity within the embedding\nspace. Next, we build a series of datasets with a priori known diversity\npreferences for each facet. Using these datasets, we demonstrate that our\nproposed metrics achieve a stronger agreement with the ground-truth diversity\nthan baselines. Finally, we showcase the applicability of our proposed metrics\nacross several real-life evaluation scenarios. MAD Speech will be made publicly\naccessible.\n","authors":["Matthieu Futeral","Andrea Agostinelli","Marco Tagliasacchi","Neil Zeghidour","Eugene Kharitonov"],"pdf_url":"https://arxiv.org/pdf/2404.10419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03848v4","updated":"2024-04-16T09:14:46Z","published":"2024-02-06T09:50:08Z","title":"ANLS* -- A Universal Document Processing Metric for Generative Large\n  Language Models","summary":"  Traditionally, discriminative models have been the predominant choice for\ntasks like document classification and information extraction. These models\nmake predictions that fall into a limited number of predefined classes,\nfacilitating a binary true or false evaluation and enabling the direct\ncalculation of metrics such as the F1 score. However, recent advancements in\ngenerative large language models (GLLMs) have prompted a shift in the field due\nto their enhanced zero-shot capabilities, which eliminate the need for a\ndownstream dataset and computationally expensive fine-tuning. However,\nevaluating GLLMs presents a challenge as the binary true or false evaluation\nused for discriminative models is not applicable to the predictions made by\nGLLMs.\n  This paper introduces a new metric for generative models called ANLS* for\nevaluating a wide variety of tasks, including information extraction and\nclassification tasks. The ANLS* metric extends existing ANLS metrics as a\ndrop-in-replacement and is still compatible with previously reported ANLS\nscores. An evaluation of 7 different datasets, 6 different GLLMs and 3\ndifferent prompting methods using the ANLS* metric is also provided,\ndemonstrating the importance of the proposed metric.\n  We also benchmark a novel approach to generate prompts for documents, called\nSFT, against other prompting techniques such as LATIN. In 27 out of 35 cases,\nSFT outperforms other techniques and improves the state-of-the-art, sometimes\nby as much as $18$ percentage points.\n  Sources are available at https://github.com/deepopinion/anls_star_metric\n","authors":["David Peer","Philemon Sch√∂pf","Volckmar Nebendahl","Alexander Rietzler","Sebastian Stabinger"],"pdf_url":"https://arxiv.org/pdf/2402.03848v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11809v2","updated":"2024-04-16T08:36:31Z","published":"2024-02-19T03:39:10Z","title":"Generation Meets Verification: Accelerating Large Language Model\n  Inference with Smart Parallel Auto-Correct Decoding","summary":"  This research aims to accelerate the inference speed of large language models\n(LLMs) with billions of parameters. We propose \\textbf{S}mart \\textbf{P}arallel\n\\textbf{A}uto-\\textbf{C}orrect d\\textbf{E}coding (SPACE), an innovative\napproach designed for achieving lossless acceleration of LLMs. By integrating\nsemi-autoregressive inference and speculative decoding capabilities, SPACE\nuniquely enables autoregressive LLMs to parallelize token generation and\nverification. This is realized through a specialized semi-autoregressive\nsupervised fine-tuning process that equips existing LLMs with the ability to\nsimultaneously predict multiple tokens. Additionally, an auto-correct decoding\nalgorithm facilitates the simultaneous generation and verification of token\nsequences within a single model invocation. Through extensive experiments on a\nrange of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x\non HumanEval-X while maintaining output quality.\n","authors":["Hanling Yi","Feng Lin","Hongbin Li","Peiyang Ning","Xiaotian Yu","Rong Xiao"],"pdf_url":"https://arxiv.org/pdf/2402.11809v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10384v1","updated":"2024-04-16T08:28:16Z","published":"2024-04-16T08:28:16Z","title":"Reasoning on Efficient Knowledge Paths:Knowledge Graph Guides Large\n  Language Model for Domain Question Answering","summary":"  Large language models (LLMs), such as GPT3.5, GPT4 and LLAMA2 perform\nsurprisingly well and outperform human experts on many tasks. However, in many\ndomain-specific evaluations, these LLMs often suffer from hallucination\nproblems due to insufficient training of relevant corpus. Furthermore,\nfine-tuning large models may face problems such as the LLMs are not open source\nor the construction of high-quality domain instruction is difficult. Therefore,\nstructured knowledge databases such as knowledge graph can better provide\ndomain back- ground knowledge for LLMs and make full use of the reasoning and\nanalysis capabilities of LLMs. In some previous works, LLM was called multiple\ntimes to determine whether the current triplet was suitable for inclusion in\nthe subgraph when retrieving subgraphs through a question. Especially for the\nquestion that require a multi-hop reasoning path, frequent calls to LLM will\nconsume a lot of computing power. Moreover, when choosing the reasoning path,\nLLM will be called once for each step, and if one of the steps is selected\nincorrectly, it will lead to the accumulation of errors in the following steps.\nIn this paper, we integrated and optimized a pipeline for selecting reasoning\npaths from KG based on LLM, which can reduce the dependency on LLM. In\naddition, we propose a simple and effective subgraph retrieval method based on\nchain of thought (CoT) and page rank which can returns the paths most likely to\ncontain the answer. We conduct experiments on three datasets: GenMedGPT-5k\n[14], WebQuestions [2], and CMCQA [21]. Finally, RoK can demonstrate that using\nfewer LLM calls can achieve the same results as previous SOTAs models.\n","authors":["Yuqi Wang","Boran Jiang","Yi Luo","Dawei He","Peng Cheng","Liangcai Gao"],"pdf_url":"https://arxiv.org/pdf/2404.10384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09000v3","updated":"2024-04-16T07:41:43Z","published":"2023-11-15T14:41:57Z","title":"Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic\n  Fact-checkers","summary":"  The increased use of large language models (LLMs) across a variety of\nreal-world applications calls for mechanisms to verify the factual accuracy of\ntheir outputs. In this work, we present a holistic end-to-end solution for\nannotating the factuality of LLM-generated responses, which encompasses a\nmulti-stage annotation scheme designed to yield detailed labels concerning the\nverifiability and factual inconsistencies found in LLM outputs. We further\nconstruct an open-domain document-level factuality benchmark in three-level\ngranularity: claim, sentence and document, aiming to facilitate the evaluation\nof automatic fact-checking systems. Preliminary experiments show that FacTool,\nFactScore and Perplexity.ai are struggling to identify false claims, with the\nbest F1=0.63 by this annotation solution based on GPT-4. Annotation tool,\nbenchmark and code are available at https://github.com/yuxiaw/Factcheck-GPT.\n","authors":["Yuxia Wang","Revanth Gangi Reddy","Zain Muhammad Mujahid","Arnav Arora","Aleksandr Rubashevskii","Jiahui Geng","Osama Mohammed Afzal","Liangming Pan","Nadav Borenstein","Aditya Pillai","Isabelle Augenstein","Iryna Gurevych","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2311.09000v3.pdf","comment":"30 pages, 13 figures"},{"id":"http://arxiv.org/abs/2402.17302v2","updated":"2024-04-16T07:41:12Z","published":"2024-02-27T08:24:32Z","title":"Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in\n  Indonesian and Sundanese","summary":"  Large Language Models (LLMs) are increasingly being used to generate\nsynthetic data for training and evaluating models. However, it is unclear\nwhether they can generate a good quality of question answering (QA) dataset\nthat incorporates knowledge and cultural nuance embedded in a language,\nespecially for low-resource languages. In this study, we investigate the\neffectiveness of using LLMs in generating culturally relevant commonsense QA\ndatasets for Indonesian and Sundanese languages. To do so, we create datasets\nfor these languages using various methods involving both LLMs and human\nannotators, resulting in ~4.5K questions per language (~9K in total), making\nour dataset the largest of its kind. Our experiments show that automatic data\nadaptation from an existing English dataset is less effective for Sundanese.\nInterestingly, using the direct generation method on the target language, GPT-4\nTurbo can generate questions with adequate general knowledge in both languages,\nalbeit not as culturally 'deep' as humans. We also observe a higher occurrence\nof fluency errors in the Sundanese dataset, highlighting the discrepancy\nbetween medium- and lower-resource languages.\n","authors":["Rifki Afina Putri","Faiz Ghifari Haznitrama","Dea Adhista","Alice Oh"],"pdf_url":"https://arxiv.org/pdf/2402.17302v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07938v2","updated":"2024-04-16T07:39:05Z","published":"2024-02-07T21:08:49Z","title":"Large Language User Interfaces: Voice Interactive User Interfaces\n  powered by LLMs","summary":"  The evolution of Large Language Models (LLMs) has showcased remarkable\ncapacities for logical reasoning and natural language comprehension. These\ncapabilities can be leveraged in solutions that semantically and textually\nmodel complex problems. In this paper, we present our efforts toward\nconstructing a framework that can serve as an intermediary between a user and\ntheir user interface (UI), enabling dynamic and real-time interactions. We\nemploy a system that stands upon textual semantic mappings of UI components, in\nthe form of annotations. These mappings are stored, parsed, and scaled in a\ncustom data structure, supplementary to an agent-based prompting backend\nengine. Employing textual semantic mappings allows each component to not only\nexplain its role to the engine but also provide expectations. By comprehending\nthe needs of both the user and the components, our LLM engine can classify the\nmost appropriate application, extract relevant parameters, and subsequently\nexecute precise predictions of the user's expected actions. Such an integration\nevolves static user interfaces into highly dynamic and adaptable solutions,\nintroducing a new frontier of intelligent and responsive user experiences.\n","authors":["Syed Mekael Wasti","Ken Q. Pu","Ali Neshati"],"pdf_url":"https://arxiv.org/pdf/2402.07938v2.pdf","comment":"Accepted as peer-reviewed publication"},{"id":"http://arxiv.org/abs/2404.09170v2","updated":"2024-04-16T07:38:51Z","published":"2024-04-14T07:19:27Z","title":"Post-Semantic-Thinking: A Robust Strategy to Distill Reasoning Capacity\n  from Large Language Models","summary":"  Chain of thought finetuning aims to endow small student models with reasoning\ncapacity to improve their performance towards a specific task by allowing them\nto imitate the reasoning procedure of large language models (LLMs) beyond\nsimply predicting the answer to the question. However, the existing methods 1)\ngenerate rationale before the answer, making their answer correctness sensitive\nto the hallucination in the rationale;2) force the student model to repeat the\nexact LLMs rationale expression word-after-word, which could have the model\nbiased towards learning the expression in rationale but count against the model\nfrom understanding the core logic behind it. Therefore, we propose a robust\nPost-Semantic-Thinking (PST) strategy to generate answers before rationale.\nThanks to this answer-first setting, 1) the answering procedure can escape from\nthe adverse effects caused by hallucinations in the rationale; 2) the complex\nreasoning procedure is tightly bound with the relatively concise answer, making\nthe reasoning for questions easier with the prior information in the answer; 3)\nthe efficiency of the method can also benefit from the setting since users can\nstop the generation right after answers are outputted when inference is\nconducted. Furthermore, the PST strategy loose the constraint against the\ngenerated rationale to be close to the LLMs gold standard in the hidden\nsemantic space instead of the vocabulary space, thus making the small student\nmodel better comprehend the semantic reasoning logic in rationale. Extensive\nexperiments conducted across 12 reasoning tasks demonstrate the effectiveness\nof PST.\n","authors":["Xiaoshu Chen","Sihang Zhou","Ke Liang","Xinwang Liu"],"pdf_url":"https://arxiv.org/pdf/2404.09170v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10346v1","updated":"2024-04-16T07:30:11Z","published":"2024-04-16T07:30:11Z","title":"Self-Explore to Avoid the Pit: Improving the Reasoning Capabilities of\n  Language Models with Fine-grained Rewards","summary":"  Training on large amounts of rationales (i.e., CoT Fine-tuning) is effective\nat improving the reasoning capabilities of large language models (LLMs).\nHowever, acquiring human-authored rationales or augmenting rationales from\nproprietary models is costly and not scalable. In this paper, we study the\nproblem of whether LLMs could self-improve their reasoning capabilities. To\nthis end, we propose Self-Explore, where the LLM is tasked to explore the first\nwrong step (i.e., the first pit) within the rationale and use such signals as\nfine-grained rewards for further improvement. On the GSM8K and MATH test set,\nSelf-Explore achieves 11.57% and 2.89% improvement on average across three LLMs\ncompared to supervised fine-tuning (SFT). Our code is available at\nhttps://github.com/hbin0701/Self-Explore.\n","authors":["Hyeonbin Hwang","Doyoung Kim","Seungone Kim","Seonghyeon Ye","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2404.10346v1.pdf","comment":"Preprint Under Review"},{"id":"http://arxiv.org/abs/2402.18120v2","updated":"2024-04-16T07:29:36Z","published":"2024-02-28T07:18:39Z","title":"Exploring Multilingual Concepts of Human Value in Large Language Models:\n  Is Value Alignment Consistent, Transferable and Controllable across\n  Languages?","summary":"  Prior research in representation engineering has revealed that LLMs encode\nconcepts within their representation spaces, predominantly centered around\nEnglish. In this study, we extend this philosophy to a multilingual scenario,\ndelving into multilingual human value concepts in LLMs. Through our\ncomprehensive exploration covering 7 types of human values, 16 languages and 3\nLLM series with distinct multilinguality, we empirically substantiate the\nexistence of multilingual human values in LLMs. Further cross-lingual analysis\non these concepts discloses 3 traits arising from language resource\ndisparities: cross-lingual inconsistency, distorted linguistic relationships,\nand unidirectional cross-lingual transfer between high- and low-resource\nlanguages, all in terms of human value concepts. Additionally, we validate the\nfeasibility of cross-lingual control over value alignment capabilities of LLMs,\nleveraging the dominant language as a source language. Drawing from our\nfindings on multilingual value alignment, we prudently provide suggestions on\nthe composition of multilingual data for LLMs pre-training: including a limited\nnumber of dominant languages for cross-lingual alignment transfer while\navoiding their excessive prevalence, and keeping a balanced distribution of\nnon-dominant languages. We aspire that our findings would contribute to\nenhancing the safety and utility of multilingual AI.\n","authors":["Shaoyang Xu","Weilong Dong","Zishan Guo","Xinwei Wu","Deyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2402.18120v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02138v2","updated":"2024-04-16T07:28:05Z","published":"2024-04-02T17:49:40Z","title":"Topic-based Watermarks for LLM-Generated Text","summary":"  Recent advancements of large language models (LLMs) have resulted in\nindistinguishable text outputs comparable to human-generated text. Watermarking\nalgorithms are potential tools that offer a way to differentiate between LLM-\nand human-generated text by embedding detectable signatures within\nLLM-generated output. However, current watermarking schemes lack robustness\nagainst known attacks against watermarking algorithms. In addition, they are\nimpractical considering an LLM generates tens of thousands of text outputs per\nday and the watermarking algorithm needs to memorize each output it generates\nfor the detection to work. In this work, focusing on the limitations of current\nwatermarking schemes, we propose the concept of a \"topic-based watermarking\nalgorithm\" for LLMs. The proposed algorithm determines how to generate tokens\nfor the watermarked LLM output based on extracted topics of an input prompt or\nthe output of a non-watermarked LLM. Inspired from previous work, we propose\nusing a pair of lists (that are generated based on the specified extracted\ntopic(s)) that specify certain tokens to be included or excluded while\ngenerating the watermarked output of the LLM. Using the proposed watermarking\nalgorithm, we show the practicality of a watermark detection algorithm.\nFurthermore, we discuss a wide range of attacks that can emerge against\nwatermarking algorithms for LLMs and the benefit of the proposed watermarking\nscheme for the feasibility of modeling a potential attacker considering its\nbenefit vs. loss.\n","authors":["Alexander Nemecek","Yuzhou Jiang","Erman Ayday"],"pdf_url":"https://arxiv.org/pdf/2404.02138v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2404.08801v2","updated":"2024-04-16T07:27:58Z","published":"2024-04-12T20:28:14Z","title":"Megalodon: Efficient LLM Pretraining and Inference with Unlimited\n  Context Length","summary":"  The quadratic complexity and weak length extrapolation of Transformers limits\ntheir ability to scale to long sequences, and while sub-quadratic solutions\nlike linear attention and state space models exist, they empirically\nunderperform Transformers in pretraining efficiency and downstream task\naccuracy. We introduce Megalodon, a neural architecture for efficient sequence\nmodeling with unlimited context length. Megalodon inherits the architecture of\nMega (exponential moving average with gated attention), and further introduces\nmultiple technical components to improve its capability and stability,\nincluding complex exponential moving average (CEMA), timestep normalization\nlayer, normalized attention mechanism and pre-norm with two-hop residual\nconfiguration. In a controlled head-to-head comparison with Llama2, Megalodon\nachieves better efficiency than Transformer in the scale of 7 billion\nparameters and 2 trillion training tokens. Megalodon reaches a training loss of\n1.70, landing mid-way between Llama2-7B (1.75) and 13B (1.67). Code:\nhttps://github.com/XuezheMax/megalodon\n","authors":["Xuezhe Ma","Xiaomeng Yang","Wenhan Xiong","Beidi Chen","Lili Yu","Hao Zhang","Jonathan May","Luke Zettlemoyer","Omer Levy","Chunting Zhou"],"pdf_url":"https://arxiv.org/pdf/2404.08801v2.pdf","comment":"9 pages, 6 figures and 8 tables"},{"id":"http://arxiv.org/abs/2404.09894v2","updated":"2024-04-16T07:06:16Z","published":"2024-04-15T16:06:36Z","title":"Glitch Tokens in Large Language Models: Categorization Taxonomy and\n  Effective Detection","summary":"  With the expanding application of Large Language Models (LLMs) in various\ndomains, it becomes imperative to comprehensively investigate their unforeseen\nbehaviors and consequent outcomes. In this study, we introduce and\nsystematically explore the phenomenon of \"glitch tokens\", which are anomalous\ntokens produced by established tokenizers and could potentially compromise the\nmodels' quality of response. Specifically, we experiment on seven top popular\nLLMs utilizing three distinct tokenizers and involving a totally of 182,517\ntokens. We present categorizations of the identified glitch tokens and symptoms\nexhibited by LLMs when interacting with glitch tokens. Based on our observation\nthat glitch tokens tend to cluster in the embedding space, we propose\nGlitchHunter, a novel iterative clustering-based technique, for efficient\nglitch token detection. The evaluation shows that our approach notably\noutperforms three baseline methods on eight open-source LLMs. To the best of\nour knowledge, we present the first comprehensive study on glitch tokens. Our\nnew detection further provides valuable insights into mitigating\ntokenization-related errors in LLMs.\n","authors":["Yuxi Li","Yi Liu","Gelei Deng","Ying Zhang","Wenjia Song","Ling Shi","Kailong Wang","Yuekang Li","Yang Liu","Haoyu Wang"],"pdf_url":"https://arxiv.org/pdf/2404.09894v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02797v2","updated":"2024-04-16T06:50:58Z","published":"2024-01-05T13:22:12Z","title":"PeFoMed: Parameter Efficient Fine-tuning of Multimodal Large Language\n  Models for Medical Imaging","summary":"  Multimodal large language models (MLLMs) represent an evolutionary expansion\nin the capabilities of traditional large language models, enabling them to\ntackle challenges that surpass the scope of purely text-based applications. It\nleverages the knowledge previously encoded within these language models,\nthereby enhancing their applicability and functionality in the reign of\nmultimodal contexts. Recent works investigate the adaptation of MLLMs as a\nuniversal solution to address medical multi-modal problems as a generative\ntask. In this paper, we propose a parameter efficient framework for fine-tuning\nMLLMs, specifically validated on medical visual question answering (Med-VQA)\nand medical report generation (MRG) tasks, using public benchmark datasets. We\nalso introduce an evaluation metric using the 5-point Likert scale and its\nweighted average value to measure the quality of the generated reports for MRG\ntasks, where the scale ratings are labelled by both humans manually and the\nGPT-4 model. We further assess the consistency of performance metrics across\ntraditional measures, GPT-4, and human ratings for both VQA and MRG tasks. The\nresults indicate that semantic similarity assessments using GPT-4 align closely\nwith human annotators and provide greater stability, yet they reveal a\ndiscrepancy when compared to conventional lexical similarity measurements. This\nquestions the reliability of lexical similarity metrics for evaluating the\nperformance of generative models in Med-VQA and report generation tasks.\nBesides, our fine-tuned model significantly outperforms GPT-4v. This indicates\nthat without additional fine-tuning, multi-modal models like GPT-4v do not\nperform effectively on medical imaging tasks. The code will be available here:\nhttps://github.com/jinlHe/PeFoMed.\n","authors":["Gang Liu","Jinlong He","Pengfei Li","Genrong He","Zhaolin Chen","Shenjun Zhong"],"pdf_url":"https://arxiv.org/pdf/2401.02797v2.pdf","comment":"12 pages, 8 figures, 12 tables"},{"id":"http://arxiv.org/abs/2404.10315v1","updated":"2024-04-16T06:47:49Z","published":"2024-04-16T06:47:49Z","title":"Enhancing Confidence Expression in Large Language Models Through\n  Learning from Past Experience","summary":"  Large Language Models (LLMs) have exhibited remarkable performance across\nvarious downstream tasks, but they may generate inaccurate or false information\nwith a confident tone. One of the possible solutions is to empower the LLM\nconfidence expression capability, in which the confidence expressed can be\nwell-aligned with the true probability of the generated answer being correct.\nHowever, leveraging the intrinsic ability of LLMs or the signals from the\noutput logits of answers proves challenging in accurately capturing the\nresponse uncertainty in LLMs. Therefore, drawing inspiration from cognitive\ndiagnostics, we propose a method of Learning from Past experience (LePe) to\nenhance the capability for confidence expression. Specifically, we first\nidentify three key problems: (1) How to capture the inherent confidence of the\nLLM? (2) How to teach the LLM to express confidence? (3) How to evaluate the\nconfidence expression of the LLM? Then we devise three stages in LePe to deal\nwith these problems. Besides, to accurately capture the confidence of an LLM\nwhen constructing the training data, we design a complete pipeline including\nquestion preparation and answer sampling. We also conduct experiments using the\nLlama family of LLMs to verify the effectiveness of our proposed method on four\ndatasets.\n","authors":["Haixia Han","Tingyun Li","Shisong Chen","Jie Shi","Chengyu Du","Yanghua Xiao","Jiaqing Liang","Xin Lin"],"pdf_url":"https://arxiv.org/pdf/2404.10315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00685v2","updated":"2024-04-16T06:46:18Z","published":"2024-03-31T13:30:12Z","title":"Scaling Properties of Speech Language Models","summary":"  Speech Language Models (SLMs) aim to learn language from raw audio, without\ntextual resources. Despite significant advances, our current models exhibit\nweak syntax and semantic abilities. However, if the scaling properties of\nneural language models hold for the speech modality, these abilities will\nimprove as the amount of compute used for training increases. In this paper, we\nuse models of this scaling behavior to estimate the scale at which our current\nmethods will yield a SLM with the English proficiency of text-based Large\nLanguage Models (LLMs). We establish a strong correlation between pre-training\nloss and downstream syntactic and semantic performance in SLMs and LLMs, which\nresults in predictable scaling of linguistic performance. We show that the\nlinguistic performance of SLMs scales up to three orders of magnitude more\nslowly than that of text-based LLMs. Additionally, we study the benefits of\nsynthetic data designed to boost semantic understanding and the effects of\ncoarser speech tokenization.\n","authors":["Santiago Cuervo","Ricard Marxer"],"pdf_url":"https://arxiv.org/pdf/2404.00685v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07001v3","updated":"2024-04-16T06:34:31Z","published":"2024-04-10T13:31:07Z","title":"Event Grounded Criminal Court View Generation with Cooperative (Large)\n  Language Models","summary":"  With the development of legal intelligence, Criminal Court View Generation\nhas attracted much attention as a crucial task of legal intelligence, which\naims to generate concise and coherent texts that summarize case facts and\nprovide explanations for verdicts. Existing researches explore the key\ninformation in case facts to yield the court views. Most of them employ a\ncoarse-grained approach that partitions the facts into broad segments (e.g.,\nverdict-related sentences) to make predictions. However, this approach fails to\ncapture the complex details present in the case facts, such as various criminal\nelements and legal events. To this end, in this paper, we propose an Event\nGrounded Generation (EGG) method for criminal court view generation with\ncooperative (Large) Language Models, which introduces the fine-grained event\ninformation into the generation. Specifically, we first design a LLMs-based\nextraction method that can extract events in case facts without massive\nannotated events. Then, we incorporate the extracted events into court view\ngeneration by merging case facts and events. Besides, considering the\ncomputational burden posed by the use of LLMs in the extraction phase of EGG,\nwe propose a LLMs-free EGG method that can eliminate the requirement for event\nextraction using LLMs in the inference phase. Extensive experimental results on\na real-world dataset clearly validate the effectiveness of our proposed method.\n","authors":["Linan Yue","Qi Liu","Lili Zhao","Li Wang","Weibo Gao","Yanqing An"],"pdf_url":"https://arxiv.org/pdf/2404.07001v3.pdf","comment":"Accepted to SIGIR2024"},{"id":"http://arxiv.org/abs/2404.10306v1","updated":"2024-04-16T06:27:39Z","published":"2024-04-16T06:27:39Z","title":"Balancing Speciality and Versatility: a Coarse to Fine Framework for\n  Supervised Fine-tuning Large Language Model","summary":"  Aligned Large Language Models (LLMs) showcase remarkable versatility, capable\nof handling diverse real-world tasks. Meanwhile, aligned LLMs are also expected\nto exhibit speciality, excelling in specific applications. However, fine-tuning\nwith extra data, a common practice to gain speciality, often leads to\ncatastrophic forgetting (CF) of previously acquired versatility, hindering the\nmodel's performance across diverse tasks. In response to this challenge, we\npropose CoFiTune, a coarse to fine framework in an attempt to strike the\nbalance between speciality and versatility. At the coarse-grained level, an\nempirical tree-search algorithm is utilized to pinpoint and update specific\nmodules that are crucial for speciality, while keeping other parameters frozen;\nat the fine-grained level, a soft-masking mechanism regulates the update to the\nLLMs, mitigating the CF issue without harming speciality. In an overall\nevaluation of both speciality and versatility, CoFiTune consistently\noutperforms baseline methods across diverse tasks and model scales. Compared to\nthe full-parameter SFT, CoFiTune leads to about 14% versatility improvement and\nmarginal speciality loss on a 13B model. Lastly, based on further analysis, we\nprovide a speculative insight into the information forwarding process in LLMs,\nwhich helps explain the effectiveness of the proposed method. The code is\navailable at https://github.com/rattlesnakey/CoFiTune.\n","authors":["Hengyuan Zhang","Yanru Wu","Dawei Li","Zacc Yang","Rui Zhao","Yong Jiang","Fei Tan"],"pdf_url":"https://arxiv.org/pdf/2404.10306v1.pdf","comment":"43 pages, 10 figures"},{"id":"http://arxiv.org/abs/2311.01041v2","updated":"2024-04-16T06:24:38Z","published":"2023-11-02T07:20:49Z","title":"Learn to Refuse: Making Large Language Models More Controllable and\n  Reliable through Knowledge Scope Limitation and Refusal Mechanism","summary":"  Large language models (LLMs) have demonstrated impressive language\nunderstanding and generation capabilities, enabling them to answer a wide range\nof questions across various domains. However, these models are not flawless and\noften produce responses that contain errors or misinformation. These\ninaccuracies, commonly referred to as hallucinations, render LLMs unreliable\nand even unusable in many scenarios. In this paper, our focus is on mitigating\nthe issue of hallucination in LLMs, particularly in the context of\nquestion-answering. Instead of attempting to answer all questions, we explore a\nrefusal mechanism that instructs LLMs to refuse to answer challenging questions\nin order to avoid errors. We then propose a simple yet effective solution\ncalled Learn to Refuse (L2R), which incorporates the refusal mechanism to\nenable LLMs to recognize and refuse to answer questions that they find\ndifficult to address. To achieve this, we utilize a structured knowledge base\nto represent all the LLM's understanding of the world, enabling it to provide\ntraceable gold knowledge. This knowledge base is separate from the LLM and\ninitially empty. It can be filled with validated knowledge and progressively\nexpanded. When an LLM encounters questions outside its domain, the system\nrecognizes its knowledge scope and determines whether it can answer the\nquestion independently. Additionally, we introduce a method for automatically\nand efficiently expanding the knowledge base of LLMs. Through qualitative and\nquantitative analysis, we demonstrate that our approach enhances the\ncontrollability and reliability of LLMs.\n","authors":["Lang Cao"],"pdf_url":"https://arxiv.org/pdf/2311.01041v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07989v5","updated":"2024-04-16T06:19:46Z","published":"2023-11-14T08:34:26Z","title":"Unifying the Perspectives of NLP and Software Engineering: A Survey on\n  Language Models for Code","summary":"  In this work we systematically review the recent advancements in code\nprocessing with language models, covering 50+ models, 30+ evaluation tasks,\n170+ datasets, and 800 related works. We break down code processing models into\ngeneral language models represented by the GPT family and specialized models\nthat are specifically pretrained on code, often with tailored objectives. We\ndiscuss the relations and differences between these models, and highlight the\nhistorical transition of code modeling from statistical models and RNNs to\npretrained Transformers and LLMs, which is exactly the same course that had\nbeen taken by NLP. We also discuss code-specific features such as AST, CFG, and\nunit tests, along with their application in training code language models, and\nidentify key challenges and potential future directions in this domain. We keep\nthe survey open and updated on GitHub at\nhttps://github.com/codefuse-ai/Awesome-Code-LLM.\n","authors":["Ziyin Zhang","Chaoyu Chen","Bingchang Liu","Cong Liao","Zi Gong","Hang Yu","Jianguo Li","Rui Wang"],"pdf_url":"https://arxiv.org/pdf/2311.07989v5.pdf","comment":"Repo is available at https://github.com/codefuse-ai/Awesome-Code-LLM.\n  8 figures, 10 tables, and 796 references"},{"id":"http://arxiv.org/abs/2310.10467v2","updated":"2024-04-16T06:06:43Z","published":"2023-10-16T14:46:52Z","title":"Stance Detection with Collaborative Role-Infused LLM-Based Agents","summary":"  Stance detection automatically detects the stance in a text towards a target,\nvital for content analysis in web and social media research. Despite their\npromising capabilities, LLMs encounter challenges when directly applied to\nstance detection. First, stance detection demands multi-aspect knowledge, from\ndeciphering event-related terminologies to understanding the expression styles\nin social media platforms. Second, stance detection requires advanced reasoning\nto infer authors' implicit viewpoints, as stance are often subtly embedded\nrather than overtly stated in the text. To address these challenges, we design\na three-stage framework COLA (short for Collaborative rOle-infused LLM-based\nAgents) in which LLMs are designated distinct roles, creating a collaborative\nsystem where each role contributes uniquely. Initially, in the multidimensional\ntext analysis stage, we configure the LLMs to act as a linguistic expert, a\ndomain specialist, and a social media veteran to get a multifaceted analysis of\ntexts, thus overcoming the first challenge. Next, in the reasoning-enhanced\ndebating stage, for each potential stance, we designate a specific LLM-based\nagent to advocate for it, guiding the LLM to detect logical connections between\ntext features and stance, tackling the second challenge. Finally, in the stance\nconclusion stage, a final decision maker agent consolidates prior insights to\ndetermine the stance. Our approach avoids extra annotated data and model\ntraining and is highly usable. We achieve state-of-the-art performance across\nmultiple datasets. Ablation studies validate the effectiveness of each design\nrole in handling stance detection. Further experiments have demonstrated the\nexplainability and the versatility of our approach. Our approach excels in\nusability, accuracy, effectiveness, explainability and versatility,\nhighlighting its value.\n","authors":["Xiaochong Lan","Chen Gao","Depeng Jin","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2310.10467v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10297v1","updated":"2024-04-16T05:45:52Z","published":"2024-04-16T05:45:52Z","title":"Future Language Modeling from Temporal Document History","summary":"  Predicting the future is of great interest across many aspects of human\nactivity. Businesses are interested in future trends, traders are interested in\nfuture stock prices, and companies are highly interested in future\ntechnological breakthroughs. While there are many automated systems for\npredicting future numerical data, such as weather, stock prices, and demand for\nproducts, there is relatively little work in automatically predicting textual\ndata. Humans are interested in textual data predictions because it is a natural\nformat for our consumption, and experts routinely make predictions in a textual\nformat (Christensen et al., 2004; Tetlock & Gardner, 2015; Frick, 2015).\nHowever, there has been relatively little formalization of this general problem\nin the machine learning or natural language processing communities. To address\nthis gap, we introduce the task of future language modeling: probabilistic\nmodeling of texts in the future based on a temporal history of texts. To our\nknowledge, our work is the first work to formalize the task of predicting the\nfuture in this way. We show that it is indeed possible to build future language\nmodels that improve upon strong non-temporal language model baselines, opening\nthe door to working on this important, and widely applicable problem.\n","authors":["Changmao Li","Jeffrey Flanigan"],"pdf_url":"https://arxiv.org/pdf/2404.10297v1.pdf","comment":"Accepted by ICLR 2024"},{"id":"http://arxiv.org/abs/2403.04197v2","updated":"2024-04-16T05:07:52Z","published":"2024-03-07T03:58:28Z","title":"Large Language Models are In-Context Molecule Learners","summary":"  Large Language Models (LLMs) have demonstrated exceptional performance in\nbiochemical tasks, especially the molecule caption translation task, which aims\nto bridge the gap between molecules and natural language texts. However,\nprevious methods in adapting LLMs to the molecule-caption translation task\nrequired extra domain-specific pre-training stages, suffered weak alignment\nbetween molecular and textual spaces, or imposed stringent demands on the scale\nof LLMs. To resolve the challenges, we propose In-Context Molecule Adaptation\n(ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment\nfrom context examples via In-Context Molecule Tuning. Specifically, ICMA\nincorporates the following three stages: Hybrid Context Retrieval,\nPost-retrieval Re-ranking, and In-context Molecule Tuning. Initially, Hybrid\nContext Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval\nto retrieve informative context examples. Additionally, we also propose\nPost-retrieval Re-ranking with Sequence Reversal and Random Walk to further\nimprove the quality of retrieval results. Finally, In-Context Molecule Tuning\nunlocks the in-context molecule learning capability of LLMs with retrieved\nexamples and adapts the parameters of LLMs for the molecule-caption translation\ntask. Experimental results demonstrate that ICMT can empower LLMs to achieve\nstate-of-the-art or comparable performance without extra training corpora and\nintricate structures, showing that LLMs are inherently in-context molecule\nlearners.\n","authors":["Jiatong Li","Wei Liu","Zhihao Ding","Wenqi Fan","Yuqiang Li","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2403.04197v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07641v2","updated":"2024-04-16T04:50:08Z","published":"2023-10-11T16:38:11Z","title":"Evaluating Large Language Models at Evaluating Instruction Following","summary":"  As research in large language models (LLMs) continues to accelerate,\nLLM-based evaluation has emerged as a scalable and cost-effective alternative\nto human evaluations for comparing the ever increasing list of models. This\npaper investigates the efficacy of these ``LLM evaluators'', particularly in\nusing them to assess instruction following, a metric that gauges how closely\ngenerated text adheres to the given instruction. We introduce a challenging\nmeta-evaluation benchmark, LLMBar, designed to test the ability of an LLM\nevaluator in discerning instruction-following outputs. The authors manually\ncurated 419 pairs of outputs, one adhering to instructions while the other\ndiverging, yet may possess deceptive qualities that mislead an LLM evaluator,\ne.g., a more engaging tone. Contrary to existing meta-evaluation, we discover\nthat different evaluators (i.e., combinations of LLMs and prompts) exhibit\ndistinct performance on LLMBar and even the highest-scoring ones have\nsubstantial room for improvement. We also present a novel suite of prompting\nstrategies that further close the gap between LLM and human evaluators. With\nLLMBar, we hope to offer more insight into LLM evaluators and foster future\nresearch in developing better instruction-following models.\n","authors":["Zhiyuan Zeng","Jiatong Yu","Tianyu Gao","Yu Meng","Tanya Goyal","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2310.07641v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2404.07475v2","updated":"2024-04-16T04:07:42Z","published":"2024-04-11T05:09:03Z","title":"Laissez-Faire Harms: Algorithmic Biases in Generative Language Models","summary":"  The rapid deployment of generative language models (LMs) has raised concerns\nabout social biases affecting the well-being of diverse consumers. The extant\nliterature on generative LMs has primarily examined bias via explicit identity\nprompting. However, prior research on bias in earlier language-based technology\nplatforms, including search engines, has shown that discrimination can occur\neven when identity terms are not specified explicitly. Studies of bias in LM\nresponses to open-ended prompts (where identity classifications are left\nunspecified) are lacking and have not yet been grounded in end-consumer harms.\nHere, we advance studies of generative LM bias by considering a broader set of\nnatural use cases via open-ended prompting. In this \"laissez-faire\" setting, we\nfind that synthetically generated texts from five of the most pervasive LMs\n(ChatGPT3.5, ChatGPT4, Claude2.0, Llama2, and PaLM2) perpetuate harms of\nomission, subordination, and stereotyping for minoritized individuals with\nintersectional race, gender, and/or sexual orientation identities (AI/AN,\nAsian, Black, Latine, MENA, NH/PI, Female, Non-binary, Queer). We find\nwidespread evidence of bias to an extent that such individuals are hundreds to\nthousands of times more likely to encounter LM-generated outputs that portray\ntheir identities in a subordinated manner compared to representative or\nempowering portrayals. We also document a prevalence of stereotypes (e.g.\nperpetual foreigner) in LM-generated outputs that are known to trigger\npsychological harms that disproportionately affect minoritized individuals.\nThese include stereotype threat, which leads to impaired cognitive performance\nand increased negative self-perception. Our findings highlight the urgent need\nto protect consumers from discriminatory harms caused by language models and\ninvest in critical AI education programs tailored towards empowering diverse\nconsumers.\n","authors":["Evan Shieh","Faye-Marie Vassel","Cassidy Sugimoto","Thema Monroe-White"],"pdf_url":"https://arxiv.org/pdf/2404.07475v2.pdf","comment":"16 pages (43 if including supplementals), 8 figures (23 if including\n  supplementals)"},{"id":"http://arxiv.org/abs/2404.10271v1","updated":"2024-04-16T03:59:33Z","published":"2024-04-16T03:59:33Z","title":"Social Choice for AI Alignment: Dealing with Diverse Human Feedback","summary":"  Foundation models such as GPT-4 are fine-tuned to avoid unsafe or otherwise\nproblematic behavior, so that, for example, they refuse to comply with requests\nfor help with committing crimes or with producing racist text. One approach to\nfine-tuning, called reinforcement learning from human feedback, learns from\nhumans' expressed preferences over multiple outputs. Another approach is\nconstitutional AI, in which the input from humans is a list of high-level\nprinciples. But how do we deal with potentially diverging input from humans?\nHow can we aggregate the input into consistent data about ''collective''\npreferences or otherwise use it to make collective choices about model\nbehavior? In this paper, we argue that the field of social choice is well\npositioned to address these questions, and we discuss ways forward for this\nagenda, drawing on discussions in a recent workshop on Social Choice for AI\nEthics and Safety held in Berkeley, CA, USA in December 2023.\n","authors":["Vincent Conitzer","Rachel Freedman","Jobst Heitzig","Wesley H. Holliday","Bob M. Jacobs","Nathan Lambert","Milan Moss√©","Eric Pacuit","Stuart Russell","Hailey Schoelkopf","Emanuel Tewolde","William S. Zwicker"],"pdf_url":"https://arxiv.org/pdf/2404.10271v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.16852v2","updated":"2024-04-16T03:53:28Z","published":"2024-03-25T15:15:41Z","title":"Towards Explainability in Legal Outcome Prediction Models","summary":"  Current legal outcome prediction models - a staple of legal NLP - do not\nexplain their reasoning. However, to employ these models in the real world,\nhuman legal actors need to be able to understand the model's decisions. In the\ncase of common law, legal practitioners reason towards the outcome of a case by\nreferring to past case law, known as precedent. We contend that precedent is,\ntherefore, a natural way of facilitating explainability for legal NLP models.\nIn this paper, we contribute a novel method for identifying the precedent\nemployed by legal outcome prediction models. Furthermore, by developing a\ntaxonomy of legal precedent, we are able to compare human judges and neural\nmodels with respect to the different types of precedent they rely on. We find\nthat while the models learn to predict outcomes reasonably well, their use of\nprecedent is unlike that of human judges.\n","authors":["Josef Valvoda","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2403.16852v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10268v1","updated":"2024-04-16T03:46:30Z","published":"2024-04-16T03:46:30Z","title":"Modeling Low-Resource Health Coaching Dialogues via Neuro-Symbolic Goal\n  Summarization and Text-Units-Text Generation","summary":"  Health coaching helps patients achieve personalized and lifestyle-related\ngoals, effectively managing chronic conditions and alleviating mental health\nissues. It is particularly beneficial, however cost-prohibitive, for\nlow-socioeconomic status populations due to its highly personalized and\nlabor-intensive nature. In this paper, we propose a neuro-symbolic goal\nsummarizer to support health coaches in keeping track of the goals and a\ntext-units-text dialogue generation model that converses with patients and\nhelps them create and accomplish specific goals for physical activities. Our\nmodels outperform previous state-of-the-art while eliminating the need for\npredefined schema and corresponding annotation. We also propose a new health\ncoaching dataset extending previous work and a metric to measure the\nunconventionality of the patient's response based on data difficulty,\nfacilitating potential coach alerts during deployment.\n","authors":["Yue Zhou","Barbara Di Eugenio","Brian Ziebart","Lisa Sharp","Bing Liu","Nikolaos Agadakos"],"pdf_url":"https://arxiv.org/pdf/2404.10268v1.pdf","comment":"Accepted to the main conference of LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2310.07023v2","updated":"2024-04-16T03:42:43Z","published":"2023-10-10T21:23:47Z","title":"Automatic Macro Mining from Interaction Traces at Scale","summary":"  Macros are building block tasks of our everyday smartphone activity (e.g.,\n\"login\", or \"booking a flight\"). Effectively extracting macros is important for\nunderstanding mobile interaction and enabling task automation. These macros are\nhowever difficult to extract at scale as they can be comprised of multiple\nsteps yet hidden within programmatic components of mobile apps. In this paper,\nwe introduce a novel approach based on Large Language Models (LLMs) to\nautomatically extract semantically meaningful macros from both random and\nuser-curated mobile interaction traces. The macros produced by our approach are\nautomatically tagged with natural language descriptions and are fully\nexecutable. We conduct multiple studies to validate the quality of extracted\nmacros, including user evaluation, comparative analysis against human-curated\ntasks, and automatic execution of these macros. These experiments and analyses\nshow the effectiveness of our approach and the usefulness of extracted macros\nin various downstream applications.\n","authors":["Forrest Huang","Gang Li","Tao Li","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2310.07023v2.pdf","comment":"Accepted to CHI 2024"},{"id":"http://arxiv.org/abs/2403.09131v3","updated":"2024-04-16T03:31:25Z","published":"2024-03-14T06:49:16Z","title":"ProSwitch: Knowledge-Guided Instruction Tuning to Generate Professional\n  and Non-Professional Styled Text","summary":"  Large Language Models (LLMs) have demonstrated efficacy in various linguistic\napplications, including text summarization and controlled text generation.\nHowever, studies into their capacity of switching between styles via\nfine-tuning remain underexplored. This study concentrates on textual\nprofessionalism and introduces a novel methodology, named ProSwitch, which\nequips a language model with the ability to produce both professional and\nnon-professional responses through knowledge-guided instruction tuning.\nProSwitch unfolds across three phases: data preparation for gathering domain\nknowledge and training corpus; instruction tuning for optimizing language\nmodels with multiple levels of instruction formats; and comprehensive\nevaluation for assessing the professionalism discrimination and reference-based\nquality of generated text. Comparative analysis of ProSwitch against both\ngeneral and specialized language models reveals that our approach outperforms\nbaselines in switching between professional and non-professional text\ngeneration.\n","authors":["Chang Zong","Yuyan Chen","Weiming Lu","Jian Shao","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2403.09131v3.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2404.10259v1","updated":"2024-04-16T03:26:43Z","published":"2024-04-16T03:26:43Z","title":"Uncovering Latent Arguments in Social Media Messaging by Employing\n  LLMs-in-the-Loop Strategy","summary":"  The widespread use of social media has led to a surge in popularity for\nautomated methods of analyzing public opinion. Supervised methods are adept at\ntext categorization, yet the dynamic nature of social media discussions poses a\ncontinual challenge for these techniques due to the constant shifting of the\nfocus. On the other hand, traditional unsupervised methods for extracting\nthemes from public discourse, such as topic modeling, often reveal overarching\npatterns that might not capture specific nuances. Consequently, a significant\nportion of research into social media discourse still depends on\nlabor-intensive manual coding techniques and a human-in-the-loop approach,\nwhich are both time-consuming and costly. In this work, we study the problem of\ndiscovering arguments associated with a specific theme. We propose a generic\nLLMs-in-the-Loop strategy that leverages the advanced capabilities of Large\nLanguage Models (LLMs) to extract latent arguments from social media messaging.\nTo demonstrate our approach, we apply our framework to contentious topics. We\nuse two publicly available datasets: (1) the climate campaigns dataset of 14k\nFacebook ads with 25 themes and (2) the COVID-19 vaccine campaigns dataset of\n9k Facebook ads with 14 themes. Furthermore, we analyze demographic targeting\nand the adaptation of messaging based on real-world events.\n","authors":["Tunazzina Islam","Dan Goldwasser"],"pdf_url":"https://arxiv.org/pdf/2404.10259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14866v2","updated":"2024-04-16T03:18:38Z","published":"2024-02-21T07:45:22Z","title":"APTQ: Attention-aware Post-Training Mixed-Precision Quantization for\n  Large Language Models","summary":"  Large Language Models (LLMs) have greatly advanced the natural language\nprocessing paradigm. However, the high computational load and huge model sizes\npose a grand challenge for deployment on edge devices. To this end, we propose\nAPTQ (Attention-aware Post-Training Mixed-Precision Quantization) for LLMs,\nwhich considers not only the second-order information of each layer's weights,\nbut also, for the first time, the nonlinear effect of attention outputs on the\nentire model. We leverage the Hessian trace as a sensitivity metric for\nmixed-precision quantization, ensuring an informed precision reduction that\nretains model performance. Experiments show APTQ surpasses previous\nquantization methods, achieving an average of 4 bit width a 5.22 perplexity\nnearly equivalent to full precision in the C4 dataset. In addition, APTQ\nattains state-of-the-art zero-shot accuracy of 68.24\\% and 70.48\\% at an\naverage bitwidth of 3.8 in LLaMa-7B and LLaMa-13B, respectively, demonstrating\nits effectiveness to produce high-quality quantized LLMs.\n","authors":["Ziyi Guan","Hantao Huang","Yupeng Su","Hong Huang","Ngai Wong","Hao Yu"],"pdf_url":"https://arxiv.org/pdf/2402.14866v2.pdf","comment":"6 pages, 2 figures, published to DAC 2024: 61st IEEE/ACM Design\n  Automation Conference. (DAC'24)"},{"id":"http://arxiv.org/abs/2208.08690v2","updated":"2024-04-16T03:16:22Z","published":"2022-08-18T08:03:45Z","title":"A Survey on Open Information Extraction from Rule-based Model to Large\n  Language Model","summary":"  Open information extraction is an important NLP task that targets extracting\nstructured information from unstructured text without limitations on the\nrelation type or the domain of the text. This survey paper covers open\ninformation extraction technologies from 2007 to 2022 with a focus on new\nmodels not covered by previous surveys. We propose a new categorization method\nfrom the source of information perspective to accommodate the development of\nrecent OIE technologies. In addition, we summarize three major approaches based\non task settings as well as current popular datasets and model evaluation\nmetrics. Given the comprehensive review, several future directions are shown\nfrom datasets, source of information, output form, method, and evaluation\nmetric aspects.\n","authors":["Pai Liu","Wenyang Gao","Wenjie Dong","Lin Ai","Ziwei Gong","Songfang Huang","Zongsheng Li","Ehsan Hoque","Julia Hirschberg","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2208.08690v2.pdf","comment":"The first five authors contributed to this work equally. Names are\n  ordered randomly"},{"id":"http://arxiv.org/abs/2312.15685v2","updated":"2024-04-16T02:46:58Z","published":"2023-12-25T10:29:28Z","title":"What Makes Good Data for Alignment? A Comprehensive Study of Automatic\n  Data Selection in Instruction Tuning","summary":"  Instruction tuning is a standard technique employed to align large language\nmodels to end tasks and user preferences after the initial pretraining phase.\nRecent research indicates the critical role of data engineering in instruction\ntuning -- when appropriately selected, only limited data is necessary to\nachieve superior performance. However, we still lack a principled understanding\nof what makes good instruction tuning data for alignment, and how we should\nselect data automatically and effectively. In this work, we delve deeply into\nautomatic data selection strategies for alignment. We start with controlled\nstudies to measure data across three dimensions: complexity, quality, and\ndiversity, along which we examine existing methods and introduce novel\ntechniques for enhanced data measurement. Subsequently, we propose a simple\nstrategy to select data samples based on the measurement. We present deita\n(short for Data-Efficient Instruction Tuning for Alignment), a series of models\nfine-tuned from LLaMA and Mistral models using data samples automatically\nselected with our proposed approach. Empirically, deita performs better or on\npar with the state-of-the-art open-source alignment models with only 6K SFT\ntraining data samples -- over 10x less than the data used in the baselines.\nWhen further trained with direct preference optimization (DPO),\ndeita-Mistral-7B + DPO trained with 6K SFT and 10K DPO samples achieve 7.55\nMT-Bench and 90.06% AlpacaEval scores. We anticipate this work to provide tools\non automatic data selection, facilitating data-efficient alignment. We release\nour models as well as the selected datasets for future researches to\neffectively align models more efficiently.\n","authors":["Wei Liu","Weihao Zeng","Keqing He","Yong Jiang","Junxian He"],"pdf_url":"https://arxiv.org/pdf/2312.15685v2.pdf","comment":"ICLR2024 Camera Ready. Data and model checkpoints are available at\n  https://github.com/hkust-nlp/deita"},{"id":"http://arxiv.org/abs/2402.14320v5","updated":"2024-04-16T02:46:10Z","published":"2024-02-22T06:23:37Z","title":"Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve\n  Knowledge Base Question Answering","summary":"  Recent progress with LLM-based agents has shown promising results across\nvarious tasks. However, their use in answering questions from knowledge bases\nremains largely unexplored. Implementing a KBQA system using traditional\nmethods is challenging due to the shortage of task-specific training data and\nthe complexity of creating task-focused model structures. In this paper, we\npresent Triad, a unified framework that utilizes an LLM-based agent with three\nroles for KBQA tasks. The agent is assigned three roles to tackle different\nKBQA subtasks: agent as a generalist for mastering various subtasks, as a\ndecision maker for the selection of candidates, and as an advisor for answering\nquestions with knowledge. Our KBQA framework is executed in four phases,\ninvolving the collaboration of the agent's multiple roles. We evaluated the\nperformance of our framework using three benchmark datasets, and the results\nshow that our framework outperforms state-of-the-art systems on the LC-QuAD and\nYAGO-QA benchmarks, yielding F1 scores of 11.8% and 20.7%, respectively.\n","authors":["Chang Zong","Yuchen Yan","Weiming Lu","Jian Shao","Eliot Huang","Heng Chang","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2402.14320v5.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2404.10237v1","updated":"2024-04-16T02:35:17Z","published":"2024-04-16T02:35:17Z","title":"MoE-TinyMed: Mixture of Experts for Tiny Medical Large Vision-Language\n  Models","summary":"  Mixture of Expert Tuning (MoE-Tuning) has effectively enhanced the\nperformance of general MLLMs with fewer parameters, yet its application in\nresource-limited medical settings has not been fully explored. To address this\ngap, we developed MoE-TinyMed, a model tailored for medical applications that\nsignificantly lowers parameter demands. In evaluations on the VQA-RAD, SLAKE,\nand Path-VQA datasets, MoE-TinyMed outperformed LLaVA-Med in all Med-VQA closed\nsettings with just 3.6B parameters. Additionally, a streamlined version with 2B\nparameters surpassed LLaVA-Med's performance in PathVQA, showcasing its\neffectiveness in resource-limited healthcare settings.\n","authors":["Songtao Jiang","Tuo Zheng","Yan Zhang","Yeying Jin","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2404.10237v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01373v3","updated":"2024-04-16T02:28:48Z","published":"2024-03-03T02:31:11Z","title":"Quantity Matters: Towards Assessing and Mitigating Number Hallucination\n  in Large Vision-Language Models","summary":"  Large-scale vision-language models have demonstrated impressive skill in\nhandling tasks that involve both areas. Nevertheless, these models frequently\nexperience significant issues with generating inaccurate information, which is\nhallucination. In this study, we concentrate on a specific type of\nhallucination-number hallucination, referring to models incorrectly identifying\nthe number of certain objects in pictures. We perform quantitative evaluations\nregarding number hallucination, showing it to be critical in major open-source\nlarge vision-language models. Furthermore, we utilizes two related tasks to\nconduct an in-depth analysis of number hallucination, revealing the severe\ninner and outer inconsistency among all tasks. Based on this examination, we\ndevise a training approach aimed at improving consistency to reduce number\nhallucinations, which leads to an 8% enhancement in performance over direct\nfinetuning methods. Our code and dataset will be released to the community.\n","authors":["Huixuan Zhang","Junzhe Zhang","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2403.01373v3.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2404.08262v2","updated":"2024-04-16T02:24:00Z","published":"2024-04-12T06:21:48Z","title":"Pretraining and Updating Language- and Domain-specific Large Language\n  Model: A Case Study in Japanese Business Domain","summary":"  Several previous studies have considered language- and domain-specific large\nlanguage models (LLMs) as separate topics. This study explores the combination\nof a non-English language and a high-demand industry domain, focusing on a\nJapanese business-specific LLM. This type of a model requires expertise in the\nbusiness domain, strong language skills, and regular updates of its knowledge.\nWe trained a 13-billion-parameter LLM from scratch using a new dataset of\nbusiness texts and patents, and continually pretrained it with the latest\nbusiness documents. Further we propose a new benchmark for Japanese business\ndomain question answering (QA) and evaluate our models on it. The results show\nthat our pretrained model improves QA accuracy without losing general\nknowledge, and that continual pretraining enhances adaptation to new\ninformation. Our pretrained model and business domain benchmark are publicly\navailable.\n","authors":["Kosuke Takahashi","Takahiro Omi","Kosuke Arima","Tatsuya Ishigaki"],"pdf_url":"https://arxiv.org/pdf/2404.08262v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10229v1","updated":"2024-04-16T02:19:28Z","published":"2024-04-16T02:19:28Z","title":"Generative Text Steganography with Large Language Model","summary":"  Recent advances in large language models (LLMs) have blurred the boundary of\nhigh-quality text generation between humans and machines, which is favorable\nfor generative text steganography. While, current advanced steganographic\nmapping is not suitable for LLMs since most users are restricted to accessing\nonly the black-box API or user interface of the LLMs, thereby lacking access to\nthe training vocabulary and its sampling probabilities. In this paper, we\nexplore a black-box generative text steganographic method based on the user\ninterfaces of large language models, which is called LLM-Stega. The main goal\nof LLM-Stega is that the secure covert communication between Alice (sender) and\nBob (receiver) is conducted by using the user interfaces of LLMs. Specifically,\nWe first construct a keyword set and design a new encrypted steganographic\nmapping to embed secret messages. Furthermore, to guarantee accurate extraction\nof secret messages and rich semantics of generated stego texts, an optimization\nmechanism based on reject sampling is proposed. Comprehensive experiments\ndemonstrate that the proposed LLM-Stega outperforms current state-of-the-art\nmethods.\n","authors":["Jiaxuan Wu","Zhengxian Wu","Yiming Xue","Juan Wen","Wanli Peng"],"pdf_url":"https://arxiv.org/pdf/2404.10229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10228v1","updated":"2024-04-16T02:18:30Z","published":"2024-04-16T02:18:30Z","title":"Two-Stage Stance Labeling: User-Hashtag Heuristics with Graph Neural\n  Networks","summary":"  The high volume and rapid evolution of content on social media present major\nchallenges for studying the stance of social media users. In this work, we\ndevelop a two stage stance labeling method that utilizes the user-hashtag\nbipartite graph and the user-user interaction graph. In the first stage, a\nsimple and efficient heuristic for stance labeling uses the user-hashtag\nbipartite graph to iteratively update the stance association of user and\nhashtag nodes via a label propagation mechanism. This set of soft labels is\nthen integrated with the user-user interaction graph to train a graph neural\nnetwork (GNN) model using semi-supervised learning. We evaluate this method on\ntwo large-scale datasets containing tweets related to climate change from June\n2021 to June 2022 and gun control from January 2022 to January 2023.\nExperiments demonstrate that our user-hashtag heuristic and the semi-supervised\nGNN method outperform zero-shot stance labeling using LLMs such as GPT4.\nFurther analysis illustrates how the stance labeling information and\ninteraction graph can be used for evaluating the polarization of social media\ninteractions on divisive issues such as climate change and gun control.\n","authors":["Joshua Melton","Shannon Reid","Gabriel Terejanu","Siddharth Krishnan"],"pdf_url":"https://arxiv.org/pdf/2404.10228v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10226v1","updated":"2024-04-16T02:11:46Z","published":"2024-04-16T02:11:46Z","title":"Find The Gap: Knowledge Base Reasoning For Visual Question Answering","summary":"  We analyze knowledge-based visual question answering, for which given a\nquestion, the models need to ground it into the visual modality and retrieve\nthe relevant knowledge from a given large knowledge base (KB) to be able to\nanswer. Our analysis has two folds, one based on designing neural architectures\nand training them from scratch, and another based on large pre-trained language\nmodels (LLMs). Our research questions are: 1) Can we effectively augment models\nby explicit supervised retrieval of the relevant KB information to solve the\nKB-VQA problem? 2) How do task-specific and LLM-based models perform in the\nintegration of visual and external knowledge, and multi-hop reasoning over both\nsources of information? 3) Is the implicit knowledge of LLMs sufficient for\nKB-VQA and to what extent it can replace the explicit KB? Our results\ndemonstrate the positive impact of empowering task-specific and LLM models with\nsupervised external and visual knowledge retrieval models. Our findings show\nthat though LLMs are stronger in 1-hop reasoning, they suffer in 2-hop\nreasoning in comparison with our fine-tuned NN model even if the relevant\ninformation from both modalities is available to the model. Moreover, we\nobserved that LLM models outperform the NN model for KB-related questions which\nconfirms the effectiveness of implicit knowledge in LLMs however, they do not\nalleviate the need for external KB.\n","authors":["Elham J. Barezi","Parisa Kordjamshidi"],"pdf_url":"https://arxiv.org/pdf/2404.10226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17639v3","updated":"2024-04-16T01:35:03Z","published":"2023-10-26T17:54:52Z","title":"In-Context Learning Dynamics with Random Binary Sequences","summary":"  Large language models (LLMs) trained on huge corpora of text datasets\ndemonstrate intriguing capabilities, achieving state-of-the-art performance on\ntasks they were not explicitly trained for. The precise nature of LLM\ncapabilities is often mysterious, and different prompts can elicit different\ncapabilities through in-context learning. We propose a framework that enables\nus to analyze in-context learning dynamics to understand latent concepts\nunderlying LLMs' behavioral patterns. This provides a more nuanced\nunderstanding than success-or-failure evaluation benchmarks, but does not\nrequire observing internal activations as a mechanistic interpretation of\ncircuits would. Inspired by the cognitive science of human randomness\nperception, we use random binary sequences as context and study dynamics of\nin-context learning by manipulating properties of context data, such as\nsequence length. In the latest GPT-3.5+ models, we find emergent abilities to\ngenerate seemingly random numbers and learn basic formal languages, with\nstriking in-context learning dynamics where model outputs transition sharply\nfrom seemingly random behaviors to deterministic repetition.\n","authors":["Eric J. Bigelow","Ekdeep Singh Lubana","Robert P. Dick","Hidenori Tanaka","Tomer D. Ullman"],"pdf_url":"https://arxiv.org/pdf/2310.17639v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09446v2","updated":"2024-04-16T01:33:24Z","published":"2023-03-14T09:47:23Z","title":"Controllable Prosody Generation With Partial Inputs","summary":"  We address the problem of human-in-the-loop control for generating prosody in\nthe context of text-to-speech synthesis. Controlling prosody is challenging\nbecause existing generative models lack an efficient interface through which\nusers can modify the output quickly and precisely. To solve this, we introduce\na novel framework whereby the user provides partial inputs and the generative\nmodel generates the missing features. We propose a model that is specifically\ndesigned to encode partial prosodic features and output complete audio. We show\nempirically that our model displays two essential qualities of a\nhuman-in-the-loop control mechanism: efficiency and robustness. With even a\nvery small number of input values (~4), our model enables users to improve the\nquality of the output significantly in terms of listener preference (4:1).\n","authors":["Dan Andrei Iliescu","Devang Savita Ram Mohan","Tian Huey Teh","Zack Hodari"],"pdf_url":"https://arxiv.org/pdf/2303.09446v2.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2404.09127v2","updated":"2024-04-16T01:12:09Z","published":"2024-04-14T02:40:43Z","title":"Confidence Calibration and Rationalization for LLMs via Multi-Agent\n  Deliberation","summary":"  Uncertainty estimation is a significant issue for current large language\nmodels (LLMs) that are generally poorly calibrated and over-confident,\nespecially with reinforcement learning from human feedback (RLHF). Unlike\nhumans, whose decisions and confidences not only stem from intrinsic beliefs\nbut can also be adjusted through daily observations, existing calibration\nmethods for LLMs focus on estimating or eliciting individual confidence without\ntaking full advantage of the \"Collective Wisdom\": the interaction among\nmultiple LLMs that can collectively improve both accuracy and calibration. In\nthis work, we propose Collaborative Calibration, a post-hoc training-free\ncalibration strategy that leverages the collaborative and expressive\ncapabilities of multiple tool-augmented LLM agents in a simulated group\ndeliberation process. We demonstrate the effectiveness of Collaborative\nCalibration on generative QA tasks across various domains, showing its\npotential in harnessing the rationalization of collectively calibrated\nconfidence assessments and improving the reliability of model predictions.\n","authors":["Ruixin Yang","Dheeraj Rajagopal","Shirley Anugrah Hayati","Bin Hu","Dongyeop Kang"],"pdf_url":"https://arxiv.org/pdf/2404.09127v2.pdf","comment":"Accepted at ICLR 2024 Workshop on Reliable and Responsible Foundation\n  Models"},{"id":"http://arxiv.org/abs/2404.10199v1","updated":"2024-04-16T00:50:43Z","published":"2024-04-16T00:50:43Z","title":"CULTURE-GEN: Revealing Global Cultural Perception in Language Models\n  through Natural Language Prompting","summary":"  As the utilization of large language models (LLMs) has proliferated\nworldwide, it is crucial for them to have adequate knowledge and fair\nrepresentation for diverse global cultures. In this work, we uncover culture\nperceptions of three SOTA models on 110 countries and regions on 8\nculture-related topics through culture-conditioned generations, and extract\nsymbols from these generations that are associated to each culture by the LLM.\nWe discover that culture-conditioned generation consist of linguistic \"markers\"\nthat distinguish marginalized cultures apart from default cultures. We also\ndiscover that LLMs have an uneven degree of diversity in the culture symbols,\nand that cultures from different geographic regions have different presence in\nLLMs' culture-agnostic generation. Our findings promote further research in\nstudying the knowledge and fairness of global culture perception in LLMs. Code\nand Data can be found in: https://github.com/huihanlhh/Culture-Gen/\n","authors":["Huihan Li","Liwei Jiang","Nouha Dziri","Xiang Ren","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2404.10199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10198v1","updated":"2024-04-16T00:43:03Z","published":"2024-04-16T00:43:03Z","title":"How faithful are RAG models? Quantifying the tug-of-war between RAG and\n  LLMs' internal prior","summary":"  Retrieval augmented generation (RAG) is often used to fix hallucinations and\nprovide up-to-date knowledge for large language models (LLMs). However, in\ncases when the LLM alone incorrectly answers a question, does providing the\ncorrect retrieved content always fix the error? Conversely, in cases where the\nretrieved content is incorrect, does the LLM know to ignore the wrong\ninformation, or does it recapitulate the error? To answer these questions, we\nsystematically analyze the tug-of-war between a LLM's internal knowledge (i.e.\nits prior) and the retrieved information in settings when they disagree. We\ntest GPT-4 and other LLMs on question-answering abilities across datasets with\nand without reference documents. As expected, providing the correct retrieved\ninformation fixes most model mistakes (94% accuracy). However, when the\nreference document is perturbed with increasing levels of wrong values, the LLM\nis more likely to recite the incorrect, modified information when its internal\nprior is weaker but is more resistant when its prior is stronger. Similarly, we\nalso find that the more the modified information deviates from the model's\nprior, the less likely the model is to prefer it. These results highlight an\nunderlying tension between a model's prior knowledge and the information\npresented in reference documents.\n","authors":["Kevin Wu","Eric Wu","James Zou"],"pdf_url":"https://arxiv.org/pdf/2404.10198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06149v2","updated":"2024-04-16T00:24:55Z","published":"2024-03-10T09:39:00Z","title":"Can Large Language Models Automatically Score Proficiency of Written\n  Essays?","summary":"  Although several methods were proposed to address the problem of automated\nessay scoring (AES) in the last 50 years, there is still much to desire in\nterms of effectiveness. Large Language Models (LLMs) are transformer-based\nmodels that demonstrate extraordinary capabilities on various tasks. In this\npaper, we test the ability of LLMs, given their powerful linguistic knowledge,\nto analyze and effectively score written essays. We experimented with two\npopular LLMs, namely ChatGPT and Llama. We aim to check if these models can do\nthis task and, if so, how their performance is positioned among the\nstate-of-the-art (SOTA) models across two levels, holistically and per\nindividual writing trait. We utilized prompt-engineering tactics in designing\nfour different prompts to bring their maximum potential to this task. Our\nexperiments conducted on the ASAP dataset revealed several interesting\nobservations. First, choosing the right prompt depends highly on the model and\nnature of the task. Second, the two LLMs exhibited comparable average\nperformance in AES, with a slight advantage for ChatGPT. Finally, despite the\nperformance gap between the two LLMs and SOTA models in terms of predictions,\nthey provide feedback to enhance the quality of the essays, which can\npotentially help both teachers and students.\n","authors":["Watheq Mansour","Salam Albatarni","Sohaila Eltanbouly","Tamer Elsayed"],"pdf_url":"https://arxiv.org/pdf/2403.06149v2.pdf","comment":"V2 (published version of LREC-COLING 2024)"},{"id":"http://arxiv.org/abs/2404.08555v2","updated":"2024-04-16T00:22:16Z","published":"2024-04-12T15:54:15Z","title":"RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\n  Human Feedback for LLMs","summary":"  State-of-the-art large language models (LLMs) have become indispensable tools\nfor various tasks. However, training LLMs to serve as effective assistants for\nhumans requires careful consideration. A promising approach is reinforcement\nlearning from human feedback (RLHF), which leverages human feedback to update\nthe model in accordance with human preferences and mitigate issues like\ntoxicity and hallucinations. Yet, an understanding of RLHF for LLMs is largely\nentangled with initial design choices that popularized the method and current\nresearch focuses on augmenting those choices rather than fundamentally\nimproving the framework. In this paper, we analyze RLHF through the lens of\nreinforcement learning principles to develop an understanding of its\nfundamentals, dedicating substantial focus to the core component of RLHF -- the\nreward model. Our study investigates modeling choices, caveats of function\napproximation, and their implications on RLHF training algorithms, highlighting\nthe underlying assumptions made about the expressivity of reward. Our analysis\nimproves the understanding of the role of reward models and methods for their\ntraining, concurrently revealing limitations of the current methodology. We\ncharacterize these limitations, including incorrect generalization, model\nmisspecification, and the sparsity of feedback, along with their impact on the\nperformance of a language model. The discussion and analysis are substantiated\nby a categorical review of current literature, serving as a reference for\nresearchers and practitioners to understand the challenges of RLHF and build\nupon existing efforts.\n","authors":["Shreyas Chaudhari","Pranjal Aggarwal","Vishvak Murahari","Tanmay Rajpurohit","Ashwin Kalyan","Karthik Narasimhan","Ameet Deshpande","Bruno Castro da Silva"],"pdf_url":"https://arxiv.org/pdf/2404.08555v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03048v2","updated":"2024-04-16T23:59:51Z","published":"2024-04-03T20:29:40Z","title":"Decentralised Moderation for Interoperable Social Networks: A\n  Conversation-based Approach for Pleroma and the Fediverse","summary":"  The recent development of decentralised and interoperable social networks\n(such as the \"fediverse\") creates new challenges for content moderators. This\nis because millions of posts generated on one server can easily \"spread\" to\nanother, even if the recipient server has very different moderation policies.\nAn obvious solution would be to leverage moderation tools to automatically tag\n(and filter) posts that contravene moderation policies, e.g. related to toxic\nspeech. Recent work has exploited the conversational context of a post to\nimprove this automatic tagging, e.g. using the replies to a post to help\nclassify if it contains toxic speech. This has shown particular potential in\nenvironments with large training sets that contain complete conversations.\nThis, however, creates challenges in a decentralised context, as a single\nconversation may be fragmented across multiple servers. Thus, each server only\nhas a partial view of an entire conversation because conversations are often\nfederated across servers in a non-synchronized fashion. To address this, we\npropose a decentralised conversation-aware content moderation approach suitable\nfor the fediverse. Our approach employs a graph deep learning model (GraphNLI)\ntrained locally on each server. The model exploits local data to train a model\nthat combines post and conversational information captured through random walks\nto detect toxicity. We evaluate our approach with data from Pleroma, a major\ndecentralised and interoperable micro-blogging network containing 2 million\nconversations. Our model effectively detects toxicity on larger instances,\nexclusively trained using their local post information (0.8837 macro-F1). Our\napproach has considerable scope to improve moderation in decentralised and\ninteroperable social networks such as Pleroma or Mastodon.\n","authors":["Vibhor Agarwal","Aravindh Raman","Nishanth Sastry","Ahmed M. Abdelmoniem","Gareth Tyson","Ignacio Castro"],"pdf_url":"https://arxiv.org/pdf/2404.03048v2.pdf","comment":"Accepted at International AAAI Conference on Web and Social Media\n  (ICWSM) 2024. Please cite accordingly!"},{"id":"http://arxiv.org/abs/2404.10960v1","updated":"2024-04-16T23:56:38Z","published":"2024-04-16T23:56:38Z","title":"Uncertainty-Based Abstention in LLMs Improves Safety and Reduces\n  Hallucinations","summary":"  A major barrier towards the practical deployment of large language models\n(LLMs) is their lack of reliability. Three situations where this is\nparticularly apparent are correctness, hallucinations when given unanswerable\nquestions, and safety. In all three cases, models should ideally abstain from\nresponding, much like humans, whose ability to understand uncertainty makes us\nrefrain from answering questions we don't know. Inspired by analogous\napproaches in classification, this study explores the feasibility and efficacy\nof abstaining while uncertain in the context of LLMs within the domain of\nquestion-answering. We investigate two kinds of uncertainties, statistical\nuncertainty metrics and a distinct verbalized measure, termed as In-Dialogue\nUncertainty (InDU). Using these uncertainty measures combined with models with\nand without Reinforcement Learning with Human Feedback (RLHF), we show that in\nall three situations, abstention based on the right kind of uncertainty measure\ncan boost the reliability of LLMs. By sacrificing only a few highly uncertain\nsamples we can improve correctness by 2% to 8%, avoid 50% hallucinations via\ncorrectly identifying unanswerable questions and increase safety by 70% up to\n99% with almost no additional computational overhead.\n","authors":["Christian Tomani","Kamalika Chaudhuri","Ivan Evtimov","Daniel Cremers","Mark Ibrahim"],"pdf_url":"https://arxiv.org/pdf/2404.10960v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00824v2","updated":"2024-04-16T23:32:38Z","published":"2024-02-27T00:24:42Z","title":"Information Flow Routes: Automatically Interpreting Language Models at\n  Scale","summary":"  Information flows by routes inside the network via mechanisms implemented in\nthe model. These routes can be represented as graphs where nodes correspond to\ntoken representations and edges to operations inside the network. We\nautomatically build these graphs in a top-down manner, for each prediction\nleaving only the most important nodes and edges. In contrast to the existing\nworkflows relying on activation patching, we do this through attribution: this\nallows us to efficiently uncover existing circuits with just a single forward\npass. Additionally, the applicability of our method is far beyond patching: we\ndo not need a human to carefully design prediction templates, and we can\nextract information flow routes for any prediction (not just the ones among the\nallowed templates). As a result, we can talk about model behavior in general,\nfor specific types of predictions, or different domains. We experiment with\nLlama 2 and show that the role of some attention heads is overall important,\ne.g. previous token heads and subword merging heads. Next, we find similarities\nin Llama 2 behavior when handling tokens of the same part of speech. Finally,\nwe show that some model components can be specialized on domains such as coding\nor multilingual texts.\n","authors":["Javier Ferrando","Elena Voita"],"pdf_url":"https://arxiv.org/pdf/2403.00824v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10952v1","updated":"2024-04-16T23:27:38Z","published":"2024-04-16T23:27:38Z","title":"Can Language Models Solve Olympiad Programming?","summary":"  Computing olympiads contain some of the most challenging problems for humans,\nrequiring complex algorithmic reasoning, puzzle solving, in addition to\ngenerating efficient code. However, it has been understudied as a domain to\nevaluate language models (LMs). In this paper, we introduce the USACO benchmark\nwith 307 problems from the USA Computing Olympiad, along with high-quality unit\ntests, reference code, and official analyses for each problem. These resources\nenable us to construct and test a range of LM inference methods for competitive\nprogramming for the first time. We find GPT-4 only achieves a 8.7% pass@1\naccuracy with zero-shot chain-of-thought prompting, and our best inference\nmethod improves it to 20.2% using a combination of self-reflection and\nretrieval over episodic knowledge. However, this is far from solving the\nbenchmark. To better understand the remaining challenges, we design a novel\nhuman-in-the-loop study and surprisingly find that a small number of targeted\nhints enable GPT-4 to solve 13 out of 15 problems previously unsolvable by any\nmodel and method. Our benchmark, baseline methods, quantitative results, and\nqualitative analysis serve as an initial step toward LMs with grounded,\ncreative, and algorithmic reasoning.\n","authors":["Quan Shi","Michael Tang","Karthik Narasimhan","Shunyu Yao"],"pdf_url":"https://arxiv.org/pdf/2404.10952v1.pdf","comment":"Code and data: https://princeton-nlp.github.io/USACOBench/"},{"id":"http://arxiv.org/abs/2311.07387v2","updated":"2024-04-16T22:51:51Z","published":"2023-11-13T15:11:26Z","title":"Assessing Logical Puzzle Solving in Large Language Models: Insights from\n  a Minesweeper Case Study","summary":"  Large Language Models (LLMs) have shown remarkable proficiency in language\nunderstanding and have been successfully applied to a variety of real-world\ntasks through task-specific fine-tuning or prompt engineering. Despite these\nadvancements, it remains an open question whether LLMs are fundamentally\ncapable of reasoning and planning, or if they primarily rely on recalling and\nsynthesizing information from their training data. In our research, we\nintroduce a novel task -- Minesweeper -- specifically designed in a format\nunfamiliar to LLMs and absent from their training datasets. This task\nchallenges LLMs to identify the locations of mines based on numerical clues\nprovided by adjacent opened cells. Successfully completing this task requires\nan understanding of each cell's state, discerning spatial relationships between\nthe clues and mines, and strategizing actions based on logical deductions drawn\nfrom the arrangement of the cells. Our experiments, including trials with the\nadvanced GPT-4 model, indicate that while LLMs possess the foundational\nabilities required for this task, they struggle to integrate these into a\ncoherent, multi-step logical reasoning process needed to solve Minesweeper.\nThese findings highlight the need for further research to understand the nature\nof reasoning capabilities in LLMs under similar circumstances, and to explore\npathways towards more sophisticated AI reasoning and planning models.\n","authors":["Yinghao Li","Haorui Wang","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.07387v2.pdf","comment":"23 pages, 5 figures, 4 tables, in NAACL 2024"},{"id":"http://arxiv.org/abs/2404.10939v1","updated":"2024-04-16T22:43:48Z","published":"2024-04-16T22:43:48Z","title":"More Room for Language: Investigating the Effect of Retrieval on\n  Language Models","summary":"  Retrieval-augmented language models pose a promising alternative to standard\nlanguage modeling. During pretraining, these models search in a corpus of\ndocuments for contextually relevant information that could aid the language\nmodeling objective. We introduce an 'ideal retrieval' methodology to study\nthese models in a fully controllable setting. We conduct an extensive\nevaluation to examine how retrieval augmentation affects the behavior of the\nunderlying language model. Among other things, we observe that these models: i)\nsave substantially less world knowledge in their weights, ii) are better at\nunderstanding local context and inter-word dependencies, but iii) are worse at\ncomprehending global context.\n","authors":["David Samuel","Lucas Georges Gabriel Charpentier","Sondre Wold"],"pdf_url":"https://arxiv.org/pdf/2404.10939v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2312.03740v2","updated":"2024-04-16T22:27:39Z","published":"2023-11-28T17:56:34Z","title":"A Survey on Prompting Techniques in LLMs","summary":"  Autoregressive Large Language Models have transformed the landscape of\nNatural Language Processing. Pre-train and prompt paradigm has replaced the\nconventional approach of pre-training and fine-tuning for many downstream NLP\ntasks. This shift has been possible largely due to LLMs and innovative\nprompting techniques. LLMs have shown great promise for a variety of downstream\ntasks owing to their vast parameters and huge datasets that they are\npre-trained on. However, in order to fully realize their potential, their\noutputs must be guided towards the desired outcomes. Prompting, in which a\nspecific input or instruction is provided to guide the LLMs toward the intended\noutput, has become a tool for achieving this goal. In this paper, we discuss\nthe various prompting techniques that have been applied to fully harness the\npower of LLMs. We present a taxonomy of existing literature on prompting\ntechniques and provide a concise survey based on this taxonomy. Further, we\nidentify some open problems in the realm of prompting in autoregressive LLMs\nwhich could serve as a direction for future research.\n","authors":["Prabin Bhandari"],"pdf_url":"https://arxiv.org/pdf/2312.03740v2.pdf","comment":"10 pages, 4 Figures"},{"id":"http://arxiv.org/abs/2404.10934v1","updated":"2024-04-16T22:12:36Z","published":"2024-04-16T22:12:36Z","title":"Shears: Unstructured Sparsity with Neural Low-rank Adapter Search","summary":"  Recently, several approaches successfully demonstrated that weight-sharing\nNeural Architecture Search (NAS) can effectively explore a search space of\nelastic low-rank adapters (LoRA), allowing the parameter-efficient fine-tuning\n(PEFT) and compression of large language models. In this paper, we introduce a\nnovel approach called Shears, demonstrating how the integration of\ncost-effective sparsity and a proposed Neural Low-rank adapter Search (NLS)\nalgorithm can further improve the efficiency of PEFT approaches. Results\ndemonstrate the benefits of Shears compared to other methods, reaching high\nsparsity levels while improving or with little drop in accuracy, utilizing a\nsingle GPU for a pair of hours.\n","authors":["J. Pablo Mu√±oz","Jinjie Yuan","Nilesh Jain"],"pdf_url":"https://arxiv.org/pdf/2404.10934v1.pdf","comment":"2024 Annual Conference of the North American Chapter of the\n  Association for Computational Linguistics (Industry Track)"},{"id":"http://arxiv.org/abs/2404.10933v1","updated":"2024-04-16T22:11:35Z","published":"2024-04-16T22:11:35Z","title":"LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs","summary":"  Fine-tuning pre-trained large language models (LLMs) with limited hardware\npresents challenges due to GPU memory constraints. Various distributed\nfine-tuning methods have been proposed to alleviate memory constraints on GPU.\nHowever, determining the most effective method for achieving rapid fine-tuning\nwhile preventing GPU out-of-memory issues in a given environment remains\nunclear. To address this challenge, we introduce LLMem, a solution that\nestimates the GPU memory consumption when applying distributed fine-tuning\nmethods across multiple GPUs and identifies the optimal method. We conduct GPU\nmemory usage estimation prior to fine-tuning, leveraging the fundamental\nstructure of transformer-based decoder models and the memory usage distribution\nof each method. Experimental results show that LLMem accurately estimates peak\nGPU memory usage on a single GPU, with error rates of up to 1.6%. Additionally,\nit shows an average error rate of 3.0% when applying distributed fine-tuning\nmethods to LLMs with more than a billion parameters on multi-GPU setups.\n","authors":["Taeho Kim","Yanming Wang","Vatshank Chaturvedi","Lokesh Gupta","Seyeon Kim","Yongin Kwon","Sangtae Ha"],"pdf_url":"https://arxiv.org/pdf/2404.10933v1.pdf","comment":"9 pages, 9 figures, accepted to IJCAI 2024"},{"id":"http://arxiv.org/abs/2404.10924v1","updated":"2024-04-16T21:52:55Z","published":"2024-04-16T21:52:55Z","title":"Binder: Hierarchical Concept Representation through Order Embedding of\n  Binary Vectors","summary":"  For natural language understanding and generation, embedding concepts using\nan order-based representation is an essential task. Unlike traditional point\nvector based representation, an order-based representation imposes geometric\nconstraints on the representation vectors for explicitly capturing various\nsemantic relationships that may exist between a pair of concepts. In existing\nliterature, several approaches on order-based embedding have been proposed,\nmostly focusing on capturing hierarchical relationships; examples include\nvectors in Euclidean space, complex, Hyperbolic, order, and Box Embedding. Box\nembedding creates region-based rich representation of concepts, but along the\nprocess it sacrifices simplicity, requiring a custom-made optimization scheme\nfor learning the representation. Hyperbolic embedding improves embedding\nquality by exploiting the ever-expanding property of Hyperbolic space, but it\nalso suffers from the same fate as box embedding as gradient descent like\noptimization is not simple in the Hyperbolic space. In this work, we propose\nBinder, a novel approach for order-based representation. Binder uses binary\nvectors for embedding, so the embedding vectors are compact with an order of\nmagnitude smaller footprint than other methods. Binder uses a simple and\nefficient optimization scheme for learning representation vectors with a linear\ntime complexity. Our comprehensive experimental results show that Binder is\nvery accurate, yielding competitive results on the representation task. But\nBinder stands out from its competitors on the transitive closure link\nprediction task as it can learn concept embeddings just from the direct edges,\nwhereas all existing order-based approaches rely on the indirect edges.\n","authors":["Croix Gyurek","Niloy Talukder","Mohammad Al Hasan"],"pdf_url":"https://arxiv.org/pdf/2404.10924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10922v1","updated":"2024-04-16T21:45:59Z","published":"2024-04-16T21:45:59Z","title":"Teaching a Multilingual Large Language Model to Understand Multilingual\n  Speech via Multi-Instructional Training","summary":"  Recent advancements in language modeling have led to the emergence of Large\nLanguage Models (LLMs) capable of various natural language processing tasks.\nDespite their success in text-based tasks, applying LLMs to the speech domain\nremains limited and challenging. This paper presents BLOOMZMMS, a novel model\nthat integrates a multilingual LLM with a multilingual speech encoder, aiming\nto harness the capabilities of LLMs for speech recognition and beyond.\nUtilizing a multi-instructional training approach, we demonstrate the\ntransferability of linguistic knowledge from the text to the speech modality.\nOur experiments, conducted on 1900 hours of transcribed data from 139\nlanguages, establish that a multilingual speech representation can be\neffectively learned and aligned with a multilingual LLM. While this learned\nrepresentation initially shows limitations in task generalization, we address\nthis issue by generating synthetic targets in a multi-instructional style. Our\nzero-shot evaluation results confirm the robustness of our approach across\nmultiple tasks, including speech translation and multilingual spoken language\nunderstanding, thereby opening new avenues for applying LLMs in the speech\ndomain.\n","authors":["Pavel Denisov","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2404.10922v1.pdf","comment":"NAACL Findings 2024"},{"id":"http://arxiv.org/abs/2311.12275v4","updated":"2024-04-16T21:34:29Z","published":"2023-11-21T01:34:02Z","title":"Enabling On-Device Large Language Model Personalization with\n  Self-Supervised Data Selection and Synthesis","summary":"  After a large language model (LLM) is deployed on edge devices, it is\ndesirable for these devices to learn from user-generated conversation data to\ngenerate user-specific and personalized responses in real-time. However,\nuser-generated data usually contains sensitive and private information, and\nuploading such data to the cloud for annotation is not preferred if not\nprohibited. While it is possible to obtain annotation locally by directly\nasking users to provide preferred responses, such annotations have to be sparse\nto not affect user experience. In addition, the storage of edge devices is\nusually too limited to enable large-scale fine-tuning with full user-generated\ndata. It remains an open question how to enable on-device LLM personalization,\nconsidering sparse annotation and limited on-device storage. In this paper, we\npropose a novel framework to select and store the most representative data\nonline in a self-supervised way. Such data has a small memory footprint and\nallows infrequent requests of user annotations for further fine-tuning. To\nenhance fine-tuning quality, multiple semantically similar pairs of question\ntexts and expected responses are generated using the LLM. Our experiments show\nthat the proposed framework achieves the best user-specific content-generating\ncapability (accuracy) and fine-tuning speed (performance) compared with vanilla\nbaselines. To the best of our knowledge, this is the very first on-device LLM\npersonalization framework.\n","authors":["Ruiyang Qin","Jun Xia","Zhenge Jia","Meng Jiang","Ahmed Abbasi","Peipei Zhou","Jingtong Hu","Yiyu Shi"],"pdf_url":"https://arxiv.org/pdf/2311.12275v4.pdf","comment":"Accepted by 2024 61th ACM/IEEE Design Automation Conference (DAC)"},{"id":"http://arxiv.org/abs/2404.10917v1","updated":"2024-04-16T21:33:05Z","published":"2024-04-16T21:33:05Z","title":"Which questions should I answer? Salience Prediction of Inquisitive\n  Questions","summary":"  Inquisitive questions -- open-ended, curiosity-driven questions people ask as\nthey read -- are an integral part of discourse processing (Kehler and Rohde,\n2017; Onea, 2016) and comprehension (Prince, 2004). Recent work in NLP has\ntaken advantage of question generation capabilities of LLMs to enhance a wide\nrange of applications. But the space of inquisitive questions is vast: many\nquestions can be evoked from a given context. So which of those should be\nprioritized to find answers? Linguistic theories, unfortunately, have not yet\nprovided an answer to this question. This paper presents QSALIENCE, a salience\npredictor of inquisitive questions. QSALIENCE is instruction-tuned over our\ndataset of linguist-annotated salience scores of 1,766 (context, question)\npairs. A question scores high on salience if answering it would greatly enhance\nthe understanding of the text (Van Rooy, 2003). We show that highly salient\nquestions are empirically more likely to be answered in the same article,\nbridging potential questions (Onea, 2016) with Questions Under Discussion\n(Roberts, 2012). We further validate our findings by showing that answering\nsalient questions is an indicator of summarization quality in news.\n","authors":["Yating Wu","Ritika Mangla","Alexandros G. Dimakis","Greg Durrett","Junyi Jessy Li"],"pdf_url":"https://arxiv.org/pdf/2404.10917v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03441v2","updated":"2024-04-16T21:32:47Z","published":"2024-04-04T13:39:06Z","title":"Benchmarking ChatGPT on Algorithmic Reasoning","summary":"  We evaluate ChatGPT's ability to solve algorithm problems from the CLRS\nbenchmark suite that is designed for GNNs. The benchmark requires the use of a\nspecified classical algorithm to solve a given problem. We find that ChatGPT\noutperforms specialist GNN models, using Python to successfully solve these\nproblems. This raises new points in the discussion about learning algorithms\nwith neural networks and how we think about what out of distribution testing\nlooks like with web scale training data.\n","authors":["Sean McLeish","Avi Schwarzschild","Tom Goldstein"],"pdf_url":"https://arxiv.org/pdf/2404.03441v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19708v2","updated":"2024-04-16T21:32:29Z","published":"2024-03-23T10:42:49Z","title":"AttentionStore: Cost-effective Attention Reuse across Multi-turn\n  Conversations in Large Language Model Serving","summary":"  Interacting with humans through multi-turn conversations is a fundamental\nfeature of large language models (LLMs). However, existing LLM serving engines\nfor executing multi-turn conversations are inefficient due to the need to\nrepeatedly compute the key-value (KV) caches of historical tokens, incurring\nhigh serving costs. To address the problem, this paper proposes AttentionStore,\na new attention mechanism that enables the reuse of KV caches (i.e., attention\nreuse) across multi-turn conversations, significantly reducing the repetitive\ncomputation overheads. AttentionStore maintains a hierarchical KV caching\nsystem that leverages cost-effective memory/storage mediums to save KV caches\nfor all requests. To reduce KV cache access overheads from slow mediums,\nAttentionStore employs layer-wise pre-loading and asynchronous saving schemes\nto overlap the KV cache access with the GPU computation. To ensure that the KV\ncaches to be accessed are placed in the fastest hierarchy, AttentionStore\nemploys scheduler-aware fetching and eviction schemes to consciously place the\nKV caches in different layers based on the hints from the inference job\nscheduler. To avoid the invalidation of the saved KV caches incurred by context\nwindow overflow, AttentionStore enables the saved KV caches to remain valid via\ndecoupling the positional encoding and effectively truncating the KV caches.\nExtensive experimental results demonstrate that AttentionStore significantly\ndecreases the time to the first token (TTFT) by up to 87%, improves the prompt\nprefilling throughput by 7.8$\\times$ for multi-turn conversations, and reduces\nthe end-to-end inference cost by up to 70%. For long sequence inference,\nAttentionStore reduces the TTFT by up to 95% and improves the prompt prefilling\nthroughput by 22$\\times$.\n","authors":["Bin Gao","Zhuomin He","Puru Sharma","Qingxuan Kang","Djordje Jevdjic","Junbo Deng","Xingkun Yang","Zhou Yu","Pengfei Zuo"],"pdf_url":"https://arxiv.org/pdf/2403.19708v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01132v4","updated":"2024-04-16T21:30:14Z","published":"2023-10-02T12:11:17Z","title":"Automated Evaluation of Classroom Instructional Support with LLMs and\n  BoWs: Connecting Global Predictions to Specific Feedback","summary":"  With the aim to provide teachers with more specific, frequent, and actionable\nfeedback about their teaching, we explore how Large Language Models (LLMs) can\nbe used to estimate ``Instructional Support'' domain scores of the CLassroom\nAssessment Scoring System (CLASS), a widely used observation protocol. We\ndesign a machine learning architecture that uses either zero-shot prompting of\nMeta's Llama2, and/or a classic Bag of Words (BoW) model, to classify\nindividual utterances of teachers' speech (transcribed automatically using\nOpenAI's Whisper) for the presence of Instructional Support. Then, these\nutterance-level judgments are aggregated over a 15-min observation session to\nestimate a global CLASS score. Experiments on two CLASS-coded datasets of\ntoddler and pre-kindergarten classrooms indicate that (1) automatic CLASS\nInstructional Support estimation accuracy using the proposed method (Pearson\n$R$ up to $0.48$) approaches human inter-rater reliability (up to $R=0.55$);\n(2) LLMs generally yield slightly greater accuracy than BoW for this task,\nthough the best models often combined features extracted from both LLM and BoW;\nand (3) for classifying individual utterances, there is still room for\nimprovement of automated methods compared to human-level judgments. Finally,\n(4) we illustrate how the model's outputs can be visualized at the utterance\nlevel to provide teachers with explainable feedback on which utterances were\nmost positively or negatively correlated with specific CLASS dimensions.\n","authors":["Jacob Whitehill","Jennifer LoCasale-Crouch"],"pdf_url":"https://arxiv.org/pdf/2310.01132v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17887v3","updated":"2024-04-16T20:54:01Z","published":"2024-02-27T21:01:41Z","title":"JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning\n  and Professional Question Answering Capability","summary":"  Large Language Models (LLMs) have demonstrated a remarkable potential in\nmedical knowledge acquisition and question-answering. However, LLMs can\npotentially hallucinate and yield factually incorrect outcomes, even with\ndomain-specific pretraining. Previously, retrieval augmented generation (RAG)\nhas limited success in addressing hallucinations. Unlike previous methods in\nRAG where the retrieval model was trained separately from the LLM, we introduce\nJMLR (for Jointly trains LLM and information Retrieval (IR)) during the\nfine-tuning phase. The synchronized training mechanism enhances JMLR's ability\nto retrieve clinical guidelines and leverage medical knowledge to reason and\nanswer questions and reduces the demand for computational resources. We\nevaluated JMLR on the important medical question answering application. Our\nexperimental results demonstrate that JMLR-13B (70.5%) outperforms a previous\nstate-of-the-art open-source model using conventional pre-training and\nfine-tuning Meditron-70B (68.9%) and Llama2-13B with RAG (54.9%) on a medical\nquestion-answering dataset. JMLR-13B (148 GPU hours) also trains much faster\nthan Meditron-70B (42630 GPU hours). Through this work, we provide a new and\nefficient knowledge enhancement tool for healthcare, demonstrating the\npotential of integrating IR and LLM training for medical question-answering\nsystems.\n","authors":["Junda Wang","Zhichao Yang","Zonghai Yao","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2402.17887v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.09846v3","updated":"2024-04-16T20:43:53Z","published":"2023-05-16T23:27:59Z","title":"CPL-NoViD: Context-Aware Prompt-based Learning for Norm Violation\n  Detection in Online Communities","summary":"  Detecting norm violations in online communities is critical to maintaining\nhealthy and safe spaces for online discussions. Existing machine learning\napproaches often struggle to adapt to the diverse rules and interpretations\nacross different communities due to the inherent challenges of fine-tuning\nmodels for such context-specific tasks. In this paper, we introduce\nContext-aware Prompt-based Learning for Norm Violation Detection (CPL-NoViD), a\nnovel method that employs prompt-based learning to detect norm violations\nacross various types of rules. CPL-NoViD outperforms the baseline by\nincorporating context through natural language prompts and demonstrates\nimproved performance across different rule types. Significantly, it not only\nexcels in cross-rule-type and cross-community norm violation detection but also\nexhibits adaptability in few-shot learning scenarios. Most notably, it\nestablishes a new state-of-the-art in norm violation detection, surpassing\nexisting benchmarks. Our work highlights the potential of prompt-based learning\nfor context-sensitive norm violation detection and paves the way for future\nresearch on more adaptable, context-aware models to better support online\ncommunity moderators.\n","authors":["Zihao He","Jonathan May","Kristina Lerman"],"pdf_url":"https://arxiv.org/pdf/2305.09846v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11035v2","updated":"2024-04-16T20:22:21Z","published":"2024-02-16T19:28:52Z","title":"Retrieval-Augmented Generation: Is Dense Passage Retrieval Retrieving?","summary":"  Dense passage retrieval (DPR) is the first step in the retrieval augmented\ngeneration (RAG) paradigm for improving the performance of large language\nmodels (LLM). DPR fine-tunes pre-trained networks to enhance the alignment of\nthe embeddings between queries and relevant textual data. A deeper\nunderstanding of DPR fine-tuning will be required to fundamentally unlock the\nfull potential of this approach. In this work, we explore DPR-trained models\nmechanistically by using a combination of probing, layer activation analysis,\nand model editing. Our experiments show that DPR training decentralizes how\nknowledge is stored in the network, creating multiple access pathways to the\nsame information. We also uncover a limitation in this training style: the\ninternal knowledge of the pre-trained model bounds what the retrieval model can\nretrieve. These findings suggest a few possible directions for dense retrieval:\n(1) expose the DPR training process to more knowledge so more can be\ndecentralized, (2) inject facts as decentralized representations, (3) model and\nincorporate knowledge uncertainty in the retrieval process, and (4) directly\nmap internal model knowledge to a knowledge base.\n","authors":["Benjamin Reichman","Larry Heck"],"pdf_url":"https://arxiv.org/pdf/2402.11035v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10887v1","updated":"2024-04-16T20:15:32Z","published":"2024-04-16T20:15:32Z","title":"Search Beyond Queries: Training Smaller Language Models for Web\n  Interactions via Reinforcement Learning","summary":"  Traditional search systems focus on query formulation for effective results\nbut face challenges in scenarios such as product searches where crucial product\ndetails (e.g., size, color) remain concealed until users visit specific product\npages. This highlights the need for intelligent web navigation agents capable\nof formulating queries and navigating web pages according to users' high-level\nintents. In response to this need, this work introduces a Grounded Language\nAgent for Intelligent Web Interactions, called GLAINTEL. Drawing upon\nadvancements in language modeling and reinforcement learning, GLAINTEL\ninvestigates the efficacy of transformer-based models in enhancing the search\ncapabilities of interactive web environments. Given the dynamic action space\nfor each state in web navigation, GLAINTEL employs the Flan-T5 architecture and\nincorporates language modeling and value estimation heads. This work focuses on\ntraining smaller language models as agents across various scenarios,\nsystematically evaluating the impact of human demonstrations on the training\nprocess. Specifically, we investigate scenarios where no human demonstrations\nare available and subsequently assess the effective utilization of such\ndemonstrations. We also explore unsupervised domain adaptation for situations\nwhere demonstrations are confined to a specific domain. Experimental\nevaluations across diverse setups demonstrate the effectiveness of training\nagents in unsupervised settings, outperforming in-context learning-based\napproaches that employ larger models with up to 540 billion parameters.\nSurprisingly, behavioral cloning-based methods that straightforwardly use human\ndemonstrations do not outperform unsupervised learning-based methods.\nAdditionally, combining human demonstrations with Reinforcement Learning-based\ntraining yields results comparable to models utilizing GPT-4.\n","authors":["Moghis Fereidouni","A. B. Siddique"],"pdf_url":"https://arxiv.org/pdf/2404.10887v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2404.10877v1","updated":"2024-04-16T19:53:35Z","published":"2024-04-16T19:53:35Z","title":"Incubating Text Classifiers Following User Instruction with Nothing but\n  LLM","summary":"  In this paper, we aim to generate text classification data given arbitrary\nclass definitions (i.e., user instruction), so one can train a small text\nclassifier without any human annotation or raw corpus. Compared with pioneer\nattempts, our proposed Incubator is the first framework that can handle\ncomplicated and even mutually dependent classes (e.g., \"TED Talk given by\nEducator\" and \"Other\"). Specifically, Incubator is an LLM firstly tuned on the\ninstruction-to-data mappings that we obtained from classification datasets and\ndescriptions on HuggingFace together with in-context augmentation by GPT-4. We\nthen refine Incubator by learning on the cluster centers of semantic textual\nembeddings to emphasize the uniformity and semantic diversity in generations.\nWe compare Incubator on various classification tasks with strong baselines such\nas direct LLM-based inference and training data generation by prompt\nengineering. Experiments show Incubator is able to (1) perform well on\ntraditional benchmarks, (2) take label dependency and user preference into\nconsideration, and (3) enable logical text mining by incubating multiple\nclassifiers.\n","authors":["Letian Peng","Jingbo Shang"],"pdf_url":"https://arxiv.org/pdf/2404.10877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10859v1","updated":"2024-04-16T19:17:23Z","published":"2024-04-16T19:17:23Z","title":"Forcing Diffuse Distributions out of Language Models","summary":"  Despite being trained specifically to follow user instructions, today's\nlanguage models perform poorly when instructed to produce random outputs. For\nexample, when prompted to pick a number uniformly between one and ten\nLlama-2-13B-chat disproportionately favors the number five, and when tasked\nwith picking a first name at random, Mistral-7B-Instruct chooses Avery 40 times\nmore often than we would expect based on the U.S. population. When these\nlanguage models are used for real-world tasks where diversity of outputs is\ncrucial, such as language model assisted dataset construction, their inability\nto produce diffuse distributions over valid choices is a major hurdle. In this\nwork, we propose a fine-tuning method that encourages language models to output\ndistributions that are diffuse over valid outcomes. The methods we introduce\ngeneralize across a variety of tasks and distributions and make large language\nmodels practical for synthetic dataset generation with little human\nintervention.\n","authors":["Yiming Zhang","Avi Schwarzschild","Nicholas Carlini","Zico Kolter","Daphne Ippolito"],"pdf_url":"https://arxiv.org/pdf/2404.10859v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.03175v2","updated":"2024-04-16T19:16:46Z","published":"2023-09-06T17:24:06Z","title":"Gender-specific Machine Translation with Large Language Models","summary":"  While machine translation (MT) systems have seen significant improvements, it\nis still common for translations to reflect societal biases, such as gender\nbias. Decoder-only Large Language Models (LLMs) have demonstrated potential in\nMT, albeit with performance slightly lagging behind traditional encoder-decoder\nNeural Machine Translation (NMT) systems. However, LLMs offer a unique\nadvantage: the ability to control the properties of the output through prompts.\nIn this study, we leverage this flexibility to explore LLaMa's capability to\nproduce gender-specific translations. Our results indicate that LLaMa can\ngenerate gender-specific translations with translation accuracy and gender bias\ncomparable to NLLB, a state-of-the-art multilingual NMT system. Furthermore,\nour experiments reveal that LLaMa's gender-specific translations rely on\ncoreference resolution to determine gender, showing higher gender variance in\ngender-ambiguous datasets but maintaining consistency in less ambiguous\ncontexts. This research investigates the potential and challenges of using LLMs\nfor gender-specific translations as an instance of the controllability of\noutputs offered by LLMs.\n","authors":["Eduardo S√°nchez","Pierre Andrews","Pontus Stenetorp","Mikel Artetxe","Marta R. Costa-juss√†"],"pdf_url":"https://arxiv.org/pdf/2309.03175v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10857v1","updated":"2024-04-16T19:12:03Z","published":"2024-04-16T19:12:03Z","title":"D3CODE: Disentangling Disagreements in Data across Cultures on\n  Offensiveness Detection and Evaluation","summary":"  While human annotations play a crucial role in language technologies,\nannotator subjectivity has long been overlooked in data collection. Recent\nstudies that have critically examined this issue are often situated in the\nWestern context, and solely document differences across age, gender, or racial\ngroups. As a result, NLP research on subjectivity have overlooked the fact that\nindividuals within demographic groups may hold diverse values, which can\ninfluence their perceptions beyond their group norms. To effectively\nincorporate these considerations into NLP pipelines, we need datasets with\nextensive parallel annotations from various social and cultural groups. In this\npaper we introduce the \\dataset dataset: a large-scale cross-cultural dataset\nof parallel annotations for offensive language in over 4.5K sentences annotated\nby a pool of over 4k annotators, balanced across gender and age, from across 21\ncountries, representing eight geo-cultural regions. The dataset contains\nannotators' moral values captured along six moral foundations: care, equality,\nproportionality, authority, loyalty, and purity. Our analyses reveal\nsubstantial regional variations in annotators' perceptions that are shaped by\nindividual moral values, offering crucial insights for building pluralistic,\nculturally sensitive NLP models.\n","authors":["Aida Mostafazadeh Davani","Mark D√≠az","Dylan Baker","Vinodkumar Prabhakaran"],"pdf_url":"https://arxiv.org/pdf/2404.10857v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10848v1","updated":"2024-04-16T18:50:57Z","published":"2024-04-16T18:50:57Z","title":"A LayoutLMv3-Based Model for Enhanced Relation Extraction in\n  Visually-Rich Documents","summary":"  Document Understanding is an evolving field in Natural Language Processing\n(NLP). In particular, visual and spatial features are essential in addition to\nthe raw text itself and hence, several multimodal models were developed in the\nfield of Visual Document Understanding (VDU). However, while research is mainly\nfocused on Key Information Extraction (KIE), Relation Extraction (RE) between\nidentified entities is still under-studied. For instance, RE is crucial to\nregroup entities or obtain a comprehensive hierarchy of data in a document. In\nthis paper, we present a model that, initialized from LayoutLMv3, can match or\noutperform the current state-of-the-art results in RE applied to Visually-Rich\nDocuments (VRD) on FUNSD and CORD datasets, without any specific pre-training\nand with fewer parameters. We also report an extensive ablation study performed\non FUNSD, highlighting the great impact of certain features and modelization\nchoices on the performances.\n","authors":["Wiam Adnan","Joel Tang","Yassine Bel Khayat Zouggari","Seif Edinne Laatiri","Laurent Lam","Fabien Caspani"],"pdf_url":"https://arxiv.org/pdf/2404.10848v1.pdf","comment":"Accepted at the International Conference on Document Analysis and\n  Recognition (ICDAR 2024)"},{"id":"http://arxiv.org/abs/2310.07793v5","updated":"2024-04-16T18:35:30Z","published":"2023-10-11T18:27:12Z","title":"GenTKG: Generative Forecasting on Temporal Knowledge Graph with Large\n  Language Models","summary":"  The rapid advancements in large language models (LLMs) have ignited interest\nin the temporal knowledge graph (tKG) domain, where conventional\nembedding-based and rule-based methods dominate. The question remains open of\nwhether pre-trained LLMs can understand structured temporal relational data and\nreplace them as the foundation model for temporal relational forecasting.\nTherefore, we bring temporal knowledge forecasting into the generative setting.\nHowever, challenges occur in the huge chasms between complex temporal graph\ndata structure and sequential natural expressions LLMs can handle, and between\nthe enormous data sizes of tKGs and heavy computation costs of finetuning LLMs.\nTo address these challenges, we propose a novel retrieval-augmented generation\nframework named GenTKG combining a temporal logical rule-based retrieval\nstrategy and few-shot parameter-efficient instruction tuning to solve the above\nchallenges, respectively. Extensive experiments have shown that GenTKG\noutperforms conventional methods of temporal relational forecasting with low\ncomputation resources using extremely limited training data as few as 16\nsamples. GenTKG also highlights remarkable cross-domain generalizability with\noutperforming performance on unseen datasets without re-training, and in-domain\ngeneralizability regardless of time split in the same dataset. Our work reveals\nthe huge potential of LLMs in the tKG domain and opens a new frontier for\ngenerative forecasting on tKGs. Code and data are released here:\nhttps://github.com/mayhugotong/GenTKG.\n","authors":["Ruotong Liao","Xu Jia","Yangzhe Li","Yunpu Ma","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2310.07793v5.pdf","comment":"14 pages, Findings of NAACL 2024, Spotlight on TGL@NeurIPS2023"},{"id":"http://arxiv.org/abs/2402.08702v2","updated":"2024-04-16T18:29:43Z","published":"2024-02-13T16:38:01Z","title":"PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human\n  Feedback and Preference Alignment","summary":"  Prompt optimization aims to find the best prompt to a large language model\n(LLM) for a given task. LLMs have been successfully used to help find and\nimprove prompt candidates for single-step tasks. However, realistic tasks for\nagents are multi-step and introduce new challenges: (1) Prompt content is\nlikely to be more extensive and complex, making it more difficult for LLMs to\nanalyze errors, (2) the impact of an individual step is difficult to evaluate,\nand (3) different people may have varied preferences about task execution.\nWhile humans struggle to optimize prompts, they are good at providing feedback\nabout LLM outputs; we therefore introduce a new LLM-driven discrete prompt\noptimization framework that incorporates human-designed feedback rules to\nautomatically offer direct suggestions for improvement. We also use an extra\nlearned heuristic model that predicts prompt performance to efficiently sample\nfrom prompt candidates. This approach significantly outperforms both\nhuman-engineered prompts and several other prompt optimization methods across\n11 representative multi-step tasks (an average 10.6%-29.3% improvement to\ncurrent best methods on five LLMs respectively). We further show that the score\nfunction for tasks can be modified to better align with individual preferences.\nWe believe our work can serve as a benchmark for automatic prompt optimization\nfor LLM-driven multi-step tasks.\n","authors":["Yongchao Chen","Jacob Arkin","Yilun Hao","Yang Zhang","Nicholas Roy","Chuchu Fan"],"pdf_url":"https://arxiv.org/pdf/2402.08702v2.pdf","comment":"58 pages, 13 figures"},{"id":"http://arxiv.org/abs/2404.10838v1","updated":"2024-04-16T18:22:49Z","published":"2024-04-16T18:22:49Z","title":"Dynamic Self-adaptive Multiscale Distillation from Pre-trained\n  Multimodal Large Model for Efficient Cross-modal Representation Learning","summary":"  In recent years, pre-trained multimodal large models have attracted\nwidespread attention due to their outstanding performance in various multimodal\napplications. Nonetheless, the extensive computational resources and vast\ndatasets required for their training present significant hurdles for deployment\nin environments with limited computational resources. To address this\nchallenge, we propose a novel dynamic self-adaptive multiscale distillation\nfrom pre-trained multimodal large model for efficient cross-modal\nrepresentation learning for the first time. Unlike existing distillation\nmethods, our strategy employs a multiscale perspective, enabling the extraction\nstructural knowledge across from the pre-trained multimodal large model.\nEnsuring that the student model inherits a comprehensive and nuanced\nunderstanding of the teacher knowledge. To optimize each distillation loss in a\nbalanced and efficient manner, we propose a dynamic self-adaptive distillation\nloss balancer, a novel component eliminating the need for manual loss weight\nadjustments and dynamically balances each loss item during the distillation\nprocess. Our methodology streamlines pre-trained multimodal large models using\nonly their output features and original image-level information, requiring\nminimal computational resources. This efficient approach is suited for various\napplications and allows the deployment of advanced multimodal technologies even\nin resource-limited settings. Extensive experiments has demonstrated that our\nmethod maintains high performance while significantly reducing model complexity\nand training costs. Moreover, our distilled student model utilizes only\nimage-level information to achieve state-of-the-art performance on cross-modal\nretrieval tasks, surpassing previous methods that relied on region-level\ninformation.\n","authors":["Zhengyang Liang","Meiyu Liang","Wei Huang","Yawen Li","Zhe Xue"],"pdf_url":"https://arxiv.org/pdf/2404.10838v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2404.10830v1","updated":"2024-04-16T18:08:29Z","published":"2024-04-16T18:08:29Z","title":"Fewer Truncations Improve Language Modeling","summary":"  In large language model training, input documents are typically concatenated\ntogether and then split into sequences of equal length to avoid padding tokens.\nDespite its efficiency, the concatenation approach compromises data integrity\n-- it inevitably breaks many documents into incomplete pieces, leading to\nexcessive truncations that hinder the model from learning to compose logically\ncoherent and factually consistent content that is grounded on the complete\ncontext. To address the issue, we propose Best-fit Packing, a scalable and\nefficient method that packs documents into training sequences through\nlength-aware combinatorial optimization. Our method completely eliminates\nunnecessary truncations while retaining the same training efficiency as\nconcatenation. Empirical results from both text and code pre-training show that\nour method achieves superior performance (e.g., relatively +4.7% on reading\ncomprehension; +16.8% in context following; and +9.2% on program synthesis),\nand reduces closed-domain hallucination effectively by up to 58.3%.\n","authors":["Hantian Ding","Zijian Wang","Giovanni Paolini","Varun Kumar","Anoop Deoras","Dan Roth","Stefano Soatto"],"pdf_url":"https://arxiv.org/pdf/2404.10830v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.03145v2","updated":"2024-04-16T18:04:14Z","published":"2023-04-06T15:29:57Z","title":"Evaluating the Robustness of Machine Reading Comprehension Models to Low\n  Resource Entity Renaming","summary":"  Question answering (QA) models have shown compelling results in the task of\nMachine Reading Comprehension (MRC). Recently these systems have proved to\nperform better than humans on held-out test sets of datasets e.g. SQuAD, but\ntheir robustness is not guaranteed. The QA model's brittleness is exposed when\nevaluated on adversarial generated examples by a performance drop. In this\nstudy, we explore the robustness of MRC models to entity renaming, with\nentities from low-resource regions such as Africa. We propose EntSwap, a method\nfor test-time perturbations, to create a test set whose entities have been\nrenamed. In particular, we rename entities of type: country, person,\nnationality, location, organization, and city, to create AfriSQuAD2. Using the\nperturbed test set, we evaluate the robustness of three popular MRC models. We\nfind that compared to base models, large models perform well comparatively on\nnovel entities. Furthermore, our analysis indicates that entity type person\nhighly challenges the MRC models' performance.\n","authors":["Clemencia Siro","Tunde Oluwaseyi Ajayi"],"pdf_url":"https://arxiv.org/pdf/2304.03145v2.pdf","comment":"Accepted at The AfricaNLP Workshop 2023 colocated with ICLR 2023"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2402.16846v2","updated":"2024-04-16T17:59:53Z","published":"2024-02-26T18:59:33Z","title":"GROUNDHOG: Grounding Large Language Models to Holistic Segmentation","summary":"  Most multimodal large language models (MLLMs) learn language-to-object\ngrounding through causal language modeling where grounded objects are captured\nby bounding boxes as sequences of location tokens. This paradigm lacks\npixel-level representations that are important for fine-grained visual\nunderstanding and diagnosis. In this work, we introduce GROUNDHOG, an MLLM\ndeveloped by grounding Large Language Models to holistic segmentation.\nGROUNDHOG incorporates a masked feature extractor and converts extracted\nfeatures into visual entity tokens for the MLLM backbone, which then connects\ngroundable phrases to unified grounding masks by retrieving and merging the\nentity masks. To train GROUNDHOG, we carefully curated M3G2, a grounded visual\ninstruction tuning dataset with Multi-Modal Multi-Grained Grounding, by\nharvesting a collection of segmentation-grounded datasets with rich\nannotations. Our experimental results show that GROUNDHOG achieves superior\nperformance on various language grounding tasks without task-specific\nfine-tuning, and significantly reduces object hallucination. GROUNDHOG also\ndemonstrates better grounding towards complex forms of visual input and\nprovides easy-to-understand diagnosis in failure cases.\n","authors":["Yichi Zhang","Ziqiao Ma","Xiaofeng Gao","Suhaila Shakiah","Qiaozi Gao","Joyce Chai"],"pdf_url":"https://arxiv.org/pdf/2402.16846v2.pdf","comment":"Accepted to CVPR 2024. Website: https://groundhog-mllm.github.io/"},{"id":"http://arxiv.org/abs/2404.10775v1","updated":"2024-04-16T17:59:11Z","published":"2024-04-16T17:59:11Z","title":"COMBO: Compositional World Models for Embodied Multi-Agent Cooperation","summary":"  In this paper, we investigate the problem of embodied multi-agent\ncooperation, where decentralized agents must cooperate given only partial\negocentric views of the world. To effectively plan in this setting, in contrast\nto learning world dynamics in a single-agent scenario, we must simulate world\ndynamics conditioned on an arbitrary number of agents' actions given only\npartial egocentric visual observations of the world. To address this issue of\npartial observability, we first train generative models to estimate the overall\nworld state given partial egocentric observations. To enable accurate\nsimulation of multiple sets of actions on this world state, we then propose to\nlearn a compositional world model for multi-agent cooperation by factorizing\nthe naturally composable joint actions of multiple agents and compositionally\ngenerating the video. By leveraging this compositional world model, in\ncombination with Vision Language Models to infer the actions of other agents,\nwe can use a tree search procedure to integrate these modules and facilitate\nonline cooperative planning. To evaluate the efficacy of our methods, we create\ntwo challenging embodied multi-agent long-horizon cooperation tasks using the\nThreeDWorld simulator and conduct experiments with 2-4 agents. The results show\nour compositional world model is effective and the framework enables the\nembodied agents to cooperate efficiently with different agents across various\ntasks and an arbitrary number of agents, showing the promising future of our\nproposed framework. More videos can be found at\nhttps://vis-www.cs.umass.edu/combo/.\n","authors":["Hongxin Zhang","Zeyuan Wang","Qiushi Lyu","Zheyuan Zhang","Sunli Chen","Tianmin Shu","Yilun Du","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2404.10775v1.pdf","comment":"23 pages. The first three authors contributed equally"},{"id":"http://arxiv.org/abs/2404.10772v1","updated":"2024-04-16T17:57:19Z","published":"2024-04-16T17:57:19Z","title":"Gaussian Opacity Fields: Efficient and Compact Surface Reconstruction in\n  Unbounded Scenes","summary":"  Recently, 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view\nsynthesis results, while allowing the rendering of high-resolution images in\nreal-time. However, leveraging 3D Gaussians for surface reconstruction poses\nsignificant challenges due to the explicit and disconnected nature of 3D\nGaussians. In this work, we present Gaussian Opacity Fields (GOF), a novel\napproach for efficient, high-quality, and compact surface reconstruction in\nunbounded scenes. Our GOF is derived from ray-tracing-based volume rendering of\n3D Gaussians, enabling direct geometry extraction from 3D Gaussians by\nidentifying its levelset, without resorting to Poisson reconstruction or TSDF\nfusion as in previous work. We approximate the surface normal of Gaussians as\nthe normal of the ray-Gaussian intersection plane, enabling the application of\nregularization that significantly enhances geometry. Furthermore, we develop an\nefficient geometry extraction method utilizing marching tetrahedra, where the\ntetrahedral grids are induced from 3D Gaussians and thus adapt to the scene's\ncomplexity. Our evaluations reveal that GOF surpasses existing 3DGS-based\nmethods in surface reconstruction and novel view synthesis. Further, it\ncompares favorably to, or even outperforms, neural implicit methods in both\nquality and speed.\n","authors":["Zehao Yu","Torsten Sattler","Andreas Geiger"],"pdf_url":"https://arxiv.org/pdf/2404.10772v1.pdf","comment":"Project page:\n  https://niujinshuchong.github.io/gaussian-opacity-fields"},{"id":"http://arxiv.org/abs/2312.13150v2","updated":"2024-04-16T17:56:19Z","published":"2023-12-20T16:14:58Z","title":"Splatter Image: Ultra-Fast Single-View 3D Reconstruction","summary":"  We introduce the \\method, an ultra-efficient approach for monocular 3D object\nreconstruction. Splatter Image is based on Gaussian Splatting, which allows\nfast and high-quality reconstruction of 3D scenes from multiple images. We\napply Gaussian Splatting to monocular reconstruction by learning a neural\nnetwork that, at test time, performs reconstruction in a feed-forward manner,\nat 38 FPS. Our main innovation is the surprisingly straightforward design of\nthis network, which, using 2D operators, maps the input image to one 3D\nGaussian per pixel. The resulting set of Gaussians thus has the form an image,\nthe Splatter Image. We further extend the method take several images as input\nvia cross-view attention. Owning to the speed of the renderer (588 FPS), we use\na single GPU for training while generating entire images at each iteration to\noptimize perceptual metrics like LPIPS. On several synthetic, real,\nmulti-category and large-scale benchmark datasets, we achieve better results in\nterms of PSNR, LPIPS, and other metrics while training and evaluating much\nfaster than prior works. Code, models, demo and more results are available at\nhttps://szymanowiczs.github.io/splatter-image.\n","authors":["Stanislaw Szymanowicz","Christian Rupprecht","Andrea Vedaldi"],"pdf_url":"https://arxiv.org/pdf/2312.13150v2.pdf","comment":"CVPR 2024. Project page:\n  https://szymanowiczs.github.io/splatter-image.html . Code:\n  https://github.com/szymanowiczs/splatter-image , Demo:\n  https://huggingface.co/spaces/szymanowiczs/splatter_image"},{"id":"http://arxiv.org/abs/2312.13752v2","updated":"2024-04-16T17:55:53Z","published":"2023-12-21T11:33:10Z","title":"Hunting imaging biomarkers in pulmonary fibrosis: Benchmarks of the\n  AIIB23 challenge","summary":"  Airway-related quantitative imaging biomarkers are crucial for examination,\ndiagnosis, and prognosis in pulmonary diseases. However, the manual delineation\nof airway trees remains prohibitively time-consuming. While significant efforts\nhave been made towards enhancing airway modelling, current public-available\ndatasets concentrate on lung diseases with moderate morphological variations.\nThe intricate honeycombing patterns present in the lung tissues of fibrotic\nlung disease patients exacerbate the challenges, often leading to various\nprediction errors. To address this issue, the 'Airway-Informed Quantitative CT\nImaging Biomarker for Fibrotic Lung Disease 2023' (AIIB23) competition was\norganized in conjunction with the official 2023 International Conference on\nMedical Image Computing and Computer Assisted Intervention (MICCAI). The airway\nstructures were meticulously annotated by three experienced radiologists.\nCompetitors were encouraged to develop automatic airway segmentation models\nwith high robustness and generalization abilities, followed by exploring the\nmost correlated QIB of mortality prediction. A training set of 120\nhigh-resolution computerised tomography (HRCT) scans were publicly released\nwith expert annotations and mortality status. The online validation set\nincorporated 52 HRCT scans from patients with fibrotic lung disease and the\noffline test set included 140 cases from fibrosis and COVID-19 patients. The\nresults have shown that the capacity of extracting airway trees from patients\nwith fibrotic lung disease could be enhanced by introducing voxel-wise weighted\ngeneral union loss and continuity loss. In addition to the competitive image\nbiomarkers for prognosis, a strong airway-derived biomarker (Hazard ratio>1.5,\np<0.0001) was revealed for survival prognostication compared with existing\nclinical measurements, clinician assessment and AI-based biomarkers.\n","authors":["Yang Nan","Xiaodan Xing","Shiyi Wang","Zeyu Tang","Federico N Felder","Sheng Zhang","Roberta Eufrasia Ledda","Xiaoliu Ding","Ruiqi Yu","Weiping Liu","Feng Shi","Tianyang Sun","Zehong Cao","Minghui Zhang","Yun Gu","Hanxiao Zhang","Jian Gao","Pingyu Wang","Wen Tang","Pengxin Yu","Han Kang","Junqiang Chen","Xing Lu","Boyu Zhang","Michail Mamalakis","Francesco Prinzi","Gianluca Carlini","Lisa Cuneo","Abhirup Banerjee","Zhaohu Xing","Lei Zhu","Zacharia Mesbah","Dhruv Jain","Tsiry Mayet","Hongyu Yuan","Qing Lyu","Abdul Qayyum","Moona Mazher","Athol Wells","Simon LF Walsh","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2312.13752v2.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2404.10766v1","updated":"2024-04-16T17:50:09Z","published":"2024-04-16T17:50:09Z","title":"RapidVol: Rapid Reconstruction of 3D Ultrasound Volumes from Sensorless\n  2D Scans","summary":"  Two-dimensional (2D) freehand ultrasonography is one of the most commonly\nused medical imaging modalities, particularly in obstetrics and gynaecology.\nHowever, it only captures 2D cross-sectional views of inherently 3D anatomies,\nlosing valuable contextual information. As an alternative to requiring costly\nand complex 3D ultrasound scanners, 3D volumes can be constructed from 2D scans\nusing machine learning. However this usually requires long computational time.\nHere, we propose RapidVol: a neural representation framework to speed up\nslice-to-volume ultrasound reconstruction. We use tensor-rank decomposition, to\ndecompose the typical 3D volume into sets of tri-planes, and store those\ninstead, as well as a small neural network. A set of 2D ultrasound scans, with\ntheir ground truth (or estimated) 3D position and orientation (pose) is all\nthat is required to form a complete 3D reconstruction. Reconstructions are\nformed from real fetal brain scans, and then evaluated by requesting novel\ncross-sectional views. When compared to prior approaches based on fully\nimplicit representation (e.g. neural radiance fields), our method is over 3x\nquicker, 46% more accurate, and if given inaccurate poses is more robust.\nFurther speed-up is also possible by reconstructing from a structural prior\nrather than from scratch.\n","authors":["Mark C. Eid","Pak-Hei Yeung","Madeleine K. Wyburd","Jo√£o F. Henriques","Ana I. L. Namburete"],"pdf_url":"https://arxiv.org/pdf/2404.10766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10765v1","updated":"2024-04-16T17:50:02Z","published":"2024-04-16T17:50:02Z","title":"RefFusion: Reference Adapted Diffusion Models for 3D Scene Inpainting","summary":"  Neural reconstruction approaches are rapidly emerging as the preferred\nrepresentation for 3D scenes, but their limited editability is still posing a\nchallenge. In this work, we propose an approach for 3D scene inpainting -- the\ntask of coherently replacing parts of the reconstructed scene with desired\ncontent. Scene inpainting is an inherently ill-posed task as there exist many\nsolutions that plausibly replace the missing content. A good inpainting method\nshould therefore not only enable high-quality synthesis but also a high degree\nof control. Based on this observation, we focus on enabling explicit control\nover the inpainted content and leverage a reference image as an efficient means\nto achieve this goal. Specifically, we introduce RefFusion, a novel 3D\ninpainting method based on a multi-scale personalization of an image inpainting\ndiffusion model to the given reference view. The personalization effectively\nadapts the prior distribution to the target scene, resulting in a lower\nvariance of score distillation objective and hence significantly sharper\ndetails. Our framework achieves state-of-the-art results for object removal\nwhile maintaining high controllability. We further demonstrate the generality\nof our formulation on other downstream tasks such as object insertion, scene\noutpainting, and sparse view reconstruction.\n","authors":["Ashkan Mirzaei","Riccardo De Lutio","Seung Wook Kim","David Acuna","Jonathan Kelly","Sanja Fidler","Igor Gilitschenski","Zan Gojcic"],"pdf_url":"https://arxiv.org/pdf/2404.10765v1.pdf","comment":"Project page: https://reffusion.github.io"},{"id":"http://arxiv.org/abs/2404.10763v1","updated":"2024-04-16T17:47:16Z","published":"2024-04-16T17:47:16Z","title":"LaDiC: Are Diffusion Models Really Inferior to Autoregressive\n  Counterparts for Image-to-Text Generation?","summary":"  Diffusion models have exhibited remarkable capabilities in text-to-image\ngeneration. However, their performance in image-to-text generation,\nspecifically image captioning, has lagged behind Auto-Regressive (AR) models,\ncasting doubt on their applicability for such tasks. In this work, we revisit\ndiffusion models, highlighting their capacity for holistic context modeling and\nparallel decoding. With these benefits, diffusion models can alleviate the\ninherent limitations of AR methods, including their slow inference speed, error\npropagation, and unidirectional constraints. Furthermore, we identify the prior\nunderperformance of diffusion models stemming from the absence of an effective\nlatent space for image-text alignment, and the discrepancy between continuous\ndiffusion processes and discrete textual data. In response, we introduce a\nnovel architecture, LaDiC, which utilizes a split BERT to create a dedicated\nlatent space for captions and integrates a regularization module to manage\nvarying text lengths. Our framework also includes a diffuser for semantic\nimage-to-text conversion and a Back&Refine technique to enhance token\ninteractivity during inference. LaDiC achieves state-of-the-art performance for\ndiffusion-based methods on the MS COCO dataset with 38.2 BLEU@4 and 126.2\nCIDEr, demonstrating exceptional performance without pre-training or ancillary\nmodules. This indicates strong competitiveness with AR models, revealing the\npreviously untapped potential of diffusion models in image-to-text generation.\n","authors":["Yuchi Wang","Shuhuai Ren","Rundong Gao","Linli Yao","Qingyan Guo","Kaikai An","Jianhong Bai","Xu Sun"],"pdf_url":"https://arxiv.org/pdf/2404.10763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10760v1","updated":"2024-04-16T17:38:26Z","published":"2024-04-16T17:38:26Z","title":"Learning Feature Inversion for Multi-class Anomaly Detection under\n  General-purpose COCO-AD Benchmark","summary":"  Anomaly detection (AD) is often focused on detecting anomaly areas for\nindustrial quality inspection and medical lesion examination. However, due to\nthe specific scenario targets, the data scale for AD is relatively small, and\nevaluation metrics are still deficient compared to classic vision tasks, such\nas object detection and semantic segmentation. To fill these gaps, this work\nfirst constructs a large-scale and general-purpose COCO-AD dataset by extending\nCOCO to the AD field. This enables fair evaluation and sustainable development\nfor different methods on this challenging benchmark. Moreover, current metrics\nsuch as AU-ROC have nearly reached saturation on simple datasets, which\nprevents a comprehensive evaluation of different methods. Inspired by the\nmetrics in the segmentation field, we further propose several more practical\nthreshold-dependent AD-specific metrics, ie, m$F_1$$^{.2}_{.8}$,\nmAcc$^{.2}_{.8}$, mIoU$^{.2}_{.8}$, and mIoU-max. Motivated by GAN inversion's\nhigh-quality reconstruction capability, we propose a simple but more powerful\nInvAD framework to achieve high-quality feature reconstruction. Our method\nimproves the effectiveness of reconstruction-based methods on popular MVTec AD,\nVisA, and our newly proposed COCO-AD datasets under a multi-class unsupervised\nsetting, where only a single detection model is trained to detect anomalies\nfrom different classes. Extensive ablation experiments have demonstrated the\neffectiveness of each component of our InvAD. Full codes and models are\navailable at https://github.com/zhangzjn/ader.\n","authors":["Jiangning Zhang","Chengjie Wang","Xiangtai Li","Guanzhong Tian","Zhucun Xue","Yong Liu","Guansong Pang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2404.10760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10758v1","updated":"2024-04-16T17:35:35Z","published":"2024-04-16T17:35:35Z","title":"Watch Your Step: Optimal Retrieval for Continual Learning at Scale","summary":"  One of the most widely used approaches in continual learning is referred to\nas replay. Replay methods support interleaved learning by storing past\nexperiences in a replay buffer. Although there are methods for selectively\nconstructing the buffer and reprocessing its contents, there is limited\nexploration of the problem of selectively retrieving samples from the buffer.\nCurrent solutions have been tested in limited settings and, more importantly,\nin isolation. Existing work has also not explored the impact of duplicate\nreplays on performance. In this work, we propose a framework for evaluating\nselective retrieval strategies, categorized by simple, independent class- and\nsample-selective primitives. We evaluated several combinations of existing\nstrategies for selective retrieval and present their performances. Furthermore,\nwe propose a set of strategies to prevent duplicate replays and explore whether\nnew samples with low loss values can be learned without replay. In an effort to\nmatch our problem setting to a realistic continual learning pipeline, we\nrestrict our experiments to a setting involving a large, pre-trained, open\nvocabulary object detection model, which is fully fine-tuned on a sequence of\n15 datasets.\n","authors":["Truman Hickok","Dhireesha Kudithipudi"],"pdf_url":"https://arxiv.org/pdf/2404.10758v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.17484v2","updated":"2024-04-16T16:55:35Z","published":"2024-01-30T22:37:24Z","title":"Pixel to Elevation: Learning to Predict Elevation Maps at Long Range\n  using Images for Autonomous Offroad Navigation","summary":"  Understanding terrain topology at long-range is crucial for the success of\noff-road robotic missions, especially when navigating at high-speeds. LiDAR\nsensors, which are currently heavily relied upon for geometric mapping, provide\nsparse measurements when mapping at greater distances. To address this\nchallenge, we present a novel learning-based approach capable of predicting\nterrain elevation maps at long-range using only onboard egocentric images in\nreal-time. Our proposed method is comprised of three main elements. First, a\ntransformer-based encoder is introduced that learns cross-view associations\nbetween the egocentric views and prior bird-eye-view elevation map predictions.\nSecond, an orientation-aware positional encoding is proposed to incorporate the\n3D vehicle pose information over complex unstructured terrain with multi-view\nvisual image features. Lastly, a history-augmented learn-able map embedding is\nproposed to achieve better temporal consistency between elevation map\npredictions to facilitate the downstream navigational tasks. We experimentally\nvalidate the applicability of our proposed approach for autonomous offroad\nrobotic navigation in complex and unstructured terrain using real-world offroad\ndriving data. Furthermore, the method is qualitatively and quantitatively\ncompared against the current state-of-the-art methods. Extensive field\nexperiments demonstrate that our method surpasses baseline models in accurately\npredicting terrain elevation while effectively capturing the overall terrain\ntopology at long-ranges. Finally, ablation studies are conducted to highlight\nand understand the effect of key components of the proposed approach and\nvalidate their suitability to improve offroad robotic navigation capabilities.\n","authors":["Chanyoung Chung","Georgios Georgakis","Patrick Spieler","Curtis Padgett","Shehryar Khattak"],"pdf_url":"https://arxiv.org/pdf/2401.17484v2.pdf","comment":"8 pages, 6 figures, Accepted in IEEE Robotics and Automation Letters"},{"id":"http://arxiv.org/abs/2301.13656v3","updated":"2024-04-16T16:52:18Z","published":"2023-01-31T14:18:19Z","title":"A Survey and Benchmark of Automatic Surface Reconstruction from Point\n  Clouds","summary":"  We present a comprehensive survey and benchmark of both traditional and\nlearning-based methods for surface reconstruction from point clouds. This task\nis particularly challenging for real-world acquisitions due to factors like\nnoise, outliers, non-uniform sampling, and missing data. Traditional approaches\noften simplify the problem by imposing handcrafted priors on either the input\npoint clouds or the resulting surface, a process that can necessitate tedious\nhyperparameter tuning. Conversely, deep learning models have the capability to\ndirectly learn the properties of input point clouds and desired surfaces from\ndata. We study the influence of these handcrafted and learned priors on the\nprecision and robustness of surface reconstruction techniques. We evaluate\nvarious time-tested and contemporary methods in a standardized manner. When\nboth trained and evaluated on point clouds with identical characteristics, the\nlearning-based models consistently produce superior surfaces compared to their\ntraditional counterparts$\\unicode{x2013}$even in scenarios involving novel\nshape categories. However, traditional methods demonstrate greater resilience\nto the diverse array of point cloud anomalies commonly found in real-world 3D\nacquisitions. For the benefit of the research community, we make our code and\ndatasets available, inviting further enhancements to learning-based surface\nreconstruction. This can be accessed at\nhttps://github.com/raphaelsulzer/dsr-benchmark .\n","authors":["Raphael Sulzer","Renaud Marlet","Bruno Vallet","Loic Landrieu"],"pdf_url":"https://arxiv.org/pdf/2301.13656v3.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2404.10718v1","updated":"2024-04-16T16:51:27Z","published":"2024-04-16T16:51:27Z","title":"GazeHTA: End-to-end Gaze Target Detection with Head-Target Association","summary":"  We propose an end-to-end approach for gaze target detection: predicting a\nhead-target connection between individuals and the target image regions they\nare looking at. Most of the existing methods use independent components such as\noff-the-shelf head detectors or have problems in establishing associations\nbetween heads and gaze targets. In contrast, we investigate an end-to-end\nmulti-person Gaze target detection framework with Heads and Targets Association\n(GazeHTA), which predicts multiple head-target instances based solely on input\nscene image. GazeHTA addresses challenges in gaze target detection by (1)\nleveraging a pre-trained diffusion model to extract scene features for rich\nsemantic understanding, (2) re-injecting a head feature to enhance the head\npriors for improved head understanding, and (3) learning a connection map as\nthe explicit visual associations between heads and gaze targets. Our extensive\nexperimental results demonstrate that GazeHTA outperforms state-of-the-art gaze\ntarget detection methods and two adapted diffusion-based baselines on two\nstandard datasets.\n","authors":["Zhi-Yi Lin","Jouh Yeong Chew","Jan van Gemert","Xucong Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.10718v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10717v1","updated":"2024-04-16T16:51:12Z","published":"2024-04-16T16:51:12Z","title":"Mixed Prototype Consistency Learning for Semi-supervised Medical Image\n  Segmentation","summary":"  Recently, prototype learning has emerged in semi-supervised medical image\nsegmentation and achieved remarkable performance. However, the scarcity of\nlabeled data limits the expressiveness of prototypes in previous methods,\npotentially hindering the complete representation of prototypes for class\nembedding. To address this problem, we propose the Mixed Prototype Consistency\nLearning (MPCL) framework, which includes a Mean Teacher and an auxiliary\nnetwork. The Mean Teacher generates prototypes for labeled and unlabeled data,\nwhile the auxiliary network produces additional prototypes for mixed data\nprocessed by CutMix. Through prototype fusion, mixed prototypes provide extra\nsemantic information to both labeled and unlabeled prototypes. High-quality\nglobal prototypes for each class are formed by fusing two enhanced prototypes,\noptimizing the distribution of hidden embeddings used in consistency learning.\nExtensive experiments on the left atrium and type B aortic dissection datasets\ndemonstrate MPCL's superiority over previous state-of-the-art approaches,\nconfirming the effectiveness of our framework. The code will be released soon.\n","authors":["Lijian Li"],"pdf_url":"https://arxiv.org/pdf/2404.10717v1.pdf","comment":"15 pages, 2 figures"},{"id":"http://arxiv.org/abs/2404.10716v1","updated":"2024-04-16T16:50:35Z","published":"2024-04-16T16:50:35Z","title":"MOWA: Multiple-in-One Image Warping Model","summary":"  While recent image warping approaches achieved remarkable success on existing\nbenchmarks, they still require training separate models for each specific task\nand cannot generalize well to different camera models or customized\nmanipulations. To address diverse types of warping in practice, we propose a\nMultiple-in-One image WArping model (named MOWA) in this work. Specifically, we\nmitigate the difficulty of multi-task learning by disentangling the motion\nestimation at both the region level and pixel level. To further enable dynamic\ntask-aware image warping, we introduce a lightweight point-based classifier\nthat predicts the task type, serving as prompts to modulate the feature maps\nfor better estimation. To our knowledge, this is the first work that solves\nmultiple practical warping tasks in one single model. Extensive experiments\ndemonstrate that our MOWA, which is trained on six tasks for multiple-in-one\nimage warping, outperforms state-of-the-art task-specific models across most\ntasks. Moreover, MOWA also exhibits promising potential to generalize into\nunseen scenes, as evidenced by cross-domain and zero-shot evaluations. The code\nwill be made publicly available.\n","authors":["Kang Liao","Zongsheng Yue","Zhonghua Wu","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2404.10716v1.pdf","comment":"Project page: https://kangliao929.github.io/projects/mowa/"},{"id":"http://arxiv.org/abs/2404.10714v1","updated":"2024-04-16T16:43:36Z","published":"2024-04-16T16:43:36Z","title":"AV-GAN: Attention-Based Varifocal Generative Adversarial Network for\n  Uneven Medical Image Translation","summary":"  Different types of staining highlight different structures in organs, thereby\nassisting in diagnosis. However, due to the impossibility of repeated staining,\nwe cannot obtain different types of stained slides of the same tissue area.\nTranslating the slide that is easy to obtain (e.g., H&E) to slides of staining\ntypes difficult to obtain (e.g., MT, PAS) is a promising way to solve this\nproblem. However, some regions are closely connected to other regions, and to\nmaintain this connection, they often have complex structures and are difficult\nto translate, which may lead to wrong translations. In this paper, we propose\nthe Attention-Based Varifocal Generative Adversarial Network (AV-GAN), which\nsolves multiple problems in pathologic image translation tasks, such as uneven\ntranslation difficulty in different regions, mutual interference of multiple\nresolution information, and nuclear deformation. Specifically, we develop an\nAttention-Based Key Region Selection Module, which can attend to regions with\nhigher translation difficulty. We then develop a Varifocal Module to translate\nthese regions at multiple resolutions. Experimental results show that our\nproposed AV-GAN outperforms existing image translation methods with two virtual\nkidney tissue staining tasks and improves FID values by 15.9 and 4.16\nrespectively in the H&E-MT and H&E-PAS tasks.\n","authors":["Zexin Li","Yiyang Lin","Zijie Fang","Shuyan Li","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2404.10714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10713v1","updated":"2024-04-16T16:43:14Z","published":"2024-04-16T16:43:14Z","title":"A Plausibility Study of Using Augmented Reality in the\n  Ventriculoperitoneal Shunt Operations","summary":"  The field of augmented reality (AR) has undergone substantial growth, finding\ndiverse applications in the medical industry. This paper delves into various\ntechniques employed in medical surgeries, scrutinizing factors such as cost,\nimplementation, and accessibility. The focus of this exploration is on AR-based\nsolutions, with a particular emphasis on addressing challenges and proposing an\ninnovative solution for ventriculoperitoneal shunt (VP) operations. The\nproposed solution introduces a novel flow in the pre-surgery phase, aiming to\nsubstantially reduce setup time and operation duration by creating 3D models of\nthe skull and ventricles. Experiments are conducted where the models are\nvisualized on a 3D- printed skull through an AR device, specifically the\nMicrosoft HoloLens 2. The paper then conducts an in-depth analysis of this\nproposed solution, discussing its feasibility, advantages, limitations,and\nfuture implications.\n","authors":["Tandin Dorji","Pakinee Aimmanee","Vich Yindeedej"],"pdf_url":"https://arxiv.org/pdf/2404.10713v1.pdf","comment":"Accepted for the 2024 - 16th International Conference on Knowledge\n  and Smart Technology (KST). To be published in IEEEXplore Digital Library\n  (#61284), ISBN: 979-8-3503-7073-7"},{"id":"http://arxiv.org/abs/2404.10710v1","updated":"2024-04-16T16:36:50Z","published":"2024-04-16T16:36:50Z","title":"Dual Modalities of Text: Visual and Textual Generative Pre-training","summary":"  Harnessing visual texts represents a burgeoning frontier in the evolution of\nlanguage modeling. In this paper, we introduce a novel pre-training framework\nfor a suite of pixel-based autoregressive language models, pre-training on a\ncorpus of over 400 million documents rendered as RGB images. Our approach is\ncharacterized by a dual-modality training regimen, engaging both visual data\nthrough next patch prediction with a regression head and textual data via next\ntoken prediction with a classification head. This study is particularly focused\non investigating the synergistic interplay between visual and textual\nmodalities of language. Our comprehensive evaluation across a diverse array of\nbenchmarks reveals that the confluence of visual and textual data substantially\naugments the efficacy of pixel-based language models. Notably, our findings\nshow that a unidirectional pixel-based model, devoid of textual data during\ntraining, can match the performance levels of advanced bidirectional\npixel-based models on various language understanding benchmarks. This work\nhighlights the considerable untapped potential of integrating visual and\ntextual information for language modeling purposes. We will release our code,\ndata, and checkpoints to inspire further research advancement.\n","authors":["Yekun Chai","Qingyi Liu","Jingwu Xiao","Shuohuan Wang","Yu Sun","Hua Wu"],"pdf_url":"https://arxiv.org/pdf/2404.10710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.11679v2","updated":"2024-04-16T16:28:42Z","published":"2021-12-22T06:05:02Z","title":"Ghost-dil-NetVLAD: A Lightweight Neural Network for Visual Place\n  Recognition","summary":"  Visual place recognition (VPR) is a challenging task with the unbalance\nbetween enormous computational cost and high recognition performance. Thanks to\nthe practical feature extraction ability of the lightweight convolution neural\nnetworks (CNNs) and the train-ability of the vector of locally aggregated\ndescriptors (VLAD) layer, we propose a lightweight weakly supervised end-to-end\nneural network consisting of a front-ended perception model called GhostCNN and\na learnable VLAD layer as a back-end. GhostCNN is based on Ghost modules that\nare lightweight CNN-based architectures. They can generate redundant feature\nmaps using linear operations instead of the traditional convolution process,\nmaking a good trade-off between computation resources and recognition accuracy.\nTo enhance our proposed lightweight model further, we add dilated convolutions\nto the Ghost module to get features containing more spatial semantic\ninformation, improving accuracy. Finally, rich experiments conducted on a\ncommonly used public benchmark and our private dataset validate that the\nproposed neural network reduces the FLOPs and parameters of VGG16-NetVLAD by\n99.04% and 80.16%, respectively. Besides, both models achieve similar accuracy.\n","authors":["Qingyuan Gong","Yu Liu","Liqiang Zhang","Renhe Liu"],"pdf_url":"https://arxiv.org/pdf/2112.11679v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16278v2","updated":"2024-04-16T16:26:35Z","published":"2023-11-27T19:34:04Z","title":"VehicleGAN: Pair-flexible Pose Guided Image Synthesis for Vehicle\n  Re-identification","summary":"  Vehicle Re-identification (Re-ID) has been broadly studied in the last\ndecade; however, the different camera view angle leading to confused\ndiscrimination in the feature subspace for the vehicles of various poses, is\nstill challenging for the Vehicle Re-ID models in the real world. To promote\nthe Vehicle Re-ID models, this paper proposes to synthesize a large number of\nvehicle images in the target pose, whose idea is to project the vehicles of\ndiverse poses into the unified target pose so as to enhance feature\ndiscrimination. Considering that the paired data of the same vehicles in\ndifferent traffic surveillance cameras might be not available in the real\nworld, we propose the first Pair-flexible Pose Guided Image Synthesis method\nfor Vehicle Re-ID, named as VehicleGAN in this paper, which works for both\nsupervised and unsupervised settings without the knowledge of geometric 3D\nmodels. Because of the feature distribution difference between real and\nsynthetic data, simply training a traditional metric learning based Re-ID model\nwith data-level fusion (i.e., data augmentation) is not satisfactory, therefore\nwe propose a new Joint Metric Learning (JML) via effective feature-level fusion\nfrom both real and synthetic data. Intensive experimental results on the public\nVeRi-776 and VehicleID datasets prove the accuracy and effectiveness of our\nproposed VehicleGAN and JML.\n","authors":["Baolu Li","Ping Liu","Lan Fu","Jinlong Li","Jianwu Fang","Zhigang Xu","Hongkai Yu"],"pdf_url":"https://arxiv.org/pdf/2311.16278v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10700v1","updated":"2024-04-16T16:17:48Z","published":"2024-04-16T16:17:48Z","title":"Rawformer: Unpaired Raw-to-Raw Translation for Learnable Camera ISPs","summary":"  Modern smartphone camera quality heavily relies on the image signal processor\n(ISP) to enhance captured raw images, utilizing carefully designed modules to\nproduce final output images encoded in a standard color space (e.g., sRGB).\nNeural-based end-to-end learnable ISPs offer promising advancements,\npotentially replacing traditional ISPs with their ability to adapt without\nrequiring extensive tuning for each new camera model, as is often the case for\nnearly every module in traditional ISPs. However, the key challenge with the\nrecent learning-based ISPs is the urge to collect large paired datasets for\neach distinct camera model due to the influence of intrinsic camera\ncharacteristics on the formation of input raw images. This paper tackles this\nchallenge by introducing a novel method for unpaired learning of raw-to-raw\ntranslation across diverse cameras. Specifically, we propose Rawformer, an\nunsupervised Transformer-based encoder-decoder method for raw-to-raw\ntranslation. It accurately maps raw images captured by a certain camera to the\ntarget camera, facilitating the generalization of learnable ISPs to new unseen\ncameras. Our method demonstrates superior performance on real camera datasets,\nachieving higher accuracy compared to previous state-of-the-art techniques, and\npreserving a more robust correlation between the original and translated raw\nimages.\n","authors":["Georgy Perevozchikov","Nancy Mehta","Mahmoud Afifi","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2404.10700v1.pdf","comment":"15 pages, 5 figures"},{"id":"http://arxiv.org/abs/2404.10699v1","updated":"2024-04-16T16:16:40Z","published":"2024-04-16T16:16:40Z","title":"ECLAIR: A High-Fidelity Aerial LiDAR Dataset for Semantic Segmentation","summary":"  We introduce ECLAIR (Extended Classification of Lidar for AI Recognition), a\nnew outdoor large-scale aerial LiDAR dataset designed specifically for\nadvancing research in point cloud semantic segmentation. As the most extensive\nand diverse collection of its kind to date, the dataset covers a total area of\n10$km^2$ with close to 600 million points and features eleven distinct object\ncategories. To guarantee the dataset's quality and utility, we have thoroughly\ncurated the point labels through an internal team of experts, ensuring accuracy\nand consistency in semantic labeling. The dataset is engineered to move forward\nthe fields of 3D urban modeling, scene understanding, and utility\ninfrastructure management by presenting new challenges and potential\napplications. As a benchmark, we report qualitative and quantitative analysis\nof a voxel-based point cloud segmentation approach based on the Minkowski\nEngine.\n","authors":["Iaroslav Melekhov","Anand Umashankar","Hyeong-Jin Kim","Vladislav Serkov","Dusty Argyle"],"pdf_url":"https://arxiv.org/pdf/2404.10699v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2404.10690v1","updated":"2024-04-16T16:10:23Z","published":"2024-04-16T16:10:23Z","title":"MathWriting: A Dataset For Handwritten Mathematical Expression\n  Recognition","summary":"  We introduce MathWriting, the largest online handwritten mathematical\nexpression dataset to date. It consists of 230k human-written samples and an\nadditional 400k synthetic ones. MathWriting can also be used for offline HME\nrecognition and is larger than all existing offline HME datasets like\nIM2LATEX-100K. We introduce a benchmark based on MathWriting data in order to\nadvance research on both online and offline HME recognition.\n","authors":["Philippe Gervais","Asya Fadeeva","Andrii Maksai"],"pdf_url":"https://arxiv.org/pdf/2404.10690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10688v1","updated":"2024-04-16T16:08:59Z","published":"2024-04-16T16:08:59Z","title":"Efficient Conditional Diffusion Model with Probability Flow Sampling for\n  Image Super-resolution","summary":"  Image super-resolution is a fundamentally ill-posed problem because multiple\nvalid high-resolution images exist for one low-resolution image.\nSuper-resolution methods based on diffusion probabilistic models can deal with\nthe ill-posed nature by learning the distribution of high-resolution images\nconditioned on low-resolution images, avoiding the problem of blurry images in\nPSNR-oriented methods. However, existing diffusion-based super-resolution\nmethods have high time consumption with the use of iterative sampling, while\nthe quality and consistency of generated images are less than ideal due to\nproblems like color shifting. In this paper, we propose Efficient Conditional\nDiffusion Model with Probability Flow Sampling (ECDP) for image\nsuper-resolution. To reduce the time consumption, we design a continuous-time\nconditional diffusion model for image super-resolution, which enables the use\nof probability flow sampling for efficient generation. Additionally, to improve\nthe consistency of generated images, we propose a hybrid parametrization for\nthe denoiser network, which interpolates between the data-predicting\nparametrization and the noise-predicting parametrization for different noise\nscales. Moreover, we design an image quality loss as a complement to the score\nmatching loss of diffusion models, further improving the consistency and\nquality of super-resolution. Extensive experiments on DIV2K, ImageNet, and\nCelebA demonstrate that our method achieves higher super-resolution quality\nthan existing diffusion-based image super-resolution methods while having lower\ntime consumption. Our code is available at https://github.com/Yuan-Yutao/ECDP.\n","authors":["Yutao Yuan","Chun Yuan"],"pdf_url":"https://arxiv.org/pdf/2404.10688v1.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2404.10685v1","updated":"2024-04-16T16:04:38Z","published":"2024-04-16T16:04:38Z","title":"Generating Human Interaction Motions in Scenes with Text Control","summary":"  We present TeSMo, a method for text-controlled scene-aware motion generation\nbased on denoising diffusion models. Previous text-to-motion methods focus on\ncharacters in isolation without considering scenes due to the limited\navailability of datasets that include motion, text descriptions, and\ninteractive scenes. Our approach begins with pre-training a scene-agnostic\ntext-to-motion diffusion model, emphasizing goal-reaching constraints on\nlarge-scale motion-capture datasets. We then enhance this model with a\nscene-aware component, fine-tuned using data augmented with detailed scene\ninformation, including ground plane and object shapes. To facilitate training,\nwe embed annotated navigation and interaction motions within scenes. The\nproposed method produces realistic and diverse human-object interactions, such\nas navigation and sitting, in different scenes with various object shapes,\norientations, initial body positions, and poses. Extensive experiments\ndemonstrate that our approach surpasses prior techniques in terms of the\nplausibility of human-scene interactions, as well as the realism and variety of\nthe generated motions. Code will be released upon publication of this work at\nhttps://research.nvidia.com/labs/toronto-ai/tesmo.\n","authors":["Hongwei Yi","Justus Thies","Michael J. Black","Xue Bin Peng","Davis Rempe"],"pdf_url":"https://arxiv.org/pdf/2404.10685v1.pdf","comment":"Project Page: https://research.nvidia.com/labs/toronto-ai/tesmo/"},{"id":"http://arxiv.org/abs/2404.10681v1","updated":"2024-04-16T15:58:49Z","published":"2024-04-16T15:58:49Z","title":"StyleCity: Large-Scale 3D Urban Scenes Stylization with Vision-and-Text\n  Reference via Progressive Optimization","summary":"  Creating large-scale virtual urban scenes with variant styles is inherently\nchallenging. To facilitate prototypes of virtual production and bypass the need\nfor complex materials and lighting setups, we introduce the first\nvision-and-text-driven texture stylization system for large-scale urban scenes,\nStyleCity. Taking an image and text as references, StyleCity stylizes a 3D\ntextured mesh of a large-scale urban scene in a semantics-aware fashion and\ngenerates a harmonic omnidirectional sky background. To achieve that, we\npropose to stylize a neural texture field by transferring 2D vision-and-text\npriors to 3D globally and locally. During 3D stylization, we progressively\nscale the planned training views of the input 3D scene at different levels in\norder to preserve high-quality scene content. We then optimize the scene style\nglobally by adapting the scale of the style image with the scale of the\ntraining views. Moreover, we enhance local semantics consistency by the\nsemantics-aware style loss which is crucial for photo-realistic stylization.\nBesides texture stylization, we further adopt a generative diffusion model to\nsynthesize a style-consistent omnidirectional sky image, which offers a more\nimmersive atmosphere and assists the semantic stylization process. The stylized\nneural texture field can be baked into an arbitrary-resolution texture,\nenabling seamless integration into conventional rendering pipelines and\nsignificantly easing the virtual production prototyping process. Extensive\nexperiments demonstrate our stylized scenes' superiority in qualitative and\nquantitative performance and user preferences.\n","authors":["Yingshu Chen","Huajian Huang","Tuan-Anh Vu","Ka Chun Shum","Sai-Kit Yeung"],"pdf_url":"https://arxiv.org/pdf/2404.10681v1.pdf","comment":"project page: https://chenyingshu.github.io/stylecity3d/"},{"id":"http://arxiv.org/abs/2312.02126v3","updated":"2024-04-16T15:50:56Z","published":"2023-12-04T18:53:24Z","title":"SplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM","summary":"  Dense simultaneous localization and mapping (SLAM) is crucial for robotics\nand augmented reality applications. However, current methods are often hampered\nby the non-volumetric or implicit way they represent a scene. This work\nintroduces SplaTAM, an approach that, for the first time, leverages explicit\nvolumetric representations, i.e., 3D Gaussians, to enable high-fidelity\nreconstruction from a single unposed RGB-D camera, surpassing the capabilities\nof existing methods. SplaTAM employs a simple online tracking and mapping\nsystem tailored to the underlying Gaussian representation. It utilizes a\nsilhouette mask to elegantly capture the presence of scene density. This\ncombination enables several benefits over prior representations, including fast\nrendering and dense optimization, quickly determining if areas have been\npreviously mapped, and structured map expansion by adding more Gaussians.\nExtensive experiments show that SplaTAM achieves up to 2x superior performance\nin camera pose estimation, map construction, and novel-view synthesis over\nexisting methods, paving the way for more immersive high-fidelity SLAM\napplications.\n","authors":["Nikhil Keetha","Jay Karhade","Krishna Murthy Jatavallabhula","Gengshan Yang","Sebastian Scherer","Deva Ramanan","Jonathon Luiten"],"pdf_url":"https://arxiv.org/pdf/2312.02126v3.pdf","comment":"CVPR 2024. Website: https://spla-tam.github.io/"},{"id":"http://arxiv.org/abs/2404.10667v1","updated":"2024-04-16T15:43:22Z","published":"2024-04-16T15:43:22Z","title":"VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time","summary":"  We introduce VASA, a framework for generating lifelike talking faces with\nappealing visual affective skills (VAS) given a single static image and a\nspeech audio clip. Our premiere model, VASA-1, is capable of not only producing\nlip movements that are exquisitely synchronized with the audio, but also\ncapturing a large spectrum of facial nuances and natural head motions that\ncontribute to the perception of authenticity and liveliness. The core\ninnovations include a holistic facial dynamics and head movement generation\nmodel that works in a face latent space, and the development of such an\nexpressive and disentangled face latent space using videos. Through extensive\nexperiments including evaluation on a set of new metrics, we show that our\nmethod significantly outperforms previous methods along various dimensions\ncomprehensively. Our method not only delivers high video quality with realistic\nfacial and head dynamics but also supports the online generation of 512x512\nvideos at up to 40 FPS with negligible starting latency. It paves the way for\nreal-time engagements with lifelike avatars that emulate human conversational\nbehaviors.\n","authors":["Sicheng Xu","Guojun Chen","Yu-Xiao Guo","Jiaolong Yang","Chong Li","Zhenyu Zang","Yizhong Zhang","Xin Tong","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2404.10667v1.pdf","comment":"Tech Report. Project webpage:\n  https://www.microsoft.com/en-us/research/project/vasa-1/"},{"id":"http://arxiv.org/abs/2404.10664v1","updated":"2024-04-16T15:40:18Z","published":"2024-04-16T15:40:18Z","title":"Assessing The Impact of CNN Auto Encoder-Based Image Denoising on Image\n  Classification Tasks","summary":"  Images captured from the real world are often affected by different types of\nnoise, which can significantly impact the performance of Computer Vision\nsystems and the quality of visual data. This study presents a novel approach\nfor defect detection in casting product noisy images, specifically focusing on\nsubmersible pump impellers. The methodology involves utilizing deep learning\nmodels such as VGG16, InceptionV3, and other models in both the spatial and\nfrequency domains to identify noise types and defect status. The research\nprocess begins with preprocessing images, followed by applying denoising\ntechniques tailored to specific noise categories. The goal is to enhance the\naccuracy and robustness of defect detection by integrating noise detection and\ndenoising into the classification pipeline. The study achieved remarkable\nresults using VGG16 for noise type classification in the frequency domain,\nachieving an accuracy of over 99%. Removal of salt and pepper noise resulted in\nan average SSIM of 87.9, while Gaussian noise removal had an average SSIM of\n64.0, and periodic noise removal yielded an average SSIM of 81.6. This\ncomprehensive approach showcases the effectiveness of the deep AutoEncoder\nmodel and median filter, for denoising strategies in real-world industrial\napplications. Finally, our study reports significant improvements in binary\nclassification accuracy for defect detection compared to previous methods. For\nthe VGG16 classifier, accuracy increased from 94.6% to 97.0%, demonstrating the\neffectiveness of the proposed noise detection and denoising approach.\nSimilarly, for the InceptionV3 classifier, accuracy improved from 84.7% to\n90.0%, further validating the benefits of integrating noise analysis into the\nclassification pipeline.\n","authors":["Mohsen Hami","Mahdi JameBozorg"],"pdf_url":"https://arxiv.org/pdf/2404.10664v1.pdf","comment":"13 pages, 13 figures, 13th International conference on innovative\n  technologies in the field of science, engineering and technology"},{"id":"http://arxiv.org/abs/2404.07922v3","updated":"2024-04-16T15:33:45Z","published":"2024-04-11T17:09:28Z","title":"LaVy: Vietnamese Multimodal Large Language Model","summary":"  Large Language Models (LLMs) and Multimodal Large language models (MLLMs)\nhave taken the world by storm with impressive abilities in complex reasoning\nand linguistic comprehension. Meanwhile there are plethora of works related to\nVietnamese Large Language Models, the lack of high-quality resources in\nmultimodality limits the progress of Vietnamese MLLMs. In this paper, we\npioneer in address this by introducing LaVy, a state-of-the-art Vietnamese\nMLLM, and we also introduce LaVy-Bench benchmark designated for evaluating\nMLLMs's understanding on Vietnamese visual language tasks. Our project is\npublic at https://github.com/baochi0212/LaVy\n","authors":["Chi Tran","Huong Le Thanh"],"pdf_url":"https://arxiv.org/pdf/2404.07922v3.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2404.10633v1","updated":"2024-04-16T15:04:55Z","published":"2024-04-16T15:04:55Z","title":"Contextrast: Contextual Contrastive Learning for Semantic Segmentation","summary":"  Despite great improvements in semantic segmentation, challenges persist\nbecause of the lack of local/global contexts and the relationship between them.\nIn this paper, we propose Contextrast, a contrastive learning-based semantic\nsegmentation method that allows to capture local/global contexts and comprehend\ntheir relationships. Our proposed method comprises two parts: a) contextual\ncontrastive learning (CCL) and b) boundary-aware negative (BANE) sampling.\nContextual contrastive learning obtains local/global context from multi-scale\nfeature aggregation and inter/intra-relationship of features for better\ndiscrimination capabilities. Meanwhile, BANE sampling selects embedding\nfeatures along the boundaries of incorrectly predicted regions to employ them\nas harder negative samples on our contrastive learning, resolving segmentation\nissues along the boundary region by exploiting fine-grained details. We\ndemonstrate that our Contextrast substantially enhances the performance of\nsemantic segmentation networks, outperforming state-of-the-art contrastive\nlearning approaches on diverse public datasets, e.g. Cityscapes, CamVid,\nPASCAL-C, COCO-Stuff, and ADE20K, without an increase in computational cost\nduring inference.\n","authors":["Changki Sung","Wanhee Kim","Jungho An","Wooju Lee","Hyungtae Lim","Hyun Myung"],"pdf_url":"https://arxiv.org/pdf/2404.10633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09172v2","updated":"2024-04-16T14:56:32Z","published":"2024-04-14T07:36:18Z","title":"LoopAnimate: Loopable Salient Object Animation","summary":"  Research on diffusion model-based video generation has advanced rapidly.\nHowever, limitations in object fidelity and generation length hinder its\npractical applications. Additionally, specific domains like animated wallpapers\nrequire seamless looping, where the first and last frames of the video match\nseamlessly. To address these challenges, this paper proposes LoopAnimate, a\nnovel method for generating videos with consistent start and end frames. To\nenhance object fidelity, we introduce a framework that decouples multi-level\nimage appearance and textual semantic information. Building upon an\nimage-to-image diffusion model, our approach incorporates both pixel-level and\nfeature-level information from the input image, injecting image appearance and\ntextual semantic embeddings at different positions of the diffusion model.\nExisting UNet-based video generation models require to input the entire videos\nduring training to encode temporal and positional information at once. However,\ndue to limitations in GPU memory, the number of frames is typically restricted\nto 16. To address this, this paper proposes a three-stage training strategy\nwith progressively increasing frame numbers and reducing fine-tuning modules.\nAdditionally, we introduce the Temporal E nhanced Motion Module(TEMM) to extend\nthe capacity for encoding temporal and positional information up to 36 frames.\nThe proposed LoopAnimate, which for the first time extends the single-pass\ngeneration length of UNet-based video generation models to 35 frames while\nmaintaining high-quality video generation. Experiments demonstrate that\nLoopAnimate achieves state-of-the-art performance in both objective metrics,\nsuch as fidelity and temporal consistency, and subjective evaluation results.\n","authors":["Fanyi Wang","Peng Liu","Haotian Hu","Dan Meng","Jingwen Su","Jinjin Xu","Yanhao Zhang","Xiaoming Ren","Zhiwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.09172v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10626v1","updated":"2024-04-16T14:52:15Z","published":"2024-04-16T14:52:15Z","title":"Exploring selective image matching methods for zero-shot and few-sample\n  unsupervised domain adaptation of urban canopy prediction","summary":"  We explore simple methods for adapting a trained multi-task UNet which\npredicts canopy cover and height to a new geographic setting using remotely\nsensed data without the need of training a domain-adaptive classifier and\nextensive fine-tuning. Extending previous research, we followed a selective\nalignment process to identify similar images in the two geographical domains\nand then tested an array of data-based unsupervised domain adaptation\napproaches in a zero-shot setting as well as with a small amount of\nfine-tuning. We find that the selective aligned data-based image matching\nmethods produce promising results in a zero-shot setting, and even more so with\na small amount of fine-tuning. These methods outperform both an untransformed\nbaseline and a popular data-based image-to-image translation model. The best\nperforming methods were pixel distribution adaptation and fourier domain\nadaptation on the canopy cover and height tasks respectively.\n","authors":["John Francis","Stephen Law"],"pdf_url":"https://arxiv.org/pdf/2404.10626v1.pdf","comment":"ICLR 2024 Machine Learning for Remote Sensing (ML4RS) Workshop"},{"id":"http://arxiv.org/abs/2404.10625v1","updated":"2024-04-16T14:48:40Z","published":"2024-04-16T14:48:40Z","title":"Gaussian Splatting Decoder for 3D-aware Generative Adversarial Networks","summary":"  NeRF-based 3D-aware Generative Adversarial Networks (GANs) like EG3D or\nGIRAFFE have shown very high rendering quality under large representational\nvariety. However, rendering with Neural Radiance Fields poses challenges for 3D\napplications: First, the significant computational demands of NeRF rendering\npreclude its use on low-power devices, such as mobiles and VR/AR headsets.\nSecond, implicit representations based on neural networks are difficult to\nincorporate into explicit 3D scenes, such as VR environments or video games. 3D\nGaussian Splatting (3DGS) overcomes these limitations by providing an explicit\n3D representation that can be rendered efficiently at high frame rates. In this\nwork, we present a novel approach that combines the high rendering quality of\nNeRF-based 3D-aware GANs with the flexibility and computational advantages of\n3DGS. By training a decoder that maps implicit NeRF representations to explicit\n3D Gaussian Splatting attributes, we can integrate the representational\ndiversity and quality of 3D GANs into the ecosystem of 3D Gaussian Splatting\nfor the first time. Additionally, our approach allows for a high resolution GAN\ninversion and real-time GAN editing with 3D Gaussian Splatting scenes.\n","authors":["Florian Barthel","Arian Beckmann","Wieland Morgenstern","Anna Hilsmann","Peter Eisert"],"pdf_url":"https://arxiv.org/pdf/2404.10625v1.pdf","comment":"CVPRW"},{"id":"http://arxiv.org/abs/2403.08801v5","updated":"2024-04-16T14:48:34Z","published":"2024-02-05T12:33:37Z","title":"CoBra: Complementary Branch Fusing Class and Semantic Knowledge for\n  Robust Weakly Supervised Semantic Segmentation","summary":"  Leveraging semantically precise pseudo masks derived from image-level class\nknowledge for segmentation, namely image-level Weakly Supervised Semantic\nSegmentation (WSSS), still remains challenging. While Class Activation Maps\n(CAMs) using CNNs have steadily been contributing to the success of WSSS, the\nresulting activation maps often narrowly focus on class-specific parts (e.g.,\nonly face of human). On the other hand, recent works based on vision\ntransformers (ViT) have shown promising results based on their self-attention\nmechanism to capture the semantic parts but fail in capturing complete\nclass-specific details (e.g., entire body parts of human but also with a dog\nnearby). In this work, we propose Complementary Branch (CoBra), a novel dual\nbranch framework consisting of two distinct architectures which provide\nvaluable complementary knowledge of class (from CNN) and semantic (from ViT) to\neach branch. In particular, we learn Class-Aware Projection (CAP) for the CNN\nbranch and Semantic-Aware Projection (SAP) for the ViT branch to explicitly\nfuse their complementary knowledge and facilitate a new type of extra\npatch-level supervision. Our model, through CoBra, fuses CNN and ViT's\ncomplementary outputs to create robust pseudo masks that integrate both class\nand semantic information effectively. Extensive experiments qualitatively and\nquantitatively investigate how CNN and ViT complement each other on the PASCAL\nVOC 2012 dataset, showing a state-of-the-art WSSS result. This includes not\nonly the masks generated by our model, but also the segmentation results\nderived from utilizing these masks as pseudo labels.\n","authors":["Woojung Han","Seil Kang","Kyobin Choo","Seong Jae Hwang"],"pdf_url":"https://arxiv.org/pdf/2403.08801v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14045v2","updated":"2024-04-16T14:45:44Z","published":"2024-02-21T13:06:48Z","title":"A Systematic Review of Low-Rank and Local Low-Rank Matrix Approximation\n  in Big Data Medical Imaging","summary":"  The large volume and complexity of medical imaging datasets are bottlenecks\nfor storage, transmission, and processing. To tackle these challenges, the\napplication of low-rank matrix approximation (LRMA) and its derivative, local\nLRMA (LLRMA) has demonstrated potential.\n  A detailed analysis of the literature identifies LRMA and LLRMA methods\napplied to various imaging modalities, and the challenges and limitations\nassociated with existing LRMA and LLRMA methods are addressed.\n  We note a significant shift towards a preference for LLRMA in the medical\nimaging field since 2015, demonstrating its potential and effectiveness in\ncapturing complex structures in medical data compared to LRMA. Acknowledging\nthe limitations of shallow similarity methods used with LLRMA, we suggest\nadvanced semantic image segmentation for similarity measure, explaining in\ndetail how it can measure similar patches and their feasibility.\n  We note that LRMA and LLRMA are mainly applied to unstructured medical data,\nand we propose extending their application to different medical data types,\nincluding structured and semi-structured. This paper also discusses how LRMA\nand LLRMA can be applied to regular data with missing entries and the impact of\ninaccuracies in predicting missing values and their effects. We discuss the\nimpact of patch size and propose the use of random search (RS) to determine the\noptimal patch size. To enhance feasibility, a hybrid approach using Bayesian\noptimization and RS is proposed, which could improve the application of LRMA\nand LLRMA in medical imaging.\n","authors":["Sisipho Hamlomo","Marcellin Atemkeng","Yusuf Brima","Chuneeta Nunhokee","Jeremy Baxter"],"pdf_url":"https://arxiv.org/pdf/2402.14045v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10620v1","updated":"2024-04-16T14:43:33Z","published":"2024-04-16T14:43:33Z","title":"PyTorchGeoNodes: Enabling Differentiable Shape Programs for 3D Shape\n  Reconstruction","summary":"  We propose PyTorchGeoNodes, a differentiable module for reconstructing 3D\nobjects from images using interpretable shape programs. In comparison to\ntraditional CAD model retrieval methods, the use of shape programs for 3D\nreconstruction allows for reasoning about the semantic properties of\nreconstructed objects, editing, low memory footprint, etc. However, the\nutilization of shape programs for 3D scene understanding has been largely\nneglected in past works. As our main contribution, we enable gradient-based\noptimization by introducing a module that translates shape programs designed in\nBlender, for example, into efficient PyTorch code. We also provide a method\nthat relies on PyTorchGeoNodes and is inspired by Monte Carlo Tree Search\n(MCTS) to jointly optimize discrete and continuous parameters of shape programs\nand reconstruct 3D objects for input scenes. In our experiments, we apply our\nalgorithm to reconstruct 3D objects in the ScanNet dataset and evaluate our\nresults against CAD model retrieval-based reconstructions. Our experiments\nindicate that our reconstructions match well the input scenes while enabling\nsemantic reasoning about reconstructed objects.\n","authors":["Sinisa Stekovic","Stefan Ainetter","Mattia D'Urso","Friedrich Fraundorfer","Vincent Lepetit"],"pdf_url":"https://arxiv.org/pdf/2404.10620v1.pdf","comment":"In Submission"},{"id":"http://arxiv.org/abs/2404.10618v1","updated":"2024-04-16T14:42:49Z","published":"2024-04-16T14:42:49Z","title":"Private Attribute Inference from Images with Vision-Language Models","summary":"  As large language models (LLMs) become ubiquitous in our daily tasks and\ndigital interactions, associated privacy risks are increasingly in focus. While\nLLM privacy research has primarily focused on the leakage of model training\ndata, it has recently been shown that the increase in models' capabilities has\nenabled LLMs to make accurate privacy-infringing inferences from previously\nunseen texts. With the rise of multimodal vision-language models (VLMs),\ncapable of understanding both images and text, a pertinent question is whether\nsuch results transfer to the previously unexplored domain of benign images\nposted online. To investigate the risks associated with the image reasoning\ncapabilities of newly emerging VLMs, we compile an image dataset with\nhuman-annotated labels of the image owner's personal attributes. In order to\nunderstand the additional privacy risk posed by VLMs beyond traditional human\nattribute recognition, our dataset consists of images where the inferable\nprivate attributes do not stem from direct depictions of humans. On this\ndataset, we evaluate the inferential capabilities of 7 state-of-the-art VLMs,\nfinding that they can infer various personal attributes at up to 77.6%\naccuracy. Concerningly, we observe that accuracy scales with the general\ncapabilities of the models, implying that future models can be misused as\nstronger adversaries, establishing an imperative for the development of\nadequate defenses.\n","authors":["Batuhan T√∂mek√ße","Mark Vero","Robin Staab","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2404.10618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10121v3","updated":"2024-04-16T14:35:13Z","published":"2023-11-16T10:45:46Z","title":"Slide-SAM: Medical SAM Meets Sliding Window","summary":"  The Segment Anything Model (SAM) has achieved a notable success in\ntwo-dimensional image segmentation in natural images. However, the substantial\ngap between medical and natural images hinders its direct application to\nmedical image segmentation tasks. Particularly in 3D medical images, SAM\nstruggles to learn contextual relationships between slices, limiting its\npractical applicability. Moreover, applying 2D SAM to 3D images requires\nprompting the entire volume, which is time- and label-consuming. To address\nthese problems, we propose Slide-SAM, which treats a stack of three adjacent\nslices as a prediction window. It firstly takes three slices from a 3D volume\nand point- or bounding box prompts on the central slice as inputs to predict\nsegmentation masks for all three slices. Subsequently, the masks of the top and\nbottom slices are then used to generate new prompts for adjacent slices.\nFinally, step-wise prediction can be achieved by sliding the prediction window\nforward or backward through the entire volume. Our model is trained on multiple\npublic and private medical datasets and demonstrates its effectiveness through\nextensive 3D segmetnation experiments, with the help of minimal prompts. Code\nis available at \\url{https://github.com/Curli-quan/Slide-SAM}.\n","authors":["Quan Quan","Fenghe Tang","Zikang Xu","Heqin Zhu","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2311.10121v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10603v1","updated":"2024-04-16T14:28:57Z","published":"2024-04-16T14:28:57Z","title":"Enhancing 3D Fidelity of Text-to-3D using Cross-View Correspondences","summary":"  Leveraging multi-view diffusion models as priors for 3D optimization have\nalleviated the problem of 3D consistency, e.g., the Janus face problem or the\ncontent drift problem, in zero-shot text-to-3D models. However, the 3D\ngeometric fidelity of the output remains an unresolved issue; albeit the\nrendered 2D views are realistic, the underlying geometry may contain errors\nsuch as unreasonable concavities. In this work, we propose CorrespondentDream,\nan effective method to leverage annotation-free, cross-view correspondences\nyielded from the diffusion U-Net to provide additional 3D prior to the NeRF\noptimization process. We find that these correspondences are strongly\nconsistent with human perception, and by adopting it in our loss design, we are\nable to produce NeRF models with geometries that are more coherent with common\nsense, e.g., more smoothed object surface, yielding higher 3D fidelity. We\ndemonstrate the efficacy of our approach through various comparative\nqualitative results and a solid user study.\n","authors":["Seungwook Kim","Kejie Li","Xueqing Deng","Yichun Shi","Minsu Cho","Peng Wang"],"pdf_url":"https://arxiv.org/pdf/2404.10603v1.pdf","comment":"25 pages, 22 figures, accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2404.10600v1","updated":"2024-04-16T14:26:55Z","published":"2024-04-16T14:26:55Z","title":"Intra-operative tumour margin evaluation in breast-conserving surgery\n  with deep learning","summary":"  A positive margin may result in an increased risk of local recurrences after\nbreast retention surgery for any malignant tumour. In order to reduce the\nnumber of positive margins would offer surgeon real-time intra-operative\ninformation on the presence of positive resection margins. This study aims to\ndesign an intra-operative tumour margin evaluation scheme by using specimen\nmammography in breast-conserving surgery. Total of 30 cases were evaluated and\ncompared with the manually determined contours by experienced physicians and\npathology report. The proposed method utilizes image thresholding to extract\nregions of interest and then performs a deep learning model, i.e. SegNet, to\nsegment tumour tissue. The margin width of normal tissues surrounding it is\nevaluated as the result. The desired size of margin around the tumor was set\nfor 10 mm. The smallest average difference to manual sketched margin (6.53 mm\n+- 5.84). In the all case, the SegNet architecture was utilized to obtain\ntissue specimen boundary and tumor contour, respectively. The simulation\nresults indicated that this technology is helpful in discriminating positive\nfrom negative margins in the intra-operative setting. The aim of proposed\nscheme was a potential procedure in the intra-operative measurement system. The\nexperimental results reveal that deep learning techniques can draw results that\nare consistent with pathology reports.\n","authors":["Wei-Chung Shia","Yu-Len Huang","Yi-Chun Chen","Hwa-Koon Wu","Dar-Ren Chen"],"pdf_url":"https://arxiv.org/pdf/2404.10600v1.pdf","comment":"1 pages, 6 figures and 2 tables"},{"id":"http://arxiv.org/abs/2404.10595v1","updated":"2024-04-16T14:20:55Z","published":"2024-04-16T14:20:55Z","title":"Automated Evaluation of Large Vision-Language Models on Self-driving\n  Corner Cases","summary":"  Large Vision-Language Models (LVLMs), due to the remarkable visual reasoning\nability to understand images and videos, have received widespread attention in\nthe autonomous driving domain, which significantly advances the development of\ninterpretable end-to-end autonomous driving. However, current evaluations of\nLVLMs primarily focus on the multi-faceted capabilities in common scenarios,\nlacking quantifiable and automated assessment in autonomous driving contexts,\nlet alone severe road corner cases that even the state-of-the-art autonomous\ndriving perception systems struggle to handle. In this paper, we propose\nCODA-LM, a novel vision-language benchmark for self-driving, which provides the\nfirst automatic and quantitative evaluation of LVLMs for interpretable\nautonomous driving including general perception, regional perception, and\ndriving suggestions. CODA-LM utilizes the texts to describe the road images,\nexploiting powerful text-only large language models (LLMs) without image inputs\nto assess the capabilities of LVLMs in autonomous driving scenarios, which\nreveals stronger alignment with human preferences than LVLM judges. Experiments\ndemonstrate that even the closed-sourced commercial LVLMs like GPT-4V cannot\ndeal with road corner cases well, suggesting that we are still far from a\nstrong LVLM-powered intelligent driving agent, and we hope our CODA-LM can\nbecome the catalyst to promote future development.\n","authors":["Yanze Li","Wenhua Zhang","Kai Chen","Yanxin Liu","Pengxiang Li","Ruiyuan Gao","Lanqing Hong","Meng Tian","Xinhai Zhao","Zhenguo Li","Dit-Yan Yeung","Huchuan Lu","Xu Jia"],"pdf_url":"https://arxiv.org/pdf/2404.10595v1.pdf","comment":"Project Page: https://coda-dataset.github.io/coda-lm/"},{"id":"http://arxiv.org/abs/2404.08814v2","updated":"2024-04-16T14:17:51Z","published":"2024-04-12T21:14:20Z","title":"E3: Ensemble of Expert Embedders for Adapting Synthetic Image Detectors\n  to New Generators Using Limited Data","summary":"  As generative AI progresses rapidly, new synthetic image generators continue\nto emerge at a swift pace. Traditional detection methods face two main\nchallenges in adapting to these generators: the forensic traces of synthetic\nimages from new techniques can vastly differ from those learned during\ntraining, and access to data for these new generators is often limited. To\naddress these issues, we introduce the Ensemble of Expert Embedders (E3), a\nnovel continual learning framework for updating synthetic image detectors. E3\nenables the accurate detection of images from newly emerged generators using\nminimal training data. Our approach does this by first employing transfer\nlearning to develop a suite of expert embedders, each specializing in the\nforensic traces of a specific generator. Then, all embeddings are jointly\nanalyzed by an Expert Knowledge Fusion Network to produce accurate and reliable\ndetection decisions. Our experiments demonstrate that E3 outperforms existing\ncontinual learning methods, including those developed specifically for\nsynthetic image detection.\n","authors":["Aref Azizpour","Tai D. Nguyen","Manil Shrestha","Kaidi Xu","Edward Kim","Matthew C. Stamm"],"pdf_url":"https://arxiv.org/pdf/2404.08814v2.pdf","comment":"11 pages, 4 figures, To be published in CVPRWMF24"},{"id":"http://arxiv.org/abs/2403.14421v2","updated":"2024-04-16T14:16:48Z","published":"2024-03-21T14:17:28Z","title":"DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning","summary":"  Text-to-image diffusion models have been shown to suffer from sample-level\nmemorization, possibly reproducing near-perfect replica of images that they are\ntrained on, which may be undesirable. To remedy this issue, we develop the\nfirst differentially private (DP) retrieval-augmented generation algorithm that\nis capable of generating high-quality image samples while providing provable\nprivacy guarantees. Specifically, we assume access to a text-to-image diffusion\nmodel trained on a small amount of public data, and design a DP retrieval\nmechanism to augment the text prompt with samples retrieved from a private\nretrieval dataset. Our \\emph{differentially private retrieval-augmented\ndiffusion model} (DP-RDM) requires no fine-tuning on the retrieval dataset to\nadapt to another domain, and can use state-of-the-art generative models to\ngenerate high-quality image samples while satisfying rigorous DP guarantees.\nFor instance, when evaluated on MS-COCO, our DP-RDM can generate samples with a\nprivacy budget of $\\epsilon=10$, while providing a $3.5$ point improvement in\nFID compared to public-only retrieval for up to $10,000$ queries.\n","authors":["Jonathan Lebensold","Maziar Sanjabi","Pietro Astolfi","Adriana Romero-Soriano","Kamalika Chaudhuri","Mike Rabbat","Chuan Guo"],"pdf_url":"https://arxiv.org/pdf/2403.14421v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08966v2","updated":"2024-04-16T14:16:40Z","published":"2024-04-13T11:07:53Z","title":"LoopGaussian: Creating 3D Cinemagraph with Multi-view Images via\n  Eulerian Motion Field","summary":"  Cinemagraph is a unique form of visual media that combines elements of still\nphotography and subtle motion to create a captivating experience. However, the\nmajority of videos generated by recent works lack depth information and are\nconfined to the constraints of 2D image space. In this paper, inspired by\nsignificant progress in the field of novel view synthesis (NVS) achieved by 3D\nGaussian Splatting (3D-GS), we propose LoopGaussian to elevate cinemagraph from\n2D image space to 3D space using 3D Gaussian modeling. To achieve this, we\nfirst employ the 3D-GS method to reconstruct 3D Gaussian point clouds from\nmulti-view images of static scenes,incorporating shape regularization terms to\nprevent blurring or artifacts caused by object deformation. We then adopt an\nautoencoder tailored for 3D Gaussian to project it into feature space. To\nmaintain the local continuity of the scene, we devise SuperGaussian for\nclustering based on the acquired features. By calculating the similarity\nbetween clusters and employing a two-stage estimation method, we derive an\nEulerian motion field to describe velocities across the entire scene. The 3D\nGaussian points then move within the estimated Eulerian motion field. Through\nbidirectional animation techniques, we ultimately generate a 3D Cinemagraph\nthat exhibits natural and seamlessly loopable dynamics. Experiment results\nvalidate the effectiveness of our approach, demonstrating high-quality and\nvisually appealing scene generation. The project is available at\nhttps://pokerlishao.github.io/LoopGaussian/.\n","authors":["Jiyang Li","Lechao Cheng","Zhangye Wang","Tingting Mu","Jingxuan He"],"pdf_url":"https://arxiv.org/pdf/2404.08966v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2202.13588v3","updated":"2024-04-16T14:15:40Z","published":"2022-02-28T07:44:59Z","title":"Using Multi-scale SwinTransformer-HTC with Data augmentation in CoNIC\n  Challenge","summary":"  Colorectal cancer is one of the most common cancers worldwide, so early\npathological examination is very important. However, it is time-consuming and\nlabor-intensive to identify the number and type of cells on H&E images in\nclinical. Therefore, automatic segmentation and classification task and\ncounting the cellular composition of H&E images from pathological sections is\nproposed by CoNIC Challenge 2022. We proposed a multi-scale Swin transformer\nwith HTC for this challenge, and also applied the known normalization methods\nto generate more augmentation data. Finally, our strategy showed that the\nmulti-scale played a crucial role to identify different scale features and the\naugmentation arose the recognition of model.\n","authors":["Chia-Yen Lee","Hsiang-Chin Chien","Ching-Ping Wang","Hong Yen","Kai-Wen Zhen","Hong-Kun Lin"],"pdf_url":"https://arxiv.org/pdf/2202.13588v3.pdf","comment":"Errors have been identified in the analysis"},{"id":"http://arxiv.org/abs/2404.10588v1","updated":"2024-04-16T14:13:44Z","published":"2024-04-16T14:13:44Z","title":"Do Counterfactual Examples Complicate Adversarial Training?","summary":"  We leverage diffusion models to study the robustness-performance tradeoff of\nrobust classifiers. Our approach introduces a simple, pretrained diffusion\nmethod to generate low-norm counterfactual examples (CEs): semantically altered\ndata which results in different true class membership. We report that the\nconfidence and accuracy of robust models on their clean training data are\nassociated with the proximity of the data to their CEs. Moreover, robust models\nperform very poorly when evaluated on the CEs directly, as they become\nincreasingly invariant to the low-norm, semantic changes brought by CEs. The\nresults indicate a significant overlap between non-robust and semantic\nfeatures, countering the common assumption that non-robust features are not\ninterpretable.\n","authors":["Eric Yeats","Cameron Darwin","Eduardo Ortega","Frank Liu","Hai Li"],"pdf_url":"https://arxiv.org/pdf/2404.10588v1.pdf","comment":"Accepted as a short paper to the GCV Workshop at CVPR'24"},{"id":"http://arxiv.org/abs/2404.10584v1","updated":"2024-04-16T14:10:42Z","published":"2024-04-16T14:10:42Z","title":"ReWiTe: Realistic Wide-angle and Telephoto Dual Camera Fusion Dataset\n  via Beam Splitter Camera Rig","summary":"  The fusion of images from dual camera systems featuring a wide-angle and a\ntelephoto camera has become a hotspot problem recently. By integrating\nsimultaneously captured wide-angle and telephoto images from these systems, the\nresulting fused image achieves a wide field of view (FOV) coupled with\nhigh-definition quality. Existing approaches are mostly deep learning methods,\nand predominantly rely on supervised learning, where the training dataset plays\na pivotal role. However, current datasets typically adopt a data synthesis\napproach generate input pairs of wide-angle and telephoto images alongside\nground-truth images. Notably, the wide-angle inputs are synthesized rather than\ncaptured using real wide-angle cameras, and the ground-truth image is captured\nby wide-angle camera whose quality is substantially lower than that of input\ntelephoto images captured by telephoto cameras. To address these limitations,\nwe introduce a novel hardware setup utilizing a beam splitter to simultaneously\ncapture three images, i.e. input pairs and ground-truth images, from two\nauthentic cellphones equipped with wide-angle and telephoto dual cameras.\nSpecifically, the wide-angle and telephoto images captured by cellphone 2 serve\nas the input pair, while the telephoto image captured by cellphone 1, which is\ncalibrated to match the optical path of the wide-angle image from cellphone 2,\nserves as the ground-truth image, maintaining quality on par with the input\ntelephoto image. Experiments validate the efficacy of our newly introduced\ndataset, named ReWiTe, significantly enhances the performance of various\nexisting methods for real-world wide-angle and telephoto dual image fusion\ntasks.\n","authors":["Chunli Peng","Xuan Dong","Tiantian Cao","Zhengqing Li","Kun Dong","Weixin Li"],"pdf_url":"https://arxiv.org/pdf/2404.10584v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15368v3","updated":"2024-04-16T14:08:03Z","published":"2023-03-27T16:35:28Z","title":"2S-UDF: A Novel Two-stage UDF Learning Method for Robust Non-watertight\n  Model Reconstruction from Multi-view Images","summary":"  Recently, building on the foundation of neural radiance field, various\ntechniques have emerged to learn unsigned distance fields (UDF) to reconstruct\n3D non-watertight models from multi-view images. Yet, a central challenge in\nUDF-based volume rendering is formulating a proper way to convert unsigned\ndistance values into volume density, ensuring that the resulting weight\nfunction remains unbiased and sensitive to occlusions. Falling short on these\nrequirements often results in incorrect topology or large reconstruction errors\nin resulting models. This paper addresses this challenge by presenting a novel\ntwo-stage algorithm, 2S-UDF, for learning a high-quality UDF from multi-view\nimages. Initially, the method applies an easily trainable density function\nthat, while slightly biased and transparent, aids in coarse reconstruction. The\nsubsequent stage then refines the geometry and appearance of the object to\nachieve a high-quality reconstruction by directly adjusting the weight function\nused in volume rendering to ensure that it is unbiased and occlusion-aware.\nDecoupling density and weight in two stages makes our training stable and\nrobust, distinguishing our technique from existing UDF learning approaches.\nEvaluations on the DeepFashion3D, DTU, and BlendedMVS datasets validate the\nrobustness and effectiveness of our proposed approach. In both quantitative\nmetrics and visual quality, the results indicate our superior performance over\nother UDF learning techniques in reconstructing 3D non-watertight models from\nmulti-view images. Our code is available at\nhttps://bitbucket.org/jkdeng/2sudf/.\n","authors":["Junkai Deng","Fei Hou","Xuhui Chen","Wencheng Wang","Ying He"],"pdf_url":"https://arxiv.org/pdf/2303.15368v3.pdf","comment":"accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2404.10575v1","updated":"2024-04-16T13:53:58Z","published":"2024-04-16T13:53:58Z","title":"EMC$^2$: Efficient MCMC Negative Sampling for Contrastive Learning with\n  Global Convergence","summary":"  A key challenge in contrastive learning is to generate negative samples from\na large sample set to contrast with positive samples, for learning better\nencoding of the data. These negative samples often follow a softmax\ndistribution which are dynamically updated during the training process.\nHowever, sampling from this distribution is non-trivial due to the high\ncomputational costs in computing the partition function. In this paper, we\npropose an Efficient Markov Chain Monte Carlo negative sampling method for\nContrastive learning (EMC$^2$). We follow the global contrastive learning loss\nas introduced in SogCLR, and propose EMC$^2$ which utilizes an adaptive\nMetropolis-Hastings subroutine to generate hardness-aware negative samples in\nan online fashion during the optimization. We prove that EMC$^2$ finds an\n$\\mathcal{O}(1/\\sqrt{T})$-stationary point of the global contrastive loss in\n$T$ iterations. Compared to prior works, EMC$^2$ is the first algorithm that\nexhibits global convergence (to stationarity) regardless of the choice of batch\nsize while exhibiting low computation and memory cost. Numerical experiments\nvalidate that EMC$^2$ is effective with small batch training and achieves\ncomparable or better performance than baseline algorithms. We report the\nresults for pre-training image encoders on STL-10 and Imagenet-100.\n","authors":["Chung-Yiu Yau","Hoi-To Wai","Parameswaran Raman","Soumajyoti Sarkar","Mingyi Hong"],"pdf_url":"https://arxiv.org/pdf/2404.10575v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2404.10574v1","updated":"2024-04-16T13:52:00Z","published":"2024-04-16T13:52:00Z","title":"Uncertainty-guided Open-Set Source-Free Unsupervised Domain Adaptation\n  with Target-private Class Segregation","summary":"  Standard Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from\na labeled source domain to an unlabeled target but usually requires\nsimultaneous access to both source and target data. Moreover, UDA approaches\ncommonly assume that source and target domains share the same labels space.\nYet, these two assumptions are hardly satisfied in real-world scenarios. This\npaper considers the more challenging Source-Free Open-set Domain Adaptation\n(SF-OSDA) setting, where both assumptions are dropped. We propose a novel\napproach for SF-OSDA that exploits the granularity of target-private categories\nby segregating their samples into multiple unknown classes. Starting from an\ninitial clustering-based assignment, our method progressively improves the\nsegregation of target-private samples by refining their pseudo-labels with the\nguide of an uncertainty-based sample selection module. Additionally, we propose\na novel contrastive loss, named NL-InfoNCELoss, that, integrating negative\nlearning into self-supervised contrastive learning, enhances the model\nrobustness to noisy pseudo-labels. Extensive experiments on benchmark datasets\ndemonstrate the superiority of the proposed method over existing approaches,\nestablishing new state-of-the-art performance. Notably, additional analyses\nshow that our method is able to learn the underlying semantics of novel\nclasses, opening the possibility to perform novel class discovery.\n","authors":["Mattia Litrico","Davide Talon","Sebastiano Battiato","Alessio Del Bue","Mario Valerio Giuffrida","Pietro Morerio"],"pdf_url":"https://arxiv.org/pdf/2404.10574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10572v1","updated":"2024-04-16T13:47:27Z","published":"2024-04-16T13:47:27Z","title":"Label merge-and-split: A graph-colouring approach for memory-efficient\n  brain parcellation","summary":"  Whole brain parcellation requires inferring hundreds of segmentation labels\nin large image volumes and thus presents significant practical challenges for\ndeep learning approaches. We introduce label merge-and-split, a method that\nfirst greatly reduces the effective number of labels required for\nlearning-based whole brain parcellation and then recovers original labels.\nUsing a greedy graph colouring algorithm, our method automatically groups and\nmerges multiple spatially separate labels prior to model training and\ninference. The merged labels may be semantically unrelated. A deep learning\nmodel is trained to predict merged labels. At inference time, original labels\nare restored using atlas-based influence regions. In our experiments, the\nproposed approach reduces the number of labels by up to 68% while achieving\nsegmentation accuracy comparable to the baseline method without label merging\nand splitting. Moreover, model training and inference times as well as GPU\nmemory requirements were reduced significantly. The proposed method can be\napplied to all semantic segmentation tasks with a large number of spatially\nseparate classes within an atlas-based prior.\n","authors":["Aaron Kujawa","Reuben Dorent","Sebastien Ourselin","Tom Vercauteren"],"pdf_url":"https://arxiv.org/pdf/2404.10572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10571v1","updated":"2024-04-16T13:47:21Z","published":"2024-04-16T13:47:21Z","title":"CMU-Flownet: Exploring Point Cloud Scene Flow Estimation in Occluded\n  Scenario","summary":"  Occlusions hinder point cloud frame alignment in LiDAR data, a challenge\ninadequately addressed by scene flow models tested mainly on occlusion-free\ndatasets. Attempts to integrate occlusion handling within networks often suffer\naccuracy issues due to two main limitations: a) the inadequate use of occlusion\ninformation, often merging it with flow estimation without an effective\nintegration strategy, and b) reliance on distance-weighted upsampling that\nfalls short in correcting occlusion-related errors. To address these\nchallenges, we introduce the Correlation Matrix Upsampling Flownet\n(CMU-Flownet), incorporating an occlusion estimation module within its cost\nvolume layer, alongside an Occlusion-aware Cost Volume (OCV) mechanism.\nSpecifically, we propose an enhanced upsampling approach that expands the\nsensory field of the sampling process which integrates a Correlation Matrix\ndesigned to evaluate point-level similarity. Meanwhile, our model robustly\nintegrates occlusion data within the context of scene flow, deploying this\ninformation strategically during the refinement phase of the flow estimation.\nThe efficacy of this approach is demonstrated through subsequent experimental\nvalidation. Empirical assessments reveal that CMU-Flownet establishes\nstate-of-the-art performance within the realms of occluded Flyingthings3D and\nKITTY datasets, surpassing previous methodologies across a majority of\nevaluated metrics.\n","authors":["Jingze Chen","Junfeng Yao","Qiqin Lin","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2404.10571v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2308.16215v6","updated":"2024-04-16T13:32:25Z","published":"2023-08-30T16:44:38Z","title":"Deep Video Codec Control for Vision Models","summary":"  Standardized lossy video coding is at the core of almost all real-world video\nprocessing pipelines. Rate control is used to enable standard codecs to adapt\nto different network bandwidth conditions or storage constraints. However,\nstandard video codecs (e.g., H.264) and their rate control modules aim to\nminimize video distortion w.r.t. human quality assessment. We demonstrate\nempirically that standard-coded videos vastly deteriorate the performance of\ndeep vision models. To overcome the deterioration of vision performance, this\npaper presents the first end-to-end learnable deep video codec control that\nconsiders both bandwidth constraints and downstream deep vision performance,\nwhile adhering to existing standardization. We demonstrate that our approach\nbetter preserves downstream deep vision performance than traditional standard\nvideo coding.\n","authors":["Christoph Reich","Biplob Debnath","Deep Patel","Tim Prangemeier","Daniel Cremers","Srimat Chakradhar"],"pdf_url":"https://arxiv.org/pdf/2308.16215v6.pdf","comment":"Accepted at CVPR 2024 Workshop on AI for Streaming (AIS)"},{"id":"http://arxiv.org/abs/2404.00724v2","updated":"2024-04-16T13:28:22Z","published":"2024-03-31T15:50:52Z","title":"Absolute-Unified Multi-Class Anomaly Detection via Class-Agnostic\n  Distribution Alignment","summary":"  Conventional unsupervised anomaly detection (UAD) methods build separate\nmodels for each object category. Recent studies have proposed to train a\nunified model for multiple classes, namely model-unified UAD. However, such\nmethods still implement the unified model separately on each class during\ninference with respective anomaly decision thresholds, which hinders their\napplication when the image categories are entirely unavailable. In this work,\nwe present a simple yet powerful method to address multi-class anomaly\ndetection without any class information, namely \\textit{absolute-unified} UAD.\nWe target the crux of prior works in this challenging setting: different\nobjects have mismatched anomaly score distributions. We propose Class-Agnostic\nDistribution Alignment (CADA) to align the mismatched score distribution of\neach implicit class without knowing class information, which enables unified\nanomaly detection for all classes and samples. The essence of CADA is to\npredict each class's score distribution of normal samples given any image,\nnormal or anomalous, of this class. As a general component, CADA can activate\nthe potential of nearly all UAD methods under absolute-unified setting. Our\napproach is extensively evaluated under the proposed setting on two popular UAD\nbenchmark datasets, MVTec AD and VisA, where we exceed previous\nstate-of-the-art by a large margin.\n","authors":["Jia Guo","Haonan Han","Shuai Lu","Weihang Zhang","Huiqi Li"],"pdf_url":"https://arxiv.org/pdf/2404.00724v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.00218v3","updated":"2024-04-16T13:22:08Z","published":"2022-11-01T02:00:32Z","title":"Pixel-Wise Contrastive Distillation","summary":"  We present a simple but effective pixel-level self-supervised distillation\nframework friendly to dense prediction tasks. Our method, called Pixel-Wise\nContrastive Distillation (PCD), distills knowledge by attracting the\ncorresponding pixels from student's and teacher's output feature maps. PCD\nincludes a novel design called SpatialAdaptor which ``reshapes'' a part of the\nteacher network while preserving the distribution of its output features. Our\nablation experiments suggest that this reshaping behavior enables more\ninformative pixel-to-pixel distillation. Moreover, we utilize a plug-in\nmulti-head self-attention module that explicitly relates the pixels of\nstudent's feature maps to enhance the effective receptive field, leading to a\nmore competitive student. PCD \\textbf{outperforms} previous self-supervised\ndistillation methods on various dense prediction tasks. A backbone of\n\\mbox{ResNet-18-FPN} distilled by PCD achieves $37.4$ AP$^\\text{bbox}$ and\n$34.0$ AP$^\\text{mask}$ on COCO dataset using the detector of \\mbox{Mask\nR-CNN}. We hope our study will inspire future research on how to pre-train a\nsmall model friendly to dense prediction tasks in a self-supervised fashion.\n","authors":["Junqiang Huang","Zichao Guo"],"pdf_url":"https://arxiv.org/pdf/2211.00218v3.pdf","comment":"ICCV 2023 camera-ready"},{"id":"http://arxiv.org/abs/2304.08272v4","updated":"2024-04-16T13:20:44Z","published":"2023-04-17T13:33:23Z","title":"About latent roles in forecasting players in team sports","summary":"  Forecasting players in sports has grown in popularity due to the potential\nfor a tactical advantage and the applicability of such research to multi-agent\ninteraction systems. Team sports contain a significant social component that\ninfluences interactions between teammates and opponents. However, it still\nneeds to be fully exploited. In this work, we hypothesize that each participant\nhas a specific function in each action and that role-based interaction is\ncritical for predicting players' future moves. We create RolFor, a novel\nend-to-end model for Role-based Forecasting. RolFor uses a new module we\ndeveloped called Ordering Neural Networks (OrderNN) to permute the order of the\nplayers such that each player is assigned to a latent role. The latent role is\nthen modeled with a RoleGCN. Thanks to its graph representation, it provides a\nfully learnable adjacency matrix that captures the relationships between roles\nand is subsequently used to forecast the players' future trajectories.\nExtensive experiments on a challenging NBA basketball dataset back up the\nimportance of roles and justify our goal of modeling them using optimizable\nmodels. When an oracle provides roles, the proposed RolFor compares favorably\nto the current state-of-the-art (it ranks first in terms of ADE and second in\nterms of FDE errors). However, training the end-to-end RolFor incurs the issues\nof differentiability of permutation methods, which we experimentally review.\nFinally, this work restates differentiable ranking as a difficult open problem\nand its great potential in conjunction with graph-based interaction models.\nProject is available at: https://www.pinlab.org/aboutlatentroles\n","authors":["Luca Scofano","Alessio Sampieri","Giuseppe Re","Matteo Almanza","Alessandro Panconesi","Fabio Galasso"],"pdf_url":"https://arxiv.org/pdf/2304.08272v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10548v1","updated":"2024-04-16T13:18:02Z","published":"2024-04-16T13:18:02Z","title":"Classification of Prostate Cancer in 3D Magnetic Resonance Imaging Data\n  based on Convolutional Neural Networks","summary":"  Prostate cancer is a commonly diagnosed cancerous disease among men\nworld-wide. Even with modern technology such as multi-parametric magnetic\nresonance tomography and guided biopsies, the process for diagnosing prostate\ncancer remains time consuming and requires highly trained professionals. In\nthis paper, different convolutional neural networks (CNN) are evaluated on\ntheir abilities to reliably classify whether an MRI sequence contains malignant\nlesions. Implementations of a ResNet, a ConvNet and a ConvNeXt for 3D image\ndata are trained and evaluated. The models are trained using different data\naugmentation techniques, learning rates, and optimizers. The data is taken from\na private dataset, provided by Cantonal Hospital Aarau. The best result was\nachieved by a ResNet3D, yielding an average precision score of 0.4583 and AUC\nROC score of 0.6214.\n","authors":["Malte Rippa","Ruben Schulze","Marian Himstedt","Felice Burn"],"pdf_url":"https://arxiv.org/pdf/2404.10548v1.pdf","comment":"Previous version published in Buzug T.M., Handels H., M\\\"uller S.,\n  H\\\"ubner C., Mertins A., Rostalski P.: Student Conference Proceedings 2023,\n  Infinite Science Publishing, 2023 (ISBN/EAN 978-3-945954-72-0). 7 pages, 2\n  figures"},{"id":"http://arxiv.org/abs/2311.15658v2","updated":"2024-04-16T12:58:57Z","published":"2023-11-27T09:40:14Z","title":"Regularization by Texts for Latent Diffusion Inverse Solvers","summary":"  The recent advent of diffusion models has led to significant progress in\nsolving inverse problems, leveraging these models as effective generative\npriors. Nonetheless, there remain challenges related to the ill-posed nature of\nsuch problems, often due to inherent ambiguities in measurements or intrinsic\nsystem symmetries. To address this, drawing inspiration from the human ability\nto resolve visual ambiguities through perceptual biases, here we introduce a\nnovel latent diffusion inverse solver by regularization by texts (TReg).\nSpecifically, TReg applies the textual description of the preconception of the\nsolution during the reverse diffusion sampling, of which the description is\ndynamically reinforced through null-text optimization for adaptive negation.\nOur comprehensive experimental results demonstrate that TReg successfully\nmitigates ambiguity in the inverse problems, enhancing their effectiveness and\naccuracy.\n","authors":["Jeongsol Kim","Geon Yeong Park","Hyungjin Chung","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2311.15658v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10527v1","updated":"2024-04-16T12:55:15Z","published":"2024-04-16T12:55:15Z","title":"SPVLoc: Semantic Panoramic Viewport Matching for 6D Camera Localization\n  in Unseen Environments","summary":"  In this paper, we present SPVLoc, a global indoor localization method that\naccurately determines the six-dimensional (6D) camera pose of a query image and\nrequires minimal scene-specific prior knowledge and no scene-specific training.\nOur approach employs a novel matching procedure to localize the perspective\ncamera's viewport, given as an RGB image, within a set of panoramic semantic\nlayout representations of the indoor environment. The panoramas are rendered\nfrom an untextured 3D reference model, which only comprises approximate\nstructural information about room shapes, along with door and window\nannotations. We demonstrate that a straightforward convolutional network\nstructure can successfully achieve image-to-panorama and ultimately\nimage-to-model matching. Through a viewport classification score, we rank\nreference panoramas and select the best match for the query image. Then, a 6D\nrelative pose is estimated between the chosen panorama and query image. Our\nexperiments demonstrate that this approach not only efficiently bridges the\ndomain gap but also generalizes well to previously unseen scenes that are not\npart of the training data. Moreover, it achieves superior localization accuracy\ncompared to the state of the art methods and also estimates more degrees of\nfreedom of the camera pose. We will make our source code publicly available at\nhttps://github.com/fraunhoferhhi/spvloc .\n","authors":["Niklas Gard","Anna Hilsmann","Peter Eisert"],"pdf_url":"https://arxiv.org/pdf/2404.10527v1.pdf","comment":"This submission includes the paper and supplementary material. 24\n  pages, 11 figures"},{"id":"http://arxiv.org/abs/2312.02155v3","updated":"2024-04-16T12:43:35Z","published":"2023-12-04T18:59:55Z","title":"GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for\n  Real-time Human Novel View Synthesis","summary":"  We present a new approach, termed GPS-Gaussian, for synthesizing novel views\nof a character in a real-time manner. The proposed method enables 2K-resolution\nrendering under a sparse-view camera setting. Unlike the original Gaussian\nSplatting or neural implicit rendering methods that necessitate per-subject\noptimizations, we introduce Gaussian parameter maps defined on the source views\nand regress directly Gaussian Splatting properties for instant novel view\nsynthesis without any fine-tuning or optimization. To this end, we train our\nGaussian parameter regression module on a large amount of human scan data,\njointly with a depth estimation module to lift 2D parameter maps to 3D space.\nThe proposed framework is fully differentiable and experiments on several\ndatasets demonstrate that our method outperforms state-of-the-art methods while\nachieving an exceeding rendering speed.\n","authors":["Shunyuan Zheng","Boyao Zhou","Ruizhi Shao","Boning Liu","Shengping Zhang","Liqiang Nie","Yebin Liu"],"pdf_url":"https://arxiv.org/pdf/2312.02155v3.pdf","comment":"Accepted by CVPR 2024 (Highlight). Project page:\n  https://shunyuanzheng.github.io/GPS-Gaussian"},{"id":"http://arxiv.org/abs/2404.10518v1","updated":"2024-04-16T12:41:25Z","published":"2024-04-16T12:41:25Z","title":"MobileNetV4 - Universal Models for the Mobile Ecosystem","summary":"  We present the latest generation of MobileNets, known as MobileNetV4 (MNv4),\nfeaturing universally efficient architecture designs for mobile devices. At its\ncore, we introduce the Universal Inverted Bottleneck (UIB) search block, a\nunified and flexible structure that merges Inverted Bottleneck (IB), ConvNext,\nFeed Forward Network (FFN), and a novel Extra Depthwise (ExtraDW) variant.\nAlongside UIB, we present Mobile MQA, an attention block tailored for mobile\naccelerators, delivering a significant 39% speedup. An optimized neural\narchitecture search (NAS) recipe is also introduced which improves MNv4 search\neffectiveness. The integration of UIB, Mobile MQA and the refined NAS recipe\nresults in a new suite of MNv4 models that are mostly Pareto optimal across\nmobile CPUs, DSPs, GPUs, as well as specialized accelerators like Apple Neural\nEngine and Google Pixel EdgeTPU - a characteristic not found in any other\nmodels tested. Finally, to further boost accuracy, we introduce a novel\ndistillation technique. Enhanced by this technique, our MNv4-Hybrid-Large model\ndelivers 87% ImageNet-1K accuracy, with a Pixel 8 EdgeTPU runtime of just\n3.8ms.\n","authors":["Danfeng Qin","Chas Leichner","Manolis Delakis","Marco Fornoni","Shixin Luo","Fan Yang","Weijun Wang","Colby Banbury","Chengxi Ye","Berkin Akin","Vaibhav Aggarwal","Tenghui Zhu","Daniele Moro","Andrew Howard"],"pdf_url":"https://arxiv.org/pdf/2404.10518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.14950v2","updated":"2024-04-16T12:40:41Z","published":"2022-11-27T22:01:47Z","title":"Leveraging Image Matching Toward End-to-End Relative Camera Pose\n  Regression","summary":"  This paper proposes a generalizable, end-to-end deep learning-based method\nfor relative pose regression between two images. Given two images of the same\nscene captured from different viewpoints, our method predicts the relative\nrotation and translation (including direction and scale) between the two\nrespective cameras. Inspired by the classical pipeline, our method leverages\nImage Matching (IM) as a pre-trained task for relative pose regression.\nSpecifically, we use LoFTR, an architecture that utilizes an attention-based\nnetwork pre-trained on Scannet, to extract semi-dense feature maps, which are\nthen warped and fed into a pose regression network. Notably, we use a loss\nfunction that utilizes separate terms to account for the translation direction\nand scale. We believe such a separation is important because translation\ndirection is determined by point correspondences while the scale is inferred\nfrom prior on shape sizes. Our ablations further support this choice. We\nevaluate our method on several datasets and show that it outperforms previous\nend-to-end methods. The method also generalizes well to unseen datasets.\n","authors":["Fadi Khatib","Yuval Margalit","Meirav Galun","Ronen Basri"],"pdf_url":"https://arxiv.org/pdf/2211.14950v2.pdf","comment":"Project webpage: https://fadikhatib.github.io/GRelPose"},{"id":"http://arxiv.org/abs/2404.10501v1","updated":"2024-04-16T12:19:54Z","published":"2024-04-16T12:19:54Z","title":"Self-Supervised Visual Preference Alignment","summary":"  This paper makes the first attempt towards unsupervised preference alignment\nin Vision-Language Models (VLMs). We generate chosen and rejected responses\nwith regard to the original and augmented image pairs, and conduct preference\nalignment with direct preference optimization. It is based on a core idea:\nproperly designed augmentation to the image input will induce VLM to generate\nfalse but hard negative responses, which helps the model to learn from and\nproduce more robust and powerful answers. The whole pipeline no longer hinges\non supervision from GPT4 or human involvement during alignment, and is highly\nefficient with few lines of code. With only 8k randomly sampled unsupervised\ndata, it achieves 90\\% relative score to GPT-4 on complex reasoning in\nLLaVA-Bench, and improves LLaVA-7B/13B by 6.7\\%/5.6\\% score on complex\nmulti-modal benchmark MM-Vet. Visualizations shows its improved ability to\nalign with user-intentions. A series of ablations are firmly conducted to\nreveal the latent mechanism of the approach, which also indicates its potential\ntowards further scaling. Code will be available.\n","authors":["Ke Zhu","Liang Zhao","Zheng Ge","Xiangyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.10501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10499v1","updated":"2024-04-16T12:18:08Z","published":"2024-04-16T12:18:08Z","title":"Robust Noisy Label Learning via Two-Stream Sample Distillation","summary":"  Noisy label learning aims to learn robust networks under the supervision of\nnoisy labels, which plays a critical role in deep learning. Existing work\neither conducts sample selection or label correction to deal with noisy labels\nduring the model training process. In this paper, we design a simple yet\neffective sample selection framework, termed Two-Stream Sample Distillation\n(TSSD), for noisy label learning, which can extract more high-quality samples\nwith clean labels to improve the robustness of network training. Firstly, a\nnovel Parallel Sample Division (PSD) module is designed to generate a certain\ntraining set with sufficient reliable positive and negative samples by jointly\nconsidering the sample structure in feature space and the human prior in loss\nspace. Secondly, a novel Meta Sample Purification (MSP) module is further\ndesigned to mine adequate semi-hard samples from the remaining uncertain\ntraining set by learning a strong meta classifier with extra golden data. As a\nresult, more and more high-quality samples will be distilled from the noisy\ntraining set to train networks robustly in every iteration. Extensive\nexperiments on four benchmark datasets, including CIFAR-10, CIFAR-100,\nTiny-ImageNet, and Clothing-1M, show that our method has achieved\nstate-of-the-art results over its competitors.\n","authors":["Sihan Bai","Sanping Zhou","Zheng Qin","Le Wang","Nanning Zheng"],"pdf_url":"https://arxiv.org/pdf/2404.10499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10498v1","updated":"2024-04-16T12:12:06Z","published":"2024-04-16T12:12:06Z","title":"LAECIPS: Large Vision Model Assisted Adaptive Edge-Cloud Collaboration\n  for IoT-based Perception System","summary":"  Recent large vision models (e.g., SAM) enjoy great potential to facilitate\nintelligent perception with high accuracy. Yet, the resource constraints in the\nIoT environment tend to limit such large vision models to be locally deployed,\nincurring considerable inference latency thereby making it difficult to support\nreal-time applications, such as autonomous driving and robotics. Edge-cloud\ncollaboration with large-small model co-inference offers a promising approach\nto achieving high inference accuracy and low latency. However, existing\nedge-cloud collaboration methods are tightly coupled with the model\narchitecture and cannot adapt to the dynamic data drifts in heterogeneous IoT\nenvironments. To address the issues, we propose LAECIPS, a new edge-cloud\ncollaboration framework. In LAECIPS, both the large vision model on the cloud\nand the lightweight model on the edge are plug-and-play. We design an\nedge-cloud collaboration strategy based on hard input mining, optimized for\nboth high accuracy and low latency. We propose to update the edge model and its\ncollaboration strategy with the cloud under the supervision of the large vision\nmodel, so as to adapt to the dynamic IoT data streams. Theoretical analysis of\nLAECIPS proves its feasibility. Experiments conducted in a robotic semantic\nsegmentation system using real-world datasets show that LAECIPS outperforms its\nstate-of-the-art competitors in accuracy, latency, and communication overhead\nwhile having better adaptability to dynamic environments.\n","authors":["Shijing Hu","Ruijun Deng","Xin Du","Zhihui Lu","Qiang Duan","Yi He","Shih-Chia Huang","Jie Wu"],"pdf_url":"https://arxiv.org/pdf/2404.10498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06665v2","updated":"2024-04-16T12:09:24Z","published":"2024-04-10T00:25:09Z","title":"Deep Generative Data Assimilation in Multimodal Setting","summary":"  Robust integration of physical knowledge and data is key to improve\ncomputational simulations, such as Earth system models. Data assimilation is\ncrucial for achieving this goal because it provides a systematic framework to\ncalibrate model outputs with observations, which can include remote sensing\nimagery and ground station measurements, with uncertainty quantification.\nConventional methods, including Kalman filters and variational approaches,\ninherently rely on simplifying linear and Gaussian assumptions, and can be\ncomputationally expensive. Nevertheless, with the rapid adoption of data-driven\nmethods in many areas of computational sciences, we see the potential of\nemulating traditional data assimilation with deep learning, especially\ngenerative models. In particular, the diffusion-based probabilistic framework\nhas large overlaps with data assimilation principles: both allows for\nconditional generation of samples with a Bayesian inverse framework. These\nmodels have shown remarkable success in text-conditioned image generation or\nimage-controlled video synthesis. Likewise, one can frame data assimilation as\nobservation-conditioned state calibration. In this work, we propose SLAMS:\nScore-based Latent Assimilation in Multimodal Setting. Specifically, we\nassimilate in-situ weather station data and ex-situ satellite imagery to\ncalibrate the vertical temperature profiles, globally. Through extensive\nablation, we demonstrate that SLAMS is robust even in low-resolution, noisy,\nand sparse data settings. To our knowledge, our work is the first to apply deep\ngenerative framework for multimodal data assimilation using real-world\ndatasets; an important step for building robust computational simulators,\nincluding the next-generation Earth system models. Our code is available at:\nhttps://github.com/yongquan-qu/SLAMS\n","authors":["Yongquan Qu","Juan Nathaniel","Shuolin Li","Pierre Gentine"],"pdf_url":"https://arxiv.org/pdf/2404.06665v2.pdf","comment":"CVPR2024 EarthVision"},{"id":"http://arxiv.org/abs/2312.07039v2","updated":"2024-04-16T12:05:55Z","published":"2023-12-12T07:52:33Z","title":"Open-Pose 3D Zero-Shot Learning: Benchmark and Challenges","summary":"  With the explosive 3D data growth, the urgency of utilizing zero-shot\nlearning to facilitate data labeling becomes evident. Recently, methods\ntransferring language or language-image pre-training models like Contrastive\nLanguage-Image Pre-training (CLIP) to 3D vision have made significant progress\nin the 3D zero-shot classification task. These methods primarily focus on 3D\nobject classification with an aligned pose; such a setting is, however, rather\nrestrictive, which overlooks the recognition of 3D objects with open poses\ntypically encountered in real-world scenarios, such as an overturned chair or a\nlying teddy bear. To this end, we propose a more realistic and challenging\nscenario named open-pose 3D zero-shot classification, focusing on the\nrecognition of 3D objects regardless of their orientation. First, we revisit\nthe current research on 3D zero-shot classification, and propose two benchmark\ndatasets specifically designed for the open-pose setting. We empirically\nvalidate many of the most popular methods in the proposed open-pose benchmark.\nOur investigations reveal that most current 3D zero-shot classification models\nsuffer from poor performance, indicating a substantial exploration room towards\nthe new direction. Furthermore, we study a concise pipeline with an iterative\nangle refinement mechanism that automatically optimizes one ideal angle to\nclassify these open-pose 3D objects. In particular, to make validation more\ncompelling and not just limited to existing CLIP-based methods, we also pioneer\nthe exploration of knowledge transfer based on Diffusion models. While the\nproposed solutions can serve as a new benchmark for open-pose 3D zero-shot\nclassification, we discuss the complexities and challenges of this scenario\nthat remain for further research development. The code is available publicly at\nhttps://github.com/weiguangzhao/Diff-OP3D.\n","authors":["Weiguang Zhao","Guanyu Yang","Rui Zhang","Chenru Jiang","Chaolong Yang","Yuyao Yan","Amir Hussain","Kaizhu Huang"],"pdf_url":"https://arxiv.org/pdf/2312.07039v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04016v2","updated":"2024-04-16T12:04:01Z","published":"2023-12-07T03:10:03Z","title":"PartDistill: 3D Shape Part Segmentation by Vision-Language Model\n  Distillation","summary":"  This paper proposes a cross-modal distillation framework, PartDistill, which\ntransfers 2D knowledge from vision-language models (VLMs) to facilitate 3D\nshape part segmentation. PartDistill addresses three major challenges in this\ntask: the lack of 3D segmentation in invisible or undetected regions in the 2D\nprojections, inconsistent 2D predictions by VLMs, and the lack of knowledge\naccumulation across different 3D shapes. PartDistill consists of a teacher\nnetwork that uses a VLM to make 2D predictions and a student network that\nlearns from the 2D predictions while extracting geometrical features from\nmultiple 3D shapes to carry out 3D part segmentation. A bi-directional\ndistillation, including forward and backward distillations, is carried out\nwithin the framework, where the former forward distills the 2D predictions to\nthe student network, and the latter improves the quality of the 2D predictions,\nwhich subsequently enhances the final 3D segmentation. Moreover, PartDistill\ncan exploit generative models that facilitate effortless 3D shape creation for\ngenerating knowledge sources to be distilled. Through extensive experiments,\nPartDistill boosts the existing methods with substantial margins on widely used\nShapeNetPart and PartNetE datasets, by more than 15% and 12% higher mIoU\nscores, respectively. The code for this work is available at\nhttps://github.com/ardianumam/PartDistill.\n","authors":["Ardian Umam","Cheng-Kun Yang","Min-Hung Chen","Jen-Hui Chuang","Yen-Yu Lin"],"pdf_url":"https://arxiv.org/pdf/2312.04016v2.pdf","comment":"CVPR 2024 Accepted"},{"id":"http://arxiv.org/abs/2404.10490v1","updated":"2024-04-16T11:57:03Z","published":"2024-04-16T11:57:03Z","title":"Teaching Chinese Sign Language with Feedback in Mixed Reality","summary":"  Traditional sign language teaching methods face challenges such as limited\nfeedback and diverse learning scenarios. Although 2D resources lack real-time\nfeedback, classroom teaching is constrained by a scarcity of teacher. Methods\nbased on VR and AR have relatively primitive interaction feedback mechanisms.\nThis study proposes an innovative teaching model that uses real-time monocular\nvision and mixed reality technology. First, we introduce an improved\nhand-posture reconstruction method to achieve sign language semantic retention\nand real-time feedback. Second, a ternary system evaluation algorithm is\nproposed for a comprehensive assessment, maintaining good consistency with\nexperts in sign language. Furthermore, we use mixed reality technology to\nconstruct a scenario-based 3D sign language classroom and explore the user\nexperience of scenario teaching. Overall, this paper presents a novel teaching\nmethod that provides an immersive learning experience, advanced posture\nreconstruction, and precise feedback, achieving positive feedback on user\nexperience and learning effectiveness.\n","authors":["Hongli Wen","Yang Xu","Lin Li","Xudong Ru"],"pdf_url":"https://arxiv.org/pdf/2404.10490v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.17701v3","updated":"2024-04-16T11:46:39Z","published":"2024-03-26T13:40:18Z","title":"Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical\n  Image Segmentation","summary":"  Image segmentation holds a vital position in the realms of diagnosis and\ntreatment within the medical domain. Traditional convolutional neural networks\n(CNNs) and Transformer models have made significant advancements in this realm,\nbut they still encounter challenges because of limited receptive field or high\ncomputing complexity. Recently, State Space Models (SSMs), particularly Mamba\nand its variants, have demonstrated notable performance in the field of vision.\nHowever, their feature extraction methods may not be sufficiently effective and\nretain some redundant structures, leaving room for parameter reduction.\nMotivated by previous spatial and channel attention methods, we propose Triplet\nMamba-UNet. The method leverages residual VSS Blocks to extract intensive\ncontextual features, while Triplet SSM is employed to fuse features across\nspatial and channel dimensions. We conducted experiments on ISIC17, ISIC18,\nCVC-300, CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB, and Kvasir-Instrument datasets,\ndemonstrating the superior segmentation performance of our proposed TM-UNet.\nAdditionally, compared to the previous VM-UNet, our model achieves a one-third\nreduction in parameters.\n","authors":["Hao Tang","Lianglun Cheng","Guoheng Huang","Zhengguang Tan","Junhao Lu","Kaihong Wu"],"pdf_url":"https://arxiv.org/pdf/2403.17701v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10484v1","updated":"2024-04-16T11:44:12Z","published":"2024-04-16T11:44:12Z","title":"AbsGS: Recovering Fine Details for 3D Gaussian Splatting","summary":"  3D Gaussian Splatting (3D-GS) technique couples 3D Gaussian primitives with\ndifferentiable rasterization to achieve high-quality novel view synthesis\nresults while providing advanced real-time rendering performance. However, due\nto the flaw of its adaptive density control strategy in 3D-GS, it frequently\nsuffers from over-reconstruction issue in intricate scenes containing\nhigh-frequency details, leading to blurry rendered images. The underlying\nreason for the flaw has still been under-explored. In this work, we present a\ncomprehensive analysis of the cause of aforementioned artifacts, namely\ngradient collision, which prevents large Gaussians in over-reconstructed\nregions from splitting. To address this issue, we propose the novel\nhomodirectional view-space positional gradient as the criterion for\ndensification. Our strategy efficiently identifies large Gaussians in\nover-reconstructed regions, and recovers fine details by splitting. We evaluate\nour proposed method on various challenging datasets. The experimental results\nindicate that our approach achieves the best rendering quality with reduced or\nsimilar memory consumption. Our method is easy to implement and can be\nincorporated into a wide variety of most recent Gaussian Splatting-based\nmethods. We will open source our codes upon formal publication. Our project\npage is available at: https://ty424.github.io/AbsGS.github.io/\n","authors":["Zongxin Ye","Wenyu Li","Sidun Liu","Peng Qiao","Yong Dou"],"pdf_url":"https://arxiv.org/pdf/2404.10484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10476v1","updated":"2024-04-16T11:38:44Z","published":"2024-04-16T11:38:44Z","title":"Efficient optimal dispersed Haar-like filters for face detection","summary":"  This paper introduces a new dispersed Haar-like filter for efficiently\ndetection face. The basic idea for finding the filter is maximising\nbetween-class and minimising within-class variance. The proposed filters can be\nconsidered as an optimal configuration dispersed Haar-like filters; filters\nwith disjoint black and white parts.\n","authors":["Zeinab Sedaghatjoo","Hossein Hosseinzadeh","Ahmad shirzadi"],"pdf_url":"https://arxiv.org/pdf/2404.10476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02612v2","updated":"2024-04-16T11:35:37Z","published":"2023-11-05T10:01:18Z","title":"GPT-4V-AD: Exploring Grounding Potential of VQA-oriented GPT-4V for\n  Zero-shot Anomaly Detection","summary":"  Large Multimodal Model (LMM) GPT-4V(ision) endows GPT-4 with visual grounding\ncapabilities, making it possible to handle certain tasks through the Visual\nQuestion Answering (VQA) paradigm. This paper explores the potential of\nVQA-oriented GPT-4V in the recently popular visual Anomaly Detection (AD) and\nis the first to conduct qualitative and quantitative evaluations on the popular\nMVTec AD and VisA datasets. Considering that this task requires both\nimage-/pixel-level evaluations, the proposed GPT-4V-AD framework contains three\ncomponents: \\textbf{\\textit{1)}} Granular Region Division, \\textbf{\\textit{2)}}\nPrompt Designing, \\textbf{\\textit{3)}} Text2Segmentation for easy quantitative\nevaluation, and have made some different attempts for comparative analysis. The\nresults show that GPT-4V can achieve certain results in the zero-shot AD task\nthrough a VQA paradigm, such as achieving image-level 77.1/88.0 and pixel-level\n68.0/76.6 AU-ROCs on MVTec AD and VisA datasets, respectively. However, its\nperformance still has a certain gap compared to the state-of-the-art zero-shot\nmethod, \\eg, WinCLIP and CLIP-AD, and further researches are needed. This study\nprovides a baseline reference for the research of VQA-oriented LMM in the\nzero-shot AD task, and we also post several possible future works. Code is\navailable at \\url{https://github.com/zhangzjn/GPT-4V-AD}.\n","authors":["Jiangning Zhang","Haoyang He","Xuhai Chen","Zhucun Xue","Yabiao Wang","Chengjie Wang","Lei Xie","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2311.02612v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10474v1","updated":"2024-04-16T11:29:43Z","published":"2024-04-16T11:29:43Z","title":"Toward a Realistic Benchmark for Out-of-Distribution Detection","summary":"  Deep neural networks are increasingly used in a wide range of technologies\nand services, but remain highly susceptible to out-of-distribution (OOD)\nsamples, that is, drawn from a different distribution than the original\ntraining set. A common approach to address this issue is to endow deep neural\nnetworks with the ability to detect OOD samples. Several benchmarks have been\nproposed to design and validate OOD detection techniques. However, many of them\nare based on far-OOD samples drawn from very different distributions, and thus\nlack the complexity needed to capture the nuances of real-world scenarios. In\nthis work, we introduce a comprehensive benchmark for OOD detection, based on\nImageNet and Places365, that assigns individual classes as in-distribution or\nout-of-distribution depending on the semantic similarity with the training set.\nSeveral techniques can be used to determine which classes should be considered\nin-distribution, yielding benchmarks with varying properties. Experimental\nresults on different OOD detection techniques show how their measured efficacy\ndepends on the selected benchmark and how confidence-based techniques may\noutperform classifier-based ones on near-OOD samples.\n","authors":["Pietro Recalcati","Fabio Garcea","Luca Piano","Fabrizio Lamberti","Lia Morra"],"pdf_url":"https://arxiv.org/pdf/2404.10474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.11029v4","updated":"2024-04-16T11:24:36Z","published":"2023-06-19T15:46:41Z","title":"RemoteCLIP: A Vision Language Foundation Model for Remote Sensing","summary":"  General-purpose foundation models have led to recent breakthroughs in\nartificial intelligence. In remote sensing, self-supervised learning (SSL) and\nMasked Image Modeling (MIM) have been adopted to build foundation models.\nHowever, these models primarily learn low-level features and require annotated\ndata for fine-tuning. Moreover, they are inapplicable for retrieval and\nzero-shot applications due to the lack of language understanding. To address\nthese limitations, we propose RemoteCLIP, the first vision-language foundation\nmodel for remote sensing that aims to learn robust visual features with rich\nsemantics and aligned text embeddings for seamless downstream application. To\naddress the scarcity of pre-training data, we leverage data scaling which\nconverts heterogeneous annotations into a unified image-caption data format\nbased on Box-to-Caption (B2C) and Mask-to-Box (M2B) conversion. By further\nincorporating UAV imagery, we produce a 12 $\\times$ larger pretraining dataset\nthan the combination of all available datasets. RemoteCLIP can be applied to a\nvariety of downstream tasks, including zero-shot image classification, linear\nprobing, $\\textit{k}$-NN classification, few-shot classification, image-text\nretrieval, and object counting in remote sensing images. Evaluation on 16\ndatasets, including a newly introduced RemoteCount benchmark to test the object\ncounting ability, shows that RemoteCLIP consistently outperforms baseline\nfoundation models across different model scales. Impressively, RemoteCLIP beats\nthe state-of-the-art method by 9.14% mean recall on the RSITMD dataset and\n8.92% on the RSICD dataset. For zero-shot classification, our RemoteCLIP\noutperforms the CLIP baseline by up to 6.39% average accuracy on 12 downstream\ndatasets. Project website: https://github.com/ChenDelong1999/RemoteCLIP\n","authors":["Fan Liu","Delong Chen","Zhangqingyun Guan","Xiaocong Zhou","Jiale Zhu","Qiaolin Ye","Liyong Fu","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2306.11029v4.pdf","comment":"Accepted by IEEE Transactions on Geoscience and Remote Sensing (TGRS)"},{"id":"http://arxiv.org/abs/2404.10454v1","updated":"2024-04-16T10:50:16Z","published":"2024-04-16T10:50:16Z","title":"A Computer Vision-Based Quality Assessment Technique for the automatic\n  control of consumables for analytical laboratories","summary":"  The rapid growth of the Industry 4.0 paradigm is increasing the pressure to\ndevelop effective automated monitoring systems. Artificial Intelligence (AI) is\na convenient tool to improve the efficiency of industrial processes while\nreducing errors and waste. In fact, it allows the use of real-time data to\nincrease the effectiveness of monitoring systems, minimize errors, make the\nproduction process more sustainable, and save costs. In this paper, a novel\nautomatic monitoring system is proposed in the context of production process of\nplastic consumables used in analysis laboratories, with the aim to increase the\neffectiveness of the control process currently performed by a human operator.\nIn particular, we considered the problem of classifying the presence or absence\nof a transparent anticoagulant substance inside test tubes. Specifically, a\nhand-designed deep network model is used and compared with some\nstate-of-the-art models for its ability to categorize different images of vials\nthat can be either filled with the anticoagulant or empty. Collected results\nindicate that the proposed approach is competitive with state-of-the-art models\nin terms of accuracy. Furthermore, we increased the complexity of the task by\ntraining the models on the ability to discriminate not only the presence or\nabsence of the anticoagulant inside the vial, but also the size of the test\ntube. The analysis performed in the latter scenario confirms the\ncompetitiveness of our approach. Moreover, our model is remarkably superior in\nterms of its generalization ability and requires significantly fewer resources.\nThese results suggest the possibility of successfully implementing such a model\nin the production process of a plastic consumables company.\n","authors":["Meriam Zribi","Paolo Pagliuca","Francesca Pitolli"],"pdf_url":"https://arxiv.org/pdf/2404.10454v1.pdf","comment":"31 pages, 13 figures, 10 tables"},{"id":"http://arxiv.org/abs/2404.09342v2","updated":"2024-04-16T10:33:36Z","published":"2024-04-14T19:51:32Z","title":"Face-voice Association in Multilingual Environments (FAME) Challenge\n  2024 Evaluation Plan","summary":"  The advancements of technology have led to the use of multimodal systems in\nvarious real-world applications. Among them, the audio-visual systems are one\nof the widely used multimodal systems. In the recent years, associating face\nand voice of a person has gained attention due to presence of unique\ncorrelation between them. The Face-voice Association in Multilingual\nEnvironments (FAME) Challenge 2024 focuses on exploring face-voice association\nunder a unique condition of multilingual scenario. This condition is inspired\nfrom the fact that half of the world's population is bilingual and most often\npeople communicate under multilingual scenario. The challenge uses a dataset\nnamely, Multilingual Audio-Visual (MAV-Celeb) for exploring face-voice\nassociation in multilingual environments. This report provides the details of\nthe challenge, dataset, baselines and task details for the FAME Challenge.\n","authors":["Muhammad Saad Saeed","Shah Nawaz","Muhammad Salman Tahir","Rohan Kumar Das","Muhammad Zaigham Zaheer","Marta Moscati","Markus Schedl","Muhammad Haris Khan","Karthik Nandakumar","Muhammad Haroon Yousaf"],"pdf_url":"https://arxiv.org/pdf/2404.09342v2.pdf","comment":"ACM Multimedia Conference - Grand Challenge"},{"id":"http://arxiv.org/abs/2404.10441v1","updated":"2024-04-16T10:26:57Z","published":"2024-04-16T10:26:57Z","title":"1st Place Solution for ICCV 2023 OmniObject3D Challenge: Sparse-View\n  Reconstruction","summary":"  In this report, we present the 1st place solution for ICCV 2023 OmniObject3D\nChallenge: Sparse-View Reconstruction. The challenge aims to evaluate\napproaches for novel view synthesis and surface reconstruction using only a few\nposed images of each object. We utilize Pixel-NeRF as the basic model, and\napply depth supervision as well as coarse-to-fine positional encoding. The\nexperiments demonstrate the effectiveness of our approach in improving\nsparse-view reconstruction quality. We ranked first in the final test with a\nPSNR of 25.44614.\n","authors":["Hang Du","Yaping Xue","Weidong Dai","Xuejun Yan","Jingjing Wang"],"pdf_url":"https://arxiv.org/pdf/2404.10441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08504v2","updated":"2024-04-16T10:18:56Z","published":"2024-04-12T14:34:24Z","title":"3D Human Scan With A Moving Event Camera","summary":"  Capturing a 3D human body is one of the important tasks in computer vision\nwith a wide range of applications such as virtual reality and sports analysis.\nHowever, conventional frame cameras are limited by their temporal resolution\nand dynamic range, which imposes constraints in real-world application setups.\nEvent cameras have the advantages of high temporal resolution and high dynamic\nrange (HDR), but the development of event-based methods is necessary to handle\ndata with different characteristics. This paper proposes a novel event-based\nmethod for 3D pose estimation and human mesh recovery. Prior work on\nevent-based human mesh recovery require frames (images) as well as event data.\nThe proposed method solely relies on events; it carves 3D voxels by moving the\nevent camera around a stationary body, reconstructs the human pose and mesh by\nattenuated rays, and fit statistical body models, preserving high-frequency\ndetails. The experimental results show that the proposed method outperforms\nconventional frame-based methods in the estimation accuracy of both pose and\nbody mesh. We also demonstrate results in challenging situations where a\nconventional camera has motion blur. This is the first to demonstrate\nevent-only human mesh recovery, and we hope that it is the first step toward\nachieving robust and accurate 3D human body scanning from vision sensors.\nhttps://florpeng.github.io/event-based-human-scan/\n","authors":["Kai Kohyama","Shintaro Shiba","Yoshimitsu Aoki"],"pdf_url":"https://arxiv.org/pdf/2404.08504v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10438v1","updated":"2024-04-16T10:04:38Z","published":"2024-04-16T10:04:38Z","title":"The Unreasonable Effectiveness of Pre-Trained Features for Camera Pose\n  Refinement","summary":"  Pose refinement is an interesting and practically relevant research\ndirection. Pose refinement can be used to (1) obtain a more accurate pose\nestimate from an initial prior (e.g., from retrieval), (2) as pre-processing,\ni.e., to provide a better starting point to a more expensive pose estimator,\n(3) as post-processing of a more accurate localizer. Existing approaches focus\non learning features / scene representations for the pose refinement task. This\ninvolves training an implicit scene representation or learning features while\noptimizing a camera pose-based loss. A natural question is whether training\nspecific features / representations is truly necessary or whether similar\nresults can be already achieved with more generic features. In this work, we\npresent a simple approach that combines pre-trained features with a particle\nfilter and a renderable representation of the scene. Despite its simplicity, it\nachieves state-of-the-art results, demonstrating that one can easily build a\npose refiner without the need for specific training. The code is at\nhttps://github.com/ga1i13o/mcloc_poseref\n","authors":["Gabriele Trivigno","Carlo Masone","Barbara Caputo","Torsten Sattler"],"pdf_url":"https://arxiv.org/pdf/2404.10438v1.pdf","comment":"Accepted to CVPR2024 (Highlight)"},{"id":"http://arxiv.org/abs/2404.05468v3","updated":"2024-04-16T10:02:17Z","published":"2024-04-08T12:46:39Z","title":"Mind-to-Image: Projecting Visual Mental Imagination of the Brain from\n  fMRI","summary":"  The reconstruction of images observed by subjects from fMRI data collected\nduring visual stimuli has made significant strides in the past decade, thanks\nto the availability of extensive fMRI datasets and advancements in generative\nmodels for image generation. However, the application of visual reconstruction\nhas remained limited. Reconstructing visual imagination presents a greater\nchallenge, with potentially revolutionary applications ranging from aiding\nindividuals with disabilities to verifying witness accounts in court. The\nprimary hurdles in this field are the absence of data collection protocols for\nvisual imagery and the lack of datasets on the subject. Traditionally,\nfMRI-to-image relies on data collected from subjects exposed to visual stimuli,\nwhich poses issues for generating visual imagery based on the difference of\nbrain activity between visual stimulation and visual imagery. For the first\ntime, we have compiled a substantial dataset (around 6h of scans) on visual\nimagery along with a proposed data collection protocol. We then train a\nmodified version of an fMRI-to-image model and demonstrate the feasibility of\nreconstructing images from two modes of imagination: from memory and from pure\nimagination. This marks an important step towards creating a technology that\nallow direct reconstruction of visual imagery.\n","authors":["Hugo Caselles-Dupr√©","Charles Mellerio","Paul H√©rent","Aliz√©e Lopez-Persem","Benoit B√©ranger","Mathieu Soularue","Pierre Fautrel","Gauthier Vernier","Matthieu Cord"],"pdf_url":"https://arxiv.org/pdf/2404.05468v3.pdf","comment":"Pre-print to be updated. Work in progress"},{"id":"http://arxiv.org/abs/2404.10433v1","updated":"2024-04-16T09:56:08Z","published":"2024-04-16T09:56:08Z","title":"Explainable concept mappings of MRI: Revealing the mechanisms underlying\n  deep learning-based brain disease classification","summary":"  Motivation. While recent studies show high accuracy in the classification of\nAlzheimer's disease using deep neural networks, the underlying learned concepts\nhave not been investigated.\n  Goals. To systematically identify changes in brain regions through concepts\nlearned by the deep neural network for model validation.\n  Approach. Using quantitative R2* maps we separated Alzheimer's patients\n(n=117) from normal controls (n=219) by using a convolutional neural network\nand systematically investigated the learned concepts using Concept Relevance\nPropagation and compared these results to a conventional region of\ninterest-based analysis.\n  Results. In line with established histological findings and the region of\ninterest-based analyses, highly relevant concepts were primarily found in and\nadjacent to the basal ganglia.\n  Impact. The identification of concepts learned by deep neural networks for\ndisease classification enables validation of the models and could potentially\nimprove reliability.\n","authors":["Christian Tinauer","Anna Damulina","Maximilian Sackl","Martin Soellradl","Reduan Achtibat","Maximilian Dreyer","Frederik Pahde","Sebastian Lapuschkin","Reinhold Schmidt","Stefan Ropele","Wojciech Samek","Christian Langkammer"],"pdf_url":"https://arxiv.org/pdf/2404.10433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18975v2","updated":"2024-04-16T09:49:15Z","published":"2024-02-29T09:27:40Z","title":"Theoretically Achieving Continuous Representation of Oriented Bounding\n  Boxes","summary":"  Considerable efforts have been devoted to Oriented Object Detection (OOD).\nHowever, one lasting issue regarding the discontinuity in Oriented Bounding Box\n(OBB) representation remains unresolved, which is an inherent bottleneck for\nextant OOD methods. This paper endeavors to completely solve this issue in a\ntheoretically guaranteed manner and puts an end to the ad-hoc efforts in this\ndirection. Prior studies typically can only address one of the two cases of\ndiscontinuity: rotation and aspect ratio, and often inadvertently introduce\ndecoding discontinuity, e.g. Decoding Incompleteness (DI) and Decoding\nAmbiguity (DA) as discussed in literature. Specifically, we propose a novel\nrepresentation method called Continuous OBB (COBB), which can be readily\nintegrated into existing detectors e.g. Faster-RCNN as a plugin. It can\ntheoretically ensure continuity in bounding box regression which to our best\nknowledge, has not been achieved in literature for rectangle-based object\nrepresentation. For fairness and transparency of experiments, we have developed\na modularized benchmark based on the open-source deep learning framework\nJittor's detection toolbox JDet for OOD evaluation. On the popular DOTA\ndataset, by integrating Faster-RCNN as the same baseline model, our new method\noutperforms the peer method Gliding Vertex by 1.13% mAP50 (relative improvement\n1.54%), and 2.46% mAP75 (relative improvement 5.91%), without any tricks.\n","authors":["Zi-Kai Xiao","Guo-Ye Yang","Xue Yang","Tai-Jiang Mu","Junchi Yan","Shi-min Hu"],"pdf_url":"https://arxiv.org/pdf/2402.18975v2.pdf","comment":"17 pages, 12 tables, 8 figures. Accepted by CVPR'24. Code:\n  https://github.com/514flowey/JDet-COBB"},{"id":"http://arxiv.org/abs/2404.10411v1","updated":"2024-04-16T09:28:54Z","published":"2024-04-16T09:28:54Z","title":"Camera clustering for scalable stream-based active distillation","summary":"  We present a scalable framework designed to craft efficient lightweight\nmodels for video object detection utilizing self-training and knowledge\ndistillation techniques. We scrutinize methodologies for the ideal selection of\ntraining images from video streams and the efficacy of model sharing across\nnumerous cameras. By advocating for a camera clustering methodology, we aim to\ndiminish the requisite number of models for training while augmenting the\ndistillation dataset. The findings affirm that proper camera clustering notably\namplifies the accuracy of distilled models, eclipsing the methodologies that\nemploy distinct models for each camera or a universal model trained on the\naggregate camera data.\n","authors":["Dani Manjah","Davide Cacciarelli","Christophe De Vleeschouwer","Benoit Macq"],"pdf_url":"https://arxiv.org/pdf/2404.10411v1.pdf","comment":"This manuscript is currently under review at IEEE Transactions on\n  Circuits and Systems for Video Technology"},{"id":"http://arxiv.org/abs/2404.10408v1","updated":"2024-04-16T09:19:23Z","published":"2024-04-16T09:19:23Z","title":"Adversarial Identity Injection for Semantic Face Image Synthesis","summary":"  Nowadays, deep learning models have reached incredible performance in the\ntask of image generation. Plenty of literature works address the task of face\ngeneration and editing, with human and automatic systems that struggle to\ndistinguish what's real from generated. Whereas most systems reached excellent\nvisual generation quality, they still face difficulties in preserving the\nidentity of the starting input subject. Among all the explored techniques,\nSemantic Image Synthesis (SIS) methods, whose goal is to generate an image\nconditioned on a semantic segmentation mask, are the most promising, even\nthough preserving the perceived identity of the input subject is not their main\nconcern. Therefore, in this paper, we investigate the problem of identity\npreservation in face image generation and present an SIS architecture that\nexploits a cross-attention mechanism to merge identity, style, and semantic\nfeatures to generate faces whose identities are as similar as possible to the\ninput ones. Experimental results reveal that the proposed method is not only\nsuitable for preserving the identity but is also effective in the face\nrecognition adversarial attack, i.e. hiding a second identity in the generated\nfaces.\n","authors":["Giuseppe Tarollo","Tomaso Fontanini","Claudio Ferrari","Guido Borghi","Andrea Prati"],"pdf_url":"https://arxiv.org/pdf/2404.10408v1.pdf","comment":"Paper accepted at CVPR 2024 Biometrics Workshop"},{"id":"http://arxiv.org/abs/2404.10407v1","updated":"2024-04-16T09:19:11Z","published":"2024-04-16T09:19:11Z","title":"Comprehensive Survey of Model Compression and Speed up for Vision\n  Transformers","summary":"  Vision Transformers (ViT) have marked a paradigm shift in computer vision,\noutperforming state-of-the-art models across diverse tasks. However, their\npractical deployment is hampered by high computational and memory demands. This\nstudy addresses the challenge by evaluating four primary model compression\ntechniques: quantization, low-rank approximation, knowledge distillation, and\npruning. We methodically analyze and compare the efficacy of these techniques\nand their combinations in optimizing ViTs for resource-constrained\nenvironments. Our comprehensive experimental evaluation demonstrates that these\nmethods facilitate a balanced compromise between model accuracy and\ncomputational efficiency, paving the way for wider application in edge\ncomputing devices.\n","authors":["Feiyang Chen","Ziqian Luo","Lisang Zhou","Xueting Pan","Ying Jiang"],"pdf_url":"https://arxiv.org/pdf/2404.10407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10405v1","updated":"2024-04-16T09:12:16Z","published":"2024-04-16T09:12:16Z","title":"Integration of Self-Supervised BYOL in Semi-Supervised Medical Image\n  Recognition","summary":"  Image recognition techniques heavily rely on abundant labeled data,\nparticularly in medical contexts. Addressing the challenges associated with\nobtaining labeled data has led to the prominence of self-supervised learning\nand semi-supervised learning, especially in scenarios with limited annotated\ndata. In this paper, we proposed an innovative approach by integrating\nself-supervised learning into semi-supervised models to enhance medical image\nrecognition. Our methodology commences with pre-training on unlabeled data\nutilizing the BYOL method. Subsequently, we merge pseudo-labeled and labeled\ndatasets to construct a neural network classifier, refining it through\niterative fine-tuning. Experimental results on three different datasets\ndemonstrate that our approach optimally leverages unlabeled data, outperforming\nexisting methods in terms of accuracy for medical image recognition.\n","authors":["Hao Feng","Yuanzhe Jia","Ruijia Xu","Mukesh Prasad","Ali Anaissi","Ali Braytee"],"pdf_url":"https://arxiv.org/pdf/2404.10405v1.pdf","comment":"Accepted by ICCS 2024"},{"id":"http://arxiv.org/abs/2205.10120v7","updated":"2024-04-16T09:03:32Z","published":"2022-05-17T14:00:58Z","title":"Privacy Preserving Image Registration","summary":"  Image registration is a key task in medical imaging applications, allowing to\nrepresent medical images in a common spatial reference frame. Current\napproaches to image registration are generally based on the assumption that the\ncontent of the images is usually accessible in clear form, from which the\nspatial transformation is subsequently estimated. This common assumption may\nnot be met in practical applications, since the sensitive nature of medical\nimages may ultimately require their analysis under privacy constraints,\npreventing to openly share the image content.In this work, we formulate the\nproblem of image registration under a privacy preserving regime, where images\nare assumed to be confidential and cannot be disclosed in clear. We derive our\nprivacy preserving image registration framework by extending classical\nregistration paradigms to account for advanced cryptographic tools, such as\nsecure multi-party computation and homomorphic encryption, that enable the\nexecution of operations without leaking the underlying data. To overcome the\nproblem of performance and scalability of cryptographic tools in high\ndimensions, we propose several techniques to optimize the image registration\noperations by using gradient approximations, and by revisiting the use of\nhomomorphic encryption trough packing, to allow the efficient encryption and\nmultiplication of large matrices. We demonstrate our privacy preserving\nframework in linear and non-linear registration problems, evaluating its\naccuracy and scalability with respect to standard, non-private counterparts.\nOur results show that privacy preserving image registration is feasible and can\nbe adopted in sensitive medical imaging applications.\n","authors":["Riccardo Taiello","Melek √ñnen","Francesco Capano","Olivier Humbert","Marco Lorenzi"],"pdf_url":"https://arxiv.org/pdf/2205.10120v7.pdf","comment":"v4 Accepted at Medical Image Computing and Computer Assisted\n  Intervention (2022) 130-140"},{"id":"http://arxiv.org/abs/2404.10394v1","updated":"2024-04-16T08:52:42Z","published":"2024-04-16T08:52:42Z","title":"Portrait3D: Text-Guided High-Quality 3D Portrait Generation Using\n  Pyramid Representation and GANs Prior","summary":"  Existing neural rendering-based text-to-3D-portrait generation methods\ntypically make use of human geometry prior and diffusion models to obtain\nguidance. However, relying solely on geometry information introduces issues\nsuch as the Janus problem, over-saturation, and over-smoothing. We present\nPortrait3D, a novel neural rendering-based framework with a novel joint\ngeometry-appearance prior to achieve text-to-3D-portrait generation that\novercomes the aforementioned issues. To accomplish this, we train a 3D portrait\ngenerator, 3DPortraitGAN-Pyramid, as a robust prior. This generator is capable\nof producing 360{\\deg} canonical 3D portraits, serving as a starting point for\nthe subsequent diffusion-based generation process. To mitigate the \"grid-like\"\nartifact caused by the high-frequency information in the feature-map-based 3D\nrepresentation commonly used by most 3D-aware GANs, we integrate a novel\npyramid tri-grid 3D representation into 3DPortraitGAN-Pyramid. To generate 3D\nportraits from text, we first project a randomly generated image aligned with\nthe given prompt into the pre-trained 3DPortraitGAN-Pyramid's latent space. The\nresulting latent code is then used to synthesize a pyramid tri-grid. Beginning\nwith the obtained pyramid tri-grid, we use score distillation sampling to\ndistill the diffusion model's knowledge into the pyramid tri-grid. Following\nthat, we utilize the diffusion model to refine the rendered images of the 3D\nportrait and then use these refined images as training data to further optimize\nthe pyramid tri-grid, effectively eliminating issues with unrealistic color and\nunnatural artifacts. Our experimental results show that Portrait3D can produce\nrealistic, high-quality, and canonical 3D portraits that align with the prompt.\n","authors":["Yiqian Wu","Hao Xu","Xiangjun Tang","Xien Chen","Siyu Tang","Zhebin Zhang","Chen Li","Xiaogang Jin"],"pdf_url":"https://arxiv.org/pdf/2404.10394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10387v1","updated":"2024-04-16T08:39:29Z","published":"2024-04-16T08:39:29Z","title":"CNN-based explanation ensembling for dataset, representation and\n  explanations evaluation","summary":"  Explainable Artificial Intelligence has gained significant attention due to\nthe widespread use of complex deep learning models in high-stake domains such\nas medicine, finance, and autonomous cars. However, different explanations\noften present different aspects of the model's behavior. In this research\nmanuscript, we explore the potential of ensembling explanations generated by\ndeep classification models using convolutional model. Through experimentation\nand analysis, we aim to investigate the implications of combining explanations\nto uncover a more coherent and reliable patterns of the model's behavior,\nleading to the possibility of evaluating the representation learned by the\nmodel. With our method, we can uncover problems of under-representation of\nimages in a certain class. Moreover, we discuss other side benefits like\nfeatures' reduction by replacing the original image with its explanations\nresulting in the removal of some sensitive information. Through the use of\ncarefully selected evaluation metrics from the Quantus library, we demonstrated\nthe method's superior performance in terms of Localisation and Faithfulness,\ncompared to individual explanations.\n","authors":["Weronika Hryniewska-Guzik","Luca Longo","Przemys≈Çaw Biecek"],"pdf_url":"https://arxiv.org/pdf/2404.10387v1.pdf","comment":"accepted at 2nd World Conference on eXplainable Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2404.10383v1","updated":"2024-04-16T08:25:36Z","published":"2024-04-16T08:25:36Z","title":"Learning to Score Sign Language with Two-stage Method","summary":"  Human action recognition and performance assessment have been hot research\ntopics in recent years. Recognition problems have mature solutions in the field\nof sign language, but past research in performance analysis has focused on\ncompetitive sports and medical training, overlooking the scoring assessment\n,which is an important part of sign language teaching digitalization. In this\npaper, we analyze the existing technologies for performance assessment and\nadopt methods that perform well in human pose reconstruction tasks combined\nwith motion rotation embedded expressions, proposing a two-stage sign language\nperformance evaluation pipeline. Our analysis shows that choosing\nreconstruction tasks in the first stage can provide more expressive features,\nand using smoothing methods can provide an effective reference for assessment.\nExperiments show that our method provides good score feedback mechanisms and\nhigh consistency with professional assessments compared to end-to-end\nevaluations.\n","authors":["Wen Hongli","Xu Yang"],"pdf_url":"https://arxiv.org/pdf/2404.10383v1.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2404.10378v1","updated":"2024-04-16T08:15:10Z","published":"2024-04-16T08:15:10Z","title":"Second Edition FRCSyn Challenge at CVPR 2024: Face Recognition Challenge\n  in the Era of Synthetic Data","summary":"  Synthetic data is gaining increasing relevance for training machine learning\nmodels. This is mainly motivated due to several factors such as the lack of\nreal data and intra-class variability, time and errors produced in manual\nlabeling, and in some cases privacy concerns, among others. This paper presents\nan overview of the 2nd edition of the Face Recognition Challenge in the Era of\nSynthetic Data (FRCSyn) organized at CVPR 2024. FRCSyn aims to investigate the\nuse of synthetic data in face recognition to address current technological\nlimitations, including data privacy concerns, demographic biases,\ngeneralization to novel scenarios, and performance constraints in challenging\nsituations such as aging, pose variations, and occlusions. Unlike the 1st\nedition, in which synthetic data from DCFace and GANDiffFace methods was only\nallowed to train face recognition systems, in this 2nd edition we propose new\nsub-tasks that allow participants to explore novel face generative methods. The\noutcomes of the 2nd FRCSyn Challenge, along with the proposed experimental\nprotocol and benchmarking contribute significantly to the application of\nsynthetic data to face recognition.\n","authors":["Ivan DeAndres-Tame","Ruben Tolosana","Pietro Melzi","Ruben Vera-Rodriguez","Minchul Kim","Christian Rathgeb","Xiaoming Liu","Aythami Morales","Julian Fierrez","Javier Ortega-Garcia","Zhizhou Zhong","Yuge Huang","Yuxi Mi","Shouhong Ding","Shuigeng Zhou","Shuai He","Lingzhi Fu","Heng Cong","Rongyu Zhang","Zhihong Xiao","Evgeny Smirnov","Anton Pimenov","Aleksei Grigorev","Denis Timoshenko","Kaleb Mesfin Asfaw","Cheng Yaw Low","Hao Liu","Chuyi Wang","Qing Zuo","Zhixiang He","Hatef Otroshi Shahreza","Anjith George","Alexander Unnervik","Parsa Rahimi","S√©bastien Marcel","Pedro C. Neto","Marco Huber","Jan Niklas Kolf","Naser Damer","Fadi Boutros","Jaime S. Cardoso","Ana F. Sequeira","Andrea Atzori","Gianni Fenu","Mirko Marras","Vitomir ≈†truc","Jiang Yu","Zhangjie Li","Jichun Li","Weisong Zhao","Zhen Lei","Xiangyu Zhu","Xiao-Yu Zhang","Bernardo Biesseck","Pedro Vidal","Luiz Coelho","Roger Granada","David Menotti"],"pdf_url":"https://arxiv.org/pdf/2404.10378v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2311.10476"},{"id":"http://arxiv.org/abs/2404.10370v1","updated":"2024-04-16T08:08:47Z","published":"2024-04-16T08:08:47Z","title":"Know Yourself Better: Diverse Discriminative Feature Learning Improves\n  Open Set Recognition","summary":"  Open set recognition (OSR) is a critical aspect of machine learning,\naddressing the challenge of detecting novel classes during inference. Within\nthe realm of deep learning, neural classifiers trained on a closed set of data\ntypically struggle to identify novel classes, leading to erroneous predictions.\nTo address this issue, various heuristic methods have been proposed, allowing\nmodels to express uncertainty by stating \"I don't know.\" However, a gap in the\nliterature remains, as there has been limited exploration of the underlying\nmechanisms of these methods. In this paper, we conduct an analysis of open set\nrecognition methods, focusing on the aspect of feature diversity. Our research\nreveals a significant correlation between learning diverse discriminative\nfeatures and enhancing OSR performance. Building on this insight, we propose a\nnovel OSR approach that leverages the advantages of feature diversity. The\nefficacy of our method is substantiated through rigorous evaluation on a\nstandard OSR testbench, demonstrating a substantial improvement over\nstate-of-the-art methods.\n","authors":["Jiawen Xu"],"pdf_url":"https://arxiv.org/pdf/2404.10370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08514v2","updated":"2024-04-16T07:56:01Z","published":"2024-04-12T14:54:26Z","title":"NIR-Assisted Image Denoising: A Selective Fusion Approach and A\n  Real-World Benchmark Datase","summary":"  Despite the significant progress in image denoising, it is still challenging\nto restore fine-scale details while removing noise, especially in extremely\nlow-light environments. Leveraging near-infrared (NIR) images to assist visible\nRGB image denoising shows the potential to address this issue, becoming a\npromising technology. Nonetheless, existing works still struggle with taking\nadvantage of NIR information effectively for real-world image denoising, due to\nthe content inconsistency between NIR-RGB images and the scarcity of real-world\npaired datasets. To alleviate the problem, we propose an efficient Selective\nFusion Module (SFM), which can be plug-and-played into the advanced denoising\nnetworks to merge the deep NIR-RGB features. Specifically, we sequentially\nperform the global and local modulation for NIR and RGB features, and then\nintegrate the two modulated features. Furthermore, we present a Real-world\nNIR-Assisted Image Denoising (Real-NAID) dataset, which covers diverse\nscenarios as well as various noise levels. Extensive experiments on both\nsynthetic and our real-world datasets demonstrate that the proposed method\nachieves better results than state-of-the-art ones.\n","authors":["Rongjian Xu","Zhilu Zhang","Renlong Wu","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2404.08514v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2311.12815v2","updated":"2024-04-16T07:51:12Z","published":"2023-09-24T02:57:56Z","title":"Proposing an intelligent mesh smoothing method with graph neural\n  networks","summary":"  In CFD, mesh smoothing methods are commonly utilized to refine the mesh\nquality to achieve high-precision numerical simulations. Specifically,\noptimization-based smoothing is used for high-quality mesh smoothing, but it\nincurs significant computational overhead. Pioneer works improve its smoothing\nefficiency by adopting supervised learning to learn smoothing methods from\nhigh-quality meshes. However, they pose difficulty in smoothing the mesh nodes\nwith varying degrees and also need data augmentation to address the node input\nsequence problem. Additionally, the required labeled high-quality meshes\nfurther limit the applicability of the proposed method. In this paper, we\npresent GMSNet, a lightweight neural network model for intelligent mesh\nsmoothing. GMSNet adopts graph neural networks to extract features of the\nnode's neighbors and output the optimal node position. During smoothing, we\nalso introduce a fault-tolerance mechanism to prevent GMSNet from generating\nnegative volume elements. With a lightweight model, GMSNet can effectively\nsmoothing mesh nodes with varying degrees and remain unaffected by the order of\ninput data. A novel loss function, MetricLoss, is also developed to eliminate\nthe need for high-quality meshes, which provides a stable and rapid convergence\nduring training. We compare GMSNet with commonly used mesh smoothing methods on\ntwo-dimensional triangle meshes. The experimental results show that GMSNet\nachieves outstanding mesh smoothing performances with 5% model parameters of\nthe previous model, and attains 13.56 times faster than optimization-based\nsmoothing.\n","authors":["Zhichao Wang","Xinhai Chen","Junjun Yan","Jie Liu"],"pdf_url":"https://arxiv.org/pdf/2311.12815v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.16016v3","updated":"2024-04-16T07:47:19Z","published":"2023-06-28T08:44:00Z","title":"Positive Label Is All You Need for Multi-Label Classification","summary":"  Multi-label classification (MLC) faces challenges from label noise in\ntraining data due to annotating diverse semantic labels for each image. Current\nmethods mainly target identifying and correcting label mistakes using trained\nMLC models, but still struggle with persistent noisy labels during training,\nresulting in imprecise recognition and reduced performance. Our paper addresses\nlabel noise in MLC by introducing a positive and unlabeled multi-label\nclassification (PU-MLC) method. To counteract noisy labels, we directly discard\nnegative labels, focusing on the abundance of negative labels and the origin of\nmost noisy labels. PU-MLC employs positive-unlabeled learning, training the\nmodel with only positive labels and unlabeled data. The method incorporates\nadaptive re-balance factors and temperature coefficients in the loss function\nto address label distribution imbalance and prevent over-smoothing of\nprobabilities during training. Additionally, we introduce a local-global\nconvolution module to capture both local and global dependencies in the image\nwithout requiring backbone retraining. PU-MLC proves effective on MLC and MLC\nwith partial labels (MLC-PL) tasks, demonstrating significant improvements on\nMS-COCO and PASCAL VOC datasets with fewer annotations. Code is available at:\nhttps://github.com/TAKELAMAG/PU-MLC.\n","authors":["Zhixiang Yuan","Kaixin Zhang","Tao Huang"],"pdf_url":"https://arxiv.org/pdf/2306.16016v3.pdf","comment":"ICME 2024"},{"id":"http://arxiv.org/abs/2404.10358v1","updated":"2024-04-16T07:46:55Z","published":"2024-04-16T07:46:55Z","title":"Improving Bracket Image Restoration and Enhancement with Flow-guided\n  Alignment and Enhanced Feature Aggregation","summary":"  In this paper, we address the Bracket Image Restoration and Enhancement\n(BracketIRE) task using a novel framework, which requires restoring a\nhigh-quality high dynamic range (HDR) image from a sequence of noisy, blurred,\nand low dynamic range (LDR) multi-exposure RAW inputs. To overcome this\nchallenge, we present the IREANet, which improves the multiple exposure\nalignment and aggregation with a Flow-guide Feature Alignment Module (FFAM) and\nan Enhanced Feature Aggregation Module (EFAM). Specifically, the proposed FFAM\nincorporates the inter-frame optical flow as guidance to facilitate the\ndeformable alignment and spatial attention modules for better feature\nalignment. The EFAM further employs the proposed Enhanced Residual Block (ERB)\nas a foundational component, wherein a unidirectional recurrent network\naggregates the aligned temporal features to better reconstruct the results. To\nimprove model generalization and performance, we additionally employ the Bayer\npreserving augmentation (BayerAug) strategy to augment the multi-exposure RAW\ninputs. Our experimental evaluations demonstrate that the proposed IREANet\nshows state-of-the-art performance compared with previous methods.\n","authors":["Wenjie Lin","Zhen Liu","Chengzhi Jiang","Mingyan Han","Ting Jiang","Shuaicheng Liu"],"pdf_url":"https://arxiv.org/pdf/2404.10358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10357v1","updated":"2024-04-16T07:44:52Z","published":"2024-04-16T07:44:52Z","title":"Optimization of Prompt Learning via Multi-Knowledge Representation for\n  Vision-Language Models","summary":"  Vision-Language Models (VLMs), such as CLIP, play a foundational role in\nvarious cross-modal applications. To fully leverage VLMs' potential in adapting\nto downstream tasks, context optimization methods like Prompt Tuning are\nessential. However, one key limitation is the lack of diversity in prompt\ntemplates, whether they are hand-crafted or learned through additional modules.\nThis limitation restricts the capabilities of pretrained VLMs and can result in\nincorrect predictions in downstream tasks. To address this challenge, we\npropose Context Optimization with Multi-Knowledge Representation (CoKnow), a\nframework that enhances Prompt Learning for VLMs with rich contextual\nknowledge. To facilitate CoKnow during inference, we trained lightweight\nsemantic knowledge mappers, which are capable of generating Multi-Knowledge\nRepresentation for an input image without requiring additional priors.\nExperimentally, We conducted extensive experiments on 11 publicly available\ndatasets, demonstrating that CoKnow outperforms a series of previous methods.\nWe will make all resources open-source: https://github.com/EMZucas/CoKnow.\n","authors":["Enming Zhang","Bingke zhu","Yingying Chen","Qinghai Miao","Ming Tang","Jinqiao Wang"],"pdf_url":"https://arxiv.org/pdf/2404.10357v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06207v2","updated":"2024-04-16T07:41:49Z","published":"2024-04-09T10:56:46Z","title":"Leveraging edge detection and neural networks for better UAV\n  localization","summary":"  We propose a novel method for geolocalizing Unmanned Aerial Vehicles (UAVs)\nin environments lacking Global Navigation Satellite Systems (GNSS). Current\nstate-of-the-art techniques employ an offline-trained encoder to generate a\nvector representation (embedding) of the UAV's current view, which is then\ncompared with pre-computed embeddings of geo-referenced images to determine the\nUAV's position. Here, we demonstrate that the performance of these methods can\nbe significantly enhanced by preprocessing the images to extract their edges,\nwhich exhibit robustness to seasonal and illumination variations. Furthermore,\nwe establish that utilizing edges enhances resilience to orientation and\naltitude inaccuracies. Additionally, we introduce a confidence criterion for\nlocalization. Our findings are substantiated through synthetic experiments.\n","authors":["Theo Di Piazza","Enric Meinhardt-Llopis","Gabriele Facciolo","Benedicte Bascle","Corentin Abgrall","Jean-Clement Devaux"],"pdf_url":"https://arxiv.org/pdf/2404.06207v2.pdf","comment":"Accepted for publication in IGARSS2024. 4 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2401.09450v2","updated":"2024-04-16T07:35:41Z","published":"2023-12-22T11:15:16Z","title":"Joining Forces for Pathology Diagnostics with AI Assistance: The EMPAIA\n  Initiative","summary":"  Over the past decade, artificial intelligence (AI) methods in pathology have\nadvanced substantially. However, integration into routine clinical practice has\nbeen slow due to numerous challenges, including technical and regulatory\nhurdles in translating research results into clinical diagnostic products and\nthe lack of standardized interfaces. The open and vendor-neutral EMPAIA\ninitiative addresses these challenges. Here, we provide an overview of EMPAIA's\nachievements and lessons learned. EMPAIA integrates various stakeholders of the\npathology AI ecosystem, i.e., pathologists, computer scientists, and industry.\nIn close collaboration, we developed technical interoperability standards,\nrecommendations for AI testing and product development, and explainability\nmethods. We implemented the modular and open-source EMPAIA platform and\nsuccessfully integrated 14 AI-based image analysis apps from 8 different\nvendors, demonstrating how different apps can use a single standardized\ninterface. We prioritized requirements and evaluated the use of AI in real\nclinical settings with 14 different pathology laboratories in Europe and Asia.\nIn addition to technical developments, we created a forum for all stakeholders\nto share information and experiences on digital pathology and AI. Commercial,\nclinical, and academic stakeholders can now adopt EMPAIA's common open-source\ninterfaces, providing a unique opportunity for large-scale standardization and\nstreamlining of processes. Further efforts are needed to effectively and\nbroadly establish AI assistance in routine laboratory use. To this end, a\nsustainable infrastructure, the non-profit association EMPAIA International,\nhas been established to continue standardization and support broad\nimplementation and advocacy for an AI-assisted digital pathology future.\n","authors":["Norman Zerbe","Lars Ole Schwen","Christian Gei√üler","Katja Wiesemann","Tom Bisson","Peter Boor","Rita Carvalho","Michael Franz","Christoph Jansen","Tim-Rasmus Kiehl","Bj√∂rn Lindequist","Nora Charlotte Pohlan","Sarah Schmell","Klaus Strohmenger","Falk Zakrzewski","Markus Plass","Michael Takla","Tobias K√ºster","Andr√© Homeyer","Peter Hufnagl"],"pdf_url":"https://arxiv.org/pdf/2401.09450v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10343v1","updated":"2024-04-16T07:26:20Z","published":"2024-04-16T07:26:20Z","title":"The Ninth NTIRE 2024 Efficient Super-Resolution Challenge Report","summary":"  This paper provides a comprehensive review of the NTIRE 2024 challenge,\nfocusing on efficient single-image super-resolution (ESR) solutions and their\noutcomes. The task of this challenge is to super-resolve an input image with a\nmagnification factor of x4 based on pairs of low and corresponding\nhigh-resolution images. The primary objective is to develop networks that\noptimize various aspects such as runtime, parameters, and FLOPs, while still\nmaintaining a peak signal-to-noise ratio (PSNR) of approximately 26.90 dB on\nthe DIV2K_LSDIR_valid dataset and 26.99 dB on the DIV2K_LSDIR_test dataset. In\naddition, this challenge has 4 tracks including the main track (overall\nperformance), sub-track 1 (runtime), sub-track 2 (FLOPs), and sub-track 3\n(parameters). In the main track, all three metrics (ie runtime, FLOPs, and\nparameter count) were considered. The ranking of the main track is calculated\nbased on a weighted sum-up of the scores of all other sub-tracks. In sub-track\n1, the practical runtime performance of the submissions was evaluated, and the\ncorresponding score was used to determine the ranking. In sub-track 2, the\nnumber of FLOPs was considered. The score calculated based on the corresponding\nFLOPs was used to determine the ranking. In sub-track 3, the number of\nparameters was considered. The score calculated based on the corresponding\nparameters was used to determine the ranking. RLFN is set as the baseline for\nefficiency measurement. The challenge had 262 registered participants, and 34\nteams made valid submissions. They gauge the state-of-the-art in efficient\nsingle-image super-resolution. To facilitate the reproducibility of the\nchallenge and enable other researchers to build upon these findings, the code\nand the pre-trained model of validated solutions are made publicly available at\nhttps://github.com/Amazingren/NTIRE2024_ESR/.\n","authors":["Bin Ren","Yawei Li","Nancy Mehta","Radu Timofte","Hongyuan Yu","Cheng Wan","Yuxin Hong","Bingnan Han","Zhuoyuan Wu","Yajun Zou","Yuqing Liu","Jizhe Li","Keji He","Chao Fan","Heng Zhang","Xiaolin Zhang","Xuanwu Yin","Kunlong Zuo","Bohao Liao","Peizhe Xia","Long Peng","Zhibo Du","Xin Di","Wangkai Li","Yang Wang","Wei Zhai","Renjing Pei","Jiaming Guo","Songcen Xu","Yang Cao","Zhengjun Zha","Yan Wang","Yi Liu","Qing Wang","Gang Zhang","Liou Zhang","Shijie Zhao","Long Sun","Jinshan Pan","Jiangxin Dong","Jinhui Tang","Xin Liu","Min Yan","Qian Wang","Menghan Zhou","Yiqiang Yan","Yixuan Liu","Wensong Chan","Dehua Tang","Dong Zhou","Li Wang","Lu Tian","Barsoum Emad","Bohan Jia","Junbo Qiao","Yunshuai Zhou","Yun Zhang","Wei Li","Shaohui Lin","Shenglong Zhou","Binbin Chen","Jincheng Liao","Suiyi Zhao","Zhao Zhang","Bo Wang","Yan Luo","Yanyan Wei","Feng Li","Mingshen Wang","Yawei Li","Jinhan Guan","Dehua Hu","Jiawei Yu","Qisheng Xu","Tao Sun","Long Lan","Kele Xu","Xin Lin","Jingtong Yue","Lehan Yang","Shiyi Du","Lu Qi","Chao Ren","Zeyu Han","Yuhan Wang","Chaolin Chen","Haobo Li","Mingjun Zheng","Zhongbao Yang","Lianhong Song","Xingzhuo Yan","Minghan Fu","Jingyi Zhang","Baiang Li","Qi Zhu","Xiaogang Xu","Dan Guo","Chunle Guo","Jiadi Chen","Huanhuan Long","Chunjiang Duanmu","Xiaoyan Lei","Jie Liu","Weilin Jia","Weifeng Cao","Wenlong Zhang","Yanyu Mao","Ruilong Guo","Nihao Zhang","Qian Wang","Manoj Pandey","Maksym Chernozhukov","Giang Le","Shuli Cheng","Hongyuan Wang","Ziyan Wei","Qingting Tang","Liejun Wang","Yongming Li","Yanhui Guo","Hao Xu","Akram Khatami-Rizi","Ahmad Mahmoudi-Aznaveh","Chih-Chung Hsu","Chia-Ming Lee","Yi-Shiuan Chou","Amogh Joshi","Nikhil Akalwadi","Sampada Malagi","Palani Yashaswini","Chaitra Desai","Ramesh Ashok Tabib","Ujwala Patil","Uma Mudenagudi"],"pdf_url":"https://arxiv.org/pdf/2404.10343v1.pdf","comment":"The report paper of NTIRE2024 Efficient Super-resolution, accepted by\n  CVPRW2024"},{"id":"http://arxiv.org/abs/2404.10342v1","updated":"2024-04-16T07:25:17Z","published":"2024-04-16T07:25:17Z","title":"Referring Flexible Image Restoration","summary":"  In reality, images often exhibit multiple degradations, such as rain and fog\nat night (triple degradations). However, in many cases, individuals may not\nwant to remove all degradations, for instance, a blurry lens revealing a\nbeautiful snowy landscape (double degradations). In such scenarios, people may\nonly desire to deblur. These situations and requirements shed light on a new\nchallenge in image restoration, where a model must perceive and remove specific\ndegradation types specified by human commands in images with multiple\ndegradations. We term this task Referring Flexible Image Restoration (RFIR). To\naddress this, we first construct a large-scale synthetic dataset called RFIR,\ncomprising 153,423 samples with the degraded image, text prompt for specific\ndegradation removal and restored image. RFIR consists of five basic degradation\ntypes: blur, rain, haze, low light and snow while six main sub-categories are\nincluded for varying degrees of degradation removal. To tackle the challenge,\nwe propose a novel transformer-based multi-task model named TransRFIR, which\nsimultaneously perceives degradation types in the degraded image and removes\nspecific degradation upon text prompt. TransRFIR is based on two devised\nattention modules, Multi-Head Agent Self-Attention (MHASA) and Multi-Head Agent\nCross Attention (MHACA), where MHASA and MHACA introduce the agent token and\nreach the linear complexity, achieving lower computation cost than vanilla\nself-attention and cross-attention and obtaining competitive performances. Our\nTransRFIR achieves state-of-the-art performances compared with other\ncounterparts and is proven as an effective architecture for image restoration.\nWe release our project at https://github.com/GuanRunwei/FIR-CP.\n","authors":["Runwei Guan","Rongsheng Hu","Zhuhao Zhou","Tianlang Xue","Ka Lok Man","Jeremy Smith","Eng Gee Lim","Weiping Ding","Yutao Yue"],"pdf_url":"https://arxiv.org/pdf/2404.10342v1.pdf","comment":"15 pages, 19 figures"},{"id":"http://arxiv.org/abs/2404.10335v1","updated":"2024-04-16T07:19:52Z","published":"2024-04-16T07:19:52Z","title":"Efficiently Adversarial Examples Generation for Visual-Language Models\n  under Targeted Transfer Scenarios using Diffusion Models","summary":"  Targeted transfer-based attacks involving adversarial examples pose a\nsignificant threat to large visual-language models (VLMs). However, the\nstate-of-the-art (SOTA) transfer-based attacks incur high costs due to\nexcessive iteration counts. Furthermore, the generated adversarial examples\nexhibit pronounced adversarial noise and demonstrate limited efficacy in\nevading defense methods such as DiffPure. To address these issues, inspired by\nscore matching, we introduce AdvDiffVLM, which utilizes diffusion models to\ngenerate natural, unrestricted adversarial examples. Specifically, AdvDiffVLM\nemploys Adaptive Ensemble Gradient Estimation to modify the score during the\ndiffusion model's reverse generation process, ensuring the adversarial examples\nproduced contain natural adversarial semantics and thus possess enhanced\ntransferability. Simultaneously, to enhance the quality of adversarial examples\nfurther, we employ the GradCAM-guided Mask method to disperse adversarial\nsemantics throughout the image, rather than concentrating them in a specific\narea. Experimental results demonstrate that our method achieves a speedup\nranging from 10X to 30X compared to existing transfer-based attack methods,\nwhile maintaining superior quality of adversarial examples. Additionally, the\ngenerated adversarial examples possess strong transferability and exhibit\nincreased robustness against adversarial defense methods. Notably, AdvDiffVLM\ncan successfully attack commercial VLMs, including GPT-4V, in a black-box\nmanner.\n","authors":["Qi Guo","Shanmin Pang","Xiaojun Jia","Qing Guo"],"pdf_url":"https://arxiv.org/pdf/2404.10335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10332v1","updated":"2024-04-16T07:14:32Z","published":"2024-04-16T07:14:32Z","title":"Prescribing the Right Remedy: Mitigating Hallucinations in Large\n  Vision-Language Models via Targeted Instruction Tuning","summary":"  Despite achieving outstanding performance on various cross-modal tasks,\ncurrent large vision-language models (LVLMs) still suffer from hallucination\nissues, manifesting as inconsistencies between their generated responses and\nthe corresponding images. Prior research has implicated that the low quality of\ninstruction data, particularly the skewed balance between positive and negative\nsamples, is a significant contributor to model hallucinations. Recently,\nresearchers have proposed high-quality instruction datasets, such as\nLRV-Instruction, to mitigate model hallucination. Nonetheless, our\ninvestigation reveals that hallucinatory concepts from different LVLMs exhibit\nspecificity, i.e. the distribution of hallucinatory concepts varies\nsignificantly across models. Existing datasets did not consider the\nhallucination specificity of different models in the design processes, thereby\ndiminishing their efficacy in mitigating model hallucination. In this paper, we\npropose a targeted instruction data generation framework named DFTG that\ntailored to the hallucination specificity of different models. Concretely, DFTG\nconsists of two stages: hallucination diagnosis, which extracts the necessary\ninformation from the model's responses and images for hallucination diagnosis;\nand targeted data generation, which generates targeted instruction data based\non diagnostic results. The experimental results on hallucination benchmarks\ndemonstrate that the targeted instruction data generated by our method are more\neffective in mitigating hallucinations compared to previous datasets.\n","authors":["Rui Hu","Yahan Tu","Jitao Sang"],"pdf_url":"https://arxiv.org/pdf/2404.10332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10322v1","updated":"2024-04-16T07:07:40Z","published":"2024-04-16T07:07:40Z","title":"Domain-Rectifying Adapter for Cross-Domain Few-Shot Segmentation","summary":"  Few-shot semantic segmentation (FSS) has achieved great success on segmenting\nobjects of novel classes, supported by only a few annotated samples. However,\nexisting FSS methods often underperform in the presence of domain shifts,\nespecially when encountering new domain styles that are unseen during training.\nIt is suboptimal to directly adapt or generalize the entire model to new\ndomains in the few-shot scenario. Instead, our key idea is to adapt a small\nadapter for rectifying diverse target domain styles to the source domain.\nConsequently, the rectified target domain features can fittingly benefit from\nthe well-optimized source domain segmentation model, which is intently trained\non sufficient source domain data. Training domain-rectifying adapter requires\nsufficiently diverse target domains. We thus propose a novel local-global style\nperturbation method to simulate diverse potential target domains by\nperturbating the feature channel statistics of the individual images and\ncollective statistics of the entire source domain, respectively. Additionally,\nwe propose a cyclic domain alignment module to facilitate the adapter\neffectively rectifying domains using a reverse domain rectification\nsupervision. The adapter is trained to rectify the image features from diverse\nsynthesized target domains to align with the source domain. During testing on\ntarget domains, we start by rectifying the image features and then conduct\nfew-shot segmentation on the domain-rectified features. Extensive experiments\ndemonstrate the effectiveness of our method, achieving promising results on\ncross-domain few-shot semantic segmentation tasks. Our code is available at\nhttps://github.com/Matt-Su/DR-Adapter.\n","authors":["Jiapeng Su","Qi Fan","Guangming Lu","Fanglin Chen","Wenjie Pei"],"pdf_url":"https://arxiv.org/pdf/2404.10322v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2404.10319v1","updated":"2024-04-16T06:59:26Z","published":"2024-04-16T06:59:26Z","title":"Application of Deep Learning Methods to Processing of Noisy Medical\n  Video Data","summary":"  Cells count become a challenging problem when the cells move in a continuous\nstream, and their boundaries are difficult for visual detection. To resolve\nthis problem we modified the training and decision making processes using\ncurriculum learning and multi-view predictions techniques, respectively.\n","authors":["Danil Afonchikov","Elena Kornaeva","Irina Makovik","Alexey Kornaev"],"pdf_url":"https://arxiv.org/pdf/2404.10319v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10318v1","updated":"2024-04-16T06:58:30Z","published":"2024-04-16T06:58:30Z","title":"SRGS: Super-Resolution 3D Gaussian Splatting","summary":"  Recently, 3D Gaussian Splatting (3DGS) has gained popularity as a novel\nexplicit 3D representation. This approach relies on the representation power of\nGaussian primitives to provide a high-quality rendering. However, primitives\noptimized at low resolution inevitably exhibit sparsity and texture deficiency,\nposing a challenge for achieving high-resolution novel view synthesis (HRNVS).\nTo address this problem, we propose Super-Resolution 3D Gaussian Splatting\n(SRGS) to perform the optimization in a high-resolution (HR) space. The\nsub-pixel constraint is introduced for the increased viewpoints in HR space,\nexploiting the sub-pixel cross-view information of the multiple low-resolution\n(LR) views. The gradient accumulated from more viewpoints will facilitate the\ndensification of primitives. Furthermore, a pre-trained 2D super-resolution\nmodel is integrated with the sub-pixel constraint, enabling these dense\nprimitives to learn faithful texture features. In general, our method focuses\non densification and texture learning to effectively enhance the representation\nability of primitives. Experimentally, our method achieves high rendering\nquality on HRNVS only with LR inputs, outperforming state-of-the-art methods on\nchallenging datasets such as Mip-NeRF 360 and Tanks & Temples. Related codes\nwill be released upon acceptance.\n","authors":["Xiang Feng","Yongbo He","Yubo Wang","Yan Yang","Zhenzhong Kuang","Yu Jun","Jianping Fan","Jiajun ding"],"pdf_url":"https://arxiv.org/pdf/2404.10318v1.pdf","comment":"submit ACM MM 2024"},{"id":"http://arxiv.org/abs/2404.10314v1","updated":"2024-04-16T06:40:51Z","published":"2024-04-16T06:40:51Z","title":"Awareness of uncertainty in classification using a multivariate model\n  and multi-views","summary":"  One of the ways to make artificial intelligence more natural is to give it\nsome room for doubt. Two main questions should be resolved in that way. First,\nhow to train a model to estimate uncertainties of its own predictions? And\nthen, what to do with the uncertain predictions if they appear? First, we\nproposed an uncertainty-aware negative log-likelihood loss for the case of\nN-dimensional multivariate normal distribution with spherical variance matrix\nto the solution of N-classes classification tasks. The loss is similar to the\nheteroscedastic regression loss. The proposed model regularizes uncertain\npredictions, and trains to calculate both the predictions and their uncertainty\nestimations. The model fits well with the label smoothing technique. Second, we\nexpanded the limits of data augmentation at the training and test stages, and\nmade the trained model to give multiple predictions for a given number of\naugmented versions of each test sample. Given the multi-view predictions\ntogether with their uncertainties and confidences, we proposed several methods\nto calculate final predictions, including mode values and bin counts with soft\nand hard weights. For the latter method, we formalized the model tuning task in\nthe form of multimodal optimization with non-differentiable criteria of maximum\naccuracy, and applied particle swarm optimization to solve the tuning task. The\nproposed methodology was tested using CIFAR-10 dataset with clean and noisy\nlabels and demonstrated good results in comparison with other uncertainty\nestimation methods related to sample selection, co-teaching, and label\nsmoothing.\n","authors":["Alexey Kornaev","Elena Kornaeva","Oleg Ivanov","Ilya Pershin","Danis Alukaev"],"pdf_url":"https://arxiv.org/pdf/2404.10314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10312v1","updated":"2024-04-16T06:39:37Z","published":"2024-04-16T06:39:37Z","title":"OmniSSR: Zero-shot Omnidirectional Image Super-Resolution using Stable\n  Diffusion Model","summary":"  Omnidirectional images (ODIs) are commonly used in real-world visual tasks,\nand high-resolution ODIs help improve the performance of related visual tasks.\nMost existing super-resolution methods for ODIs use end-to-end learning\nstrategies, resulting in inferior realness of generated images and a lack of\neffective out-of-domain generalization capabilities in training methods. Image\ngeneration methods represented by diffusion model provide strong priors for\nvisual tasks and have been proven to be effectively applied to image\nrestoration tasks. Leveraging the image priors of the Stable Diffusion (SD)\nmodel, we achieve omnidirectional image super-resolution with both fidelity and\nrealness, dubbed as OmniSSR. Firstly, we transform the equirectangular\nprojection (ERP) images into tangent projection (TP) images, whose distribution\napproximates the planar image domain. Then, we use SD to iteratively sample\ninitial high-resolution results. At each denoising iteration, we further\ncorrect and update the initial results using the proposed Octadecaplex Tangent\nInformation Interaction (OTII) and Gradient Decomposition (GD) technique to\nensure better consistency. Finally, the TP images are transformed back to\nobtain the final high-resolution results. Our method is zero-shot, requiring no\ntraining or fine-tuning. Experiments of our method on two benchmark datasets\ndemonstrate the effectiveness of our proposed method.\n","authors":["Runyi Li","Xuhan Sheng","Weiqi Li","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.10312v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.15406v2","updated":"2024-04-16T06:33:09Z","published":"2023-12-24T04:49:06Z","title":"Objects as volumes: A stochastic geometry view of opaque solids","summary":"  We develop a theory for the representation of opaque solids as volumes.\nStarting from a stochastic representation of opaque solids as random indicator\nfunctions, we prove the conditions under which such solids can be modeled using\nexponential volumetric transport. We also derive expressions for the volumetric\nattenuation coefficient as a functional of the probability distributions of the\nunderlying indicator functions. We generalize our theory to account for\nisotropic and anisotropic scattering at different parts of the solid, and for\nrepresentations of opaque solids as stochastic implicit surfaces. We derive our\nvolumetric representation from first principles, which ensures that it\nsatisfies physical constraints such as reciprocity and reversibility. We use\nour theory to explain, compare, and correct previous volumetric\nrepresentations, as well as propose meaningful extensions that lead to improved\nperformance in 3D reconstruction tasks.\n","authors":["Bailey Miller","Hanyu Chen","Alice Lai","Ioannis Gkioulekas"],"pdf_url":"https://arxiv.org/pdf/2312.15406v2.pdf","comment":"project page: https://imaging.cs.cmu.edu/volumetric_opaque_solids"},{"id":"http://arxiv.org/abs/2404.10307v1","updated":"2024-04-16T06:33:08Z","published":"2024-04-16T06:33:08Z","title":"Learnable Prompt for Few-Shot Semantic Segmentation in Remote Sensing\n  Domain","summary":"  Few-shot segmentation is a task to segment objects or regions of novel\nclasses within an image given only a few annotated examples. In the generalized\nsetting, the task extends to segment both the base and the novel classes. The\nmain challenge is how to train the model such that the addition of novel\nclasses does not hurt the base classes performance, also known as catastrophic\nforgetting. To mitigate this issue, we use SegGPT as our base model and train\nit on the base classes. Then, we use separate learnable prompts to handle\npredictions for each novel class. To handle various object sizes which\ntypically present in remote sensing domain, we perform patch-based prediction.\nTo address the discontinuities along patch boundaries, we propose a\npatch-and-stitch technique by re-framing the problem as an image inpainting\ntask. During inference, we also utilize image similarity search over image\nembeddings for prompt selection and novel class filtering to reduce false\npositive predictions. Based on our experiments, our proposed method boosts the\nweighted mIoU of a simple fine-tuned SegGPT from 15.96 to 35.08 on the\nvalidation set of few-shot OpenEarthMap dataset given in the challenge.\n","authors":["Steve Andreas Immanuel","Hagai Raja Sinulingga"],"pdf_url":"https://arxiv.org/pdf/2404.10307v1.pdf","comment":"Accepted to CVPRW 2024"},{"id":"http://arxiv.org/abs/2303.16242v4","updated":"2024-04-16T06:26:46Z","published":"2023-03-28T18:36:19Z","title":"CuNeRF: Cube-Based Neural Radiance Field for Zero-Shot Medical Image\n  Arbitrary-Scale Super Resolution","summary":"  Medical image arbitrary-scale super-resolution (MIASSR) has recently gained\nwidespread attention, aiming to super sample medical volumes at arbitrary\nscales via a single model. However, existing MIASSR methods face two major\nlimitations: (i) reliance on high-resolution (HR) volumes and (ii) limited\ngeneralization ability, which restricts their application in various scenarios.\nTo overcome these limitations, we propose Cube-based Neural Radiance Field\n(CuNeRF), a zero-shot MIASSR framework that can yield medical images at\narbitrary scales and viewpoints in a continuous domain. Unlike existing MIASSR\nmethods that fit the mapping between low-resolution (LR) and HR volumes, CuNeRF\nfocuses on building a coordinate-intensity continuous representation from LR\nvolumes without the need for HR references. This is achieved by the proposed\ndifferentiable modules: including cube-based sampling, isotropic volume\nrendering, and cube-based hierarchical rendering. Through extensive experiments\non magnetic resource imaging (MRI) and computed tomography (CT) modalities, we\ndemonstrate that CuNeRF outperforms state-of-the-art MIASSR methods. CuNeRF\nyields better visual verisimilitude and reduces aliasing artifacts at various\nupsampling factors. Moreover, our CuNeRF does not need any LR-HR training\npairs, which is more flexible and easier to be used than others. Our code is\nreleased at https://github.com/NarcissusEx/CuNeRF.\n","authors":["Zixuan Chen","Jian-Huang Lai","Lingxiao Yang","Xiaohua Xie"],"pdf_url":"https://arxiv.org/pdf/2303.16242v4.pdf","comment":"This paper is accepted by the International Conference on Computer\n  Vision (ICCV) 2023"},{"id":"http://arxiv.org/abs/2404.10305v1","updated":"2024-04-16T06:24:53Z","published":"2024-04-16T06:24:53Z","title":"TC-OCR: TableCraft OCR for Efficient Detection & Recognition of Table\n  Structure & Content","summary":"  The automatic recognition of tabular data in document images presents a\nsignificant challenge due to the diverse range of table styles and complex\nstructures. Tables offer valuable content representation, enhancing the\npredictive capabilities of various systems such as search engines and Knowledge\nGraphs. Addressing the two main problems, namely table detection (TD) and table\nstructure recognition (TSR), has traditionally been approached independently.\nIn this research, we propose an end-to-end pipeline that integrates deep\nlearning models, including DETR, CascadeTabNet, and PP OCR v2, to achieve\ncomprehensive image-based table recognition. This integrated approach\neffectively handles diverse table styles, complex structures, and image\ndistortions, resulting in improved accuracy and efficiency compared to existing\nmethods like Table Transformers. Our system achieves simultaneous table\ndetection (TD), table structure recognition (TSR), and table content\nrecognition (TCR), preserving table structures and accurately extracting\ntabular data from document images. The integration of multiple models addresses\nthe intricacies of table recognition, making our approach a promising solution\nfor image-based table understanding, data extraction, and information retrieval\napplications. Our proposed approach achieves an IOU of 0.96 and an OCR Accuracy\nof 78%, showcasing a remarkable improvement of approximately 25% in the OCR\nAccuracy compared to the previous Table Transformer approach.\n","authors":["Avinash Anand","Raj Jaiswal","Pijush Bhuyan","Mohit Gupta","Siddhesh Bangar","Md. Modassir Imam","Rajiv Ratn Shah","Shin'ichi Satoh"],"pdf_url":"https://arxiv.org/pdf/2404.10305v1.pdf","comment":"8 pages, 2 figures, Workshop of 1st MMIR Deep Multimodal Learning for\n  Information Retrieval"},{"id":"http://arxiv.org/abs/2404.09406v2","updated":"2024-04-16T05:58:39Z","published":"2024-04-15T01:47:44Z","title":"Human-in-the-Loop Segmentation of Multi-species Coral Imagery","summary":"  Broad-scale marine surveys performed by underwater vehicles significantly\nincrease the availability of coral reef imagery, however it is costly and\ntime-consuming for domain experts to label images. Point label propagation is\nan approach used to leverage existing image data labeled with sparse point\nlabels. The resulting augmented ground truth generated is then used to train a\nsemantic segmentation model. Here, we first demonstrate that recent advances in\nfoundation models enable generation of multi-species coral augmented ground\ntruth masks using denoised DINOv2 features and K-Nearest Neighbors (KNN),\nwithout the need for any pre-training or custom-designed algorithms. For\nextremely sparsely labeled images, we propose a labeling regime based on\nhuman-in-the-loop principles, resulting in significant improvement in\nannotation efficiency: If only 5 point labels per image are available, our\nproposed human-in-the-loop approach improves on the state-of-the-art by 17.3%\nfor pixel accuracy and 22.6% for mIoU; and by 10.6% and 19.1% when 10 point\nlabels per image are available. Even if the human-in-the-loop labeling regime\nis not used, the denoised DINOv2 features with a KNN outperforms the prior\nstate-of-the-art by 3.5% for pixel accuracy and 5.7% for mIoU (5 grid points).\nWe also provide a detailed analysis of how point labeling style and the\nquantity of points per image affects the point label propagation quality and\nprovide general recommendations on maximizing point label efficiency.\n","authors":["Scarlett Raine","Ross Marchant","Brano Kusy","Frederic Maire","Niko Suenderhauf","Tobias Fischer"],"pdf_url":"https://arxiv.org/pdf/2404.09406v2.pdf","comment":"Accepted at the CVPR2024 3rd Workshop on Learning with Limited\n  Labelled Data for Image and Video Understanding (L3D-IVU), 10 pages, 6\n  figures, an additional 4 pages of supplementary material"},{"id":"http://arxiv.org/abs/2404.10292v1","updated":"2024-04-16T05:29:14Z","published":"2024-04-16T05:29:14Z","title":"From Data Deluge to Data Curation: A Filtering-WoRA Paradigm for\n  Efficient Text-based Person Search","summary":"  In text-based person search endeavors, data generation has emerged as a\nprevailing practice, addressing concerns over privacy preservation and the\narduous task of manual annotation. Although the number of synthesized data can\nbe infinite in theory, the scientific conundrum persists that how much\ngenerated data optimally fuels subsequent model training. We observe that only\na subset of the data in these constructed datasets plays a decisive role.\nTherefore, we introduce a new Filtering-WoRA paradigm, which contains a\nfiltering algorithm to identify this crucial data subset and WoRA (Weighted\nLow-Rank Adaptation) learning strategy for light fine-tuning. The filtering\nalgorithm is based on the cross-modality relevance to remove the lots of coarse\nmatching synthesis pairs. As the number of data decreases, we do not need to\nfine-tune the entire model. Therefore, we propose a WoRA learning strategy to\nefficiently update a minimal portion of model parameters. WoRA streamlines the\nlearning process, enabling heightened efficiency in extracting knowledge from\nfewer, yet potent, data instances. Extensive experimentation validates the\nefficacy of pretraining, where our model achieves advanced and efficient\nretrieval performance on challenging real-world benchmarks. Notably, on the\nCUHK-PEDES dataset, we have achieved a competitive mAP of 67.02% while reducing\nmodel training time by 19.82%.\n","authors":["Jintao Sun","Zhedong Zheng","Gangyi Ding"],"pdf_url":"https://arxiv.org/pdf/2404.10292v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10290v1","updated":"2024-04-16T05:28:07Z","published":"2024-04-16T05:28:07Z","title":"NeuroMorphix: A Novel Brain MRI Asymmetry-specific Feature Construction\n  Approach For Seizure Recurrence Prediction","summary":"  Seizure recurrence is an important concern after an initial unprovoked\nseizure; without drug treatment, it occurs within 2 years in 40-50% of cases.\nThe decision to treat currently relies on predictors of seizure recurrence risk\nthat are inaccurate, resulting in unnecessary, possibly harmful, treatment in\nsome patients and potentially preventable seizures in others. Because of the\nlink between brain lesions and seizure recurrence, we developed a recurrence\nprediction tool using machine learning and clinical 3T brain MRI. We developed\nNeuroMorphix, a feature construction approach based on MRI brain anatomy. Each\nof seven NeuroMorphix features measures the absolute or relative difference\nbetween corresponding regions in each cerebral hemisphere. FreeSurfer was used\nto segment brain regions and to generate values for morphometric parameters (8\nfor each cortical region and 5 for each subcortical region). The parameters\nwere then mapped to whole brain NeuroMorphix features, yielding a total of 91\nfeatures per subject. Features were generated for a first seizure patient\ncohort (n = 169) categorised into seizure recurrence and non-recurrence\nsubgroups. State-of-the-art classification algorithms were trained and tested\nusing NeuroMorphix features to predict seizure recurrence. Classification\nmodels using the top 5 features, ranked by sequential forward selection,\ndemonstrated excellent performance in predicting seizure recurrence, with area\nunder the ROC curve of 88-93%, accuracy of 83-89%, and F1 score of 83-90%.\nHighly ranked features aligned with structural alterations known to be\nassociated with epilepsy. This study highlights the potential for targeted,\ndata-driven approaches to aid clinical decision-making in brain disorders.\n","authors":["Soumen Ghosh","Viktor Vegh","Shahrzad Moinian","Hamed Moradi","Alice-Ann Sullivan","John Phamnguyen","David Reutens"],"pdf_url":"https://arxiv.org/pdf/2404.10290v1.pdf","comment":"This work has been submitted to the IEEE TMI for possible publication"},{"id":"http://arxiv.org/abs/2404.10282v1","updated":"2024-04-16T04:52:41Z","published":"2024-04-16T04:52:41Z","title":"Tripod: Three Complementary Inductive Biases for Disentangled\n  Representation Learning","summary":"  Inductive biases are crucial in disentangled representation learning for\nnarrowing down an underspecified solution set. In this work, we consider\nendowing a neural network autoencoder with three select inductive biases from\nthe literature: data compression into a grid-like latent space via\nquantization, collective independence amongst latents, and minimal functional\ninfluence of any latent on how other latents determine data generation. In\nprinciple, these inductive biases are deeply complementary: they most directly\nspecify properties of the latent space, encoder, and decoder, respectively. In\npractice, however, naively combining existing techniques instantiating these\ninductive biases fails to yield significant benefits. To address this, we\npropose adaptations to the three techniques that simplify the learning problem,\nequip key regularization terms with stabilizing invariances, and quash\ndegenerate incentives. The resulting model, Tripod, achieves state-of-the-art\nresults on a suite of four image disentanglement benchmarks. We also verify\nthat Tripod significantly improves upon its naive incarnation and that all\nthree of its \"legs\" are necessary for best performance.\n","authors":["Kyle Hsu","Jubayer Ibn Hamid","Kaylee Burns","Chelsea Finn","Jiajun Wu"],"pdf_url":"https://arxiv.org/pdf/2404.10282v1.pdf","comment":"22 pages, 10 figures, code available at\n  https://github.com/kylehkhsu/tripod"},{"id":"http://arxiv.org/abs/2404.10279v1","updated":"2024-04-16T04:44:16Z","published":"2024-04-16T04:44:16Z","title":"EucliDreamer: Fast and High-Quality Texturing for 3D Models with\n  Depth-Conditioned Stable Diffusion","summary":"  We present EucliDreamer, a simple and effective method to generate textures\nfor 3D models given text prompts and meshes. The texture is parametrized as an\nimplicit function on the 3D surface, which is optimized with the Score\nDistillation Sampling (SDS) process and differentiable rendering. To generate\nhigh-quality textures, we leverage a depth-conditioned Stable Diffusion model\nguided by the depth image rendered from the mesh. We test our approach on 3D\nmodels in Objaverse and conducted a user study, which shows its superior\nquality compared to existing texturing methods like Text2Tex. In addition, our\nmethod converges 2 times faster than DreamFusion. Through text prompting,\ntextures of diverse art styles can be produced. We hope Euclidreamer proides a\nviable solution to automate a labor-intensive stage in 3D content creation.\n","authors":["Cindy Le","Congrui Hetang","Chendi Lin","Ang Cao","Yihui He"],"pdf_url":"https://arxiv.org/pdf/2404.10279v1.pdf","comment":"Short version of arXiv:2311.15573"},{"id":"http://arxiv.org/abs/2403.14987v2","updated":"2024-04-16T04:15:32Z","published":"2024-03-22T06:45:45Z","title":"Generative Active Learning for Image Synthesis Personalization","summary":"  This paper presents a pilot study that explores the application of active\nlearning, traditionally studied in the context of discriminative models, to\ngenerative models. We specifically focus on image synthesis personalization\ntasks. The primary challenge in conducting active learning on generative models\nlies in the open-ended nature of querying, which differs from the closed form\nof querying in discriminative models that typically target a single concept. We\nintroduce the concept of anchor directions to transform the querying process\ninto a semi-open problem. We propose a direction-based uncertainty sampling\nstrategy to enable generative active learning and tackle the\nexploitation-exploration dilemma. Extensive experiments are conducted to\nvalidate the effectiveness of our approach, demonstrating that an open-source\nmodel can achieve superior performance compared to closed-source models\ndeveloped by large companies, such as Google's StyleDrop. The source code is\navailable at https://github.com/zhangxulu1996/GAL4Personalization.\n","authors":["Xulu Zhang","Wengyu Zhang","Xiao-Yong Wei","Jinlin Wu","Zhaoxiang Zhang","Zhen Lei","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2403.14987v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10272v1","updated":"2024-04-16T04:05:33Z","published":"2024-04-16T04:05:33Z","title":"Plug-and-Play Acceleration of Occupancy Grid-based NeRF Rendering using\n  VDB Grid and Hierarchical Ray Traversal","summary":"  Transmittance estimators such as Occupancy Grid (OG) can accelerate the\ntraining and rendering of Neural Radiance Field (NeRF) by predicting important\nsamples that contributes much to the generated image. However, OG manages\noccupied regions in the form of the dense binary grid, in which there are many\nblocks with the same values that cause redundant examination of voxels'\nemptiness in ray-tracing. In our work, we introduce two techniques to improve\nthe efficiency of ray-tracing in trained OG without fine-tuning. First, we\nreplace the dense grids with VDB grids to reduce the spatial redundancy.\nSecond, we use hierarchical digital differential analyzer (HDDA) to efficiently\ntrace voxels in the VDB grids. Our experiments on NeRF-Synthetic and Mip-NeRF\n360 datasets show that our proposed method successfully accelerates rendering\nNeRF-Synthetic dataset by 12% in average and Mip-NeRF 360 dataset by 4% in\naverage, compared to a fast implementation of OG, NerfAcc, without losing the\nquality of rendered images.\n","authors":["Yoshio Kato","Shuhei Tarashima"],"pdf_url":"https://arxiv.org/pdf/2404.10272v1.pdf","comment":"Short paper for CVPR Neural Rendering Intelligence Workshop 2024.\n  Code: https://github.com/Yosshi999/faster-occgrid"},{"id":"http://arxiv.org/abs/2305.00635v2","updated":"2024-04-16T03:46:03Z","published":"2023-05-01T02:51:38Z","title":"Learning Self-Prior for Mesh Inpainting Using Self-Supervised Graph\n  Convolutional Networks","summary":"  In this paper, we present a self-prior-based mesh inpainting framework that\nrequires only an incomplete mesh as input, without the need for any training\ndatasets. Additionally, our method maintains the polygonal mesh format\nthroughout the inpainting process without converting the shape format to an\nintermediate one, such as a voxel grid, a point cloud, or an implicit function,\nwhich are typically considered easier for deep neural networks to process. To\nachieve this goal, we introduce two graph convolutional networks (GCNs):\nsingle-resolution GCN (SGCN) and multi-resolution GCN (MGCN), both trained in a\nself-supervised manner. Our approach refines a watertight mesh obtained from\nthe initial hole filling to generate a complete output mesh. Specifically, we\ntrain the GCNs to deform an oversmoothed version of the input mesh into the\nexpected complete shape. The deformation is described by vertex displacements,\nand the GCNs are supervised to obtain accurate displacements at vertices in\nreal holes. To this end, we specify several connected regions of the mesh as\nfake holes, thereby generating meshes with various sets of fake holes. The\ncorrect displacements of vertices are known in these fake holes, thus enabling\ntraining GCNs with loss functions that assess the accuracy of vertex\ndisplacements. We demonstrate that our method outperforms traditional\ndataset-independent approaches and exhibits greater robustness compared with\nother deep-learning-based methods for shapes that infrequently appear in shape\ndatasets. Our code and test data are available at\nhttps://github.com/astaka-pe/SeMIGCN.\n","authors":["Shota Hattori","Tatsuya Yatagawa","Yutaka Ohtake","Hiromasa Suzuki"],"pdf_url":"https://arxiv.org/pdf/2305.00635v2.pdf","comment":"18 pages, 18 figures, 8 tables"},{"id":"http://arxiv.org/abs/2404.10267v1","updated":"2024-04-16T03:45:45Z","published":"2024-04-16T03:45:45Z","title":"OneActor: Consistent Character Generation via Cluster-Conditioned\n  Guidance","summary":"  Text-to-image diffusion models benefit artists with high-quality image\ngeneration. Yet its stochastic nature prevent artists from creating consistent\nimages of the same character. Existing methods try to tackle this challenge and\ngenerate consistent content in various ways. However, they either depend on\nexternal data or require expensive tuning of the diffusion model. For this\nissue, we argue that a lightweight but intricate guidance is enough to\nfunction. Aiming at this, we lead the way to formalize the objective of\nconsistent generation, derive a clustering-based score function and propose a\nnovel paradigm, OneActor. We design a cluster-conditioned model which\nincorporates posterior samples to guide the denoising trajectories towards the\ntarget cluster. To overcome the overfitting challenge shared by one-shot tuning\npipelines, we devise auxiliary components to simultaneously augment the tuning\nand regulate the inference. This technique is later verified to significantly\nenhance the content diversity of generated images. Comprehensive experiments\nshow that our method outperforms a variety of baselines with satisfactory\ncharacter consistency, superior prompt conformity as well as high image\nquality. And our method is at least 4 times faster than tuning-based baselines.\nFurthermore, to our best knowledge, we first prove that the semantic space has\nthe same interpolation property as the latent space dose. This property can\nserve as another promising tool for fine generation control.\n","authors":["Jiahao Wang","Caixia Yan","Haonan Lin","Weizhan Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.10267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13127v4","updated":"2024-04-16T03:43:43Z","published":"2023-11-22T03:31:31Z","title":"MetaCloak: Preventing Unauthorized Subject-driven Text-to-image\n  Diffusion-based Synthesis via Meta-learning","summary":"  Text-to-image diffusion models allow seamless generation of personalized\nimages from scant reference photos. Yet, these tools, in the wrong hands, can\nfabricate misleading or harmful content, endangering individuals. To address\nthis problem, existing poisoning-based approaches perturb user images in an\nimperceptible way to render them \"unlearnable\" from malicious uses. We identify\ntwo limitations of these defending approaches: i) sub-optimal due to the\nhand-crafted heuristics for solving the intractable bilevel optimization and\nii) lack of robustness against simple data transformations like Gaussian\nfiltering. To solve these challenges, we propose MetaCloak, which solves the\nbi-level poisoning problem with a meta-learning framework with an additional\ntransformation sampling process to craft transferable and robust perturbation.\nSpecifically, we employ a pool of surrogate diffusion models to craft\ntransferable and model-agnostic perturbation. Furthermore, by incorporating an\nadditional transformation process, we design a simple denoising-error\nmaximization loss that is sufficient for causing transformation-robust semantic\ndistortion and degradation in a personalized generation. Extensive experiments\non the VGGFace2 and CelebA-HQ datasets show that MetaCloak outperforms existing\napproaches. Notably, MetaCloak can successfully fool online training services\nlike Replicate, in a black-box manner, demonstrating the effectiveness of\nMetaCloak in real-world scenarios. Our code is available at\nhttps://github.com/liuyixin-louis/MetaCloak.\n","authors":["Yixin Liu","Chenrui Fan","Yutong Dai","Xun Chen","Pan Zhou","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2311.13127v4.pdf","comment":"Accepted to CVPR 2024 (Oral)"},{"id":"http://arxiv.org/abs/2404.09640v2","updated":"2024-04-16T03:43:11Z","published":"2024-04-15T10:19:39Z","title":"CREST: Cross-modal Resonance through Evidential Deep Learning for\n  Enhanced Zero-Shot Learning","summary":"  Zero-shot learning (ZSL) enables the recognition of novel classes by\nleveraging semantic knowledge transfer from known to unknown categories. This\nknowledge, typically encapsulated in attribute descriptions, aids in\nidentifying class-specific visual features, thus facilitating visual-semantic\nalignment and improving ZSL performance. However, real-world challenges such as\ndistribution imbalances and attribute co-occurrence among instances often\nhinder the discernment of local variances in images, a problem exacerbated by\nthe scarcity of fine-grained, region-specific attribute annotations. Moreover,\nthe variability in visual presentation within categories can also skew\nattribute-category associations. In response, we propose a bidirectional\ncross-modal ZSL approach CREST. It begins by extracting representations for\nattribute and visual localization and employs Evidential Deep Learning (EDL) to\nmeasure underlying epistemic uncertainty, thereby enhancing the model's\nresilience against hard negatives. CREST incorporates dual learning pathways,\nfocusing on both visual-category and attribute-category alignments, to ensure\nrobust correlation between latent and observable spaces. Moreover, we introduce\nan uncertainty-informed cross-modal fusion technique to refine visual-attribute\ninference. Extensive experiments demonstrate our model's effectiveness and\nunique explainability across multiple datasets. Our code and data are available\nat: https://github.com/JethroJames/CREST.\n","authors":["Haojian Huang","Xiaozhen Qiao","Zhuo Chen","Haodong Chen","Bingyu Li","Zhe Sun","Mulin Chen","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2404.09640v2.pdf","comment":"Ongoing work; 10 pages, 2 Tables, 9 Figures; Repo is available at:\n  https://github.com/JethroJames/CREST"},{"id":"http://arxiv.org/abs/2404.09378v2","updated":"2024-04-16T03:39:27Z","published":"2024-04-14T23:30:35Z","title":"Orientation-conditioned Facial Texture Mapping for Video-based Facial\n  Remote Photoplethysmography Estimation","summary":"  Camera-based remote photoplethysmography (rPPG) enables contactless\nmeasurement of important physiological signals such as pulse rate (PR).\nHowever, dynamic and unconstrained subject motion introduces significant\nvariability into the facial appearance in video, confounding the ability of\nvideo-based methods to accurately extract the rPPG signal. In this study, we\nleverage the 3D facial surface to construct a novel orientation-conditioned\nfacial texture video representation which improves the motion robustness of\nexisting video-based facial rPPG estimation methods. Our proposed method\nachieves a significant 18.2% performance improvement in cross-dataset testing\non MMPD over our baseline using the PhysNet model trained on PURE, highlighting\nthe efficacy and generalization benefits of our designed video representation.\nWe demonstrate significant performance improvements of up to 29.6% in all\ntested motion scenarios in cross-dataset testing on MMPD, even in the presence\nof dynamic and unconstrained subject motion, emphasizing the benefits of\ndisentangling motion through modeling the 3D facial surface for motion robust\nfacial rPPG estimation. We validate the efficacy of our design decisions and\nthe impact of different video processing steps through an ablation study. Our\nfindings illustrate the potential strengths of exploiting the 3D facial surface\nas a general strategy for addressing dynamic and unconstrained subject motion\nin videos. The code is available at\nhttps://samcantrill.github.io/orientation-uv-rppg/.\n","authors":["Sam Cantrill","David Ahmedt-Aristizabal","Lars Petersson","Hanna Suominen","Mohammad Ali Armin"],"pdf_url":"https://arxiv.org/pdf/2404.09378v2.pdf","comment":"12 pages, 8 figures, 6 tables; corrected abstract typo"},{"id":"http://arxiv.org/abs/2404.00231v2","updated":"2024-04-16T03:38:31Z","published":"2024-03-30T03:23:52Z","title":"Attention-based Shape-Deformation Networks for Artifact-Free Geometry\n  Reconstruction of Lumbar Spine from MR Images","summary":"  Lumbar disc degeneration, a progressive structural wear and tear of lumbar\nintervertebral disc, is regarded as an essential role on low back pain, a\nsignificant global health concern. Automated lumbar spine geometry\nreconstruction from MR images will enable fast measurement of medical\nparameters to evaluate the lumbar status, in order to determine a suitable\ntreatment. Existing image segmentation-based techniques often generate\nerroneous segments or unstructured point clouds, unsuitable for medical\nparameter measurement. In this work, we present TransDeformer: a novel\nattention-based deep learning approach that reconstructs the geometry of the\nlumbar spine with high spatial accuracy and mesh correspondence across\npatients, and we also present a variant of TransDeformer for error estimation.\nSpecially, we devise new attention modules with a new attention formula, which\nintegrate image features and tokenized contour features to predict the\ndisplacements of the points on a shape template without the need for image\nsegmentation. The deformed template reveals the lumbar spine geometry in an\nimage. Experiment results show that our TransDeformer generates artifact-free\ngeometry outputs, and its variant predicts the error of a reconstructed\ngeometry. Our code is available at\nhttps://github.com/linchenq/TransDeformer-Mesh.\n","authors":["Linchen Qian","Jiasong Chen","Linhai Ma","Timur Urakov","Weiyong Gu","Liang Liang"],"pdf_url":"https://arxiv.org/pdf/2404.00231v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10263v1","updated":"2024-04-16T03:34:35Z","published":"2024-04-16T03:34:35Z","title":"PreGSU-A Generalized Traffic Scene Understanding Model for Autonomous\n  Driving based on Pre-trained Graph Attention Network","summary":"  Scene understanding, defined as learning, extraction, and representation of\ninteractions among traffic elements, is one of the critical challenges toward\nhigh-level autonomous driving (AD). Current scene understanding methods mainly\nfocus on one concrete single task, such as trajectory prediction and risk level\nevaluation. Although they perform well on specific metrics, the generalization\nability is insufficient to adapt to the real traffic complexity and downstream\ndemand diversity. In this study, we propose PreGSU, a generalized pre-trained\nscene understanding model based on graph attention network to learn the\nuniversal interaction and reasoning of traffic scenes to support various\ndownstream tasks. After the feature engineering and sub-graph module, all\nelements are embedded as nodes to form a dynamic weighted graph. Then, four\ngraph attention layers are applied to learn the relationships among agents and\nlanes. In the pre-train phase, the understanding model is trained on two\nself-supervised tasks: Virtual Interaction Force (VIF) modeling and Masked Road\nModeling (MRM). Based on the artificial potential field theory, VIF modeling\nenables PreGSU to capture the agent-to-agent interactions while MRM extracts\nagent-to-road connections. In the fine-tuning process, the pre-trained\nparameters are loaded to derive detailed understanding outputs. We conduct\nvalidation experiments on two downstream tasks, i.e., trajectory prediction in\nurban scenario, and intention recognition in highway scenario, to verify the\ngeneralized ability and understanding ability. Results show that compared with\nthe baselines, PreGSU achieves better accuracy on both tasks, indicating the\npotential to be generalized to various scenes and targets. Ablation study shows\nthe effectiveness of pre-train task design.\n","authors":["Yuning Wang","Zhiyuan Liu","Haotian Lin","Junkai Jiang","Shaobing Xu","Jianqiang Wang"],"pdf_url":"https://arxiv.org/pdf/2404.10263v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2404.09301v2","updated":"2024-04-16T03:27:00Z","published":"2024-04-14T16:55:23Z","title":"A Simple Strategy for Body Estimation from Partial-View Images","summary":"  Virtual try-on and product personalization have become increasingly important\nin modern online shopping, highlighting the need for accurate body measurement\nestimation. Although previous research has advanced in estimating 3D body\nshapes from RGB images, the task is inherently ambiguous as the observed scale\nof human subjects in the images depends on two unknown factors: capture\ndistance and body dimensions. This ambiguity is particularly pronounced in\npartial-view scenarios. To address this challenge, we propose a modular and\nsimple height normalization solution. This solution relocates the subject\nskeleton to the desired position, thereby normalizing the scale and\ndisentangling the relationship between the two variables. Our experimental\nresults demonstrate that integrating this technique into state-of-the-art human\nmesh reconstruction models significantly enhances partial body measurement\nestimation. Additionally, we illustrate the applicability of this approach to\nmulti-view settings, showcasing its versatility.\n","authors":["Yafei Mao","Xuelu Li","Brandon Smith","Jinjin Li","Raja Bala"],"pdf_url":"https://arxiv.org/pdf/2404.09301v2.pdf","comment":"Accepted to CVPRW 2024 Computer Vision for Fashion, Art, and Design"},{"id":"http://arxiv.org/abs/2401.15914v2","updated":"2024-04-16T03:25:25Z","published":"2024-01-29T06:57:48Z","title":"Overcoming the Pitfalls of Vision-Language Model Finetuning for OOD\n  Generalization","summary":"  Existing vision-language models exhibit strong generalization on a variety of\nvisual domains and tasks. However, such models mainly perform zero-shot\nrecognition in a closed-set manner, and thus struggle to handle open-domain\nvisual concepts by design. There are recent finetuning methods, such as prompt\nlearning, that not only study the discrimination between in-distribution (ID)\nand out-of-distribution (OOD) samples, but also show some improvements in both\nID and OOD accuracies. In this paper, we first demonstrate that vision-language\nmodels, after long enough finetuning but without proper regularization, tend to\noverfit the known classes in the given dataset, with degraded performance on\nunknown classes. Then we propose a novel approach OGEN to address this pitfall,\nwith the main focus on improving the OOD GENeralization of finetuned models.\nSpecifically, a class-conditional feature generator is introduced to synthesize\nOOD features using just the class name of any unknown class. Such synthesized\nfeatures will provide useful knowledge about unknowns and help regularize the\ndecision boundary between ID and OOD data when optimized jointly. Equally\nimportant is our adaptive self-distillation mechanism to regularize our feature\ngeneration model during joint optimization, i.e., adaptively transferring\nknowledge between model states to further prevent overfitting. Experiments\nvalidate that our method yields convincing gains in OOD generalization\nperformance in different settings. Code: https://github.com/apple/ml-ogen.\n","authors":["Yuhang Zang","Hanlin Goh","Josh Susskind","Chen Huang"],"pdf_url":"https://arxiv.org/pdf/2401.15914v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2312.06797v2","updated":"2024-04-16T03:10:34Z","published":"2023-12-11T19:13:38Z","title":"Improving the Robustness of 3D Human Pose Estimation: A Benchmark and\n  Learning from Noisy Input","summary":"  Despite the promising performance of current 3D human pose estimation\ntechniques, understanding and enhancing their generalization on challenging\nin-the-wild videos remain an open problem. In this work, we focus on the\nrobustness of 2D-to-3D pose lifters. To this end, we develop two benchmark\ndatasets, namely Human3.6M-C and HumanEva-I-C, to examine the robustness of\nvideo-based 3D pose lifters to a wide range of common video corruptions\nincluding temporary occlusion, motion blur, and pixel-level noise. We observe\nthe poor generalization of state-of-the-art 3D pose lifters in the presence of\ncorruption and establish two techniques to tackle this issue. First, we\nintroduce Temporal Additive Gaussian Noise (TAGN) as a simple yet effective 2D\ninput pose data augmentation. Additionally, to incorporate the confidence\nscores output by the 2D pose detectors, we design a confidence-aware\nconvolution (CA-Conv) block. Extensively tested on corrupted videos, the\nproposed strategies consistently boost the robustness of 3D pose lifters and\nserve as new baselines for future research.\n","authors":["Trung-Hieu Hoang","Mona Zehni","Huy Phan","Duc Minh Vo","Minh N. Do"],"pdf_url":"https://arxiv.org/pdf/2312.06797v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17521v2","updated":"2024-04-16T03:02:04Z","published":"2024-02-27T14:05:05Z","title":"AVS-Net: Point Sampling with Adaptive Voxel Size for 3D Scene\n  Understanding","summary":"  The recent advancements in point cloud learning have enabled intelligent\nvehicles and robots to comprehend 3D environments better. However, processing\nlarge-scale 3D scenes remains a challenging problem, such that efficient\ndownsampling methods play a crucial role in point cloud learning. Existing\ndownsampling methods either require a huge computational burden or sacrifice\nfine-grained geometric information. For such purpose, this paper presents an\nadvanced sampler that achieves both high accuracy and efficiency. The proposed\nmethod utilizes voxel centroid sampling as a foundation but effectively\naddresses the challenges regarding voxel size determination and the\npreservation of critical geometric cues. Specifically, we propose a Voxel\nAdaptation Module that adaptively adjusts voxel sizes with the reference of\npoint-based downsampling ratio. This ensures that the sampling results exhibit\na favorable distribution for comprehending various 3D objects or scenes.\nMeanwhile, we introduce a network compatible with arbitrary voxel sizes for\nsampling and feature extraction while maintaining high efficiency. The proposed\napproach is demonstrated with 3D object detection and 3D semantic segmentation.\nCompared to existing state-of-the-art methods, our approach achieves better\naccuracy on outdoor and indoor large-scale datasets, e.g. Waymo and ScanNet,\nwith promising efficiency.\n","authors":["Hongcheng Yang","Dingkang Liang","Dingyuan Zhang","Zhe Liu","Zhikang Zou","Xingyu Jiang","Yingying Zhu"],"pdf_url":"https://arxiv.org/pdf/2402.17521v2.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2404.10242v1","updated":"2024-04-16T02:42:06Z","published":"2024-04-16T02:42:06Z","title":"Masked Autoencoders for Microscopy are Scalable Learners of Cellular\n  Biology","summary":"  Featurizing microscopy images for use in biological research remains a\nsignificant challenge, especially for large-scale experiments spanning millions\nof images. This work explores the scaling properties of weakly supervised\nclassifiers and self-supervised masked autoencoders (MAEs) when training with\nincreasingly larger model backbones and microscopy datasets. Our results show\nthat ViT-based MAEs outperform weakly supervised classifiers on a variety of\ntasks, achieving as much as a 11.5% relative improvement when recalling known\nbiological relationships curated from public databases. Additionally, we\ndevelop a new channel-agnostic MAE architecture (CA-MAE) that allows for\ninputting images of different numbers and orders of channels at inference time.\nWe demonstrate that CA-MAEs effectively generalize by inferring and evaluating\non a microscopy image dataset (JUMP-CP) generated under different experimental\nconditions with a different channel structure than our pretraining data\n(RPI-93M). Our findings motivate continued research into scaling\nself-supervised learning on microscopy data in order to create powerful\nfoundation models of cellular biology that have the potential to catalyze\nadvancements in drug discovery and beyond.\n","authors":["Oren Kraus","Kian Kenyon-Dean","Saber Saberian","Maryam Fallah","Peter McLean","Jess Leung","Vasudev Sharma","Ayla Khan","Jia Balakrishnan","Safiye Celik","Dominique Beaini","Maciej Sypetkowski","Chi Vicky Cheng","Kristen Morse","Maureen Makes","Ben Mabey","Berton Earnshaw"],"pdf_url":"https://arxiv.org/pdf/2404.10242v1.pdf","comment":"CVPR 2024 Highlight. arXiv admin note: text overlap with\n  arXiv:2309.16064"},{"id":"http://arxiv.org/abs/2404.10241v1","updated":"2024-04-16T02:40:35Z","published":"2024-04-16T02:40:35Z","title":"Vision-and-Language Navigation via Causal Learning","summary":"  In the pursuit of robust and generalizable environment perception and\nlanguage understanding, the ubiquitous challenge of dataset bias continues to\nplague vision-and-language navigation (VLN) agents, hindering their performance\nin unseen environments. This paper introduces the generalized cross-modal\ncausal transformer (GOAT), a pioneering solution rooted in the paradigm of\ncausal inference. By delving into both observable and unobservable confounders\nwithin vision, language, and history, we propose the back-door and front-door\nadjustment causal learning (BACL and FACL) modules to promote unbiased learning\nby comprehensively mitigating potential spurious correlations. Additionally, to\ncapture global confounder features, we propose a cross-modal feature pooling\n(CFP) module supervised by contrastive learning, which is also shown to be\neffective in improving cross-modal representations during pre-training.\nExtensive experiments across multiple VLN datasets (R2R, REVERIE, RxR, and\nSOON) underscore the superiority of our proposed method over previous\nstate-of-the-art approaches. Code is available at\nhttps://github.com/CrystalSixone/VLN-GOAT.\n","authors":["Liuyi Wang","Zongtao He","Ronghao Dang","Mengjiao Shen","Chengju Liu","Qijun Chen"],"pdf_url":"https://arxiv.org/pdf/2404.10241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.08123v3","updated":"2024-04-16T02:37:47Z","published":"2023-07-16T18:42:01Z","title":"Solving Inverse Problems with Latent Diffusion Models via Hard Data\n  Consistency","summary":"  Diffusion models have recently emerged as powerful generative priors for\nsolving inverse problems. However, training diffusion models in the pixel space\nare both data-intensive and computationally demanding, which restricts their\napplicability as priors for high-dimensional real-world data such as medical\nimages. Latent diffusion models, which operate in a much lower-dimensional\nspace, offer a solution to these challenges. However, incorporating latent\ndiffusion models to solve inverse problems remains a challenging problem due to\nthe nonlinearity of the encoder and decoder. To address these issues, we\npropose \\textit{ReSample}, an algorithm that can solve general inverse problems\nwith pre-trained latent diffusion models. Our algorithm incorporates data\nconsistency by solving an optimization problem during the reverse sampling\nprocess, a concept that we term as hard data consistency. Upon solving this\noptimization problem, we propose a novel resampling scheme to map the\nmeasurement-consistent sample back onto the noisy data manifold and\ntheoretically demonstrate its benefits. Lastly, we apply our algorithm to solve\na wide range of linear and nonlinear inverse problems in both natural and\nmedical images, demonstrating that our approach outperforms existing\nstate-of-the-art approaches, including those based on pixel-space diffusion\nmodels.\n","authors":["Bowen Song","Soo Min Kwon","Zecheng Zhang","Xinyu Hu","Qing Qu","Liyue Shen"],"pdf_url":"https://arxiv.org/pdf/2307.08123v3.pdf","comment":"27 pages, 20 figures"},{"id":"http://arxiv.org/abs/2404.10237v1","updated":"2024-04-16T02:35:17Z","published":"2024-04-16T02:35:17Z","title":"MoE-TinyMed: Mixture of Experts for Tiny Medical Large Vision-Language\n  Models","summary":"  Mixture of Expert Tuning (MoE-Tuning) has effectively enhanced the\nperformance of general MLLMs with fewer parameters, yet its application in\nresource-limited medical settings has not been fully explored. To address this\ngap, we developed MoE-TinyMed, a model tailored for medical applications that\nsignificantly lowers parameter demands. In evaluations on the VQA-RAD, SLAKE,\nand Path-VQA datasets, MoE-TinyMed outperformed LLaVA-Med in all Med-VQA closed\nsettings with just 3.6B parameters. Additionally, a streamlined version with 2B\nparameters surpassed LLaVA-Med's performance in PathVQA, showcasing its\neffectiveness in resource-limited healthcare settings.\n","authors":["Songtao Jiang","Tuo Zheng","Yan Zhang","Yeying Jin","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2404.10237v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10234v1","updated":"2024-04-16T02:29:00Z","published":"2024-04-16T02:29:00Z","title":"Compressible and Searchable: AI-native Multi-Modal Retrieval System with\n  Learned Image Compression","summary":"  The burgeoning volume of digital content across diverse modalities\nnecessitates efficient storage and retrieval methods. Conventional approaches\nstruggle to cope with the escalating complexity and scale of multimedia data.\nIn this paper, we proposed framework addresses this challenge by fusing\nAI-native multi-modal search capabilities with neural image compression. First\nwe analyze the intricate relationship between compressibility and\nsearchability, recognizing the pivotal role each plays in the efficiency of\nstorage and retrieval systems. Through the usage of simple adapter is to bridge\nthe feature of Learned Image Compression(LIC) and Contrastive Language-Image\nPretraining(CLIP) while retaining semantic fidelity and retrieval of\nmulti-modal data. Experimental evaluations on Kodak datasets demonstrate the\nefficacy of our approach, showcasing significant enhancements in compression\nefficiency and search accuracy compared to existing methodologies. Our work\nmarks a significant advancement towards scalable and efficient multi-modal\nsearch systems in the era of big data.\n","authors":["Jixiang Luo"],"pdf_url":"https://arxiv.org/pdf/2404.10234v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10227v1","updated":"2024-04-16T02:18:18Z","published":"2024-04-16T02:18:18Z","title":"MS-MANO: Enabling Hand Pose Tracking with Biomechanical Constraints","summary":"  This work proposes a novel learning framework for visual hand dynamics\nanalysis that takes into account the physiological aspects of hand motion. The\nexisting models, which are simplified joint-actuated systems, often produce\nunnatural motions. To address this, we integrate a musculoskeletal system with\na learnable parametric hand model, MANO, to create a new model, MS-MANO. This\nmodel emulates the dynamics of muscles and tendons to drive the skeletal\nsystem, imposing physiologically realistic constraints on the resulting torque\ntrajectories. We further propose a simulation-in-the-loop pose refinement\nframework, BioPR, that refines the initial estimated pose through a multi-layer\nperceptron (MLP) network. Our evaluation of the accuracy of MS-MANO and the\nefficacy of the BioPR is conducted in two separate parts. The accuracy of\nMS-MANO is compared with MyoSuite, while the efficacy of BioPR is benchmarked\nagainst two large-scale public datasets and two recent state-of-the-art\nmethods. The results demonstrate that our approach consistently improves the\nbaseline methods both quantitatively and qualitatively.\n","authors":["Pengfei Xie","Wenqiang Xu","Tutian Tang","Zhenjun Yu","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2404.10227v1.pdf","comment":"11 pages, 5 figures; CVPR 2024"},{"id":"http://arxiv.org/abs/2307.03157v2","updated":"2024-04-16T02:12:11Z","published":"2023-07-06T17:32:38Z","title":"Achieving Reliable and Fair Skin Lesion Diagnosis via Unsupervised\n  Domain Adaptation","summary":"  The development of reliable and fair diagnostic systems is often constrained\nby the scarcity of labeled data. To address this challenge, our work explores\nthe feasibility of unsupervised domain adaptation (UDA) to integrate large\nexternal datasets for developing reliable classifiers. The adoption of UDA with\nmultiple sources can simultaneously enrich the training set and bridge the\ndomain gap between different skin lesion datasets, which vary due to distinct\nacquisition protocols. Particularly, UDA shows practical promise for improving\ndiagnostic reliability when training with a custom skin lesion dataset, where\nonly limited labeled data are available from the target domain. In this study,\nwe investigate three UDA training schemes based on source data utilization:\nsingle-source, combined-source, and multi-source UDA. Our findings demonstrate\nthe effectiveness of applying UDA on multiple sources for binary and\nmulti-class classification. A strong correlation between test error and label\nshift in multi-class tasks has been observed in the experiment. Crucially, our\nstudy shows that UDA can effectively mitigate bias against minority groups and\nenhance fairness in diagnostic systems, while maintaining superior\nclassification performance. This is achieved even without directly implementing\nfairness-focused techniques. This success is potentially attributed to the\nincreased and well-adapted demographic information obtained from multiple\nsources.\n","authors":["Janet Wang","Yunbei Zhang","Zhengming Ding","Jihun Hamm"],"pdf_url":"https://arxiv.org/pdf/2307.03157v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10226v1","updated":"2024-04-16T02:11:46Z","published":"2024-04-16T02:11:46Z","title":"Find The Gap: Knowledge Base Reasoning For Visual Question Answering","summary":"  We analyze knowledge-based visual question answering, for which given a\nquestion, the models need to ground it into the visual modality and retrieve\nthe relevant knowledge from a given large knowledge base (KB) to be able to\nanswer. Our analysis has two folds, one based on designing neural architectures\nand training them from scratch, and another based on large pre-trained language\nmodels (LLMs). Our research questions are: 1) Can we effectively augment models\nby explicit supervised retrieval of the relevant KB information to solve the\nKB-VQA problem? 2) How do task-specific and LLM-based models perform in the\nintegration of visual and external knowledge, and multi-hop reasoning over both\nsources of information? 3) Is the implicit knowledge of LLMs sufficient for\nKB-VQA and to what extent it can replace the explicit KB? Our results\ndemonstrate the positive impact of empowering task-specific and LLM models with\nsupervised external and visual knowledge retrieval models. Our findings show\nthat though LLMs are stronger in 1-hop reasoning, they suffer in 2-hop\nreasoning in comparison with our fine-tuned NN model even if the relevant\ninformation from both modalities is available to the model. Moreover, we\nobserved that LLM models outperform the NN model for KB-related questions which\nconfirms the effectiveness of implicit knowledge in LLMs however, they do not\nalleviate the need for external KB.\n","authors":["Elham J. Barezi","Parisa Kordjamshidi"],"pdf_url":"https://arxiv.org/pdf/2404.10226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10220v1","updated":"2024-04-16T02:01:56Z","published":"2024-04-16T02:01:56Z","title":"Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V","summary":"  Autonomous robot navigation and manipulation in open environments require\nreasoning and replanning with closed-loop feedback. We present COME-robot, the\nfirst closed-loop framework utilizing the GPT-4V vision-language foundation\nmodel for open-ended reasoning and adaptive planning in real-world scenarios.\nWe meticulously construct a library of action primitives for robot exploration,\nnavigation, and manipulation, serving as callable execution modules for GPT-4V\nin task planning. On top of these modules, GPT-4V serves as the brain that can\naccomplish multimodal reasoning, generate action policy with code, verify the\ntask progress, and provide feedback for replanning. Such design enables\nCOME-robot to (i) actively perceive the environments, (ii) perform situated\nreasoning, and (iii) recover from failures. Through comprehensive experiments\ninvolving 8 challenging real-world tabletop and manipulation tasks, COME-robot\ndemonstrates a significant improvement in task success rate (~25%) compared to\nstate-of-the-art baseline methods. We further conduct comprehensive analyses to\nelucidate how COME-robot's design facilitates failure recovery, free-form\ninstruction following, and long-horizon task planning.\n","authors":["Peiyuan Zhi","Zhiyuan Zhang","Muzhi Han","Zeyu Zhang","Zhitian Li","Ziyuan Jiao","Baoxiong Jia","Siyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2404.10220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.16654v2","updated":"2024-04-16T01:55:19Z","published":"2023-06-29T03:31:46Z","title":"Self-Supervised MRI Reconstruction with Unrolled Diffusion Models","summary":"  Magnetic Resonance Imaging (MRI) produces excellent soft tissue contrast,\nalbeit it is an inherently slow imaging modality. Promising deep learning\nmethods have recently been proposed to reconstruct accelerated MRI scans.\nHowever, existing methods still suffer from various limitations regarding image\nfidelity, contextual sensitivity, and reliance on fully-sampled acquisitions\nfor model training. To comprehensively address these limitations, we propose a\nnovel self-supervised deep reconstruction model, named Self-Supervised\nDiffusion Reconstruction (SSDiffRecon). SSDiffRecon expresses a conditional\ndiffusion process as an unrolled architecture that interleaves cross-attention\ntransformers for reverse diffusion steps with data-consistency blocks for\nphysics-driven processing. Unlike recent diffusion methods for MRI\nreconstruction, a self-supervision strategy is adopted to train SSDiffRecon\nusing only undersampled k-space data. Comprehensive experiments on public brain\nMR datasets demonstrates the superiority of SSDiffRecon against\nstate-of-the-art supervised, and self-supervised baselines in terms of\nreconstruction speed and quality. Implementation will be available at\nhttps://github.com/yilmazkorkmaz1/SSDiffRecon.\n","authors":["Yilmaz Korkmaz","Tolga Cukur","Vishal M. Patel"],"pdf_url":"https://arxiv.org/pdf/2306.16654v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.12308v5","updated":"2024-04-16T01:52:00Z","published":"2023-04-24T17:57:15Z","title":"Segment Anything in 3D with Radiance Fields","summary":"  The Segment Anything Model (SAM) emerges as a powerful vision foundation\nmodel to generate high-quality 2D segmentation results. This paper aims to\ngeneralize SAM to segment 3D objects. Rather than replicating the data\nacquisition and annotation procedure which is costly in 3D, we design an\nefficient solution, leveraging the radiance field as a cheap and off-the-shelf\nprior that connects multi-view 2D images to the 3D space. We refer to the\nproposed solution as SA3D, short for Segment Anything in 3D. With SA3D, the\nuser is only required to provide a 2D segmentation prompt (e.g., rough points)\nfor the target object in a single view, which is used to generate its\ncorresponding 2D mask with SAM. Next, SA3D alternately performs mask inverse\nrendering and cross-view self-prompting across various views to iteratively\nrefine the 3D mask of the target object. For one view, mask inverse rendering\nprojects the 2D mask obtained by SAM into the 3D space with guidance of the\ndensity distribution learned by the radiance field for 3D mask refinement;\nThen, cross-view self-prompting extracts reliable prompts automatically as the\ninput to SAM from the rendered 2D mask of the inaccurate 3D mask for a new\nview. We show in experiments that SA3D adapts to various scenes and achieves 3D\nsegmentation within seconds. Our research reveals a potential methodology to\nlift the ability of a 2D segmentation model to 3D. Our code is available at\nhttps://github.com/Jumpat/SegmentAnythingin3D.\n","authors":["Jiazhong Cen","Jiemin Fang","Zanwei Zhou","Chen Yang","Lingxi Xie","Xiaopeng Zhang","Wei Shen","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2304.12308v5.pdf","comment":"Extension version of SA3D (NeurIPS 2023). Project page:\n  https://jumpat.github.io/SA3D/"},{"id":"http://arxiv.org/abs/2404.10213v1","updated":"2024-04-16T01:50:10Z","published":"2024-04-16T01:50:10Z","title":"GaitPoint+: A Gait Recognition Network Incorporating Point Cloud\n  Analysis and Recycling","summary":"  Gait is a behavioral biometric modality that can be used to recognize\nindividuals by the way they walk from a far distance. Most existing gait\nrecognition approaches rely on either silhouettes or skeletons, while their\njoint use is underexplored. Features from silhouettes and skeletons can provide\ncomplementary information for more robust recognition against appearance\nchanges or pose estimation errors. To exploit the benefits of both silhouette\nand skeleton features, we propose a new gait recognition network, referred to\nas the GaitPoint+. Our approach models skeleton key points as a 3D point cloud,\nand employs a computational complexity-conscious 3D point processing approach\nto extract skeleton features, which are then combined with silhouette features\nfor improved accuracy. Since silhouette- or CNN-based methods already require\nconsiderable amount of computational resources, it is preferable that the key\npoint learning module is faster and more lightweight. We present a detailed\nanalysis of the utilization of every human key point after the use of\ntraditional max-pooling, and show that while elbow and ankle points are used\nmost commonly, many useful points are discarded by max-pooling. Thus, we\npresent a method to recycle some of the discarded points by a Recycling\nMax-Pooling module, during processing of skeleton point clouds, and achieve\nfurther performance improvement. We provide a comprehensive set of experimental\nresults showing that (i) incorporating skeleton features obtained by a\npoint-based 3D point cloud processing approach boosts the performance of three\ndifferent state-of-the-art silhouette- and CNN-based baselines; (ii) recycling\nthe discarded points increases the accuracy further. Ablation studies are also\nprovided to show the effectiveness and contribution of different components of\nour approach.\n","authors":["Huantao Ren","Jiajing Chen","Senem Velipasalar"],"pdf_url":"https://arxiv.org/pdf/2404.10213v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10212v1","updated":"2024-04-16T01:49:35Z","published":"2024-04-16T01:49:35Z","title":"LWIRPOSE: A novel LWIR Thermal Image Dataset and Benchmark","summary":"  Human pose estimation faces hurdles in real-world applications due to factors\nlike lighting changes, occlusions, and cluttered environments. We introduce a\nunique RGB-Thermal Nearly Paired and Annotated 2D Pose Dataset, comprising over\n2,400 high-quality LWIR (thermal) images. Each image is meticulously annotated\nwith 2D human poses, offering a valuable resource for researchers and\npractitioners. This dataset, captured from seven actors performing diverse\neveryday activities like sitting, eating, and walking, facilitates pose\nestimation on occlusion and other challenging scenarios. We benchmark\nstate-of-the-art pose estimation methods on the dataset to showcase its\npotential, establishing a strong baseline for future research. Our results\ndemonstrate the dataset's effectiveness in promoting advancements in pose\nestimation for various applications, including surveillance, healthcare, and\nsports analytics. The dataset and code are available at\nhttps://github.com/avinres/LWIRPOSE\n","authors":["Avinash Upadhyay","Bhipanshu Dhupar","Manoj Sharma","Ankit Shukla","Ajith Abraham"],"pdf_url":"https://arxiv.org/pdf/2404.10212v1.pdf","comment":"Submitted in ICIP2024"},{"id":"http://arxiv.org/abs/2404.10210v1","updated":"2024-04-16T01:41:22Z","published":"2024-04-16T01:41:22Z","title":"MK-SGN: A Spiking Graph Convolutional Network with Multimodal Fusion and\n  Knowledge Distillation for Skeleton-based Action Recognition","summary":"  In recent years, skeleton-based action recognition, leveraging multimodal\nGraph Convolutional Networks (GCN), has achieved remarkable results. However,\ndue to their deep structure and reliance on continuous floating-point\noperations, GCN-based methods are energy-intensive. To address this issue, we\npropose an innovative Spiking Graph Convolutional Network with Multimodal\nFusion and Knowledge Distillation (MK-SGN). By merging the energy efficiency of\nSpiking Neural Network (SNN) with the graph representation capability of GCN,\nthe proposed MK-SGN reduces energy consumption while maintaining recognition\naccuracy. Firstly, we convert GCN into Spiking Graph Convolutional Network\n(SGN) and construct a foundational Base-SGN for skeleton-based action\nrecognition, establishing a new benchmark and paving the way for future\nresearch exploration. Secondly, we further propose a Spiking Multimodal Fusion\nmodule (SMF), leveraging mutual information to process multimodal data more\nefficiently. Additionally, we introduce a spiking attention mechanism and\ndesign a Spatio Graph Convolution module with a Spatial Global Spiking\nAttention mechanism (SA-SGC), enhancing feature learning capability.\nFurthermore, we delve into knowledge distillation methods from multimodal GCN\nto SGN and propose a novel, integrated method that simultaneously focuses on\nboth intermediate layer distillation and soft label distillation to improve the\nperformance of SGN. On two challenging datasets for skeleton-based action\nrecognition, MK-SGN outperforms the state-of-the-art GCN-like frameworks in\nreducing computational load and energy consumption. In contrast, typical GCN\nmethods typically consume more than 35mJ per action sample, while MK-SGN\nreduces energy consumption by more than 98%.\n","authors":["Naichuan Zheng","Hailun Xia","Zeyu Liang"],"pdf_url":"https://arxiv.org/pdf/2404.10210v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.10523v3","updated":"2024-04-16T01:34:34Z","published":"2023-04-20T17:52:58Z","title":"GenCorres: Consistent Shape Matching via Coupled Implicit-Explicit Shape\n  Generative Models","summary":"  This paper introduces GenCorres, a novel unsupervised joint shape matching\n(JSM) approach. Our key idea is to learn a mesh generator to fit an unorganized\ndeformable shape collection while constraining deformations between adjacent\nsynthetic shapes to preserve geometric structures such as local rigidity and\nlocal conformality. GenCorres presents three appealing advantages over existing\nJSM techniques. First, GenCorres performs JSM among a synthetic shape\ncollection whose size is much bigger than the input shapes and fully leverages\nthe datadriven power of JSM. Second, GenCorres unifies consistent shape\nmatching and pairwise matching (i.e., by enforcing deformation priors between\nadjacent synthetic shapes). Third, the generator provides a concise encoding of\nconsistent shape correspondences. However, learning a mesh generator from an\nunorganized shape collection is challenging, requiring a good initialization.\nGenCorres addresses this issue by learning an implicit generator from the input\nshapes, which provides intermediate shapes between two arbitrary shapes. We\nintroduce a novel approach for computing correspondences between adjacent\nimplicit surfaces, which we use to regularize the implicit generator. Synthetic\nshapes of the implicit generator then guide initial fittings (i.e., via\ntemplate-based deformation) for learning the mesh generator. Experimental\nresults show that GenCorres considerably outperforms state-of-the-art JSM\ntechniques. The synthetic shapes of GenCorres also achieve salient performance\ngains against state-of-the-art deformable shape generators.\n","authors":["Haitao Yang","Xiangru Huang","Bo Sun","Chandrajit Bajaj","Qixing Huang"],"pdf_url":"https://arxiv.org/pdf/2304.10523v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2404.08197v2","updated":"2024-04-16T01:13:35Z","published":"2024-04-12T02:04:34Z","title":"Scaling (Down) CLIP: A Comprehensive Analysis of Data, Architecture, and\n  Training Strategies","summary":"  This paper investigates the performance of the Contrastive Language-Image\nPre-training (CLIP) when scaled down to limited computation budgets. We explore\nCLIP along three dimensions: data, architecture, and training strategies. With\nregards to data, we demonstrate the significance of high-quality training data\nand show that a smaller dataset of high-quality data can outperform a larger\ndataset with lower quality. We also examine how model performance varies with\ndifferent dataset sizes, suggesting that smaller ViT models are better suited\nfor smaller datasets, while larger models perform better on larger datasets\nwith fixed compute. Additionally, we provide guidance on when to choose a\nCNN-based architecture or a ViT-based architecture for CLIP training. We\ncompare four CLIP training strategies - SLIP, FLIP, CLIP, and CLIP+Data\nAugmentation - and show that the choice of training strategy depends on the\navailable compute resource. Our analysis reveals that CLIP+Data Augmentation\ncan achieve comparable performance to CLIP using only half of the training\ndata. This work provides practical insights into how to effectively train and\ndeploy CLIP models, making them more accessible and affordable for practical\nuse in various applications.\n","authors":["Zichao Li","Cihang Xie","Ekin Dogus Cubuk"],"pdf_url":"https://arxiv.org/pdf/2404.08197v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10193v1","updated":"2024-04-16T00:28:26Z","published":"2024-04-16T00:28:26Z","title":"Consistency and Uncertainty: Identifying Unreliable Responses From\n  Black-Box Vision-Language Models for Selective Visual Question Answering","summary":"  The goal of selective prediction is to allow an a model to abstain when it\nmay not be able to deliver a reliable prediction, which is important in\nsafety-critical contexts. Existing approaches to selective prediction typically\nrequire access to the internals of a model, require retraining a model or study\nonly unimodal models. However, the most powerful models (e.g. GPT-4) are\ntypically only available as black boxes with inaccessible internals, are not\nretrainable by end-users, and are frequently used for multimodal tasks. We\nstudy the possibility of selective prediction for vision-language models in a\nrealistic, black-box setting. We propose using the principle of\n\\textit{neighborhood consistency} to identify unreliable responses from a\nblack-box vision-language model in question answering tasks. We hypothesize\nthat given only a visual question and model response, the consistency of the\nmodel's responses over the neighborhood of a visual question will indicate\nreliability. It is impossible to directly sample neighbors in feature space in\na black-box setting. Instead, we show that it is possible to use a smaller\nproxy model to approximately sample from the neighborhood. We find that\nneighborhood consistency can be used to identify model responses to visual\nquestions that are likely unreliable, even in adversarial settings or settings\nthat are out-of-distribution to the proxy model.\n","authors":["Zaid Khan","Yun Fu"],"pdf_url":"https://arxiv.org/pdf/2404.10193v1.pdf","comment":"CVPR 2024"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2404.08137v2","updated":"2024-04-16T14:40:31Z","published":"2024-04-11T21:48:54Z","title":"Generative Information Retrieval Evaluation","summary":"  This paper is a draft of a chapter intended to appear in a forthcoming book\non generative information retrieval, co-edited by Chirag Shah and Ryen White.\nIn this chapter, we consider generative information retrieval evaluation from\ntwo distinct but interrelated perspectives. First, large language models (LLMs)\nthemselves are rapidly becoming tools for evaluation, with current research\nindicating that LLMs may be superior to crowdsource workers and other paid\nassessors on basic relevance judgement tasks. We review past and ongoing\nrelated research, including speculation on the future of shared task\ninitiatives, such as TREC, and a discussion on the continuing need for human\nassessments. Second, we consider the evaluation of emerging LLM-based\ngenerative information retrieval (GenIR) systems, including retrieval augmented\ngeneration (RAG) systems. We consider approaches that focus both on the\nend-to-end evaluation of GenIR systems and on the evaluation of a retrieval\ncomponent as an element in a RAG system. Going forward, we expect the\nevaluation of GenIR systems to be at least partially based on LLM-based\nassessment, creating an apparent circularity, with a system seemingly\nevaluating its own output. We resolve this apparent circularity in two ways: 1)\nby viewing LLM-based assessment as a form of \"slow search\", where a slower IR\nsystem is used for evaluation and training of a faster production IR system;\nand 2) by recognizing a continuing need to ground evaluation in human\nassessment, even if the characteristics of that human assessment must change.\n","authors":["Marwah Alaofi","Negar Arabzadeh","Charles L. A. Clarke","Mark Sanderson"],"pdf_url":"https://arxiv.org/pdf/2404.08137v2.pdf","comment":"Draft of a chapter intended to appear in a forthcoming book on\n  generative information retrieval, co-edited by Chirag Shah and Ryen White"},{"id":"http://arxiv.org/abs/2307.16089v2","updated":"2024-04-16T14:33:21Z","published":"2023-07-29T22:40:59Z","title":"Train Once, Use Flexibly: A Modular Framework for Multi-Aspect Neural\n  News Recommendation","summary":"  Recent neural news recommenders (NNRs) extend content-based recommendation\n(1) by aligning additional aspects (e.g., topic, sentiment) between candidate\nnews and user history or (2) by diversifying recommendations w.r.t. these\naspects. This customization is achieved by \"hardcoding\" additional constraints\ninto the NNR's architecture and/or training objectives: any change in the\ndesired recommendation behavior thus requires retraining the model with a\nmodified objective. This impedes widespread adoption of multi-aspect news\nrecommenders. In this work, we introduce MANNeR, a modular framework for\nmulti-aspect neural news recommendation that supports on-the-fly customization\nover individual aspects at inference time. With metric-based learning as its\nbackbone, MANNeR learns aspect-specialized news encoders and then flexibly and\nlinearly combines the resulting aspect-specific similarity scores into\ndifferent ranking functions, alleviating the need for ranking function-specific\nretraining of the model. Extensive experimental results show that MANNeR\nconsistently outperforms state-of-the-art NNRs on both standard content-based\nrecommendation and single- and multi-aspect customization. Lastly, we validate\nthat MANNeR's aspect-customization module is robust to language and domain\ntransfer.\n","authors":["Andreea Iana","Goran Glava≈°","Heiko Paulheim"],"pdf_url":"https://arxiv.org/pdf/2307.16089v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10496v1","updated":"2024-04-16T12:10:01Z","published":"2024-04-16T12:10:01Z","title":"Spiral of Silences: How is Large Language Model Killing Information\n  Retrieval? -- A Case Study on Open Domain Question Answering","summary":"  The practice of Retrieval-Augmented Generation (RAG), which integrates Large\nLanguage Models (LLMs) with retrieval systems, has become increasingly\nprevalent. However, the repercussions of LLM-derived content infiltrating the\nweb and influencing the retrieval-generation feedback loop are largely\nuncharted territories. In this study, we construct and iteratively run a\nsimulation pipeline to deeply investigate the short-term and long-term effects\nof LLM text on RAG systems. Taking the trending Open Domain Question Answering\n(ODQA) task as a point of entry, our findings reveal a potential digital\n\"Spiral of Silence\" effect, with LLM-generated text consistently outperforming\nhuman-authored content in search rankings, thereby diminishing the presence and\nimpact of human contributions online. This trend risks creating an imbalanced\ninformation ecosystem, where the unchecked proliferation of erroneous\nLLM-generated content may result in the marginalization of accurate\ninformation. We urge the academic community to take heed of this potential\nissue, ensuring a diverse and authentic digital information landscape.\n","authors":["Xiaoyang Chen","Ben He","Hongyu Lin","Xianpei Han","Tianshu Wang","Boxi Cao","Le Sun","Yingfei Sun"],"pdf_url":"https://arxiv.org/pdf/2404.10496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10384v1","updated":"2024-04-16T08:28:16Z","published":"2024-04-16T08:28:16Z","title":"Reasoning on Efficient Knowledge Paths:Knowledge Graph Guides Large\n  Language Model for Domain Question Answering","summary":"  Large language models (LLMs), such as GPT3.5, GPT4 and LLAMA2 perform\nsurprisingly well and outperform human experts on many tasks. However, in many\ndomain-specific evaluations, these LLMs often suffer from hallucination\nproblems due to insufficient training of relevant corpus. Furthermore,\nfine-tuning large models may face problems such as the LLMs are not open source\nor the construction of high-quality domain instruction is difficult. Therefore,\nstructured knowledge databases such as knowledge graph can better provide\ndomain back- ground knowledge for LLMs and make full use of the reasoning and\nanalysis capabilities of LLMs. In some previous works, LLM was called multiple\ntimes to determine whether the current triplet was suitable for inclusion in\nthe subgraph when retrieving subgraphs through a question. Especially for the\nquestion that require a multi-hop reasoning path, frequent calls to LLM will\nconsume a lot of computing power. Moreover, when choosing the reasoning path,\nLLM will be called once for each step, and if one of the steps is selected\nincorrectly, it will lead to the accumulation of errors in the following steps.\nIn this paper, we integrated and optimized a pipeline for selecting reasoning\npaths from KG based on LLM, which can reduce the dependency on LLM. In\naddition, we propose a simple and effective subgraph retrieval method based on\nchain of thought (CoT) and page rank which can returns the paths most likely to\ncontain the answer. We conduct experiments on three datasets: GenMedGPT-5k\n[14], WebQuestions [2], and CMCQA [21]. Finally, RoK can demonstrate that using\nfewer LLM calls can achieve the same results as previous SOTAs models.\n","authors":["Yuqi Wang","Boran Jiang","Yi Luo","Dawei He","Peng Cheng","Liangcai Gao"],"pdf_url":"https://arxiv.org/pdf/2404.10384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10371v1","updated":"2024-04-16T08:08:55Z","published":"2024-04-16T08:08:55Z","title":"Promoting the linguistic diversity of TEI in the Maghreb and the Arab\n  region","summary":"  The project targets both oral corpus and the rich text resources written in\nthe Maghreb region. It focuses particularly on the continuity, for more than 12\ncenturies, of a classical still alive Arabic language and on the extreme\nhybridization of vernacular languages sustained by the rich Libyan, Roman,\nHebrew and Ottoman influences and by the more recent French, Spanish and\nItalian linguistic interference. In short, the Maghreb is a place of extremely\nabundant, but much unexploited, textual studies.\n","authors":["Henri Hudrisier","Rachid Zghibi","Sihem Zghidi","Mokhtar Ben Henda"],"pdf_url":"https://arxiv.org/pdf/2404.10371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10327v1","updated":"2024-04-16T07:11:48Z","published":"2024-04-16T07:11:48Z","title":"Exact and Efficient Unlearning for Large Language Model-based\n  Recommendation","summary":"  The evolving paradigm of Large Language Model-based Recom- mendation (LLMRec)\ncustomizes Large Language Models (LLMs) through parameter-efficient fine-tuning\n(PEFT) using recommenda- tion data. The inclusion of user data in LLMs raises\nprivacy concerns. To protect users, the unlearning process in LLMRec,\nspecifically removing unusable data (e.g., historical behaviors) from\nestablished LLMRec models, becomes crucial. However, existing unlearning\nmethods are insufficient for the unique characteristics of LLM- Rec, mainly due\nto high computational costs or incomplete data erasure. In this study, we\nintroduce the Adapter Partition and Ag- gregation (APA) framework for exact and\nefficient unlearning while maintaining recommendation performance. APA achieves\nthis by establishing distinct adapters for partitioned training data shards and\nretraining only the adapters impacted by unusable data for un- learning. To\npreserve recommendation performance and mitigate considerable inference costs,\nAPA employs parameter-level adapter aggregation with sample-adaptive attention\nfor individual testing samples. Extensive experiments substantiate the\neffectiveness and efficiency of our proposed framework\n","authors":["Zhiyu Hu","Yang Zhang","Minghao Xiao","Wenjie Wang","Fuli Feng","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2404.10327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10321v1","updated":"2024-04-16T07:05:16Z","published":"2024-04-16T07:05:16Z","title":"Cluster-based Graph Collaborative Filtering","summary":"  Graph Convolution Networks (GCNs) have significantly succeeded in learning\nuser and item representations for recommendation systems. The core of their\nefficacy is the ability to explicitly exploit the collaborative signals from\nboth the first- and high-order neighboring nodes. However, most existing\nGCN-based methods overlook the multiple interests of users while performing\nhigh-order graph convolution. Thus, the noisy information from unreliable\nneighbor nodes (e.g., users with dissimilar interests) negatively impacts the\nrepresentation learning of the target node. Additionally, conducting graph\nconvolution operations without differentiating high-order neighbors suffers the\nover-smoothing issue when stacking more layers, resulting in performance\ndegradation. In this paper, we aim to capture more valuable information from\nhigh-order neighboring nodes while avoiding noise for better representation\nlearning of the target node. To achieve this goal, we propose a novel GCN-based\nrecommendation model, termed Cluster-based Graph Collaborative Filtering\n(ClusterGCF). This model performs high-order graph convolution on\ncluster-specific graphs, which are constructed by capturing the multiple\ninterests of users and identifying the common interests among them.\nSpecifically, we design an unsupervised and optimizable soft node clustering\napproach to classify user and item nodes into multiple clusters. Based on the\nsoft node clustering results and the topology of the user-item interaction\ngraph, we assign the nodes with probabilities for different clusters to\nconstruct the cluster-specific graphs. To evaluate the effectiveness of\nClusterGCF, we conducted extensive experiments on four publicly available\ndatasets. Experimental results demonstrate that our model can significantly\nimprove recommendation performance.\n","authors":["Fan Liu","Shuai Zhao","Zhiyong Cheng","Liqiang Nie","Mohan Kankanhalli"],"pdf_url":"https://arxiv.org/pdf/2404.10321v1.pdf","comment":"22 pages, 8 figures"},{"id":"http://arxiv.org/abs/2404.10234v1","updated":"2024-04-16T02:29:00Z","published":"2024-04-16T02:29:00Z","title":"Compressible and Searchable: AI-native Multi-Modal Retrieval System with\n  Learned Image Compression","summary":"  The burgeoning volume of digital content across diverse modalities\nnecessitates efficient storage and retrieval methods. Conventional approaches\nstruggle to cope with the escalating complexity and scale of multimedia data.\nIn this paper, we proposed framework addresses this challenge by fusing\nAI-native multi-modal search capabilities with neural image compression. First\nwe analyze the intricate relationship between compressibility and\nsearchability, recognizing the pivotal role each plays in the efficiency of\nstorage and retrieval systems. Through the usage of simple adapter is to bridge\nthe feature of Learned Image Compression(LIC) and Contrastive Language-Image\nPretraining(CLIP) while retaining semantic fidelity and retrieval of\nmulti-modal data. Experimental evaluations on Kodak datasets demonstrate the\nefficacy of our approach, showcasing significant enhancements in compression\nefficiency and search accuracy compared to existing methodologies. Our work\nmarks a significant advancement towards scalable and efficient multi-modal\nsearch systems in the era of big data.\n","authors":["Jixiang Luo"],"pdf_url":"https://arxiv.org/pdf/2404.10234v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10944v1","updated":"2024-04-16T22:57:06Z","published":"2024-04-16T22:57:06Z","title":"Threat Behavior Textual Search by Attention Graph Isomorphism","summary":"  Cyber attacks cause over \\$1 trillion loss every year. An important task for\ncyber security analysts is attack forensics. It entails understanding malware\nbehaviors and attack origins. However, existing automated or manual malware\nanalysis can only disclose a subset of behaviors due to inherent difficulties\n(e.g., malware cloaking and obfuscation). As such, analysts often resort to\ntext search techniques to identify existing malware reports based on the\nsymptoms they observe, exploiting the fact that malware samples share a lot of\nsimilarity, especially those from the same origin. In this paper, we propose a\nnovel malware behavior search technique that is based on graph isomorphism at\nthe attention layers of Transformer models. We also compose a large dataset\ncollected from various agencies to facilitate such research. Our technique\noutperforms state-of-the-art methods, such as those based on sentence\nembeddings and keywords by 6-14%. In the case study of 10 real-world malwares,\nour technique can correctly attribute 8 of them to their ground truth origins\nwhile using Google only works for 3 cases.\n","authors":["Chanwoo Bae","Guanhong Tao","Zhuo Zhang","Xiangyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.10944v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17887v3","updated":"2024-04-16T20:54:01Z","published":"2024-02-27T21:01:41Z","title":"JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning\n  and Professional Question Answering Capability","summary":"  Large Language Models (LLMs) have demonstrated a remarkable potential in\nmedical knowledge acquisition and question-answering. However, LLMs can\npotentially hallucinate and yield factually incorrect outcomes, even with\ndomain-specific pretraining. Previously, retrieval augmented generation (RAG)\nhas limited success in addressing hallucinations. Unlike previous methods in\nRAG where the retrieval model was trained separately from the LLM, we introduce\nJMLR (for Jointly trains LLM and information Retrieval (IR)) during the\nfine-tuning phase. The synchronized training mechanism enhances JMLR's ability\nto retrieve clinical guidelines and leverage medical knowledge to reason and\nanswer questions and reduces the demand for computational resources. We\nevaluated JMLR on the important medical question answering application. Our\nexperimental results demonstrate that JMLR-13B (70.5%) outperforms a previous\nstate-of-the-art open-source model using conventional pre-training and\nfine-tuning Meditron-70B (68.9%) and Llama2-13B with RAG (54.9%) on a medical\nquestion-answering dataset. JMLR-13B (148 GPU hours) also trains much faster\nthan Meditron-70B (42630 GPU hours). Through this work, we provide a new and\nefficient knowledge enhancement tool for healthcare, demonstrating the\npotential of integrating IR and LLM training for medical question-answering\nsystems.\n","authors":["Junda Wang","Zhichao Yang","Zonghai Yao","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2402.17887v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11035v2","updated":"2024-04-16T20:22:21Z","published":"2024-02-16T19:28:52Z","title":"Retrieval-Augmented Generation: Is Dense Passage Retrieval Retrieving?","summary":"  Dense passage retrieval (DPR) is the first step in the retrieval augmented\ngeneration (RAG) paradigm for improving the performance of large language\nmodels (LLM). DPR fine-tunes pre-trained networks to enhance the alignment of\nthe embeddings between queries and relevant textual data. A deeper\nunderstanding of DPR fine-tuning will be required to fundamentally unlock the\nfull potential of this approach. In this work, we explore DPR-trained models\nmechanistically by using a combination of probing, layer activation analysis,\nand model editing. Our experiments show that DPR training decentralizes how\nknowledge is stored in the network, creating multiple access pathways to the\nsame information. We also uncover a limitation in this training style: the\ninternal knowledge of the pre-trained model bounds what the retrieval model can\nretrieve. These findings suggest a few possible directions for dense retrieval:\n(1) expose the DPR training process to more knowledge so more can be\ndecentralized, (2) inject facts as decentralized representations, (3) model and\nincorporate knowledge uncertainty in the retrieval process, and (4) directly\nmap internal model knowledge to a knowledge base.\n","authors":["Benjamin Reichman","Larry Heck"],"pdf_url":"https://arxiv.org/pdf/2402.11035v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10890v1","updated":"2024-04-16T20:22:12Z","published":"2024-04-16T20:22:12Z","title":"Exploring Augmentation and Cognitive Strategies for AI based Synthetic\n  Personae","summary":"  Large language models (LLMs) hold potential for innovative HCI research,\nincluding the creation of synthetic personae. However, their black-box nature\nand propensity for hallucinations pose challenges. To address these\nlimitations, this position paper advocates for using LLMs as data augmentation\nsystems rather than zero-shot generators. We further propose the development of\nrobust cognitive and memory frameworks to guide LLM responses. Initial\nexplorations suggest that data enrichment, episodic memory, and self-reflection\ntechniques can improve the reliability of synthetic personae and open up new\navenues for HCI research.\n","authors":["Rafael Arias Gonzalez","Steve DiPaola"],"pdf_url":"https://arxiv.org/pdf/2404.10890v1.pdf","comment":"This paper was accepted for publication: Proceedings of ACM Conf on\n  Human Factors in Computing Systems (CHI 24), Rafael Arias Gonzalez, Steve\n  DiPaola. Exploring Augmentation and Cognitive Strategies for Synthetic\n  Personae. ACM SigCHI, in Challenges and Opportunities of LLM-Based Synthetic\n  Personae and Data in HCI Workshop, 2024"},{"id":"http://arxiv.org/abs/2404.10876v1","updated":"2024-04-16T19:52:57Z","published":"2024-04-16T19:52:57Z","title":"Course Recommender Systems Need to Consider the Job Market","summary":"  Current course recommender systems primarily leverage learner-course\ninteractions, course content, learner preferences, and supplementary course\ndetails like instructor, institution, ratings, and reviews, to make their\nrecommendation. However, these systems often overlook a critical aspect: the\nevolving skill demand of the job market. This paper focuses on the perspective\nof academic researchers, working in collaboration with the industry, aiming to\ndevelop a course recommender system that incorporates job market skill demands.\nIn light of the job market's rapid changes and the current state of research in\ncourse recommender systems, we outline essential properties for course\nrecommender systems to address these demands effectively, including\nexplainable, sequential, unsupervised, and aligned with the job market and\nuser's goals. Our discussion extends to the challenges and research questions\nthis objective entails, including unsupervised skill extraction from job\nlistings, course descriptions, and resumes, as well as predicting\nrecommendations that align with learner objectives and the job market and\ndesigning metrics to evaluate this alignment. Furthermore, we introduce an\ninitial system that addresses some existing limitations of course recommender\nsystems using large Language Models (LLMs) for skill extraction and\nReinforcement Learning (RL) for alignment with the job market. We provide\nempirical results using open-source data to demonstrate its effectiveness.\n","authors":["Jibril Frej","Anna Dai","Syrielle Montariol","Antoine Bosselut","Tanja K√§ser"],"pdf_url":"https://arxiv.org/pdf/2404.10876v1.pdf","comment":"accepted at SIGIR 2024 as a perspective paper. Camera Ready will come\n  soon"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2404.10776v1","updated":"2024-04-16T17:59:55Z","published":"2024-04-16T17:59:55Z","title":"Nearly Optimal Algorithms for Contextual Dueling Bandits from\n  Adversarial Feedback","summary":"  Learning from human feedback plays an important role in aligning generative\nmodels, such as large language models (LLM). However, the effectiveness of this\napproach can be influenced by adversaries, who may intentionally provide\nmisleading preferences to manipulate the output in an undesirable or harmful\ndirection. To tackle this challenge, we study a specific model within this\nproblem domain--contextual dueling bandits with adversarial feedback, where the\ntrue preference label can be flipped by an adversary. We propose an algorithm\nnamely robust contextual dueling bandit (\\algo), which is based on\nuncertainty-weighted maximum likelihood estimation. Our algorithm achieves an\n$\\tilde O(d\\sqrt{T}+dC)$ regret bound, where $T$ is the number of rounds, $d$\nis the dimension of the context, and $ 0 \\le C \\le T$ is the total number of\nadversarial feedback. We also prove a lower bound to show that our regret bound\nis nearly optimal, both in scenarios with and without ($C=0$) adversarial\nfeedback. Additionally, we conduct experiments to evaluate our proposed\nalgorithm against various types of adversarial feedback. Experimental results\ndemonstrate its superiority over the state-of-the-art dueling bandit algorithms\nin the presence of adversarial feedback.\n","authors":["Qiwei Di","Jiafan He","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2404.10776v1.pdf","comment":"24pages, 5 figures"},{"id":"http://arxiv.org/abs/2404.10771v1","updated":"2024-04-16T17:55:31Z","published":"2024-04-16T17:55:31Z","title":"TENG: Time-Evolving Natural Gradient for Solving PDEs with Deep Neural\n  Net","summary":"  Partial differential equations (PDEs) are instrumental for modeling dynamical\nsystems in science and engineering. The advent of neural networks has initiated\na significant shift in tackling these complexities though challenges in\naccuracy persist, especially for initial value problems. In this paper, we\nintroduce the $\\textit{Time-Evolving Natural Gradient (TENG)}$, generalizing\ntime-dependent variational principles and optimization-based time integration,\nleveraging natural gradient optimization to obtain high accuracy in\nneural-network-based PDE solutions. Our comprehensive development includes\nalgorithms like TENG-Euler and its high-order variants, such as TENG-Heun,\ntailored for enhanced precision and efficiency. TENG's effectiveness is further\nvalidated through its performance, surpassing current leading methods and\nachieving machine precision in step-by-step optimizations across a spectrum of\nPDEs, including the heat equation, Allen-Cahn equation, and Burgers' equation.\n","authors":["Zhuo Chen","Jacob McCarran","Esteban Vizcaino","Marin Soljaƒçiƒá","Di Luo"],"pdf_url":"https://arxiv.org/pdf/2404.10771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17722v2","updated":"2024-04-16T17:54:06Z","published":"2023-10-26T18:32:05Z","title":"Large Language Models as Generalizable Policies for Embodied Tasks","summary":"  We show that large language models (LLMs) can be adapted to be generalizable\npolicies for embodied visual tasks. Our approach, called Large LAnguage model\nReinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take\nas input text instructions and visual egocentric observations and output\nactions directly in the environment. Using reinforcement learning, we train\nLLaRP to see and act solely through environmental interactions. We show that\nLLaRP is robust to complex paraphrasings of task instructions and can\ngeneralize to new tasks that require novel optimal behavior. In particular, on\n1,000 unseen tasks it achieves 42% success rate, 1.7x the success rate of other\ncommon learned baselines or zero-shot applications of LLMs. Finally, to aid the\ncommunity in studying language conditioned, massively multi-task, embodied AI\nproblems we release a novel benchmark, Language Rearrangement, consisting of\n150,000 training and 1,000 testing tasks for language-conditioned\nrearrangement. Video examples of LLaRP in unseen Language Rearrangement\ninstructions are at https://llm-rl.github.io.\n","authors":["Andrew Szot","Max Schwarzer","Harsh Agrawal","Bogdan Mazoure","Walter Talbott","Katherine Metcalf","Natalie Mackraz","Devon Hjelm","Alexander Toshev"],"pdf_url":"https://arxiv.org/pdf/2310.17722v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10769v1","updated":"2024-04-16T17:53:59Z","published":"2024-04-16T17:53:59Z","title":"Finite-dimensional approximations of push-forwards on locally analytic\n  functionals and truncation of least-squares polynomials","summary":"  This paper introduces a theoretical framework for investigating analytic maps\nfrom finite discrete data, elucidating mathematical machinery underlying the\npolynomial approximation with least-squares in multivariate situations. Our\napproach is to consider the push-forward on the space of locally analytic\nfunctionals, instead of directly handling the analytic map itself. We establish\na methodology enabling appropriate finite-dimensional approximation of the\npush-forward from finite discrete data, through the theory of the\nFourier--Borel transform and the Fock space. Moreover, we prove a rigorous\nconvergence result with a convergence rate. As an application, we prove that it\nis not the least-squares polynomial, but the polynomial obtained by truncating\nits higher-degree terms, that approximates analytic functions and further\nallows for approximation beyond the support of the data distribution. One\nadvantage of our theory is that it enables us to apply linear algebraic\noperations to the finite-dimensional approximation of the push-forward.\nUtilizing this, we prove the convergence of a method for approximating an\nanalytic vector field from finite data of the flow map of an ordinary\ndifferential equation.\n","authors":["Isao Ishikawa"],"pdf_url":"https://arxiv.org/pdf/2404.10769v1.pdf","comment":"30 pages. 2 figures. Comments are welcome"},{"id":"http://arxiv.org/abs/2310.09753v2","updated":"2024-04-16T17:53:37Z","published":"2023-10-15T06:45:38Z","title":"When can transformers reason with abstract symbols?","summary":"  We investigate the capabilities of transformer models on relational reasoning\ntasks. In these tasks, models are trained on a set of strings encoding abstract\nrelations, and are then tested out-of-distribution on data that contains\nsymbols that did not appear in the training dataset. We prove that for any\nrelational reasoning task in a large family of tasks, transformers learn the\nabstract relations and generalize to the test set when trained by gradient\ndescent on sufficiently large quantities of training data. This is in contrast\nto classical fully-connected networks, which we prove fail to learn to reason.\nOur results inspire modifications of the transformer architecture that add only\ntwo trainable parameters per head, and that we empirically demonstrate improve\ndata efficiency for learning to reason.\n","authors":["Enric Boix-Adsera","Omid Saremi","Emmanuel Abbe","Samy Bengio","Etai Littwin","Joshua Susskind"],"pdf_url":"https://arxiv.org/pdf/2310.09753v2.pdf","comment":"25 figures"},{"id":"http://arxiv.org/abs/2404.10764v1","updated":"2024-04-16T17:47:27Z","published":"2024-04-16T17:47:27Z","title":"Confidential Federated Computations","summary":"  Federated Learning and Analytics (FLA) have seen widespread adoption by\ntechnology platforms for processing sensitive on-device data. However, basic\nFLA systems have privacy limitations: they do not necessarily require\nanonymization mechanisms like differential privacy (DP), and provide limited\nprotections against a potentially malicious service provider. Adding DP to a\nbasic FLA system currently requires either adding excessive noise to each\ndevice's updates, or assuming an honest service provider that correctly\nimplements the mechanism and only uses the privatized outputs. Secure\nmultiparty computation (SMPC) -based oblivious aggregations can limit the\nservice provider's access to individual user updates and improve DP tradeoffs,\nbut the tradeoffs are still suboptimal, and they suffer from scalability\nchallenges and susceptibility to Sybil attacks. This paper introduces a novel\nsystem architecture that leverages trusted execution environments (TEEs) and\nopen-sourcing to both ensure confidentiality of server-side computations and\nprovide externally verifiable privacy properties, bolstering the robustness and\ntrustworthiness of private federated computations.\n","authors":["Hubert Eichner","Daniel Ramage","Kallista Bonawitz","Dzmitry Huba","Tiziano Santoro","Brett McLarnon","Timon Van Overveldt","Nova Fallen","Peter Kairouz","Albert Cheu","Katharine Daly","Adria Gascon","Marco Gruteser","Brendan McMahan"],"pdf_url":"https://arxiv.org/pdf/2404.10764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10761v1","updated":"2024-04-16T17:41:17Z","published":"2024-04-16T17:41:17Z","title":"TorchSurv: A Lightweight Package for Deep Survival Analysis","summary":"  TorchSurv is a Python package that serves as a companion tool to perform deep\nsurvival modeling within the PyTorch environment. Unlike existing libraries\nthat impose specific parametric forms, TorchSurv enables the use of custom\nPyTorch-based deep survival mod- els. With its lightweight design, minimal\ninput requirements, full PyTorch backend, and freedom from restrictive survival\nmodel parameterizations, TorchSurv facilitates efficient deep survival model\nimplementation and is particularly beneficial for high-dimensional and complex\ninput data scenarios\n","authors":["Melodie Monod","Peter Krusche","Qian Cao","Berkman Sahiner","Nicholas Petrick","David Ohlssen","Thibaud Coroller"],"pdf_url":"https://arxiv.org/pdf/2404.10761v1.pdf","comment":"https://opensource.nibr.com/torchsurv/"},{"id":"http://arxiv.org/abs/2403.13269v3","updated":"2024-04-16T17:37:12Z","published":"2024-03-20T03:07:50Z","title":"AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient\n  Fine-Tuning of Large Models","summary":"  We present a novel Parameter-Efficient Fine-Tuning (PEFT) method, dubbed as\nAdaptive Freezing of Low Rank Adaptation (AFLoRA). Specifically, for each\npre-trained frozen weight tensor, we add a parallel path of trainable low-rank\nmatrices, namely a down-projection and an up-projection matrix, each of which\nis followed by a feature transformation vector. Based on a novel freezing\nscore, we the incrementally freeze these projection matrices during fine-tuning\nto reduce the computation and alleviate over-fitting. Our experimental results\ndemonstrate that we can achieve state-of-the-art performance with an average\nimprovement of up to $0.85\\%$ as evaluated on GLUE benchmark while yeilding up\nto $9.5\\times$ fewer average trainable parameters. While compared in terms of\nruntime, AFLoRA can yield up to $1.86\\times$ improvement as opposed to similar\nPEFT alternatives. Besides the practical utility of our approach, we provide\ninsights on the trainability requirements of LoRA paths at different modules\nand the freezing schedule for the different projection matrices. Code will be\nreleased.\n","authors":["Zeyu Liu","Souvik Kundu","Anni Li","Junrui Wan","Lianghao Jiang","Peter Anthony Beerel"],"pdf_url":"https://arxiv.org/pdf/2403.13269v3.pdf","comment":"5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2404.08495v3","updated":"2024-04-16T17:36:39Z","published":"2024-04-12T14:25:49Z","title":"Dataset Reset Policy Optimization for RLHF","summary":"  Reinforcement Learning (RL) from Human Preference-based feedback is a popular\nparadigm for fine-tuning generative models, which has produced impressive\nmodels such as GPT-4 and Claude3 Opus. This framework often consists of two\nsteps: learning a reward model from an offline preference dataset followed by\nrunning online RL to optimize the learned reward model. In this work,\nleveraging the idea of reset, we propose a new RLHF algorithm with provable\nguarantees. Motivated by the fact that offline preference dataset provides\ninformative states (i.e., data that is preferred by the labelers), our new\nalgorithm, Dataset Reset Policy Optimization (DR-PO), integrates the existing\noffline preference dataset into the online policy training procedure via\ndataset reset: it directly resets the policy optimizer to the states in the\noffline dataset, instead of always starting from the initial state\ndistribution. In theory, we show that DR-PO learns to perform at least as good\nas any policy that is covered by the offline dataset under general function\napproximation with finite sample complexity. In experiments, we demonstrate\nthat on both the TL;DR summarization and the Anthropic Helpful Harmful (HH)\ndataset, the generation from DR-PO is better than that from Proximal Policy\nOptimization (PPO) and Direction Preference Optimization (DPO), under the\nmetric of GPT4 win-rate. Code for this work can be found at\nhttps://github.com/Cornell-RL/drpo.\n","authors":["Jonathan D. Chang","Wenhao Zhan","Owen Oertell","Kiant√© Brantley","Dipendra Misra","Jason D. Lee","Wen Sun"],"pdf_url":"https://arxiv.org/pdf/2404.08495v3.pdf","comment":"28 pages, 6 tables, 3 Figures, 3 Algorithms"},{"id":"http://arxiv.org/abs/2404.10759v1","updated":"2024-04-16T17:36:21Z","published":"2024-04-16T17:36:21Z","title":"Laplace-HDC: Understanding the geometry of binary hyperdimensional\n  computing","summary":"  This paper studies the geometry of binary hyperdimensional computing (HDC), a\ncomputational scheme in which data are encoded using high-dimensional binary\nvectors. We establish a result about the similarity structure induced by the\nHDC binding operator and show that the Laplace kernel naturally arises in this\nsetting, motivating our new encoding method Laplace-HDC, which improves upon\nprevious methods. We describe how our results indicate limitations of binary\nHDC in encoding spatial information from images and discuss potential\nsolutions, including using Haar convolutional features and the definition of a\ntranslation-equivariant HDC encoding. Several numerical experiments\nhighlighting the improved accuracy of Laplace-HDC in contrast to alternative\nmethods are presented. We also numerically study other aspects of the proposed\nframework such as robustness and the underlying translation-equivariant\nencoding.\n","authors":["Saeid Pourmand","Wyatt D. Whiting","Alireza Aghasi","Nicholas F. Marshall"],"pdf_url":"https://arxiv.org/pdf/2404.10759v1.pdf","comment":"23 pages, 7 figures"},{"id":"http://arxiv.org/abs/2404.10757v1","updated":"2024-04-16T17:35:25Z","published":"2024-04-16T17:35:25Z","title":"Deep Learning and LLM-based Methods Applied to Stellar Lightcurve\n  Classification","summary":"  Light curves serve as a valuable source of information on stellar formation\nand evolution. With the rapid advancement of machine learning techniques, it\ncan be effectively processed to extract astronomical patterns and information.\nIn this study, we present a comprehensive evaluation of deep-learning and large\nlanguage model (LLM) based models for the automatic classification of variable\nstar light curves, based on large datasets from the Kepler and K2 missions.\nSpecial emphasis is placed on Cepheids, RR Lyrae, and eclipsing binaries,\nexamining the influence of observational cadence and phase distribution on\nclassification precision. Employing AutoDL optimization, we achieve striking\nperformance with the 1D-Convolution+BiLSTM architecture and the Swin\nTransformer, hitting accuracies of 94\\% and 99\\% correspondingly, with the\nlatter demonstrating a notable 83\\% accuracy in discerning the elusive Type II\nCepheids-comprising merely 0.02\\% of the total dataset.We unveil StarWhisper\nLightCurve (LC), an innovative Series comprising three LLM-based models: LLM,\nmultimodal large language model (MLLM), and Large Audio Language Model (LALM).\nEach model is fine-tuned with strategic prompt engineering and customized\ntraining methods to explore the emergent abilities of these models for\nastronomical data. Remarkably, StarWhisper LC Series exhibit high accuracies\naround 90\\%, significantly reducing the need for explicit feature engineering,\nthereby paving the way for streamlined parallel data processing and the\nprogression of multifaceted multimodal models in astronomical applications. The\nstudy furnishes two detailed catalogs illustrating the impacts of phase and\nsampling intervals on deep learning classification accuracy, showing that a\nsubstantial decrease of up to 14\\% in observation duration and 21\\% in sampling\npoints can be realized without compromising accuracy by more than 10\\%.\n","authors":["Yu-Yang Li","Yu Bai","Cunshi Wang","Mengwei Qu","Ziteng Lu","Roberto Soria","Jifeng Liu"],"pdf_url":"https://arxiv.org/pdf/2404.10757v1.pdf","comment":"35 pages, 20 figures"},{"id":"http://arxiv.org/abs/2012.15019v3","updated":"2024-04-16T17:27:34Z","published":"2020-12-30T03:22:35Z","title":"Privacy-Constrained Policies via Mutual Information Regularized Policy\n  Gradients","summary":"  As reinforcement learning techniques are increasingly applied to real-world\ndecision problems, attention has turned to how these algorithms use potentially\nsensitive information. We consider the task of training a policy that maximizes\nreward while minimizing disclosure of certain sensitive state variables through\nthe actions. We give examples of how this setting covers real-world problems in\nprivacy for sequential decision-making. We solve this problem in the policy\ngradients framework by introducing a regularizer based on the mutual\ninformation (MI) between the sensitive state and the actions. We develop a\nmodel-based stochastic gradient estimator for optimization of\nprivacy-constrained policies. We also discuss an alternative MI regularizer\nthat serves as an upper bound to our main MI regularizer and can be optimized\nin a model-free setting, and a powerful direct estimator that can be used in an\nenvironment with differentiable dynamics. We contrast previous work in\ndifferentially-private RL to our mutual-information formulation of information\ndisclosure. Experimental results show that our training method results in\npolicies that hide the sensitive state, even in challenging high-dimensional\ntasks.\n","authors":["Chris Cundy","Rishi Desai","Stefano Ermon"],"pdf_url":"https://arxiv.org/pdf/2012.15019v3.pdf","comment":"Accepted to AISTATS 2024"},{"id":"http://arxiv.org/abs/2305.14502v2","updated":"2024-04-16T17:25:25Z","published":"2023-05-23T20:15:56Z","title":"RetICL: Sequential Retrieval of In-Context Examples with Reinforcement\n  Learning","summary":"  Recent developments in large pre-trained language models have enabled\nunprecedented performance on a variety of downstream tasks. Achieving best\nperformance with these models often leverages in-context learning, where a\nmodel performs a (possibly new) task given one or more examples. However,\nrecent work has shown that the choice of examples can have a large impact on\ntask performance and that finding an optimal set of examples is non-trivial.\nWhile there are many existing methods for selecting in-context examples, they\ngenerally score examples independently, ignoring the dependency between them\nand the order in which they are provided to the model. In this work, we propose\nRetrieval for In-Context Learning (RetICL), a learnable method for modeling and\noptimally selecting examples sequentially for in-context learning. We frame the\nproblem of sequential example selection as a Markov decision process and train\nan example retriever using reinforcement learning. We evaluate RetICL on math\nword problem solving and scientific question answering tasks and show that it\nconsistently outperforms or matches heuristic and learnable baselines. We also\nuse case studies to show that RetICL implicitly learns representations of\nproblem solving strategies.\n","authors":["Alexander Scarlatos","Andrew Lan"],"pdf_url":"https://arxiv.org/pdf/2305.14502v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10746v1","updated":"2024-04-16T17:24:22Z","published":"2024-04-16T17:24:22Z","title":"Interpolation and differentiation of alchemical degrees of freedom in\n  machine learning interatomic potentials","summary":"  Machine learning interatomic potentials (MLIPs) have become a workhorse of\nmodern atomistic simulations, and recently published universal MLIPs,\npre-trained on large datasets, have demonstrated remarkable accuracy and\ngeneralizability. However, the computational cost of MLIPs limits their\napplicability to chemically disordered systems requiring large simulation cells\nor to sample-intensive statistical methods. Here, we report the use of\ncontinuous and differentiable alchemical degrees of freedom in atomistic\nmaterials simulations, exploiting the fact that graph neural network MLIPs\nrepresent discrete elements as real-valued tensors. The proposed method\nintroduces alchemical atoms with corresponding weights into the input graph,\nalongside modifications to the message-passing and readout mechanisms of MLIPs,\nand allows smooth interpolation between the compositional states of materials.\nThe end-to-end differentiability of MLIPs enables efficient calculation of the\ngradient of energy with respect to the compositional weights. Leveraging these\ngradients, we propose methodologies for optimizing the composition of solid\nsolutions towards target macroscopic properties and conducting alchemical free\nenergy simulations to quantify the free energy of vacancy formation and\ncomposition changes. The approach offers an avenue for extending the\ncapabilities of universal MLIPs in the modeling of compositional disorder and\ncharacterizing the phase stabilities of complex materials systems.\n","authors":["Juno Nam","Rafael G√≥mez-Bombarelli"],"pdf_url":"https://arxiv.org/pdf/2404.10746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10745v1","updated":"2024-04-16T17:23:19Z","published":"2024-04-16T17:23:19Z","title":"Settling Constant Regrets in Linear Markov Decision Processes","summary":"  We study the constant regret guarantees in reinforcement learning (RL). Our\nobjective is to design an algorithm that incurs only finite regret over\ninfinite episodes with high probability. We introduce an algorithm,\nCert-LSVI-UCB, for misspecified linear Markov decision processes (MDPs) where\nboth the transition kernel and the reward function can be approximated by some\nlinear function up to misspecification level $\\zeta$. At the core of\nCert-LSVI-UCB is an innovative certified estimator, which facilitates a\nfine-grained concentration analysis for multi-phase value-targeted regression,\nenabling us to establish an instance-dependent regret bound that is constant\nw.r.t. the number of episodes. Specifically, we demonstrate that for an MDP\ncharacterized by a minimal suboptimality gap $\\Delta$, Cert-LSVI-UCB has a\ncumulative regret of $\\tilde{\\mathcal{O}}(d^3H^5/\\Delta)$ with high\nprobability, provided that the misspecification level $\\zeta$ is below\n$\\tilde{\\mathcal{O}}(\\Delta / (\\sqrt{d}H^2))$. Remarkably, this regret bound\nremains constant relative to the number of episodes $K$. To the best of our\nknowledge, Cert-LSVI-UCB is the first algorithm to achieve a constant,\ninstance-dependent, high-probability regret bound in RL with linear function\napproximation for infinite runs without relying on prior distribution\nassumptions. This not only highlights the robustness of Cert-LSVI-UCB to model\nmisspecification but also introduces novel algorithmic designs and analytical\ntechniques of independent interest.\n","authors":["Weitong Zhang","Zhiyuan Fan","Jiafan He","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2404.10745v1.pdf","comment":"46 pages, 2 tables"},{"id":"http://arxiv.org/abs/2312.12193v2","updated":"2024-04-16T17:06:37Z","published":"2023-12-19T14:27:26Z","title":"Gaussian process learning of nonlinear dynamics","summary":"  One of the pivotal tasks in scientific machine learning is to represent\nunderlying dynamical systems from time series data. Many methods for such\ndynamics learning explicitly require the derivatives of state data, which are\nnot directly available and can be approximated conventionally by finite\ndifferences. However, the discrete approximations of time derivatives may\nresult in poor estimations when state data are scarce and/or corrupted by\nnoise, thus compromising the predictiveness of the learned dynamical models. To\novercome this technical hurdle, we propose a new method that learns nonlinear\ndynamics through a Bayesian inference of characterizing model parameters. This\nmethod leverages a Gaussian process representation of states, and constructs a\nlikelihood function using the correlation between state data and their\nderivatives, yet prevents explicit evaluations of time derivatives. Through a\nBayesian scheme, a probabilistic estimate of the model parameters is given by\nthe posterior distribution, and thus a quantification is facilitated for\nuncertainties from noisy state data and the learning process. Specifically, we\nwill discuss the applicability of the proposed method to several typical\nscenarios for dynamical systems: identification and estimation with an affine\nparametrization, nonlinear parametric approximation without prior knowledge,\nand general parameter estimation for a given dynamical system.\n","authors":["Dongwei Ye","Mengwu Guo"],"pdf_url":"https://arxiv.org/pdf/2312.12193v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10730v1","updated":"2024-04-16T17:02:52Z","published":"2024-04-16T17:02:52Z","title":"Insight Gained from Migrating a Machine Learning Model to Intelligence\n  Processing Units","summary":"  The discoveries in this paper show that Intelligence Processing Units (IPUs)\noffer a viable accelerator alternative to GPUs for machine learning (ML)\napplications within the fields of materials science and battery research. We\ninvestigate the process of migrating a model from GPU to IPU and explore\nseveral optimization techniques, including pipelining and gradient\naccumulation, aimed at enhancing the performance of IPU-based models.\nFurthermore, we have effectively migrated a specialized model to the IPU\nplatform. This model is employed for predicting effective conductivity, a\nparameter crucial in ion transport processes, which govern the performance of\nmultiple charge and discharge cycles of batteries. The model utilizes a\nConvolutional Neural Network (CNN) architecture to perform prediction tasks for\neffective conductivity. The performance of this model on the IPU is found to be\ncomparable to its execution on GPUs. We also analyze the utilization and\nperformance of Graphcore's Bow IPU. Through benchmark tests, we observe\nsignificantly improved performance with the Bow IPU when compared to its\npredecessor, the Colossus IPU.\n","authors":["Hieu Le","Zhenhua He","Mai Le","Dhruva K. Chakravorty","Lisa M. Perez","Akhil Chilumuru","Yan Yao","Jiefu Chen"],"pdf_url":"https://arxiv.org/pdf/2404.10730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10728v1","updated":"2024-04-16T17:01:38Z","published":"2024-04-16T17:01:38Z","title":"Randomized Exploration in Cooperative Multi-Agent Reinforcement Learning","summary":"  We present the first study on provably efficient randomized exploration in\ncooperative multi-agent reinforcement learning (MARL). We propose a unified\nalgorithm framework for randomized exploration in parallel Markov Decision\nProcesses (MDPs), and two Thompson Sampling (TS)-type algorithms, CoopTS-PHE\nand CoopTS-LMC, incorporating the perturbed-history exploration (PHE) strategy\nand the Langevin Monte Carlo exploration (LMC) strategy respectively, which are\nflexible in design and easy to implement in practice. For a special class of\nparallel MDPs where the transition is (approximately) linear, we theoretically\nprove that both CoopTS-PHE and CoopTS-LMC achieve a\n$\\widetilde{\\mathcal{O}}(d^{3/2}H^2\\sqrt{MK})$ regret bound with communication\ncomplexity $\\widetilde{\\mathcal{O}}(dHM^2)$, where $d$ is the feature\ndimension, $H$ is the horizon length, $M$ is the number of agents, and $K$ is\nthe number of episodes. This is the first theoretical result for randomized\nexploration in cooperative MARL. We evaluate our proposed method on multiple\nparallel RL environments, including a deep exploration problem (\\textit{i.e.,}\n$N$-chain), a video game, and a real-world problem in energy systems. Our\nexperimental results support that our framework can achieve better performance,\neven under conditions of misspecified transition models. Additionally, we\nestablish a connection between our unified framework and the practical\napplication of federated learning.\n","authors":["Hao-Lun Hsu","Weixin Wang","Miroslav Pajic","Pan Xu"],"pdf_url":"https://arxiv.org/pdf/2404.10728v1.pdf","comment":"80 pages, 14 figures, 1 table. Hao-Lun Hsu and Weixin Wang\n  contributed equally to this work"},{"id":"http://arxiv.org/abs/2404.10727v1","updated":"2024-04-16T17:01:27Z","published":"2024-04-16T17:01:27Z","title":"How Deep Networks Learn Sparse and Hierarchical Data: the Sparse Random\n  Hierarchy Model","summary":"  Understanding what makes high-dimensional data learnable is a fundamental\nquestion in machine learning. On the one hand, it is believed that the success\nof deep learning lies in its ability to build a hierarchy of representations\nthat become increasingly more abstract with depth, going from simple features\nlike edges to more complex concepts. On the other hand, learning to be\ninsensitive to invariances of the task, such as smooth transformations for\nimage datasets, has been argued to be important for deep networks and it\nstrongly correlates with their performance. In this work, we aim to explain\nthis correlation and unify these two viewpoints. We show that by introducing\nsparsity to generative hierarchical models of data, the task acquires\ninsensitivity to spatial transformations that are discrete versions of smooth\ntransformations. In particular, we introduce the Sparse Random Hierarchy Model\n(SRHM), where we observe and rationalize that a hierarchical representation\nmirroring the hierarchical model is learnt precisely when such insensitivity is\nlearnt, thereby explaining the strong correlation between the latter and\nperformance. Moreover, we quantify how the sample complexity of CNNs learning\nthe SRHM depends on both the sparsity and hierarchical structure of the task.\n","authors":["Umberto Tomasini","Matthieu Wyart"],"pdf_url":"https://arxiv.org/pdf/2404.10727v1.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2404.10726v1","updated":"2024-04-16T16:59:50Z","published":"2024-04-16T16:59:50Z","title":"Automatic re-calibration of quantum devices by reinforcement learning","summary":"  During their operation, due to shifts in environmental conditions, devices\nundergo various forms of detuning from their optimal settings. Typically, this\nis addressed through control loops, which monitor variables and the device\nperformance, to maintain settings at their optimal values. Quantum devices are\nparticularly challenging since their functionality relies on precisely tuning\ntheir parameters. At the same time, the detailed modeling of the environmental\nbehavior is often computationally unaffordable, while a direct measure of the\nparameters defining the system state is costly and introduces extra noise in\nthe mechanism. In this study, we investigate the application of reinforcement\nlearning techniques to develop a model-free control loop for continuous\nrecalibration of quantum device parameters. Furthermore, we explore the\nadvantages of incorporating minimal environmental noise models. As an example,\nthe application to numerical simulations of a Kennedy receiver-based\nlong-distance quantum communication protocol is presented.\n","authors":["T. Crosta","L. Reb√≥n","F. Vilari√±o","J. M. Matera","M. Bilkis"],"pdf_url":"https://arxiv.org/pdf/2404.10726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.08630v4","updated":"2024-04-16T16:57:12Z","published":"2023-09-12T23:20:19Z","title":"PCN: A Deep Learning Approach to Jet Tagging Utilizing Novel Graph\n  Construction Methods and Chebyshev Graph Convolutions","summary":"  Jet tagging is a classification problem in high-energy physics experiments\nthat aims to identify the collimated sprays of subatomic particles, jets, from\nparticle collisions and tag them to their emitter particle. Advances in jet\ntagging present opportunities for searches of new physics beyond the Standard\nModel. Current approaches use deep learning to uncover hidden patterns in\ncomplex collision data. However, the representation of jets as inputs to a deep\nlearning model have been varied, and often, informative features are withheld\nfrom models. In this study, we propose a graph-based representation of a jet\nthat encodes the most information possible. To learn best from this\nrepresentation, we design Particle Chebyshev Network (PCN), a graph neural\nnetwork (GNN) using Chebyshev graph convolutions (ChebConv). ChebConv has been\ndemonstrated as an effective alternative to classical graph convolutions in\nGNNs and has yet to be explored in jet tagging. PCN achieves a substantial\nimprovement in accuracy over existing taggers and opens the door to future\nstudies into graph-based representations of jets and ChebConv layers in\nhigh-energy physics experiments. Code is available at\nhttps://github.com/YVSemlani/PCN-Jet-Tagging.\n","authors":["Yash Semlani","Mihir Relan","Krithik Ramesh"],"pdf_url":"https://arxiv.org/pdf/2309.08630v4.pdf","comment":"16 pages, 2 figures, and 7 tables"},{"id":"http://arxiv.org/abs/2401.17484v2","updated":"2024-04-16T16:55:35Z","published":"2024-01-30T22:37:24Z","title":"Pixel to Elevation: Learning to Predict Elevation Maps at Long Range\n  using Images for Autonomous Offroad Navigation","summary":"  Understanding terrain topology at long-range is crucial for the success of\noff-road robotic missions, especially when navigating at high-speeds. LiDAR\nsensors, which are currently heavily relied upon for geometric mapping, provide\nsparse measurements when mapping at greater distances. To address this\nchallenge, we present a novel learning-based approach capable of predicting\nterrain elevation maps at long-range using only onboard egocentric images in\nreal-time. Our proposed method is comprised of three main elements. First, a\ntransformer-based encoder is introduced that learns cross-view associations\nbetween the egocentric views and prior bird-eye-view elevation map predictions.\nSecond, an orientation-aware positional encoding is proposed to incorporate the\n3D vehicle pose information over complex unstructured terrain with multi-view\nvisual image features. Lastly, a history-augmented learn-able map embedding is\nproposed to achieve better temporal consistency between elevation map\npredictions to facilitate the downstream navigational tasks. We experimentally\nvalidate the applicability of our proposed approach for autonomous offroad\nrobotic navigation in complex and unstructured terrain using real-world offroad\ndriving data. Furthermore, the method is qualitatively and quantitatively\ncompared against the current state-of-the-art methods. Extensive field\nexperiments demonstrate that our method surpasses baseline models in accurately\npredicting terrain elevation while effectively capturing the overall terrain\ntopology at long-ranges. Finally, ablation studies are conducted to highlight\nand understand the effect of key components of the proposed approach and\nvalidate their suitability to improve offroad robotic navigation capabilities.\n","authors":["Chanyoung Chung","Georgios Georgakis","Patrick Spieler","Curtis Padgett","Shehryar Khattak"],"pdf_url":"https://arxiv.org/pdf/2401.17484v2.pdf","comment":"8 pages, 6 figures, Accepted in IEEE Robotics and Automation Letters"},{"id":"http://arxiv.org/abs/2306.08538v2","updated":"2024-04-16T16:48:07Z","published":"2023-06-14T14:38:25Z","title":"Fast and Private Inference of Deep Neural Networks by Co-designing\n  Activation Functions","summary":"  Machine Learning as a Service (MLaaS) is an increasingly popular design where\na company with abundant computing resources trains a deep neural network and\noffers query access for tasks like image classification. The challenge with\nthis design is that MLaaS requires the client to reveal their potentially\nsensitive queries to the company hosting the model. Multi-party computation\n(MPC) protects the client's data by allowing encrypted inferences. However,\ncurrent approaches suffer from prohibitively large inference times. The\ninference time bottleneck in MPC is the evaluation of non-linear layers such as\nReLU activation functions. Motivated by the success of previous work\nco-designing machine learning and MPC, we develop an activation function\nco-design. We replace all ReLUs with a polynomial approximation and evaluate\nthem with single-round MPC protocols, which give state-of-the-art inference\ntimes in wide-area networks. Furthermore, to address the accuracy issues\npreviously encountered with polynomial activations, we propose a novel training\nalgorithm that gives accuracy competitive with plaintext models. Our evaluation\nshows between $3$ and $110\\times$ speedups in inference time on large models\nwith up to $23$ million parameters while maintaining competitive inference\naccuracy.\n","authors":["Abdulrahman Diaa","Lucas Fenaux","Thomas Humphries","Marian Dietz","Faezeh Ebrahimianghazani","Bailey Kacsmar","Xinda Li","Nils Lukas","Rasoul Akhavan Mahdavi","Simon Oya","Ehsan Amjadian","Florian Kerschbaum"],"pdf_url":"https://arxiv.org/pdf/2306.08538v2.pdf","comment":"To appear at USENIX Security 2024"},{"id":"http://arxiv.org/abs/2404.10715v1","updated":"2024-04-16T16:45:47Z","published":"2024-04-16T16:45:47Z","title":"Dynamic Frequency-Based Fingerprinting Attacks against Modern Sandbox\n  Environments","summary":"  The cloud computing landscape has evolved significantly in recent years,\nembracing various sandboxes to meet the diverse demands of modern cloud\napplications. These sandboxes encompass container-based technologies like\nDocker and gVisor, microVM-based solutions like Firecracker, and\nsecurity-centric sandboxes relying on Trusted Execution Environments (TEEs)\nsuch as Intel SGX and AMD SEV. However, the practice of placing multiple\ntenants on shared physical hardware raises security and privacy concerns, most\nnotably side-channel attacks.\n  In this paper, we investigate the possibility of fingerprinting containers\nthrough CPU frequency reporting sensors in Intel and AMD CPUs. One key enabler\nof our attack is that the current CPU frequency information can be accessed by\nuser-space attackers. We demonstrate that Docker images exhibit a unique\nfrequency signature, enabling the distinction of different containers with up\nto 84.5% accuracy even when multiple containers are running simultaneously in\ndifferent cores. Additionally, we assess the effectiveness of our attack when\nperformed against several sandboxes deployed in cloud environments, including\nGoogle's gVisor, AWS' Firecracker, and TEE-based platforms like Gramine\n(utilizing Intel SGX) and AMD SEV. Our empirical results show that these\nattacks can also be carried out successfully against all of these sandboxes in\nless than 40 seconds, with an accuracy of over 70% in all cases. Finally, we\npropose a noise injection-based countermeasure to mitigate the proposed attack\non cloud environments.\n","authors":["Debopriya Roy Dipta","Thore Tiemann","Berk Gulmezoglu","Eduard Marin Fabregas","Thomas Eisenbarth"],"pdf_url":"https://arxiv.org/pdf/2404.10715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18742v4","updated":"2024-04-16T16:38:37Z","published":"2024-03-27T16:39:28Z","title":"Understanding the Learning Dynamics of Alignment with Human Feedback","summary":"  Aligning large language models (LLMs) with human intentions has become a\ncritical task for safely deploying models in real-world systems. While existing\nalignment approaches have seen empirical success, theoretically understanding\nhow these methods affect model behavior remains an open question. Our work\nprovides an initial attempt to theoretically analyze the learning dynamics of\nhuman preference alignment. We formally show how the distribution of preference\ndatasets influences the rate of model updates and provide rigorous guarantees\non the training accuracy. Our theory also reveals an intricate phenomenon where\nthe optimization is prone to prioritizing certain behaviors with higher\npreference distinguishability. We empirically validate our findings on\ncontemporary LLMs and alignment tasks, reinforcing our theoretical insights and\nshedding light on considerations for future alignment approaches. Disclaimer:\nThis paper contains potentially offensive text; reader discretion is advised.\n","authors":["Shawn Im","Yixuan Li"],"pdf_url":"https://arxiv.org/pdf/2403.18742v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08886v2","updated":"2024-04-16T16:37:28Z","published":"2024-01-16T23:45:14Z","title":"RiemannONets: Interpretable Neural Operators for Riemann Problems","summary":"  Developing the proper representations for simulating high-speed flows with\nstrong shock waves, rarefactions, and contact discontinuities has been a\nlong-standing question in numerical analysis. Herein, we employ neural\noperators to solve Riemann problems encountered in compressible flows for\nextreme pressure jumps (up to $10^{10}$ pressure ratio). In particular, we\nfirst consider the DeepONet that we train in a two-stage process, following the\nrecent work of \\cite{lee2023training}, wherein the first stage, a basis is\nextracted from the trunk net, which is orthonormalized and subsequently is used\nin the second stage in training the branch net. This simple modification of\nDeepONet has a profound effect on its accuracy, efficiency, and robustness and\nleads to very accurate solutions to Riemann problems compared to the vanilla\nversion. It also enables us to interpret the results physically as the\nhierarchical data-driven produced basis reflects all the flow features that\nwould otherwise be introduced using ad hoc feature expansion layers. We also\ncompare the results with another neural operator based on the U-Net for low,\nintermediate, and very high-pressure ratios that are very accurate for Riemann\nproblems, especially for large pressure ratios, due to their multiscale nature\nbut computationally more expensive. Overall, our study demonstrates that simple\nneural network architectures, if properly pre-trained, can achieve very\naccurate solutions of Riemann problems for real-time forecasting. The source\ncode, along with its corresponding data, can be found at the following URL:\nhttps://github.com/apey236/RiemannONet/tree/main\n","authors":["Ahmad Peyvan","Vivek Oommen","Ameya D. Jagtap","George Em Karniadakis"],"pdf_url":"https://arxiv.org/pdf/2401.08886v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.12481v3","updated":"2024-04-16T16:27:15Z","published":"2022-06-24T19:43:33Z","title":"Analyzing Explainer Robustness via Probabilistic Lipschitzness of\n  Prediction Functions","summary":"  Machine learning methods have significantly improved in their predictive\ncapabilities, but at the same time they are becoming more complex and less\ntransparent. As a result, explainers are often relied on to provide\ninterpretability to these black-box prediction models. As crucial diagnostics\ntools, it is important that these explainers themselves are robust. In this\npaper we focus on one particular aspect of robustness, namely that an explainer\nshould give similar explanations for similar data inputs. We formalize this\nnotion by introducing and defining explainer astuteness, analogous to\nastuteness of prediction functions. Our formalism allows us to connect\nexplainer robustness to the predictor's probabilistic Lipschitzness, which\ncaptures the probability of local smoothness of a function. We provide lower\nbound guarantees on the astuteness of a variety of explainers (e.g., SHAP,\nRISE, CXPlain) given the Lipschitzness of the prediction function. These\ntheoretical results imply that locally smooth prediction functions lend\nthemselves to locally robust explanations. We evaluate these results\nempirically on simulated as well as real datasets.\n","authors":["Zulqarnain Khan","Davin Hill","Aria Masoomi","Joshua Bone","Jennifer Dy"],"pdf_url":"https://arxiv.org/pdf/2206.12481v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10700v1","updated":"2024-04-16T16:17:48Z","published":"2024-04-16T16:17:48Z","title":"Rawformer: Unpaired Raw-to-Raw Translation for Learnable Camera ISPs","summary":"  Modern smartphone camera quality heavily relies on the image signal processor\n(ISP) to enhance captured raw images, utilizing carefully designed modules to\nproduce final output images encoded in a standard color space (e.g., sRGB).\nNeural-based end-to-end learnable ISPs offer promising advancements,\npotentially replacing traditional ISPs with their ability to adapt without\nrequiring extensive tuning for each new camera model, as is often the case for\nnearly every module in traditional ISPs. However, the key challenge with the\nrecent learning-based ISPs is the urge to collect large paired datasets for\neach distinct camera model due to the influence of intrinsic camera\ncharacteristics on the formation of input raw images. This paper tackles this\nchallenge by introducing a novel method for unpaired learning of raw-to-raw\ntranslation across diverse cameras. Specifically, we propose Rawformer, an\nunsupervised Transformer-based encoder-decoder method for raw-to-raw\ntranslation. It accurately maps raw images captured by a certain camera to the\ntarget camera, facilitating the generalization of learnable ISPs to new unseen\ncameras. Our method demonstrates superior performance on real camera datasets,\nachieving higher accuracy compared to previous state-of-the-art techniques, and\npreserving a more robust correlation between the original and translated raw\nimages.\n","authors":["Georgy Perevozchikov","Nancy Mehta","Mahmoud Afifi","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2404.10700v1.pdf","comment":"15 pages, 5 figures"},{"id":"http://arxiv.org/abs/2302.03439v6","updated":"2024-04-16T16:13:00Z","published":"2023-02-07T12:51:20Z","title":"Ensemble Value Functions for Efficient Exploration in Multi-Agent\n  Reinforcement Learning","summary":"  Existing value-based algorithms for cooperative multi-agent reinforcement\nlearning (MARL) commonly rely on random exploration, such as $\\epsilon$-greedy,\nto explore the environment. However, such exploration is inefficient at finding\neffective joint actions in states that require cooperation of multiple agents.\nIn this work, we propose ensemble value functions for multi-agent exploration\n(EMAX), a general framework to seamlessly extend value-based MARL algorithms\nwith ensembles of value functions. EMAX leverages the ensemble of value\nfunctions to guide the exploration of agents, stabilises their optimisation,\nand makes their policies more robust to miscoordination. These benefits are\nachieved by using a combination of three techniques. (1) EMAX uses the\nuncertainty of value estimates across the ensemble in a UCB policy to guide the\nexploration. This exploration policy focuses on parts of the environment which\nrequire cooperation across agents and, thus, enables agents to more efficiently\nlearn how to cooperate. (2) During the optimisation, EMAX computes target\nvalues as average value estimates across the ensemble. These targets exhibit\nlower variance compared to commonly applied target networks, leading to\nsignificant benefits in MARL which commonly suffers from high variance caused\nby the exploration and non-stationary policies of other agents. (3) During\nevaluation, EMAX selects actions following a majority vote across the ensemble,\nwhich reduces the likelihood of selecting sub-optimal actions. We instantiate\nthree value-based MARL algorithms with EMAX, independent DQN, VDN and QMIX, and\nevaluate them in 21 tasks across four environments. Using ensembles of five\nvalue functions, EMAX improves sample efficiency and final evaluation returns\nof these algorithms by 60%, 47%, and 539%, respectively, averaged across 21\ntasks.\n","authors":["Lukas Sch√§fer","Oliver Slumbers","Stephen McAleer","Yali Du","Stefano V. Albrecht","David Mguni"],"pdf_url":"https://arxiv.org/pdf/2302.03439v6.pdf","comment":"Preprint. Previously presented at the Adaptive and Learning Agents\n  Workshop (ALA) at the AAMAS conference 2023"},{"id":"http://arxiv.org/abs/2404.10690v1","updated":"2024-04-16T16:10:23Z","published":"2024-04-16T16:10:23Z","title":"MathWriting: A Dataset For Handwritten Mathematical Expression\n  Recognition","summary":"  We introduce MathWriting, the largest online handwritten mathematical\nexpression dataset to date. It consists of 230k human-written samples and an\nadditional 400k synthetic ones. MathWriting can also be used for offline HME\nrecognition and is larger than all existing offline HME datasets like\nIM2LATEX-100K. We introduce a benchmark based on MathWriting data in order to\nadvance research on both online and offline HME recognition.\n","authors":["Philippe Gervais","Asya Fadeeva","Andrii Maksai"],"pdf_url":"https://arxiv.org/pdf/2404.10690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10689v1","updated":"2024-04-16T16:09:38Z","published":"2024-04-16T16:09:38Z","title":"Network architecture search of X-ray based scientific applications","summary":"  X-ray and electron diffraction-based microscopy use bragg peak detection and\nptychography to perform 3-D imaging at an atomic resolution. Typically, these\ntechniques are implemented using computationally complex tasks such as a\nPsuedo-Voigt function or solving a complex inverse problem. Recently, the use\nof deep neural networks has improved the existing state-of-the-art approaches.\nHowever, the design and development of the neural network models depends on\ntime and labor intensive tuning of the model by application experts. To that\nend, we propose a hyperparameter (HPS) and neural architecture search (NAS)\napproach to automate the design and optimization of the neural network models\nfor model size, energy consumption and throughput. We demonstrate the improved\nperformance of the auto-tuned models when compared to the manually tuned\nBraggNN and PtychoNN benchmark. We study and demonstrate the importance of the\nexploring the search space of tunable hyperparameters in enhancing the\nperformance of bragg peak detection and ptychographic reconstruction. Our NAS\nand HPS of (1) BraggNN achieves a 31.03\\% improvement in bragg peak detection\naccuracy with a 87.57\\% reduction in model size, and (2) PtychoNN achieves a\n16.77\\% improvement in model accuracy and a 12.82\\% reduction in model size\nwhen compared to the baseline PtychoNN model. When inferred on the Orin-AGX\nplatform, the optimized Braggnn and Ptychonn models demonstrate a 10.51\\% and\n9.47\\% reduction in inference latency and a 44.18\\% and 15.34\\% reduction in\nenergy consumption when compared to their respective baselines, when inferred\nin the Orin-AGX edge platform.\n","authors":["Adarsha Balaji","Ramyad Hadidi","Gregory Kollmer","Mohammed E. Fouda","Prasanna Balaprakash"],"pdf_url":"https://arxiv.org/pdf/2404.10689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10688v1","updated":"2024-04-16T16:08:59Z","published":"2024-04-16T16:08:59Z","title":"Efficient Conditional Diffusion Model with Probability Flow Sampling for\n  Image Super-resolution","summary":"  Image super-resolution is a fundamentally ill-posed problem because multiple\nvalid high-resolution images exist for one low-resolution image.\nSuper-resolution methods based on diffusion probabilistic models can deal with\nthe ill-posed nature by learning the distribution of high-resolution images\nconditioned on low-resolution images, avoiding the problem of blurry images in\nPSNR-oriented methods. However, existing diffusion-based super-resolution\nmethods have high time consumption with the use of iterative sampling, while\nthe quality and consistency of generated images are less than ideal due to\nproblems like color shifting. In this paper, we propose Efficient Conditional\nDiffusion Model with Probability Flow Sampling (ECDP) for image\nsuper-resolution. To reduce the time consumption, we design a continuous-time\nconditional diffusion model for image super-resolution, which enables the use\nof probability flow sampling for efficient generation. Additionally, to improve\nthe consistency of generated images, we propose a hybrid parametrization for\nthe denoiser network, which interpolates between the data-predicting\nparametrization and the noise-predicting parametrization for different noise\nscales. Moreover, we design an image quality loss as a complement to the score\nmatching loss of diffusion models, further improving the consistency and\nquality of super-resolution. Extensive experiments on DIV2K, ImageNet, and\nCelebA demonstrate that our method achieves higher super-resolution quality\nthan existing diffusion-based image super-resolution methods while having lower\ntime consumption. Our code is available at https://github.com/Yuan-Yutao/ECDP.\n","authors":["Yutao Yuan","Chun Yuan"],"pdf_url":"https://arxiv.org/pdf/2404.10688v1.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2308.16360v3","updated":"2024-04-16T16:08:28Z","published":"2023-08-30T23:26:33Z","title":"Emoji Promotes Developer Participation and Issue Resolution on GitHub","summary":"  Although remote working is increasingly adopted during the pandemic, many are\nconcerned by the low-efficiency in the remote working. Missing in text-based\ncommunication are non-verbal cues such as facial expressions and body language,\nwhich hinders the effective communication and negatively impacts the work\noutcomes. Prevalent on social media platforms, emojis, as alternative\nnon-verbal cues, are gaining popularity in the virtual workspaces well. In this\npaper, we study how emoji usage influences developer participation and issue\nresolution in virtual workspaces. To this end, we collect GitHub issues for a\none-year period and apply causal inference techniques to measure the causal\neffect of emojis on the outcome of issues, controlling for confounders such as\nissue content, repository, and author information. We find that emojis can\nsignificantly reduce the resolution time of issues and attract more user\nparticipation. We also compare the heterogeneous effect on different types of\nissues. These findings deepen our understanding of the developer communities,\nand they provide design implications on how to facilitate interactions and\nbroaden developer participation.\n","authors":["Yuhang Zhou","Xuan Lu","Ge Gao","Qiaozhu Mei","Wei Ai"],"pdf_url":"https://arxiv.org/pdf/2308.16360v3.pdf","comment":"Accepted by the 18th International AAAI Conference on Web and Social\n  Media (ICWSM 2024)"},{"id":"http://arxiv.org/abs/2312.13035v2","updated":"2024-04-16T16:08:01Z","published":"2023-12-20T13:59:43Z","title":"Evolutionary Optimization of 1D-CNN for Non-contact Respiration Pattern\n  Classification","summary":"  In this study, we present a deep learning-based approach for time-series\nrespiration data classification. The dataset contains regular breathing\npatterns as well as various forms of abnormal breathing, obtained through\nnon-contact incoherent light-wave sensing (LWS) technology. Given the\none-dimensional (1D) nature of the data, we employed a 1D convolutional neural\nnetwork (1D-CNN) for classification purposes. Genetic algorithm was employed to\noptimize the 1D-CNN architecture to maximize classification accuracy.\nAddressing the computational complexity associated with training the 1D-CNN\nacross multiple generations, we implemented transfer learning from a\npre-trained model. This approach significantly reduced the computational time\nrequired for training, thereby enhancing the efficiency of the optimization\nprocess. This study contributes valuable insights into the potential\napplications of deep learning methodologies for enhancing respiratory anomaly\ndetection through precise and efficient respiration classification.\n","authors":["Md Zobaer Islam","Sabit Ekin","John F. O'Hara","Gary Yen"],"pdf_url":"https://arxiv.org/pdf/2312.13035v2.pdf","comment":"7 pages, 8 figures, accepted in International Conference on Fuzzy\n  Systems, Soft Computing, and Explainable AI (NAFIPS2024)"},{"id":"http://arxiv.org/abs/2404.10684v1","updated":"2024-04-16T16:04:11Z","published":"2024-04-16T16:04:11Z","title":"Driver Fatigue Prediction using Randomly Activated Neural Networks for\n  Smart Ridesharing Platforms","summary":"  Drivers in ridesharing platforms exhibit cognitive atrophy and fatigue as\nthey accept ride offers along the day, which can have a significant impact on\nthe overall efficiency of the ridesharing platform. In contrast to the current\nliterature which focuses primarily on modeling and learning driver's\npreferences across different ride offers, this paper proposes a novel Dynamic\nDiscounted Satisficing (DDS) heuristic to model and predict driver's sequential\nride decisions during a given shift. Based on DDS heuristic, a novel stochastic\nneural network with random activations is proposed to model DDS heuristic and\npredict the final decision made by a given driver. The presence of random\nactivations in the network necessitated the development of a novel training\nalgorithm called Sampling-Based Back Propagation Through Time (SBPTT), where\ngradients are computed for independent instances of neural networks (obtained\nvia sampling the distribution of activation threshold) and aggregated to update\nthe network parameters. Using both simulation experiments as well as on real\nChicago taxi dataset, this paper demonstrates the improved performance of the\nproposed approach, when compared to state-of-the-art methods.\n","authors":["Sree Pooja Akula","Mukund Telukunta","Venkata Sriram Siddhardh Nadendla"],"pdf_url":"https://arxiv.org/pdf/2404.10684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.03713v4","updated":"2024-04-16T16:00:09Z","published":"2023-01-09T23:19:40Z","title":"Noncontact Respiratory Anomaly Detection Using Infrared Light-Wave\n  Sensing","summary":"  Human respiratory rate and its pattern convey essential information about the\nphysical and psychological states of the subject. Abnormal breathing can\nindicate fatal health issues leading to further diagnosis and treatment.\nWireless light-wave sensing (LWS) using incoherent infrared light shows promise\nin safe, discreet, efficient, and non-invasive human breathing monitoring\nwithout raising privacy concerns. The respiration monitoring system needs to be\ntrained on different types of breathing patterns to identify breathing\nanomalies.The system must also validate the collected data as a breathing\nwaveform, discarding any faulty data caused by external interruption, user\nmovement, or system malfunction. To address these needs, this study simulated\nnormal and different types of abnormal respiration using a robot that mimics\nhuman breathing patterns. Then, time-series respiration data were collected\nusing infrared light-wave sensing technology. Three machine learning\nalgorithms, decision tree, random forest and XGBoost, were applied to detect\nbreathing anomalies and faulty data. Model performances were evaluated through\ncross-validation, assessing classification accuracy, precision and recall\nscores. The random forest model achieved the highest classification accuracy of\n96.75% with data collected at a 0.5m distance. In general, ensemble models like\nrandom forest and XGBoost performed better than a single model in classifying\nthe data collected at multiple distances from the light-wave sensing setup.\n","authors":["Md Zobaer Islam","Brenden Martin","Carly Gotcher","Tyler Martinez","John F. O'Hara","Sabit Ekin"],"pdf_url":"https://arxiv.org/pdf/2301.03713v4.pdf","comment":"12 pages, 15 figures, published in IEEE Transactions on Human-Machine\n  Systems"},{"id":"http://arxiv.org/abs/2404.10678v1","updated":"2024-04-16T15:53:41Z","published":"2024-04-16T15:53:41Z","title":"Automating REST API Postman Test Cases Using LLM","summary":"  In the contemporary landscape of technological advancements, the automation\nof manual processes is crucial, compelling the demand for huge datasets to\neffectively train and test machines. This research paper is dedicated to the\nexploration and implementation of an automated approach to generate test cases\nspecifically using Large Language Models. The methodology integrates the use of\nOpen AI to enhance the efficiency and effectiveness of test case generation for\ntraining and evaluating Large Language Models. This formalized approach with\nLLMs simplifies the testing process, making it more efficient and\ncomprehensive. Leveraging natural language understanding, LLMs can\nintelligently formulate test cases that cover a broad range of REST API\nproperties, ensuring comprehensive testing. The model that is developed during\nthe research is trained using manually collected postman test cases or\ninstances for various Rest APIs. LLMs enhance the creation of Postman test\ncases by automating the generation of varied and intricate test scenarios.\nPostman test cases offer streamlined automation, collaboration, and dynamic\ndata handling, providing a user-friendly and efficient approach to API testing\ncompared to traditional test cases. Thus, the model developed not only conforms\nto current technological standards but also holds the promise of evolving into\nan idea of substantial importance in future technological advancements.\n","authors":["S Deepika Sri","Mohammed Aadil S","Sanjjushri Varshini R","Raja CSP Raman","Gopinath Rajagopal","S Taranath Chan"],"pdf_url":"https://arxiv.org/pdf/2404.10678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10664v1","updated":"2024-04-16T15:40:18Z","published":"2024-04-16T15:40:18Z","title":"Assessing The Impact of CNN Auto Encoder-Based Image Denoising on Image\n  Classification Tasks","summary":"  Images captured from the real world are often affected by different types of\nnoise, which can significantly impact the performance of Computer Vision\nsystems and the quality of visual data. This study presents a novel approach\nfor defect detection in casting product noisy images, specifically focusing on\nsubmersible pump impellers. The methodology involves utilizing deep learning\nmodels such as VGG16, InceptionV3, and other models in both the spatial and\nfrequency domains to identify noise types and defect status. The research\nprocess begins with preprocessing images, followed by applying denoising\ntechniques tailored to specific noise categories. The goal is to enhance the\naccuracy and robustness of defect detection by integrating noise detection and\ndenoising into the classification pipeline. The study achieved remarkable\nresults using VGG16 for noise type classification in the frequency domain,\nachieving an accuracy of over 99%. Removal of salt and pepper noise resulted in\nan average SSIM of 87.9, while Gaussian noise removal had an average SSIM of\n64.0, and periodic noise removal yielded an average SSIM of 81.6. This\ncomprehensive approach showcases the effectiveness of the deep AutoEncoder\nmodel and median filter, for denoising strategies in real-world industrial\napplications. Finally, our study reports significant improvements in binary\nclassification accuracy for defect detection compared to previous methods. For\nthe VGG16 classifier, accuracy increased from 94.6% to 97.0%, demonstrating the\neffectiveness of the proposed noise detection and denoising approach.\nSimilarly, for the InceptionV3 classifier, accuracy improved from 84.7% to\n90.0%, further validating the benefits of integrating noise analysis into the\nclassification pipeline.\n","authors":["Mohsen Hami","Mahdi JameBozorg"],"pdf_url":"https://arxiv.org/pdf/2404.10664v1.pdf","comment":"13 pages, 13 figures, 13th International conference on innovative\n  technologies in the field of science, engineering and technology"},{"id":"http://arxiv.org/abs/2404.10662v1","updated":"2024-04-16T15:39:11Z","published":"2024-04-16T15:39:11Z","title":"Continual Offline Reinforcement Learning via Diffusion-based Dual\n  Generative Replay","summary":"  We study continual offline reinforcement learning, a practical paradigm that\nfacilitates forward transfer and mitigates catastrophic forgetting to tackle\nsequential offline tasks. We propose a dual generative replay framework that\nretains previous knowledge by concurrent replay of generated pseudo-data.\nFirst, we decouple the continual learning policy into a diffusion-based\ngenerative behavior model and a multi-head action evaluation model, allowing\nthe policy to inherit distributional expressivity for encompassing a\nprogressive range of diverse behaviors. Second, we train a task-conditioned\ndiffusion model to mimic state distributions of past tasks. Generated states\nare paired with corresponding responses from the behavior generator to\nrepresent old tasks with high-fidelity replayed samples. Finally, by\ninterleaving pseudo samples with real ones of the new task, we continually\nupdate the state and behavior generators to model progressively diverse\nbehaviors, and regularize the multi-head critic via behavior cloning to\nmitigate forgetting. Experiments demonstrate that our method achieves better\nforward transfer with less forgetting, and closely approximates the results of\nusing previous ground-truth data due to its high-fidelity replay of the sample\nspace. Our code is available at\n\\href{https://github.com/NJU-RL/CuGRO}{https://github.com/NJU-RL/CuGRO}.\n","authors":["Jinmei Liu","Wenbin Li","Xiangyu Yue","Shilin Zhang","Chunlin Chen","Zhi Wang"],"pdf_url":"https://arxiv.org/pdf/2404.10662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07922v3","updated":"2024-04-16T15:33:45Z","published":"2024-04-11T17:09:28Z","title":"LaVy: Vietnamese Multimodal Large Language Model","summary":"  Large Language Models (LLMs) and Multimodal Large language models (MLLMs)\nhave taken the world by storm with impressive abilities in complex reasoning\nand linguistic comprehension. Meanwhile there are plethora of works related to\nVietnamese Large Language Models, the lack of high-quality resources in\nmultimodality limits the progress of Vietnamese MLLMs. In this paper, we\npioneer in address this by introducing LaVy, a state-of-the-art Vietnamese\nMLLM, and we also introduce LaVy-Bench benchmark designated for evaluating\nMLLMs's understanding on Vietnamese visual language tasks. Our project is\npublic at https://github.com/baochi0212/LaVy\n","authors":["Chi Tran","Huong Le Thanh"],"pdf_url":"https://arxiv.org/pdf/2404.07922v3.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2310.10745v2","updated":"2024-04-16T15:22:04Z","published":"2023-10-16T18:22:02Z","title":"Mori-Zwanzig latent space Koopman closure for nonlinear autoencoder","summary":"  The Koopman operator presents an attractive approach to achieve global\nlinearization of nonlinear systems, making it a valuable method for simplifying\nthe understanding of complex dynamics. While data-driven methodologies have\nexhibited promise in approximating finite Koopman operators, they grapple with\nvarious challenges, such as the judicious selection of observables,\ndimensionality reduction, and the ability to predict complex system behaviors\naccurately. This study presents a novel approach termed Mori-Zwanzig\nautoencoder (MZ-AE) to robustly approximate the Koopman operator in\nlow-dimensional spaces. The proposed method leverages a nonlinear autoencoder\nto extract key observables for approximating a finite invariant Koopman\nsubspace and integrates a non-Markovian correction mechanism using the\nMori-Zwanzig formalism. Consequently, this approach yields a closed\nrepresentation of dynamics within the latent manifold of the nonlinear\nautoencoder, thereby enhancing the precision and stability of the Koopman\noperator approximation. Demonstrations showcase the technique's ability to\ncapture regime transitions in the flow around a cylinder. It also provides a\nlow dimensional approximation for Kuramoto-Sivashinsky with promising\nshort-term predictability and robust long-term statistical performance. By\nbridging the gap between data-driven techniques and the mathematical\nfoundations of Koopman theory, MZ-AE offers a promising avenue for improved\nunderstanding and prediction of complex nonlinear dynamics.\n","authors":["Priyam Gupta","Peter J. Schmid","Denis Sipp","Taraneh Sayadi","Georgios Rigas"],"pdf_url":"https://arxiv.org/pdf/2310.10745v2.pdf","comment":"22 pages, 11 figures"},{"id":"http://arxiv.org/abs/2404.10645v1","updated":"2024-04-16T15:18:40Z","published":"2024-04-16T15:18:40Z","title":"Continuous Control Reinforcement Learning: Distributed Distributional\n  DrQ Algorithms","summary":"  Distributed Distributional DrQ is a model-free and off-policy RL algorithm\nfor continuous control tasks based on the state and observation of the agent,\nwhich is an actor-critic method with the data-augmentation and the\ndistributional perspective of critic value function. Aim to learn to control\nthe agent and master some tasks in a high-dimensional continuous space. DrQ-v2\nuses DDPG as the backbone and achieves out-performance in various continuous\ncontrol tasks. Here Distributed Distributional DrQ uses Distributed\nDistributional DDPG as the backbone, and this modification aims to achieve\nbetter performance in some hard continuous control tasks through the better\nexpression ability of distributional value function and distributed actor\npolicies.\n","authors":["Zehao Zhou"],"pdf_url":"https://arxiv.org/pdf/2404.10645v1.pdf","comment":"11 pages, 12 figures"},{"id":"http://arxiv.org/abs/2404.10642v1","updated":"2024-04-16T15:16:22Z","published":"2024-04-16T15:16:22Z","title":"Self-playing Adversarial Language Game Enhances LLM Reasoning","summary":"  We explore the self-play training procedure of large language models (LLMs)\nin a two-player adversarial language game called Adversarial Taboo. In this\ngame, an attacker and a defender communicate with respect to a target word only\nvisible to the attacker. The attacker aims to induce the defender to utter the\ntarget word unconsciously, while the defender tries to infer the target word\nfrom the attacker's utterances. To win the game, both players should have\nsufficient knowledge about the target word and high-level reasoning ability to\ninfer and express in this information-reserved conversation. Hence, we are\ncurious about whether LLMs' reasoning ability can be further enhanced by\nSelf-Play in this Adversarial language Game (SPAG). With this goal, we let LLMs\nact as the attacker and play with a copy of itself as the defender on an\nextensive range of target words. Through reinforcement learning on the game\noutcomes, we observe that the LLMs' performance uniformly improves on a broad\nrange of reasoning benchmarks. Furthermore, iteratively adopting this self-play\nprocess can continuously promote LLM's reasoning ability. The code is at\nhttps://github.com/Linear95/SPAG.\n","authors":["Pengyu Cheng","Tianhao Hu","Han Xu","Zhisong Zhang","Yong Dai","Lei Han","Nan Du"],"pdf_url":"https://arxiv.org/pdf/2404.10642v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2404.05316v2","updated":"2024-04-16T15:14:50Z","published":"2024-04-08T09:06:16Z","title":"HOEG: A New Approach for Object-Centric Predictive Process Monitoring","summary":"  Predictive Process Monitoring focuses on predicting future states of ongoing\nprocess executions, such as forecasting the remaining time. Recent developments\nin Object-Centric Process Mining have enriched event data with objects and\ntheir explicit relations between events. To leverage this enriched data, we\npropose the Heterogeneous Object Event Graph encoding (HOEG), which integrates\nevents and objects into a graph structure with diverse node types. It does so\nwithout aggregating object features, thus creating a more nuanced and\ninformative representation. We then adopt a heterogeneous Graph Neural Network\narchitecture, which incorporates these diverse object features in prediction\ntasks. We evaluate the performance and scalability of HOEG in predicting\nremaining time, benchmarking it against two established graph-based encodings\nand two baseline models. Our evaluation uses three Object-Centric Event Logs\n(OCELs), including one from a real-life process at a major Dutch financial\ninstitution. The results indicate that HOEG competes well with existing models\nand surpasses them when OCELs contain informative object attributes and\nevent-object interactions.\n","authors":["Tim K. Smit","Hajo A. Reijers","Xixi Lu"],"pdf_url":"https://arxiv.org/pdf/2404.05316v2.pdf","comment":"accepted to 36th International Conference on Advanced Information\n  Systems Engineering (CAISE), 2024"},{"id":"http://arxiv.org/abs/2307.13854v4","updated":"2024-04-16T15:13:18Z","published":"2023-07-25T22:59:32Z","title":"WebArena: A Realistic Web Environment for Building Autonomous Agents","summary":"  With advances in generative AI, there is now potential for autonomous agents\nto manage daily tasks via natural language commands. However, current agents\nare primarily created and tested in simplified synthetic environments, leading\nto a disconnect with real-world scenarios. In this paper, we build an\nenvironment for language-guided agents that is highly realistic and\nreproducible. Specifically, we focus on agents that perform tasks on the web,\nand create an environment with fully functional websites from four common\ndomains: e-commerce, social forum discussions, collaborative software\ndevelopment, and content management. Our environment is enriched with tools\n(e.g., a map) and external knowledge bases (e.g., user manuals) to encourage\nhuman-like task-solving. Building upon our environment, we release a set of\nbenchmark tasks focusing on evaluating the functional correctness of task\ncompletions. The tasks in our benchmark are diverse, long-horizon, and designed\nto emulate tasks that humans routinely perform on the internet. We experiment\nwith several baseline agents, integrating recent techniques such as reasoning\nbefore acting. The results demonstrate that solving complex tasks is\nchallenging: our best GPT-4-based agent only achieves an end-to-end task\nsuccess rate of 14.41%, significantly lower than the human performance of\n78.24%. These results highlight the need for further development of robust\nagents, that current state-of-the-art large language models are far from\nperfect performance in these real-life tasks, and that WebArena can be used to\nmeasure such progress.\n","authors":["Shuyan Zhou","Frank F. Xu","Hao Zhu","Xuhui Zhou","Robert Lo","Abishek Sridhar","Xianyi Cheng","Tianyue Ou","Yonatan Bisk","Daniel Fried","Uri Alon","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2307.13854v4.pdf","comment":"Our code, data, environment reproduction resources, and video\n  demonstrations are publicly available at https://webarena.dev/"},{"id":"http://arxiv.org/abs/2404.10630v1","updated":"2024-04-16T15:02:46Z","published":"2024-04-16T15:02:46Z","title":"HLAT: High-quality Large Language Model Pre-trained on AWS Trainium","summary":"  Getting large language models (LLMs) to perform well on the downstream tasks\nrequires pre-training over trillions of tokens. This typically demands a large\nnumber of powerful computational devices in addition to a stable distributed\ntraining framework to accelerate the training. The growing number of\napplications leveraging AI/ML had led to a scarcity of the expensive\nconventional accelerators (such as GPUs), which begs the need for the\nalternative specialized-accelerators that are scalable and cost-efficient. AWS\nTrainium is the second-generation machine learning accelerator that has been\npurposely built for training large deep learning models. Its corresponding\ninstance, Amazon EC2 trn1, is an alternative to GPU instances for LLM training.\nHowever, training LLMs with billions of parameters on trn1 is challenging due\nto its relatively nascent software ecosystem. In this paper, we showcase HLAT:\na 7 billion parameter decoder-only LLM pre-trained using trn1 instances over\n1.8 trillion tokens. The performance of HLAT is benchmarked against popular\nopen source baseline models including LLaMA and OpenLLaMA, which have been\ntrained on NVIDIA GPUs and Google TPUs, respectively. On various evaluation\ntasks, we show that HLAT achieves model quality on par with the baselines. We\nalso share the best practice of using the Neuron Distributed Training Library\n(NDTL), a customized distributed training library for AWS Trainium to achieve\nefficient training. Our work demonstrates that AWS Trainium powered by the NDTL\nis able to successfully pre-train state-of-the-art LLM models with high\nperformance and cost-effectiveness.\n","authors":["Haozheng Fan","Hao Zhou","Guangtai Huang","Parameswaran Raman","Xinwei Fu","Gaurav Gupta","Dhananjay Ram","Yida Wang","Jun Huan"],"pdf_url":"https://arxiv.org/pdf/2404.10630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09841v2","updated":"2024-04-16T14:55:13Z","published":"2024-04-15T14:48:43Z","title":"Anatomy of Industrial Scale Multilingual ASR","summary":"  This paper describes AssemblyAI's industrial-scale automatic speech\nrecognition (ASR) system, designed to meet the requirements of large-scale,\nmultilingual ASR serving various application needs. Our system leverages a\ndiverse training dataset comprising unsupervised (12.5M hours), supervised\n(188k hours), and pseudo-labeled (1.6M hours) data across four languages. We\nprovide a detailed description of our model architecture, consisting of a\nfull-context 600M-parameter Conformer encoder pre-trained with BEST-RQ and an\nRNN-T decoder fine-tuned jointly with the encoder. Our extensive evaluation\ndemonstrates competitive word error rates (WERs) against larger and more\ncomputationally expensive models, such as Whisper large and Canary-1B.\nFurthermore, our architectural choices yield several key advantages, including\nan improved code-switching capability, a 5x inference speedup compared to an\noptimized Whisper baseline, a 30% reduction in hallucination rate on speech\ndata, and a 90% reduction in ambient noise compared to Whisper, along with\nsignificantly improved time-stamp accuracy. Throughout this work, we adopt a\nsystem-centric approach to analyzing various aspects of fully-fledged ASR\nmodels to gain practically relevant insights useful for real-world services\noperating at scale.\n","authors":["Francis McCann Ramirez","Luka Chkhetiani","Andrew Ehrenberg","Robert McHardy","Rami Botros","Yash Khare","Andrea Vanzo","Taufiquzzaman Peyash","Gabriel Oexle","Michael Liang","Ilya Sklyar","Enver Fakhan","Ahmed Etefy","Daniel McCrystal","Sam Flamini","Domenic Donato","Takuya Yoshioka"],"pdf_url":"https://arxiv.org/pdf/2404.09841v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14045v2","updated":"2024-04-16T14:45:44Z","published":"2024-02-21T13:06:48Z","title":"A Systematic Review of Low-Rank and Local Low-Rank Matrix Approximation\n  in Big Data Medical Imaging","summary":"  The large volume and complexity of medical imaging datasets are bottlenecks\nfor storage, transmission, and processing. To tackle these challenges, the\napplication of low-rank matrix approximation (LRMA) and its derivative, local\nLRMA (LLRMA) has demonstrated potential.\n  A detailed analysis of the literature identifies LRMA and LLRMA methods\napplied to various imaging modalities, and the challenges and limitations\nassociated with existing LRMA and LLRMA methods are addressed.\n  We note a significant shift towards a preference for LLRMA in the medical\nimaging field since 2015, demonstrating its potential and effectiveness in\ncapturing complex structures in medical data compared to LRMA. Acknowledging\nthe limitations of shallow similarity methods used with LLRMA, we suggest\nadvanced semantic image segmentation for similarity measure, explaining in\ndetail how it can measure similar patches and their feasibility.\n  We note that LRMA and LLRMA are mainly applied to unstructured medical data,\nand we propose extending their application to different medical data types,\nincluding structured and semi-structured. This paper also discusses how LRMA\nand LLRMA can be applied to regular data with missing entries and the impact of\ninaccuracies in predicting missing values and their effects. We discuss the\nimpact of patch size and propose the use of random search (RS) to determine the\noptimal patch size. To enhance feasibility, a hybrid approach using Bayesian\noptimization and RS is proposed, which could improve the application of LRMA\nand LLRMA in medical imaging.\n","authors":["Sisipho Hamlomo","Marcellin Atemkeng","Yusuf Brima","Chuneeta Nunhokee","Jeremy Baxter"],"pdf_url":"https://arxiv.org/pdf/2402.14045v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10620v1","updated":"2024-04-16T14:43:33Z","published":"2024-04-16T14:43:33Z","title":"PyTorchGeoNodes: Enabling Differentiable Shape Programs for 3D Shape\n  Reconstruction","summary":"  We propose PyTorchGeoNodes, a differentiable module for reconstructing 3D\nobjects from images using interpretable shape programs. In comparison to\ntraditional CAD model retrieval methods, the use of shape programs for 3D\nreconstruction allows for reasoning about the semantic properties of\nreconstructed objects, editing, low memory footprint, etc. However, the\nutilization of shape programs for 3D scene understanding has been largely\nneglected in past works. As our main contribution, we enable gradient-based\noptimization by introducing a module that translates shape programs designed in\nBlender, for example, into efficient PyTorch code. We also provide a method\nthat relies on PyTorchGeoNodes and is inspired by Monte Carlo Tree Search\n(MCTS) to jointly optimize discrete and continuous parameters of shape programs\nand reconstruct 3D objects for input scenes. In our experiments, we apply our\nalgorithm to reconstruct 3D objects in the ScanNet dataset and evaluate our\nresults against CAD model retrieval-based reconstructions. Our experiments\nindicate that our reconstructions match well the input scenes while enabling\nsemantic reasoning about reconstructed objects.\n","authors":["Sinisa Stekovic","Stefan Ainetter","Mattia D'Urso","Friedrich Fraundorfer","Vincent Lepetit"],"pdf_url":"https://arxiv.org/pdf/2404.10620v1.pdf","comment":"In Submission"},{"id":"http://arxiv.org/abs/2404.10618v1","updated":"2024-04-16T14:42:49Z","published":"2024-04-16T14:42:49Z","title":"Private Attribute Inference from Images with Vision-Language Models","summary":"  As large language models (LLMs) become ubiquitous in our daily tasks and\ndigital interactions, associated privacy risks are increasingly in focus. While\nLLM privacy research has primarily focused on the leakage of model training\ndata, it has recently been shown that the increase in models' capabilities has\nenabled LLMs to make accurate privacy-infringing inferences from previously\nunseen texts. With the rise of multimodal vision-language models (VLMs),\ncapable of understanding both images and text, a pertinent question is whether\nsuch results transfer to the previously unexplored domain of benign images\nposted online. To investigate the risks associated with the image reasoning\ncapabilities of newly emerging VLMs, we compile an image dataset with\nhuman-annotated labels of the image owner's personal attributes. In order to\nunderstand the additional privacy risk posed by VLMs beyond traditional human\nattribute recognition, our dataset consists of images where the inferable\nprivate attributes do not stem from direct depictions of humans. On this\ndataset, we evaluate the inferential capabilities of 7 state-of-the-art VLMs,\nfinding that they can infer various personal attributes at up to 77.6%\naccuracy. Concerningly, we observe that accuracy scales with the general\ncapabilities of the models, implying that future models can be misused as\nstronger adversaries, establishing an imperative for the development of\nadequate defenses.\n","authors":["Batuhan T√∂mek√ße","Mark Vero","Robin Staab","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2404.10618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10600v1","updated":"2024-04-16T14:26:55Z","published":"2024-04-16T14:26:55Z","title":"Intra-operative tumour margin evaluation in breast-conserving surgery\n  with deep learning","summary":"  A positive margin may result in an increased risk of local recurrences after\nbreast retention surgery for any malignant tumour. In order to reduce the\nnumber of positive margins would offer surgeon real-time intra-operative\ninformation on the presence of positive resection margins. This study aims to\ndesign an intra-operative tumour margin evaluation scheme by using specimen\nmammography in breast-conserving surgery. Total of 30 cases were evaluated and\ncompared with the manually determined contours by experienced physicians and\npathology report. The proposed method utilizes image thresholding to extract\nregions of interest and then performs a deep learning model, i.e. SegNet, to\nsegment tumour tissue. The margin width of normal tissues surrounding it is\nevaluated as the result. The desired size of margin around the tumor was set\nfor 10 mm. The smallest average difference to manual sketched margin (6.53 mm\n+- 5.84). In the all case, the SegNet architecture was utilized to obtain\ntissue specimen boundary and tumor contour, respectively. The simulation\nresults indicated that this technology is helpful in discriminating positive\nfrom negative margins in the intra-operative setting. The aim of proposed\nscheme was a potential procedure in the intra-operative measurement system. The\nexperimental results reveal that deep learning techniques can draw results that\nare consistent with pathology reports.\n","authors":["Wei-Chung Shia","Yu-Len Huang","Yi-Chun Chen","Hwa-Koon Wu","Dar-Ren Chen"],"pdf_url":"https://arxiv.org/pdf/2404.10600v1.pdf","comment":"1 pages, 6 figures and 2 tables"},{"id":"http://arxiv.org/abs/2404.08814v2","updated":"2024-04-16T14:17:51Z","published":"2024-04-12T21:14:20Z","title":"E3: Ensemble of Expert Embedders for Adapting Synthetic Image Detectors\n  to New Generators Using Limited Data","summary":"  As generative AI progresses rapidly, new synthetic image generators continue\nto emerge at a swift pace. Traditional detection methods face two main\nchallenges in adapting to these generators: the forensic traces of synthetic\nimages from new techniques can vastly differ from those learned during\ntraining, and access to data for these new generators is often limited. To\naddress these issues, we introduce the Ensemble of Expert Embedders (E3), a\nnovel continual learning framework for updating synthetic image detectors. E3\nenables the accurate detection of images from newly emerged generators using\nminimal training data. Our approach does this by first employing transfer\nlearning to develop a suite of expert embedders, each specializing in the\nforensic traces of a specific generator. Then, all embeddings are jointly\nanalyzed by an Expert Knowledge Fusion Network to produce accurate and reliable\ndetection decisions. Our experiments demonstrate that E3 outperforms existing\ncontinual learning methods, including those developed specifically for\nsynthetic image detection.\n","authors":["Aref Azizpour","Tai D. Nguyen","Manil Shrestha","Kaidi Xu","Edward Kim","Matthew C. Stamm"],"pdf_url":"https://arxiv.org/pdf/2404.08814v2.pdf","comment":"11 pages, 4 figures, To be published in CVPRWMF24"},{"id":"http://arxiv.org/abs/2403.14421v2","updated":"2024-04-16T14:16:48Z","published":"2024-03-21T14:17:28Z","title":"DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning","summary":"  Text-to-image diffusion models have been shown to suffer from sample-level\nmemorization, possibly reproducing near-perfect replica of images that they are\ntrained on, which may be undesirable. To remedy this issue, we develop the\nfirst differentially private (DP) retrieval-augmented generation algorithm that\nis capable of generating high-quality image samples while providing provable\nprivacy guarantees. Specifically, we assume access to a text-to-image diffusion\nmodel trained on a small amount of public data, and design a DP retrieval\nmechanism to augment the text prompt with samples retrieved from a private\nretrieval dataset. Our \\emph{differentially private retrieval-augmented\ndiffusion model} (DP-RDM) requires no fine-tuning on the retrieval dataset to\nadapt to another domain, and can use state-of-the-art generative models to\ngenerate high-quality image samples while satisfying rigorous DP guarantees.\nFor instance, when evaluated on MS-COCO, our DP-RDM can generate samples with a\nprivacy budget of $\\epsilon=10$, while providing a $3.5$ point improvement in\nFID compared to public-only retrieval for up to $10,000$ queries.\n","authors":["Jonathan Lebensold","Maziar Sanjabi","Pietro Astolfi","Adriana Romero-Soriano","Kamalika Chaudhuri","Mike Rabbat","Chuan Guo"],"pdf_url":"https://arxiv.org/pdf/2403.14421v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10588v1","updated":"2024-04-16T14:13:44Z","published":"2024-04-16T14:13:44Z","title":"Do Counterfactual Examples Complicate Adversarial Training?","summary":"  We leverage diffusion models to study the robustness-performance tradeoff of\nrobust classifiers. Our approach introduces a simple, pretrained diffusion\nmethod to generate low-norm counterfactual examples (CEs): semantically altered\ndata which results in different true class membership. We report that the\nconfidence and accuracy of robust models on their clean training data are\nassociated with the proximity of the data to their CEs. Moreover, robust models\nperform very poorly when evaluated on the CEs directly, as they become\nincreasingly invariant to the low-norm, semantic changes brought by CEs. The\nresults indicate a significant overlap between non-robust and semantic\nfeatures, countering the common assumption that non-robust features are not\ninterpretable.\n","authors":["Eric Yeats","Cameron Darwin","Eduardo Ortega","Frank Liu","Hai Li"],"pdf_url":"https://arxiv.org/pdf/2404.10588v1.pdf","comment":"Accepted as a short paper to the GCV Workshop at CVPR'24"},{"id":"http://arxiv.org/abs/2310.01685v3","updated":"2024-04-16T14:10:59Z","published":"2023-10-02T22:46:49Z","title":"A Framework for Interpretability in Machine Learning for Medical Imaging","summary":"  Interpretability for machine learning models in medical imaging (MLMI) is an\nimportant direction of research. However, there is a general sense of murkiness\nin what interpretability means. Why does the need for interpretability in MLMI\narise? What goals does one actually seek to address when interpretability is\nneeded? To answer these questions, we identify a need to formalize the goals\nand elements of interpretability in MLMI. By reasoning about real-world tasks\nand goals common in both medical image analysis and its intersection with\nmachine learning, we identify five core elements of interpretability:\nlocalization, visual recognizability, physical attribution, model transparency,\nand actionability. From this, we arrive at a framework for interpretability in\nMLMI, which serves as a step-by-step guide to approaching interpretability in\nthis context. Overall, this paper formalizes interpretability needs in the\ncontext of medical imaging, and our applied perspective clarifies concrete\nMLMI-specific goals and considerations in order to guide method design and\nimprove real-world usage. Our goal is to provide practical and didactic\ninformation for model designers and practitioners, inspire developers of models\nin the medical imaging field to reason more deeply about what interpretability\nis achieving, and suggest future directions of interpretability research.\n","authors":["Alan Q. Wang","Batuhan K. Karaman","Heejong Kim","Jacob Rosenthal","Rachit Saluja","Sean I. Young","Mert R. Sabuncu"],"pdf_url":"https://arxiv.org/pdf/2310.01685v3.pdf","comment":"Published in IEEE Access"},{"id":"http://arxiv.org/abs/2404.10580v1","updated":"2024-04-16T14:05:29Z","published":"2024-04-16T14:05:29Z","title":"Data-driven subgrouping of patient trajectories with chronic diseases:\n  Evidence from low back pain","summary":"  Clinical data informs the personalization of health care with a potential for\nmore effective disease management. In practice, this is achieved by\nsubgrouping, whereby clusters with similar patient characteristics are\nidentified and then receive customized treatment plans with the goal of\ntargeting subgroup-specific disease dynamics. In this paper, we propose a novel\nmixture hidden Markov model for subgrouping patient trajectories from chronic\ndiseases. Our model is probabilistic and carefully designed to capture\ndifferent trajectory phases of chronic diseases (i.e., \"severe\", \"moderate\",\nand \"mild\") through tailored latent states. We demonstrate our subgrouping\nframework based on a longitudinal study across 847 patients with non-specific\nlow back pain. Here, our subgrouping framework identifies 8 subgroups. Further,\nwe show that our subgrouping framework outperforms common baselines in terms of\ncluster validity indices. Finally, we discuss the applicability of the model to\nother chronic and long-lasting diseases.\n","authors":["Christof Naumzik","Alice Kongsted","Werner Vach","Stefan Feuerriegel"],"pdf_url":"https://arxiv.org/pdf/2404.10580v1.pdf","comment":"Forthcoming at Conference on Health, Inference, and Learning (CHIL)\n  2024"},{"id":"http://arxiv.org/abs/2404.10575v1","updated":"2024-04-16T13:53:58Z","published":"2024-04-16T13:53:58Z","title":"EMC$^2$: Efficient MCMC Negative Sampling for Contrastive Learning with\n  Global Convergence","summary":"  A key challenge in contrastive learning is to generate negative samples from\na large sample set to contrast with positive samples, for learning better\nencoding of the data. These negative samples often follow a softmax\ndistribution which are dynamically updated during the training process.\nHowever, sampling from this distribution is non-trivial due to the high\ncomputational costs in computing the partition function. In this paper, we\npropose an Efficient Markov Chain Monte Carlo negative sampling method for\nContrastive learning (EMC$^2$). We follow the global contrastive learning loss\nas introduced in SogCLR, and propose EMC$^2$ which utilizes an adaptive\nMetropolis-Hastings subroutine to generate hardness-aware negative samples in\nan online fashion during the optimization. We prove that EMC$^2$ finds an\n$\\mathcal{O}(1/\\sqrt{T})$-stationary point of the global contrastive loss in\n$T$ iterations. Compared to prior works, EMC$^2$ is the first algorithm that\nexhibits global convergence (to stationarity) regardless of the choice of batch\nsize while exhibiting low computation and memory cost. Numerical experiments\nvalidate that EMC$^2$ is effective with small batch training and achieves\ncomparable or better performance than baseline algorithms. We report the\nresults for pre-training image encoders on STL-10 and Imagenet-100.\n","authors":["Chung-Yiu Yau","Hoi-To Wai","Parameswaran Raman","Soumajyoti Sarkar","Mingyi Hong"],"pdf_url":"https://arxiv.org/pdf/2404.10575v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2404.10574v1","updated":"2024-04-16T13:52:00Z","published":"2024-04-16T13:52:00Z","title":"Uncertainty-guided Open-Set Source-Free Unsupervised Domain Adaptation\n  with Target-private Class Segregation","summary":"  Standard Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from\na labeled source domain to an unlabeled target but usually requires\nsimultaneous access to both source and target data. Moreover, UDA approaches\ncommonly assume that source and target domains share the same labels space.\nYet, these two assumptions are hardly satisfied in real-world scenarios. This\npaper considers the more challenging Source-Free Open-set Domain Adaptation\n(SF-OSDA) setting, where both assumptions are dropped. We propose a novel\napproach for SF-OSDA that exploits the granularity of target-private categories\nby segregating their samples into multiple unknown classes. Starting from an\ninitial clustering-based assignment, our method progressively improves the\nsegregation of target-private samples by refining their pseudo-labels with the\nguide of an uncertainty-based sample selection module. Additionally, we propose\na novel contrastive loss, named NL-InfoNCELoss, that, integrating negative\nlearning into self-supervised contrastive learning, enhances the model\nrobustness to noisy pseudo-labels. Extensive experiments on benchmark datasets\ndemonstrate the superiority of the proposed method over existing approaches,\nestablishing new state-of-the-art performance. Notably, additional analyses\nshow that our method is able to learn the underlying semantics of novel\nclasses, opening the possibility to perform novel class discovery.\n","authors":["Mattia Litrico","Davide Talon","Sebastiano Battiato","Alessio Del Bue","Mario Valerio Giuffrida","Pietro Morerio"],"pdf_url":"https://arxiv.org/pdf/2404.10574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10561v1","updated":"2024-04-16T13:35:24Z","published":"2024-04-16T13:35:24Z","title":"HiGraphDTI: Hierarchical Graph Representation Learning for Drug-Target\n  Interaction Prediction","summary":"  The discovery of drug-target interactions (DTIs) plays a crucial role in\npharmaceutical development. The deep learning model achieves more accurate\nresults in DTI prediction due to its ability to extract robust and expressive\nfeatures from drug and target chemical structures. However, existing deep\nlearning methods typically generate drug features via aggregating molecular\natom representations, ignoring the chemical properties carried by motifs, i.e.,\nsubstructures of the molecular graph. The atom-drug double-level molecular\nrepresentation learning can not fully exploit structure information and fails\nto interpret the DTI mechanism from the motif perspective. In addition,\nsequential model-based target feature extraction either fuses limited\ncontextual information or requires expensive computational resources. To tackle\nthe above issues, we propose a hierarchical graph representation learning-based\nDTI prediction method (HiGraphDTI). Specifically, HiGraphDTI learns\nhierarchical drug representations from triple-level molecular graphs to\nthoroughly exploit chemical information embedded in atoms, motifs, and\nmolecules. Then, an attentional feature fusion module incorporates information\nfrom different receptive fields to extract expressive target features.Last, the\nhierarchical attention mechanism identifies crucial molecular segments, which\noffers complementary views for interpreting interaction mechanisms. The\nexperiment results not only demonstrate the superiority of HiGraphDTI to the\nstate-of-the-art methods, but also confirm the practical ability of our model\nin interaction interpretation and new DTI discovery.\n","authors":["Bin Liu","Siqi Wu","Jin Wang","Xin Deng","Ao Zhou"],"pdf_url":"https://arxiv.org/pdf/2404.10561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.16215v6","updated":"2024-04-16T13:32:25Z","published":"2023-08-30T16:44:38Z","title":"Deep Video Codec Control for Vision Models","summary":"  Standardized lossy video coding is at the core of almost all real-world video\nprocessing pipelines. Rate control is used to enable standard codecs to adapt\nto different network bandwidth conditions or storage constraints. However,\nstandard video codecs (e.g., H.264) and their rate control modules aim to\nminimize video distortion w.r.t. human quality assessment. We demonstrate\nempirically that standard-coded videos vastly deteriorate the performance of\ndeep vision models. To overcome the deterioration of vision performance, this\npaper presents the first end-to-end learnable deep video codec control that\nconsiders both bandwidth constraints and downstream deep vision performance,\nwhile adhering to existing standardization. We demonstrate that our approach\nbetter preserves downstream deep vision performance than traditional standard\nvideo coding.\n","authors":["Christoph Reich","Biplob Debnath","Deep Patel","Tim Prangemeier","Daniel Cremers","Srimat Chakradhar"],"pdf_url":"https://arxiv.org/pdf/2308.16215v6.pdf","comment":"Accepted at CVPR 2024 Workshop on AI for Streaming (AIS)"},{"id":"http://arxiv.org/abs/2310.14826v2","updated":"2024-04-16T13:25:38Z","published":"2023-10-23T11:45:34Z","title":"Sharp error bounds for imbalanced classification: how many examples in\n  the minority class?","summary":"  When dealing with imbalanced classification data, reweighting the loss\nfunction is a standard procedure allowing to equilibrate between the true\npositive and true negative rates within the risk measure. Despite significant\ntheoretical work in this area, existing results do not adequately address a\nmain challenge within the imbalanced classification framework, which is the\nnegligible size of one class in relation to the full sample size and the need\nto rescale the risk function by a probability tending to zero. To address this\ngap, we present two novel contributions in the setting where the rare class\nprobability approaches zero: (1) a non asymptotic fast rate probability bound\nfor constrained balanced empirical risk minimization, and (2) a consistent\nupper bound for balanced nearest neighbors estimates. Our findings provide a\nclearer understanding of the benefits of class-weighting in realistic settings,\nopening new avenues for further research in this field.\n","authors":["Anass Aghbalou","Fran√ßois Portier","Anne Sabourin"],"pdf_url":"https://arxiv.org/pdf/2310.14826v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10550v1","updated":"2024-04-16T13:19:46Z","published":"2024-04-16T13:19:46Z","title":"Analytical Approximation of the ELBO Gradient in the Context of the\n  Clutter Problem","summary":"  We propose an analytical solution for approximating the gradient of the\nEvidence Lower Bound (ELBO) in variational inference problems where the\nstatistical model is a Bayesian network consisting of observations drawn from a\nmixture of a Gaussian distribution embedded in unrelated clutter, known as the\nclutter problem. The method employs the reparameterization trick to move the\ngradient operator inside the expectation and relies on the assumption that,\nbecause the likelihood factorizes over the observed data, the variational\ndistribution is generally more compactly supported than the Gaussian\ndistribution in the likelihood factors. This allows efficient local\napproximation of the individual likelihood factors, which leads to an\nanalytical solution for the integral defining the gradient expectation. We\nintegrate the proposed gradient approximation as the expectation step in an EM\n(Expectation Maximization) algorithm for maximizing ELBO and test against\nclassical deterministic approaches in Bayesian inference, such as the Laplace\napproximation, Expectation Propagation and Mean-Field Variational Inference.\nThe proposed method demonstrates good accuracy and rate of convergence together\nwith linear computational complexity.\n","authors":["Roumen Nikolaev Popov"],"pdf_url":"https://arxiv.org/pdf/2404.10550v1.pdf","comment":"16 pages, 4 figures, supporting code available at\n  https://github.com/rpopov42/elbo_gaa"},{"id":"http://arxiv.org/abs/2404.10548v1","updated":"2024-04-16T13:18:02Z","published":"2024-04-16T13:18:02Z","title":"Classification of Prostate Cancer in 3D Magnetic Resonance Imaging Data\n  based on Convolutional Neural Networks","summary":"  Prostate cancer is a commonly diagnosed cancerous disease among men\nworld-wide. Even with modern technology such as multi-parametric magnetic\nresonance tomography and guided biopsies, the process for diagnosing prostate\ncancer remains time consuming and requires highly trained professionals. In\nthis paper, different convolutional neural networks (CNN) are evaluated on\ntheir abilities to reliably classify whether an MRI sequence contains malignant\nlesions. Implementations of a ResNet, a ConvNet and a ConvNeXt for 3D image\ndata are trained and evaluated. The models are trained using different data\naugmentation techniques, learning rates, and optimizers. The data is taken from\na private dataset, provided by Cantonal Hospital Aarau. The best result was\nachieved by a ResNet3D, yielding an average precision score of 0.4583 and AUC\nROC score of 0.6214.\n","authors":["Malte Rippa","Ruben Schulze","Marian Himstedt","Felice Burn"],"pdf_url":"https://arxiv.org/pdf/2404.10548v1.pdf","comment":"Previous version published in Buzug T.M., Handels H., M\\\"uller S.,\n  H\\\"ubner C., Mertins A., Rostalski P.: Student Conference Proceedings 2023,\n  Infinite Science Publishing, 2023 (ISBN/EAN 978-3-945954-72-0). 7 pages, 2\n  figures"},{"id":"http://arxiv.org/abs/2404.10547v1","updated":"2024-04-16T13:16:41Z","published":"2024-04-16T13:16:41Z","title":"A/B testing under Interference with Partial Network Information","summary":"  A/B tests are often required to be conducted on subjects that might have\nsocial connections. For e.g., experiments on social media, or medical and\nsocial interventions to control the spread of an epidemic. In such settings,\nthe SUTVA assumption for randomized-controlled trials is violated due to\nnetwork interference, or spill-over effects, as treatments to group A can\npotentially also affect the control group B. When the underlying social network\nis known exactly, prior works have demonstrated how to conduct A/B tests\nadequately to estimate the global average treatment effect (GATE). However, in\npractice, it is often impossible to obtain knowledge about the exact underlying\nnetwork. In this paper, we present UNITE: a novel estimator that relax this\nassumption and can identify GATE while only relying on knowledge of the\nsuperset of neighbors for any subject in the graph. Through theoretical\nanalysis and extensive experiments, we show that the proposed approach performs\nbetter in comparison to standard estimators.\n","authors":["Shiv Shankar","Ritwik Sinha","Yash Chandak","Saayan Mitra","Madalina Fiterau"],"pdf_url":"https://arxiv.org/pdf/2404.10547v1.pdf","comment":"AISTATS 2024"},{"id":"http://arxiv.org/abs/2404.10546v1","updated":"2024-04-16T13:16:19Z","published":"2024-04-16T13:16:19Z","title":"Warm-Start Variational Quantum Policy Iteration","summary":"  Reinforcement learning is a powerful framework aiming to determine optimal\nbehavior in highly complex decision-making scenarios. This objective can be\nachieved using policy iteration, which requires to solve a typically large\nlinear system of equations. We propose the variational quantum policy iteration\n(VarQPI) algorithm, realizing this step with a NISQ-compatible quantum-enhanced\nsubroutine. Its scalability is supported by an analysis of the structure of\ngeneric reinforcement learning environments, laying the foundation for\npotential quantum advantage with utility-scale quantum computers. Furthermore,\nwe introduce the warm-start initialization variant (WS-VarQPI) that\nsignificantly reduces resource overhead. The algorithm solves a large\nFrozenLake environment with an underlying 256x256-dimensional linear system,\nindicating its practical robustness.\n","authors":["Nico Meyer","Jakob Murauer","Alexander Popov","Christian Ufrecht","Axel Plinge","Christopher Mutschler","Daniel D. Scherer"],"pdf_url":"https://arxiv.org/pdf/2404.10546v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible. 9 pages, 6 figures, 1 table"},{"id":"http://arxiv.org/abs/2311.15658v2","updated":"2024-04-16T12:58:57Z","published":"2023-11-27T09:40:14Z","title":"Regularization by Texts for Latent Diffusion Inverse Solvers","summary":"  The recent advent of diffusion models has led to significant progress in\nsolving inverse problems, leveraging these models as effective generative\npriors. Nonetheless, there remain challenges related to the ill-posed nature of\nsuch problems, often due to inherent ambiguities in measurements or intrinsic\nsystem symmetries. To address this, drawing inspiration from the human ability\nto resolve visual ambiguities through perceptual biases, here we introduce a\nnovel latent diffusion inverse solver by regularization by texts (TReg).\nSpecifically, TReg applies the textual description of the preconception of the\nsolution during the reverse diffusion sampling, of which the description is\ndynamically reinforced through null-text optimization for adaptive negation.\nOur comprehensive experimental results demonstrate that TReg successfully\nmitigates ambiguity in the inverse problems, enhancing their effectiveness and\naccuracy.\n","authors":["Jeongsol Kim","Geon Yeong Park","Hyungjin Chung","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2311.15658v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09793v2","updated":"2024-04-16T12:51:47Z","published":"2023-11-16T11:18:21Z","title":"Fossil 2.0: Formal Certificate Synthesis for the Verification and\n  Control of Dynamical Models","summary":"  This paper presents Fossil 2.0, a new major release of a software tool for\nthe synthesis of certificates (e.g., Lyapunov and barrier functions) for\ndynamical systems modelled as ordinary differential and difference equations.\nFossil 2.0 is much improved from its original release, including new\ninterfaces, a significantly expanded certificate portfolio, controller\nsynthesis and enhanced extensibility. We present these new features as part of\nthis tool paper. Fossil implements a counterexample-guided inductive synthesis\n(CEGIS) loop ensuring the soundness of the method. Our tool uses neural\nnetworks as templates to generate candidate functions, which are then formally\nproven by an SMT solver acting as an assertion verifier. Improvements with\nrespect to the first release include a wider range of certificates, synthesis\nof control laws, and support for discrete-time models.\n","authors":["Alec Edwards","Andrea Peruffo","Alessandro Abate"],"pdf_url":"https://arxiv.org/pdf/2311.09793v2.pdf","comment":"HSCC 2024 Tool Paper"},{"id":"http://arxiv.org/abs/2404.10513v1","updated":"2024-04-16T12:37:10Z","published":"2024-04-16T12:37:10Z","title":"CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\n  Granularity","summary":"  State-of-the-art performance in QA tasks is currently achieved by systems\nemploying Large Language Models (LLMs), however these models tend to\nhallucinate information in their responses. One approach focuses on enhancing\nthe generation process by incorporating attribution from the given input to the\noutput. However, the challenge of identifying appropriate attributions and\nverifying their accuracy against a source is a complex task that requires\nsignificant improvements in assessing such systems. We introduce an\nattribution-oriented Chain-of-Thought reasoning method to enhance the accuracy\nof attributions. This approach focuses the reasoning process on generating an\nattribution-centric output. Evaluations on two context-enhanced\nquestion-answering datasets using GPT-4 demonstrate improved accuracy and\ncorrectness of attributions. In addition, the combination of our method with\nfinetuning enhances the response and attribution accuracy of two smaller LLMs,\nshowing their potential to outperform GPT-4 in some cases.\n","authors":["Moshe Berchansky","Daniel Fleischer","Moshe Wasserblat","Peter Izsak"],"pdf_url":"https://arxiv.org/pdf/2404.10513v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.09827v2","updated":"2024-04-16T12:34:39Z","published":"2023-10-15T13:18:31Z","title":"VFLAIR: A Research Library and Benchmark for Vertical Federated Learning","summary":"  Vertical Federated Learning (VFL) has emerged as a collaborative training\nparadigm that allows participants with different features of the same group of\nusers to accomplish cooperative training without exposing their raw data or\nmodel parameters. VFL has gained significant attention for its research\npotential and real-world applications in recent years, but still faces\nsubstantial challenges, such as in defending various kinds of data inference\nand backdoor attacks. Moreover, most of existing VFL projects are\nindustry-facing and not easily used for keeping track of the current research\nprogress. To address this need, we present an extensible and lightweight VFL\nframework VFLAIR (available at https://github.com/FLAIR-THU/VFLAIR), which\nsupports VFL training with a variety of models, datasets and protocols, along\nwith standardized modules for comprehensive evaluations of attacks and defense\nstrategies. We also benchmark 11 attacks and 8 defenses performance under\ndifferent communication and model partition settings and draw concrete insights\nand recommendations on the choice of defense strategies for different practical\nVFL deployment scenarios.\n","authors":["Tianyuan Zou","Zixuan Gu","Yu He","Hideaki Takahashi","Yang Liu","Ya-Qin Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.09827v2.pdf","comment":"39 pages, 22 figures, 19 tabels"},{"id":"http://arxiv.org/abs/2404.10512v1","updated":"2024-04-16T12:33:44Z","published":"2024-04-16T12:33:44Z","title":"Four-hour thunderstorm nowcasting using deep diffusion models of\n  satellite","summary":"  Convection (thunderstorm) develops rapidly within hours and is highly\ndestructive, posing a significant challenge for nowcasting and resulting in\nsubstantial losses to nature and society. After the emergence of artificial\nintelligence (AI)-based methods, convection nowcasting has experienced rapid\nadvancements, with its performance surpassing that of physics-based numerical\nweather prediction and other conventional approaches. However, the lead time\nand coverage of it still leave much to be desired and hardly meet the needs of\ndisaster emergency response. Here, we propose a deep diffusion model of\nsatellite (DDMS) to establish an AI-based convection nowcasting system. On one\nhand, it employs diffusion processes to effectively simulate complicated\nspatiotemporal evolution patterns of convective clouds, significantly improving\nthe forecast lead time. On the other hand, it utilizes geostationary satellite\nbrightness temperature data, thereby achieving planetary-scale forecast\ncoverage. During long-term tests and objective validation based on the\nFengYun-4A satellite, our system achieves, for the first time, effective\nconvection nowcasting up to 4 hours, with broad coverage (about 20,000,000\nkm2), remarkable accuracy, and high resolution (15 minutes; 4 km). Its\nperformance reaches a new height in convection nowcasting compared to the\nexisting models. In terms of application, our system operates efficiently\n(forecasting 4 hours of convection in 8 minutes), and is highly transferable\nwith the potential to collaborate with multiple satellites for global\nconvection nowcasting. Furthermore, our results highlight the remarkable\ncapabilities of diffusion models in convective clouds forecasting, as well as\nthe significant value of geostationary satellite data when empowered by AI\ntechnologies.\n","authors":["Kuai Dai","Xutao Li","Junying Fang","Yunming Ye","Demin Yu","Di Xian","Danyu Qin"],"pdf_url":"https://arxiv.org/pdf/2404.10512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10501v1","updated":"2024-04-16T12:19:54Z","published":"2024-04-16T12:19:54Z","title":"Self-Supervised Visual Preference Alignment","summary":"  This paper makes the first attempt towards unsupervised preference alignment\nin Vision-Language Models (VLMs). We generate chosen and rejected responses\nwith regard to the original and augmented image pairs, and conduct preference\nalignment with direct preference optimization. It is based on a core idea:\nproperly designed augmentation to the image input will induce VLM to generate\nfalse but hard negative responses, which helps the model to learn from and\nproduce more robust and powerful answers. The whole pipeline no longer hinges\non supervision from GPT4 or human involvement during alignment, and is highly\nefficient with few lines of code. With only 8k randomly sampled unsupervised\ndata, it achieves 90\\% relative score to GPT-4 on complex reasoning in\nLLaVA-Bench, and improves LLaVA-7B/13B by 6.7\\%/5.6\\% score on complex\nmulti-modal benchmark MM-Vet. Visualizations shows its improved ability to\nalign with user-intentions. A series of ablations are firmly conducted to\nreveal the latent mechanism of the approach, which also indicates its potential\ntowards further scaling. Code will be available.\n","authors":["Ke Zhu","Liang Zhao","Zheng Ge","Xiangyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.10501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10494v1","updated":"2024-04-16T12:03:38Z","published":"2024-04-16T12:03:38Z","title":"BDAN: Mitigating Temporal Difference Across Electrodes in Cross-Subject\n  Motor Imagery Classification via Generative Bridging Domain","summary":"  Because of \"the non-repeatability of the experiment settings and conditions\"\nand \"the variability of brain patterns among subjects\", the data distributions\nacross sessions and electrodes are different in cross-subject motor imagery\n(MI) studies, eventually reducing the performance of the classification model.\nSystematically summarised based on the existing studies, a novel\ntemporal-electrode data distribution problem is investigated under both\nintra-subject and inter-subject scenarios in this paper. Based on the presented\nissue, a novel bridging domain adaptation network (BDAN) is proposed, aiming to\nminimise the data distribution difference across sessions in the aspect of the\nelectrode, thus improving and enhancing model performance. In the proposed\nBDAN, deep features of all the EEG data are extracted via a specially designed\nspatial feature extractor. With the obtained spatio-temporal features, a\nspecial generative bridging domain is established, bridging the data from all\nthe subjects across sessions. The difference across sessions and electrodes is\nthen minimized using the customized bridging loss functions, and the known\nknowledge is automatically transferred through the constructed bridging domain.\nTo show the effectiveness of the proposed BDAN, comparison experiments and\nablation studies are conducted on a public EEG dataset. The overall comparison\nresults demonstrate the superior performance of the proposed BDAN compared with\nthe other advanced deep learning and domain adaptation methods.\n","authors":["Zhige Chen","Rui Yang","Mengjie Huang","Chengxuan Qin","Zidong Wang"],"pdf_url":"https://arxiv.org/pdf/2404.10494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17701v3","updated":"2024-04-16T11:46:39Z","published":"2024-03-26T13:40:18Z","title":"Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical\n  Image Segmentation","summary":"  Image segmentation holds a vital position in the realms of diagnosis and\ntreatment within the medical domain. Traditional convolutional neural networks\n(CNNs) and Transformer models have made significant advancements in this realm,\nbut they still encounter challenges because of limited receptive field or high\ncomputing complexity. Recently, State Space Models (SSMs), particularly Mamba\nand its variants, have demonstrated notable performance in the field of vision.\nHowever, their feature extraction methods may not be sufficiently effective and\nretain some redundant structures, leaving room for parameter reduction.\nMotivated by previous spatial and channel attention methods, we propose Triplet\nMamba-UNet. The method leverages residual VSS Blocks to extract intensive\ncontextual features, while Triplet SSM is employed to fuse features across\nspatial and channel dimensions. We conducted experiments on ISIC17, ISIC18,\nCVC-300, CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB, and Kvasir-Instrument datasets,\ndemonstrating the superior segmentation performance of our proposed TM-UNet.\nAdditionally, compared to the previous VM-UNet, our model achieves a one-third\nreduction in parameters.\n","authors":["Hao Tang","Lianglun Cheng","Guoheng Huang","Zhengguang Tan","Junhao Lu","Kaihong Wu"],"pdf_url":"https://arxiv.org/pdf/2403.17701v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10483v1","updated":"2024-04-16T11:43:26Z","published":"2024-04-16T11:43:26Z","title":"Would You Trust an AI Doctor? Building Reliable Medical Predictions with\n  Kernel Dropout Uncertainty","summary":"  The growing capabilities of AI raise questions about their trustworthiness in\nhealthcare, particularly due to opaque decision-making and limited data\navailability. This paper proposes a novel approach to address these challenges,\nintroducing a Bayesian Monte Carlo Dropout model with kernel modelling. Our\nmodel is designed to enhance reliability on small medical datasets, a crucial\nbarrier to the wider adoption of AI in healthcare. This model leverages\nexisting language models for improved effectiveness and seamlessly integrates\nwith current workflows. We demonstrate significant improvements in reliability,\neven with limited data, offering a promising step towards building trust in\nAI-driven medical predictions and unlocking its potential to improve patient\ncare.\n","authors":["Ubaid Azam","Imran Razzak","Shelly Vishwakarma","Hakim Hacid","Dell Zhang","Shoaib Jameel"],"pdf_url":"https://arxiv.org/pdf/2404.10483v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10481v1","updated":"2024-04-16T11:42:06Z","published":"2024-04-16T11:42:06Z","title":"BayesJudge: Bayesian Kernel Language Modelling with Confidence\n  Uncertainty in Legal Judgment Prediction","summary":"  Predicting legal judgments with reliable confidence is paramount for\nresponsible legal AI applications. While transformer-based deep neural networks\n(DNNs) like BERT have demonstrated promise in legal tasks, accurately assessing\ntheir prediction confidence remains crucial. We present a novel Bayesian\napproach called BayesJudge that harnesses the synergy between deep learning and\ndeep Gaussian Processes to quantify uncertainty through Bayesian kernel Monte\nCarlo dropout. Our method leverages informative priors and flexible data\nmodelling via kernels, surpassing existing methods in both predictive accuracy\nand confidence estimation as indicated through brier score. Extensive\nevaluations of public legal datasets showcase our model's superior performance\nacross diverse tasks. We also introduce an optimal solution to automate the\nscrutiny of unreliable predictions, resulting in a significant increase in the\naccuracy of the model's predictions by up to 27\\%. By empowering judges and\nlegal professionals with more reliable information, our work paves the way for\ntrustworthy and transparent legal AI applications that facilitate informed\ndecisions grounded in both knowledge and quantified uncertainty.\n","authors":["Ubaid Azam","Imran Razzak","Shelly Vishwakarma","Hakim Hacid","Dell Zhang","Shoaib Jameel"],"pdf_url":"https://arxiv.org/pdf/2404.10481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.05855v3","updated":"2024-04-16T11:40:46Z","published":"2023-09-11T22:34:06Z","title":"Instabilities in Convnets for Raw Audio","summary":"  What makes waveform-based deep learning so hard? Despite numerous attempts at\ntraining convolutional neural networks (convnets) for filterbank design, they\noften fail to outperform hand-crafted baselines. These baselines are linear\ntime-invariant systems: as such, they can be approximated by convnets with wide\nreceptive fields. Yet, in practice, gradient-based optimization leads to\nsuboptimal approximations. In our article, we approach this phenomenon from the\nperspective of initialization. We present a theory of large deviations for the\nenergy response of FIR filterbanks with random Gaussian weights. We find that\ndeviations worsen for large filters and locally periodic input signals, which\nare both typical for audio signal processing applications. Numerical\nsimulations align with our theory and suggest that the condition number of a\nconvolutional layer follows a logarithmic scaling law between the number and\nlength of the filters, which is reminiscent of discrete wavelet bases.\n","authors":["Daniel Haider","Vincent Lostanlen","Martin Ehler","Peter Balazs"],"pdf_url":"https://arxiv.org/pdf/2309.05855v3.pdf","comment":"4 pages, 5 figures, 1 page appendix, published in IEEE SPL"},{"id":"http://arxiv.org/abs/2404.10474v1","updated":"2024-04-16T11:29:43Z","published":"2024-04-16T11:29:43Z","title":"Toward a Realistic Benchmark for Out-of-Distribution Detection","summary":"  Deep neural networks are increasingly used in a wide range of technologies\nand services, but remain highly susceptible to out-of-distribution (OOD)\nsamples, that is, drawn from a different distribution than the original\ntraining set. A common approach to address this issue is to endow deep neural\nnetworks with the ability to detect OOD samples. Several benchmarks have been\nproposed to design and validate OOD detection techniques. However, many of them\nare based on far-OOD samples drawn from very different distributions, and thus\nlack the complexity needed to capture the nuances of real-world scenarios. In\nthis work, we introduce a comprehensive benchmark for OOD detection, based on\nImageNet and Places365, that assigns individual classes as in-distribution or\nout-of-distribution depending on the semantic similarity with the training set.\nSeveral techniques can be used to determine which classes should be considered\nin-distribution, yielding benchmarks with varying properties. Experimental\nresults on different OOD detection techniques show how their measured efficacy\ndepends on the selected benchmark and how confidence-based techniques may\noutperform classifier-based ones on near-OOD samples.\n","authors":["Pietro Recalcati","Fabio Garcea","Luca Piano","Fabrizio Lamberti","Lia Morra"],"pdf_url":"https://arxiv.org/pdf/2404.10474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10472v1","updated":"2024-04-16T11:25:00Z","published":"2024-04-16T11:25:00Z","title":"Machine Learning Based Optimization Workflow for Tuning Numerical\n  Settings of Differential Equation Solvers for Boundary Value Problems","summary":"  Several numerical differential equation solvers have been employed\neffectively over the years as an alternative to analytical solvers to quickly\nand conveniently solve differential equations. One category of these is\nboundary value solvers, which are used to solve real-world problems formulated\nas differential equations with boundary conditions. These solvers require\ncertain numerical settings to solve the differential equations that affect\ntheir solvability and performance. A systematic fine-tuning of these settings\nis required to obtain the desired solution and performance. Currently, these\nsettings are either selected by trial and error or require domain expertise. In\nthis paper, we propose a machine learning-based optimization workflow for\nfine-tuning the numerical settings to reduce the time and domain expertise\nrequired in the process. In the evaluation section, we discuss the scalability,\nstability, and reliability of the proposed workflow. We demonstrate our\nworkflow on a numerical boundary value problem solver.\n","authors":["Viny Saajan Victor","Manuel Ettm√ºller","Andre Schmei√üer","Heike Leitte","Simone Gramsch"],"pdf_url":"https://arxiv.org/pdf/2404.10472v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10777v4","updated":"2024-04-16T10:59:11Z","published":"2023-11-16T06:01:47Z","title":"A Systematic Review of Aspect-based Sentiment Analysis (ABSA): Domains,\n  Methods, and Trends","summary":"  Aspect-based Sentiment Analysis (ABSA) is a fine-grained type of sentiment\nanalysis that identifies aspects and their associated opinions from a given\ntext. With the surge of digital opinionated text data, ABSA gained increasing\npopularity for its ability to mine more detailed and targeted insights. Many\nreview papers on ABSA subtasks and solution methodologies exist, however, few\nfocus on trends over time or systemic issues relating to research application\ndomains, datasets, and solution approaches. To fill the gap, this paper\npresents a Systematic Literature Review (SLR) of ABSA studies with a focus on\ntrends and high-level relationships among these fundamental components. This\nreview is one of the largest SLRs on ABSA, and also, to our knowledge, the\nfirst that systematically examines the trends and inter-relations among ABSA\nresearch and data distribution across domains and solution paradigms and\napproaches. Our sample includes 519 primary studies screened from 4191 search\nresults without time constraints via an innovative automatic filtering process.\nOur quantitative analysis not only identifies trends in nearly two decades of\nABSA research development but also unveils a systemic lack of dataset and\ndomain diversity as well as domain mismatch that may hinder the development of\nfuture ABSA research. We discuss these findings and their implications and\npropose suggestions for future research.\n","authors":["Yan Cathy Hua","Paul Denny","Katerina Taskova","J√∂rg Wicker"],"pdf_url":"https://arxiv.org/pdf/2311.10777v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10458v1","updated":"2024-04-16T10:56:33Z","published":"2024-04-16T10:56:33Z","title":"Advancing Long-Term Multi-Energy Load Forecasting with Patchformer: A\n  Patch and Transformer-Based Approach","summary":"  In the context of increasing demands for long-term multi-energy load\nforecasting in real-world applications, this paper introduces Patchformer, a\nnovel model that integrates patch embedding with encoder-decoder\nTransformer-based architectures. To address the limitation in existing\nTransformer-based models, which struggle with intricate temporal patterns in\nlong-term forecasting, Patchformer employs patch embedding, which predicts\nmultivariate time-series data by separating it into multiple univariate data\nand segmenting each of them into multiple patches. This method effectively\nenhances the model's ability to capture local and global semantic dependencies.\nThe numerical analysis shows that the Patchformer obtains overall better\nprediction accuracy in both multivariate and univariate long-term forecasting\non the novel Multi-Energy dataset and other benchmark datasets. In addition,\nthe positive effect of the interdependence among energy-related products on the\nperformance of long-term time-series forecasting across Patchformer and other\ncompared models is discovered, and the superiority of the Patchformer against\nother models is also demonstrated, which presents a significant advancement in\nhandling the interdependence and complexities of long-term multi-energy\nforecasting. Lastly, Patchformer is illustrated as the only model that follows\nthe positive correlation between model performance and the length of the past\nsequence, which states its ability to capture long-range past local semantic\ninformation.\n","authors":["Qiuyi Hong","Fanlin Meng","Felipe Maldonado"],"pdf_url":"https://arxiv.org/pdf/2404.10458v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10457v1","updated":"2024-04-16T10:54:48Z","published":"2024-04-16T10:54:48Z","title":"Revealing data leakage in protein interaction benchmarks","summary":"  In recent years, there has been remarkable progress in machine learning for\nprotein-protein interactions. However, prior work has predominantly focused on\nimproving learning algorithms, with less attention paid to evaluation\nstrategies and data preparation. Here, we demonstrate that further development\nof machine learning methods may be hindered by the quality of existing\ntrain-test splits. Specifically, we find that commonly used splitting\nstrategies for protein complexes, based on protein sequence or metadata\nsimilarity, introduce major data leakage. This may result in overoptimistic\nevaluation of generalization, as well as unfair benchmarking of the models,\nbiased towards assessing their overfitting capacity rather than practical\nutility. To overcome the data leakage, we recommend constructing data splits\nbased on 3D structural similarity of protein-protein interfaces and suggest\ncorresponding algorithms. We believe that addressing the data leakage problem\nis critical for further progress in this research area.\n","authors":["Anton Bushuiev","Roman Bushuiev","Jiri Sedlar","Tomas Pluskal","Jiri Damborsky","Stanislav Mazurenko","Josef Sivic"],"pdf_url":"https://arxiv.org/pdf/2404.10457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10450v1","updated":"2024-04-16T10:39:25Z","published":"2024-04-16T10:39:25Z","title":"Graph Neural Networks for Protein-Protein Interactions - A Short Survey","summary":"  Protein-protein interactions (PPIs) play key roles in a broad range of\nbiological processes. Numerous strategies have been proposed for predicting\nPPIs, and among them, graph-based methods have demonstrated promising outcomes\nowing to the inherent graph structure of PPI networks. This paper reviews\nvarious graph-based methodologies, and discusses their applications in PPI\nprediction. We classify these approaches into two primary groups based on their\nmodel structures. The first category employs Graph Neural Networks (GNN) or\nGraph Convolutional Networks (GCN), while the second category utilizes Graph\nAttention Networks (GAT), Graph Auto-Encoders and Graph-BERT. We highlight the\ndistinctive methodologies of each approach in managing the graph-structured\ndata inherent in PPI networks and anticipate future research directions in this\ndomain.\n","authors":["Mingda Xu","Peisheng Qian","Ziyuan Zhao","Zeng Zeng","Jianguo Chen","Weide Liu","Xulei Yang"],"pdf_url":"https://arxiv.org/pdf/2404.10450v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.11050v2","updated":"2024-04-16T10:31:27Z","published":"2023-01-26T11:47:10Z","title":"Minerva: A File-Based Ransomware Detector","summary":"  Ransomware attacks have caused billions of dollars in damages in recent\nyears, and are expected to cause billions more in the future. Consequently,\nsignificant effort has been devoted to ransomware detection and mitigation.\nBehavioral-based ransomware detection approaches have garnered considerable\nattention recently. These behavioral detectors typically rely on process-based\nbehavioral profiles to identify malicious behaviors. However, with an\nincreasing body of literature highlighting the vulnerability of such approaches\nto evasion attacks, a comprehensive solution to the ransomware problem remains\nelusive. This paper presents Minerva, a novel robust approach to ransomware\ndetection. Minerva is engineered to be robust by design against evasion\nattacks, with architectural and feature selection choices informed by their\nresilience to adversarial manipulation. We conduct a comprehensive analysis of\nMinerva across a diverse spectrum of ransomware types, encompassing unseen\nransomware as well as variants designed specifically to evade Minerva. Our\nevaluation showcases the ability of Minerva to accurately identify ransomware,\ngeneralize to unseen threats, and withstand evasion attacks. Furthermore,\nMinerva achieves remarkably low detection times, enabling the adoption of data\nloss prevention techniques with near-zero overhead.\n","authors":["Dorjan Hitaj","Giulio Pagnotta","Fabio De Gaspari","Lorenzo De Carli","Luigi V. Mancini"],"pdf_url":"https://arxiv.org/pdf/2301.11050v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2404.10445v1","updated":"2024-04-16T10:31:06Z","published":"2024-04-16T10:31:06Z","title":"SparseDM: Toward Sparse Efficient Diffusion Models","summary":"  Diffusion models have been extensively used in data generation tasks and are\nrecognized as one of the best generative models. However, their time-consuming\ndeployment, long inference time, and requirements on large memory limit their\napplication on mobile devices. In this paper, we propose a method based on the\nimproved Straight-Through Estimator to improve the deployment efficiency of\ndiffusion models. Specifically, we add sparse masks to the Convolution and\nLinear layers in a pre-trained diffusion model, then use design progressive\nsparsity for model training in the fine-tuning stage, and switch the inference\nmask on and off, which supports a flexible choice of sparsity during inference\naccording to the FID and MACs requirements. Experiments on four datasets\nconducted on a state-of-the-art Transformer-based diffusion model demonstrate\nthat our method reduces MACs by $50\\%$ while increasing FID by only 1.5 on\naverage. Under other MACs conditions, the FID is also lower than 1$\\sim$137\ncompared to other methods.\n","authors":["Kafeng Wang","Jianfei Chen","He Li","Zhenpeng Mi","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2404.10445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10444v1","updated":"2024-04-16T10:30:52Z","published":"2024-04-16T10:30:52Z","title":"Semi-supervised Fr√©chet Regression","summary":"  This paper explores the field of semi-supervised Fr\\'echet regression, driven\nby the significant costs associated with obtaining non-Euclidean labels.\nMethodologically, we propose two novel methods: semi-supervised NW Fr\\'echet\nregression and semi-supervised kNN Fr\\'echet regression, both based on graph\ndistance acquired from all feature instances. These methods extend the scope of\nexisting semi-supervised Euclidean regression methods. We establish their\nconvergence rates with limited labeled data and large amounts of unlabeled\ndata, taking into account the low-dimensional manifold structure of the feature\nspace. Through comprehensive simulations across diverse settings and\napplications to real data, we demonstrate the superior performance of our\nmethods over their supervised counterparts. This study addresses existing\nresearch gaps and paves the way for further exploration and advancements in the\nfield of semi-supervised Fr\\'echet regression.\n","authors":["Rui Qiu","Zhou Yu","Zhenhua Lin"],"pdf_url":"https://arxiv.org/pdf/2404.10444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10443v1","updated":"2024-04-16T10:30:48Z","published":"2024-04-16T10:30:48Z","title":"AGHINT: Attribute-Guided Representation Learning on Heterogeneous\n  Information Networks with Transformer","summary":"  Recently, heterogeneous graph neural networks (HGNNs) have achieved\nimpressive success in representation learning by capturing long-range\ndependencies and heterogeneity at the node level. However, few existing studies\nhave delved into the utilization of node attributes in heterogeneous\ninformation networks (HINs). In this paper, we investigate the impact of\ninter-node attribute disparities on HGNNs performance within the benchmark\ntask, i.e., node classification, and empirically find that typical models\nexhibit significant performance decline when classifying nodes whose attributes\nmarkedly differ from their neighbors. To alleviate this issue, we propose a\nnovel Attribute-Guided heterogeneous Information Networks representation\nlearning model with Transformer (AGHINT), which allows a more effective\naggregation of neighbor node information under the guidance of attributes.\nSpecifically, AGHINT transcends the constraints of the original graph structure\nby directly integrating higher-order similar neighbor features into the\nlearning process and modifies the message-passing mechanism between nodes based\non their attribute disparities. Extensive experimental results on three\nreal-world heterogeneous graph benchmarks with target node attributes\ndemonstrate that AGHINT outperforms the state-of-the-art.\n","authors":["Jinhui Yuan","Shan Lu","Peibo Duan","Jieyue He"],"pdf_url":"https://arxiv.org/pdf/2404.10443v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2404.07840v2","updated":"2024-04-16T10:05:27Z","published":"2024-04-11T15:27:56Z","title":"On Training Data Influence of GPT Models","summary":"  Amidst the rapid advancements in generative language models, the\ninvestigation of how training data shapes the performance of GPT models is\nstill emerging. This paper presents GPTfluence, a novel approach that leverages\na featurized simulation to assess the impact of training examples on the\ntraining dynamics of GPT models. Our approach not only traces the influence of\nindividual training instances on performance trajectories, such as loss and\nother key metrics, on targeted test points but also enables a comprehensive\ncomparison with existing methods across various training scenarios in GPT\nmodels, ranging from 14 million to 2.8 billion parameters, across a range of\ndownstream tasks. Contrary to earlier methods that struggle with generalization\nto new data, GPTfluence introduces a parameterized simulation of training\ndynamics, demonstrating robust generalization capabilities to unseen training\ndata. This adaptability is evident across both fine-tuning and\ninstruction-tuning scenarios, spanning tasks in natural language understanding\nand generation. We will make our code and data publicly available.\n","authors":["Qingyi Liu","Yekun Chai","Shuohuan Wang","Yu Sun","Qiwei Peng","Keze Wang","Hua Wu"],"pdf_url":"https://arxiv.org/pdf/2404.07840v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10436v1","updated":"2024-04-16T10:02:36Z","published":"2024-04-16T10:02:36Z","title":"Tree Bandits for Generative Bayes","summary":"  In generative models with obscured likelihood, Approximate Bayesian\nComputation (ABC) is often the tool of last resort for inference. However, ABC\ndemands many prior parameter trials to keep only a small fraction that passes\nan acceptance test. To accelerate ABC rejection sampling, this paper develops a\nself-aware framework that learns from past trials and errors. We apply\nrecursive partitioning classifiers on the ABC lookup table to sequentially\nrefine high-likelihood regions into boxes. Each box is regarded as an arm in a\nbinary bandit problem treating ABC acceptance as a reward. Each arm has a\nproclivity for being chosen for the next ABC evaluation, depending on the prior\ndistribution and past rejections. The method places more splits in those areas\nwhere the likelihood resides, shying away from low-probability regions destined\nfor ABC rejections. We provide two versions: (1) ABC-Tree for posterior\nsampling, and (2) ABC-MAP for maximum a posteriori estimation. We demonstrate\naccurate ABC approximability at much lower simulation cost. We justify the use\nof our tree-based bandit algorithms with nearly optimal regret bounds. Finally,\nwe successfully apply our approach to the problem of masked image\nclassification using deep generative models.\n","authors":["Sean O'Hagan","Jungeum Kim","Veronika Rockova"],"pdf_url":"https://arxiv.org/pdf/2404.10436v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05468v3","updated":"2024-04-16T10:02:17Z","published":"2024-04-08T12:46:39Z","title":"Mind-to-Image: Projecting Visual Mental Imagination of the Brain from\n  fMRI","summary":"  The reconstruction of images observed by subjects from fMRI data collected\nduring visual stimuli has made significant strides in the past decade, thanks\nto the availability of extensive fMRI datasets and advancements in generative\nmodels for image generation. However, the application of visual reconstruction\nhas remained limited. Reconstructing visual imagination presents a greater\nchallenge, with potentially revolutionary applications ranging from aiding\nindividuals with disabilities to verifying witness accounts in court. The\nprimary hurdles in this field are the absence of data collection protocols for\nvisual imagery and the lack of datasets on the subject. Traditionally,\nfMRI-to-image relies on data collected from subjects exposed to visual stimuli,\nwhich poses issues for generating visual imagery based on the difference of\nbrain activity between visual stimulation and visual imagery. For the first\ntime, we have compiled a substantial dataset (around 6h of scans) on visual\nimagery along with a proposed data collection protocol. We then train a\nmodified version of an fMRI-to-image model and demonstrate the feasibility of\nreconstructing images from two modes of imagination: from memory and from pure\nimagination. This marks an important step towards creating a technology that\nallow direct reconstruction of visual imagery.\n","authors":["Hugo Caselles-Dupr√©","Charles Mellerio","Paul H√©rent","Aliz√©e Lopez-Persem","Benoit B√©ranger","Mathieu Soularue","Pierre Fautrel","Gauthier Vernier","Matthieu Cord"],"pdf_url":"https://arxiv.org/pdf/2404.05468v3.pdf","comment":"Pre-print to be updated. Work in progress"},{"id":"http://arxiv.org/abs/2404.10433v1","updated":"2024-04-16T09:56:08Z","published":"2024-04-16T09:56:08Z","title":"Explainable concept mappings of MRI: Revealing the mechanisms underlying\n  deep learning-based brain disease classification","summary":"  Motivation. While recent studies show high accuracy in the classification of\nAlzheimer's disease using deep neural networks, the underlying learned concepts\nhave not been investigated.\n  Goals. To systematically identify changes in brain regions through concepts\nlearned by the deep neural network for model validation.\n  Approach. Using quantitative R2* maps we separated Alzheimer's patients\n(n=117) from normal controls (n=219) by using a convolutional neural network\nand systematically investigated the learned concepts using Concept Relevance\nPropagation and compared these results to a conventional region of\ninterest-based analysis.\n  Results. In line with established histological findings and the region of\ninterest-based analyses, highly relevant concepts were primarily found in and\nadjacent to the basal ganglia.\n  Impact. The identification of concepts learned by deep neural networks for\ndisease classification enables validation of the models and could potentially\nimprove reliability.\n","authors":["Christian Tinauer","Anna Damulina","Maximilian Sackl","Martin Soellradl","Reduan Achtibat","Maximilian Dreyer","Frederik Pahde","Sebastian Lapuschkin","Reinhold Schmidt","Stefan Ropele","Wojciech Samek","Christian Langkammer"],"pdf_url":"https://arxiv.org/pdf/2404.10433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10420v1","updated":"2024-04-16T09:37:41Z","published":"2024-04-16T09:37:41Z","title":"AudioProtoPNet: An interpretable deep learning model for bird sound\n  classification","summary":"  Recently, scientists have proposed several deep learning models to monitor\nthe diversity of bird species. These models can detect bird species with high\naccuracy by analyzing acoustic signals. However, traditional deep learning\nalgorithms are black-box models that provide no insight into their\ndecision-making process. For domain experts, such as ornithologists, it is\ncrucial that these models are not only efficient, but also interpretable in\norder to be used as assistive tools. In this study, we present an adaption of\nthe Prototypical Part Network (ProtoPNet) for audio classification that\nprovides inherent interpretability through its model architecture. Our approach\nis based on a ConvNeXt backbone architecture for feature extraction and learns\nprototypical patterns for each bird species using spectrograms of the training\ndata. Classification of new data is done by comparison with these prototypes in\nlatent space, which simultaneously serve as easily understandable explanations\nfor the model's decisions.\n","authors":["Ren√© Heinrich","Bernhard Sick","Christoph Scholz"],"pdf_url":"https://arxiv.org/pdf/2404.10420v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2404.10413v1","updated":"2024-04-16T09:31:19Z","published":"2024-04-16T09:31:19Z","title":"VDTuner: Automated Performance Tuning for Vector Data Management Systems","summary":"  Vector data management systems (VDMSs) have become an indispensable\ncornerstone in large-scale information retrieval and machine learning systems\nlike large language models. To enhance the efficiency and flexibility of\nsimilarity search, VDMS exposes many tunable index parameters and system\nparameters for users to specify. However, due to the inherent characteristics\nof VDMS, automatic performance tuning for VDMS faces several critical\nchallenges, which cannot be well addressed by the existing auto-tuning methods.\nIn this paper, we introduce VDTuner, a learning-based automatic performance\ntuning framework for VDMS, leveraging multi-objective Bayesian optimization.\nVDTuner overcomes the challenges associated with VDMS by efficiently exploring\na complex multi-dimensional parameter space without requiring any prior\nknowledge. Moreover, it is able to achieve a good balance between search speed\nand recall rate, delivering an optimal configuration. Extensive evaluations\ndemonstrate that VDTuner can markedly improve VDMS performance (14.12% in\nsearch speed and 186.38% in recall rate) compared with default setting, and is\nmore efficient compared with state-of-the-art baselines (up to 3.57 times\nfaster in terms of tuning time). In addition, VDTuner is scalable to specific\nuser preference and cost-aware optimization objective. VDTuner is available\nonline at https://github.com/tiannuo-yang/VDTuner.\n","authors":["Tiannuo Yang","Wen Hu","Wangqi Peng","Yusen Li","Jianguo Li","Gang Wang","Xiaoguang Liu"],"pdf_url":"https://arxiv.org/pdf/2404.10413v1.pdf","comment":"Accepted by ICDE 2024"},{"id":"http://arxiv.org/abs/2305.02942v3","updated":"2024-04-16T09:30:18Z","published":"2023-05-04T15:44:56Z","title":"Incentivising the federation: gradient-based metrics for data selection\n  and valuation in private decentralised training","summary":"  Obtaining high-quality data for collaborative training of machine learning\nmodels can be a challenging task due to A) regulatory concerns and B) a lack of\ndata owner incentives to participate. The first issue can be addressed through\nthe combination of distributed machine learning techniques (e.g. federated\nlearning) and privacy enhancing technologies (PET), such as the differentially\nprivate (DP) model training. The second challenge can be addressed by rewarding\nthe participants for giving access to data which is beneficial to the training\nmodel, which is of particular importance in federated settings, where the data\nis unevenly distributed. However, DP noise can adversely affect the\nunderrepresented and the atypical (yet often informative) data samples, making\nit difficult to assess their usefulness. In this work, we investigate how to\nleverage gradient information to permit the participants of private training\nsettings to select the data most beneficial for the jointly trained model. We\nassess two such methods, namely variance of gradients (VoG) and the privacy\nloss-input susceptibility score (PLIS). We show that these techniques can\nprovide the federated clients with tools for principled data selection even in\nstricter privacy settings.\n","authors":["Dmitrii Usynin","Daniel Rueckert","Georgios Kaissis"],"pdf_url":"https://arxiv.org/pdf/2305.02942v3.pdf","comment":"Accepted at EICC 2024"},{"id":"http://arxiv.org/abs/2404.09151v2","updated":"2024-04-16T09:29:02Z","published":"2024-04-14T06:09:35Z","title":"Emerging Platforms Meet Emerging LLMs: A Year-Long Journey of Top-Down\n  Development","summary":"  Deploying machine learning (ML) on diverse computing platforms is crucial to\naccelerate and broaden their applications. However, it presents significant\nsoftware engineering challenges due to the fast evolution of models, especially\nthe recent Large Language Models (LLMs), and the emergence of new computing\nplatforms. Current ML frameworks are primarily engineered for CPU and CUDA\nplatforms, leaving a big gap in enabling emerging ones like Metal, Vulkan, and\nWebGPU.\n  While a traditional bottom-up development pipeline fails to close the gap\ntimely, we introduce TapML, a top-down approach and tooling designed to\nstreamline the deployment of ML systems on diverse platforms, optimized for\ndeveloper productivity. Unlike traditional bottom-up methods, which involve\nextensive manual testing and debugging, TapML automates unit testing through\ntest carving and adopts a migration-based strategy for gradually offloading\nmodel computations from mature source platforms to emerging target platforms.\nBy leveraging realistic inputs and remote connections for gradual target\noffloading, TapML accelerates the validation and minimizes debugging scopes,\nsignificantly optimizing development efforts.\n  TapML was developed and applied through a year-long, real-world effort that\nsuccessfully deployed significant emerging models and platforms. Through\nserious deployments of 82 emerging models in 17 distinct architectures across 5\nemerging platforms, we showcase the effectiveness of TapML in enhancing\ndeveloper productivity while ensuring model reliability and efficiency.\nFurthermore, we summarize comprehensive case studies from our real-world\ndevelopment, offering best practices for developing emerging ML systems.\n","authors":["Siyuan Feng","Jiawei Liu","Ruihang Lai","Charlie F. Ruan","Yong Yu","Lingming Zhang","Tianqi Chen"],"pdf_url":"https://arxiv.org/pdf/2404.09151v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.14598v4","updated":"2024-04-16T09:23:24Z","published":"2022-10-26T10:12:31Z","title":"Manifold Gaussian Variational Bayes on the Precision Matrix","summary":"  We propose an optimization algorithm for Variational Inference (VI) in\ncomplex models. Our approach relies on natural gradient updates where the\nvariational space is a Riemann manifold. We develop an efficient algorithm for\nGaussian Variational Inference whose updates satisfy the positive definite\nconstraint on the variational covariance matrix. Our Manifold Gaussian\nVariational Bayes on the Precision matrix (MGVBP) solution provides simple\nupdate rules, is straightforward to implement, and the use of the precision\nmatrix parametrization has a significant computational advantage. Due to its\nblack-box nature, MGVBP stands as a ready-to-use solution for VI in complex\nmodels. Over five datasets, we empirically validate our feasible approach on\ndifferent statistical and econometric models, discussing its performance with\nrespect to baseline methods.\n","authors":["Martin Magris","Mostafa Shabani","Alexandros Iosifidis"],"pdf_url":"https://arxiv.org/pdf/2210.14598v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10405v1","updated":"2024-04-16T09:12:16Z","published":"2024-04-16T09:12:16Z","title":"Integration of Self-Supervised BYOL in Semi-Supervised Medical Image\n  Recognition","summary":"  Image recognition techniques heavily rely on abundant labeled data,\nparticularly in medical contexts. Addressing the challenges associated with\nobtaining labeled data has led to the prominence of self-supervised learning\nand semi-supervised learning, especially in scenarios with limited annotated\ndata. In this paper, we proposed an innovative approach by integrating\nself-supervised learning into semi-supervised models to enhance medical image\nrecognition. Our methodology commences with pre-training on unlabeled data\nutilizing the BYOL method. Subsequently, we merge pseudo-labeled and labeled\ndatasets to construct a neural network classifier, refining it through\niterative fine-tuning. Experimental results on three different datasets\ndemonstrate that our approach optimally leverages unlabeled data, outperforming\nexisting methods in terms of accuracy for medical image recognition.\n","authors":["Hao Feng","Yuanzhe Jia","Ruijia Xu","Mukesh Prasad","Ali Anaissi","Ali Braytee"],"pdf_url":"https://arxiv.org/pdf/2404.10405v1.pdf","comment":"Accepted by ICCS 2024"},{"id":"http://arxiv.org/abs/2404.10401v1","updated":"2024-04-16T09:03:13Z","published":"2024-04-16T09:03:13Z","title":"A Phone-based Distributed Ambient Temperature Measurement System with An\n  Efficient Label-free Automated Training Strategy","summary":"  Enhancing the energy efficiency of buildings significantly relies on\nmonitoring indoor ambient temperature. The potential limitations of\nconventional temperature measurement techniques, together with the omnipresence\nof smartphones, have redirected researchers' attention towards the exploration\nof phone-based ambient temperature estimation technology. Nevertheless,\nnumerous obstacles remain to be addressed in order to achieve a practical\nimplementation of this technology. This study proposes a distributed\nphone-based ambient temperature estimation system which enables collaboration\nbetween multiple phones to accurately measure the ambient temperature in each\nsmall area of an indoor space. Besides, it offers a secure, efficient, and\ncost-effective training strategy to train a new estimation model for each newly\nadded phone, eliminating the need for manual collection of labeled data. This\ninnovative training strategy can yield a high-performing estimation model for a\nnew phone with just 5 data points, requiring only a few iterations. Meanwhile,\nby crowdsourcing, our system automatically provides accurate inferred labels\nfor all newly collected data. We also highlight the potential of integrating\nfederated learning into our system to ensure privacy protection at the end of\nthis study. We believe this study has the potential to advance the practical\napplication of phone-based ambient temperature measurement, facilitating\nenergy-saving efforts in buildings.\n","authors":["Dayin Chen","Xiaodan Shi","Haoran Zhang","Xuan Song","Dongxiao Zhang","Yuntian Chen","Jinyue Yan"],"pdf_url":"https://arxiv.org/pdf/2404.10401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10393v1","updated":"2024-04-16T08:48:46Z","published":"2024-04-16T08:48:46Z","title":"Offline Trajectory Generalization for Offline Reinforcement Learning","summary":"  Offline reinforcement learning (RL) aims to learn policies from static\ndatasets of previously collected trajectories. Existing methods for offline RL\neither constrain the learned policy to the support of offline data or utilize\nmodel-based virtual environments to generate simulated rollouts. However, these\nmethods suffer from (i) poor generalization to unseen states; and (ii) trivial\nimprovement from low-qualified rollout simulation. In this paper, we propose\noffline trajectory generalization through world transformers for offline\nreinforcement learning (OTTO). Specifically, we use casual Transformers, a.k.a.\nWorld Transformers, to predict state dynamics and the immediate reward. Then we\npropose four strategies to use World Transformers to generate high-rewarded\ntrajectory simulation by perturbing the offline data. Finally, we jointly use\noffline data with simulated data to train an offline RL algorithm. OTTO serves\nas a plug-in module and can be integrated with existing offline RL methods to\nenhance them with better generalization capability of transformers and\nhigh-rewarded data augmentation. Conducting extensive experiments on D4RL\nbenchmark datasets, we verify that OTTO significantly outperforms\nstate-of-the-art offline RL methods.\n","authors":["Ziqi Zhao","Zhaochun Ren","Liu Yang","Fajie Yuan","Pengjie Ren","Zhumin Chen","jun Ma","Xin Xin"],"pdf_url":"https://arxiv.org/pdf/2404.10393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17963v3","updated":"2024-04-16T08:45:26Z","published":"2023-03-31T11:06:09Z","title":"Learning-Based Optimal Control with Performance Guarantees for Unknown\n  Systems with Latent States","summary":"  As control engineering methods are applied to increasingly complex systems,\ndata-driven approaches for system identification appear as a promising\nalternative to physics-based modeling. While the Bayesian approaches prevalent\nfor safety-critical applications usually rely on the availability of state\nmeasurements, the states of a complex system are often not directly measurable.\nIt may then be necessary to jointly estimate the dynamics and the latent state,\nmaking the quantification of uncertainties and the design of controllers with\nformal performance guarantees considerably more challenging. This paper\nproposes a novel method for the computation of an optimal input trajectory for\nunknown nonlinear systems with latent states based on a combination of particle\nMarkov chain Monte Carlo methods and scenario theory. Probabilistic performance\nguarantees are derived for the resulting input trajectory, and an approach to\nvalidate the performance of arbitrary control laws is presented. The\neffectiveness of the proposed method is demonstrated in a numerical simulation.\n","authors":["Robert Lefringhausen","Supitsana Srithasan","Armin Lederer","Sandra Hirche"],"pdf_url":"https://arxiv.org/pdf/2303.17963v3.pdf","comment":"Accepted version submitted to the 22nd European Control Conference"},{"id":"http://arxiv.org/abs/2404.10386v1","updated":"2024-04-16T08:37:36Z","published":"2024-04-16T08:37:36Z","title":"I/O in Machine Learning Applications on HPC Systems: A 360-degree Survey","summary":"  High-Performance Computing (HPC) systems excel in managing distributed\nworkloads, and the growing interest in Artificial Intelligence (AI) has\nresulted in a surge in demand for faster methods of Machine Learning (ML) model\ntraining and inference. In the past, research on HPC I/O focused on optimizing\nthe underlying storage system for modeling and simulation applications and\ncheckpointing the results, causing writes to be the dominant I/O operation.\nThese applications typically access large portions of the data written by\nsimulations or experiments. ML workloads, in contrast, perform small I/O reads\nspread across a large number of random files. This shift of I/O access patterns\nposes several challenges to HPC storage systems. In this paper, we survey I/O\nin ML applications on HPC systems, and target literature within a 6-year time\nwindow from 2019 to 2024. We provide an overview of the common phases of ML,\nreview available profilers and benchmarks, examine the I/O patterns encountered\nduring ML training, explore I/O optimizations utilized in modern ML frameworks\nand proposed in recent literature, and lastly, present gaps requiring further\nR&D. We seek to summarize the common practices used in accessing data by ML\napplications and expose research gaps that could spawn further R&D.\n","authors":["Noah Lewis","Jean Luca Bez","Suren Byna"],"pdf_url":"https://arxiv.org/pdf/2404.10386v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11809v2","updated":"2024-04-16T08:36:31Z","published":"2024-02-19T03:39:10Z","title":"Generation Meets Verification: Accelerating Large Language Model\n  Inference with Smart Parallel Auto-Correct Decoding","summary":"  This research aims to accelerate the inference speed of large language models\n(LLMs) with billions of parameters. We propose \\textbf{S}mart \\textbf{P}arallel\n\\textbf{A}uto-\\textbf{C}orrect d\\textbf{E}coding (SPACE), an innovative\napproach designed for achieving lossless acceleration of LLMs. By integrating\nsemi-autoregressive inference and speculative decoding capabilities, SPACE\nuniquely enables autoregressive LLMs to parallelize token generation and\nverification. This is realized through a specialized semi-autoregressive\nsupervised fine-tuning process that equips existing LLMs with the ability to\nsimultaneously predict multiple tokens. Additionally, an auto-correct decoding\nalgorithm facilitates the simultaneous generation and verification of token\nsequences within a single model invocation. Through extensive experiments on a\nrange of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x\non HumanEval-X while maintaining output quality.\n","authors":["Hanling Yi","Feng Lin","Hongbin Li","Peiyang Ning","Xiaotian Yu","Rong Xiao"],"pdf_url":"https://arxiv.org/pdf/2402.11809v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12076v2","updated":"2024-04-16T08:19:47Z","published":"2024-02-16T17:38:28Z","title":"Neuron-centric Hebbian Learning","summary":"  One of the most striking capabilities behind the learning mechanisms of the\nbrain is the adaptation, through structural and functional plasticity, of its\nsynapses. While synapses have the fundamental role of transmitting information\nacross the brain, several studies show that it is the neuron activations that\nproduce changes on synapses. Yet, most plasticity models devised for artificial\nNeural Networks (NNs), e.g., the ABCD rule, focus on synapses, rather than\nneurons, therefore optimizing synaptic-specific Hebbian parameters. This\napproach, however, increases the complexity of the optimization process since\neach synapse is associated to multiple Hebbian parameters. To overcome this\nlimitation, we propose a novel plasticity model, called Neuron-centric Hebbian\nLearning (NcHL), where optimization focuses on neuron- rather than\nsynaptic-specific Hebbian parameters. Compared to the ABCD rule, NcHL reduces\nthe parameters from $5W$ to $5N$, being $W$ and $N$ the number of weights and\nneurons, and usually $N \\ll W$. We also devise a ``weightless'' NcHL model,\nwhich requires less memory by approximating the weights based on a record of\nneuron activations. Our experiments on two robotic locomotion tasks reveal that\nNcHL performs comparably to the ABCD rule, despite using up to $\\sim97$ times\nless parameters, thus allowing for scalable plasticity\n","authors":["Andrea Ferigo","Elia Cunegatti","Giovanni Iacca"],"pdf_url":"https://arxiv.org/pdf/2403.12076v2.pdf","comment":"Accepted at Genetic and Evolutionary Computation Conference (GECCO\n  2024)"},{"id":"http://arxiv.org/abs/2404.10378v1","updated":"2024-04-16T08:15:10Z","published":"2024-04-16T08:15:10Z","title":"Second Edition FRCSyn Challenge at CVPR 2024: Face Recognition Challenge\n  in the Era of Synthetic Data","summary":"  Synthetic data is gaining increasing relevance for training machine learning\nmodels. This is mainly motivated due to several factors such as the lack of\nreal data and intra-class variability, time and errors produced in manual\nlabeling, and in some cases privacy concerns, among others. This paper presents\nan overview of the 2nd edition of the Face Recognition Challenge in the Era of\nSynthetic Data (FRCSyn) organized at CVPR 2024. FRCSyn aims to investigate the\nuse of synthetic data in face recognition to address current technological\nlimitations, including data privacy concerns, demographic biases,\ngeneralization to novel scenarios, and performance constraints in challenging\nsituations such as aging, pose variations, and occlusions. Unlike the 1st\nedition, in which synthetic data from DCFace and GANDiffFace methods was only\nallowed to train face recognition systems, in this 2nd edition we propose new\nsub-tasks that allow participants to explore novel face generative methods. The\noutcomes of the 2nd FRCSyn Challenge, along with the proposed experimental\nprotocol and benchmarking contribute significantly to the application of\nsynthetic data to face recognition.\n","authors":["Ivan DeAndres-Tame","Ruben Tolosana","Pietro Melzi","Ruben Vera-Rodriguez","Minchul Kim","Christian Rathgeb","Xiaoming Liu","Aythami Morales","Julian Fierrez","Javier Ortega-Garcia","Zhizhou Zhong","Yuge Huang","Yuxi Mi","Shouhong Ding","Shuigeng Zhou","Shuai He","Lingzhi Fu","Heng Cong","Rongyu Zhang","Zhihong Xiao","Evgeny Smirnov","Anton Pimenov","Aleksei Grigorev","Denis Timoshenko","Kaleb Mesfin Asfaw","Cheng Yaw Low","Hao Liu","Chuyi Wang","Qing Zuo","Zhixiang He","Hatef Otroshi Shahreza","Anjith George","Alexander Unnervik","Parsa Rahimi","S√©bastien Marcel","Pedro C. Neto","Marco Huber","Jan Niklas Kolf","Naser Damer","Fadi Boutros","Jaime S. Cardoso","Ana F. Sequeira","Andrea Atzori","Gianni Fenu","Mirko Marras","Vitomir ≈†truc","Jiang Yu","Zhangjie Li","Jichun Li","Weisong Zhao","Zhen Lei","Xiangyu Zhu","Xiao-Yu Zhang","Bernardo Biesseck","Pedro Vidal","Luiz Coelho","Roger Granada","David Menotti"],"pdf_url":"https://arxiv.org/pdf/2404.10378v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2311.10476"},{"id":"http://arxiv.org/abs/2404.10370v1","updated":"2024-04-16T08:08:47Z","published":"2024-04-16T08:08:47Z","title":"Know Yourself Better: Diverse Discriminative Feature Learning Improves\n  Open Set Recognition","summary":"  Open set recognition (OSR) is a critical aspect of machine learning,\naddressing the challenge of detecting novel classes during inference. Within\nthe realm of deep learning, neural classifiers trained on a closed set of data\ntypically struggle to identify novel classes, leading to erroneous predictions.\nTo address this issue, various heuristic methods have been proposed, allowing\nmodels to express uncertainty by stating \"I don't know.\" However, a gap in the\nliterature remains, as there has been limited exploration of the underlying\nmechanisms of these methods. In this paper, we conduct an analysis of open set\nrecognition methods, focusing on the aspect of feature diversity. Our research\nreveals a significant correlation between learning diverse discriminative\nfeatures and enhancing OSR performance. Building on this insight, we propose a\nnovel OSR approach that leverages the advantages of feature diversity. The\nefficacy of our method is substantiated through rigorous evaluation on a\nstandard OSR testbench, demonstrating a substantial improvement over\nstate-of-the-art methods.\n","authors":["Jiawen Xu"],"pdf_url":"https://arxiv.org/pdf/2404.10370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10365v1","updated":"2024-04-16T07:55:34Z","published":"2024-04-16T07:55:34Z","title":"Learning Wireless Data Knowledge Graph for Green Intelligent\n  Communications: Methodology and Experiments","summary":"  Intelligent communications have played a pivotal role in shaping the\nevolution of 6G networks. Native artificial intelligence (AI) within green\ncommunication systems must meet stringent real-time requirements. To achieve\nthis, deploying lightweight and resource-efficient AI models is necessary.\nHowever, as wireless networks generate a multitude of data fields and\nindicators during operation, only a fraction of them imposes significant impact\non the network AI models. Therefore, real-time intelligence of communication\nsystems heavily relies on a small but critical set of the data that profoundly\ninfluences the performance of network AI models. These challenges underscore\nthe need for innovative architectures and solutions. In this paper, we propose\na solution, termed the pervasive multi-level (PML) native AI architecture,\nwhich integrates the concept of knowledge graph (KG) into the intelligent\noperational manipulations of mobile networks, resulting in the establishment of\na wireless data KG. Leveraging the wireless data KG, we characterize the\nmassive and complex data collected from wireless communication networks and\nanalyze the relationships among various data fields. The obtained graph of data\nfield relations enables the on-demand generation of minimal and effective\ndatasets, referred to as feature datasets, tailored to specific application\nrequirements. Consequently, this architecture not only enhances AI training,\ninference, and validation processes but also significantly reduces resource\nwastage and overhead for communication networks. To implement this\narchitecture, we have developed a specific solution comprising a\nspatio-temporal heterogeneous graph attention neural network model (STREAM) as\nwell as a feature dataset generation algorithm. Experiments are conducted to\nvalidate the effectiveness of the proposed architecture.\n","authors":["Yongming Huang","Xiaohu You","Hang Zhan","Shiwen He","Ningning Fu","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2404.10365v1.pdf","comment":"12 pages,11 figures"},{"id":"http://arxiv.org/abs/2404.10363v1","updated":"2024-04-16T07:53:54Z","published":"2024-04-16T07:53:54Z","title":"A Survey on Data-Driven Fault Diagnostic Techniques for Marine Diesel\n  Engines","summary":"  Fault diagnosis in marine diesel engines is vital for maritime safety and\noperational efficiency.These engines are integral to marine vessels, and their\nreliable performance is crucial for safenavigation. Swift identification and\nresolution of faults are essential to prevent breakdowns,enhance safety, and\nreduce the risk of catastrophic failures at sea. Proactive fault\ndiagnosisfacilitates timely maintenance, minimizes downtime, and ensures the\noverall reliability andlongevity of marine diesel engines. This paper explores\nthe importance of fault diagnosis,emphasizing subsystems, common faults, and\nrecent advancements in data-driven approachesfor effective marine diesel engine\nmaintenance\n","authors":["Ayah Youssef","Hassan Noura","Abderrahim El Amrani","El Mostafa El Adel","Mustapha Ouladsine"],"pdf_url":"https://arxiv.org/pdf/2404.10363v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10356v1","updated":"2024-04-16T07:44:08Z","published":"2024-04-16T07:44:08Z","title":"Generating Counterfactual Trajectories with Latent Diffusion Models for\n  Concept Discovery","summary":"  Trustworthiness is a major prerequisite for the safe application of opaque\ndeep learning models in high-stakes domains like medicine. Understanding the\ndecision-making process not only contributes to fostering trust but might also\nreveal previously unknown decision criteria of complex models that could\nadvance the state of medical research. The discovery of decision-relevant\nconcepts from black box models is a particularly challenging task. This study\nproposes Concept Discovery through Latent Diffusion-based Counterfactual\nTrajectories (CDCT), a novel three-step framework for concept discovery\nleveraging the superior image synthesis capabilities of diffusion models. In\nthe first step, CDCT uses a Latent Diffusion Model (LDM) to generate a\ncounterfactual trajectory dataset. This dataset is used to derive a\ndisentangled representation of classification-relevant concepts using a\nVariational Autoencoder (VAE). Finally, a search algorithm is applied to\nidentify relevant concepts in the disentangled latent space. The application of\nCDCT to a classifier trained on the largest public skin lesion dataset revealed\nnot only the presence of several biases but also meaningful biomarkers.\nMoreover, the counterfactuals generated within CDCT show better FID scores than\nthose produced by a previously established state-of-the-art method, while being\n12 times more resource-efficient. Unsupervised concept discovery holds great\npotential for the application of trustworthy AI and the further development of\nhuman knowledge in various domains. CDCT represents a further step in this\ndirection.\n","authors":["Payal Varshney","Adriano Lucieri","Christoph Balada","Andreas Dengel","Sheraz Ahmed"],"pdf_url":"https://arxiv.org/pdf/2404.10356v1.pdf","comment":"Submitted to International Conference on Pattern Recognition (ICPR)\n  2024"},{"id":"http://arxiv.org/abs/2404.10354v1","updated":"2024-04-16T07:42:55Z","published":"2024-04-16T07:42:55Z","title":"Physical formula enhanced multi-task learning for pharmacokinetics\n  prediction","summary":"  Artificial intelligence (AI) technology has demonstrated remarkable potential\nin drug dis-covery, where pharmacokinetics plays a crucial role in determining\nthe dosage, safety, and efficacy of new drugs. A major challenge for AI-driven\ndrug discovery (AIDD) is the scarcity of high-quality data, which often\nrequires extensive wet-lab work. A typical example of this is pharmacokinetic\nexperiments. In this work, we develop a physical formula enhanced mul-ti-task\nlearning (PEMAL) method that predicts four key parameters of pharmacokinetics\nsimultaneously. By incorporating physical formulas into the multi-task\nframework, PEMAL facilitates effective knowledge sharing and target alignment\namong the pharmacokinetic parameters, thereby enhancing the accuracy of\nprediction. Our experiments reveal that PEMAL significantly lowers the data\ndemand, compared to typical Graph Neural Networks. Moreover, we demonstrate\nthat PEMAL enhances the robustness to noise, an advantage that conventional\nNeural Networks do not possess. Another advantage of PEMAL is its high\nflexibility, which can be potentially applied to other multi-task machine\nlearning scenarios. Overall, our work illustrates the benefits and potential of\nusing PEMAL in AIDD and other scenarios with data scarcity and noise.\n","authors":["Ruifeng Li","Dongzhan Zhou","Ancheng Shen","Ao Zhang","Mao Su","Mingqian Li","Hongyang Chen","Gang Chen","Yin Zhang","Shufei Zhang","Yuqiang Li","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2404.10354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10353v1","updated":"2024-04-16T07:41:29Z","published":"2024-04-16T07:41:29Z","title":"Rethinking the Graph Polynomial Filter via Positive and Negative\n  Coupling Analysis","summary":"  Recently, the optimization of polynomial filters within Spectral Graph Neural\nNetworks (GNNs) has emerged as a prominent research focus. Existing spectral\nGNNs mainly emphasize polynomial properties in filter design, introducing\ncomputational overhead and neglecting the integration of crucial graph\nstructure information. We argue that incorporating graph information into basis\nconstruction can enhance understanding of polynomial basis, and further\nfacilitate simplified polynomial filter design. Motivated by this, we first\npropose a Positive and Negative Coupling Analysis (PNCA) framework, where the\nconcepts of positive and negative activation are defined and their respective\nand mixed effects are analysed. Then, we explore PNCA from the message\npropagation perspective, revealing the subtle information hidden in the\nactivation process. Subsequently, PNCA is used to analyze the mainstream\npolynomial filters, and a novel simple basis that decouples the positive and\nnegative activation and fully utilizes graph structure information is designed.\nFinally, a simple GNN (called GSCNet) is proposed based on the new basis.\nExperimental results on the benchmark datasets for node classification verify\nthat our GSCNet obtains better or comparable results compared with existing\nstate-of-the-art GNNs while demanding relatively less computational time.\n","authors":["Haodong Wen","Bodong Du","Ruixun Liu","Deyu Meng","Xiangyong Cao"],"pdf_url":"https://arxiv.org/pdf/2404.10353v1.pdf","comment":"13 pages, 8 figures, 6 tables"},{"id":"http://arxiv.org/abs/2404.10351v1","updated":"2024-04-16T07:39:54Z","published":"2024-04-16T07:39:54Z","title":"On the Use of Relative Validity Indices for Comparing Clustering\n  Approaches","summary":"  Relative Validity Indices (RVIs) such as the Silhouette Width Criterion,\nCalinski-Harabasz and Davie's Bouldin indices are the most popular tools for\nevaluating and optimising applications of clustering. Their ability to rank\ncollections of candidate partitions has been used to guide the selection of the\nnumber of clusters, and to compare partitions from different clustering\nalgorithms. Beyond these more conventional tasks, many examples can be found in\nthe literature where RVIs have been used to compare and select other aspects of\nclustering approaches such as data normalisation procedures, data\nrepresentation methods, and distance measures. The authors are not aware of any\nstudies that have attempted to establish the suitability of RVIs for such\ncomparisons. Moreover, given the impact of these aspects on pairwise\nsimilarities, it is not even immediately obvious how RVIs should be implemented\nwhen comparing these aspects. In this study, we conducted experiments with\nseven common RVIs on over 2.7 million clustering partitions for both synthetic\nand real-world datasets, encompassing feature-vector and time-series data. Our\nfindings suggest that RVIs are not well-suited to these unconventional tasks,\nand that conclusions drawn from such applications may be misleading. It is\nrecommended that normalisation procedures, representation methods, and distance\nmeasures instead be selected using external validation on high quality labelled\ndatasets or carefully designed outcome-oriented objective criteria, both of\nwhich should be informed by relevant domain knowledge and clustering aims.\n","authors":["Luke W. Yerbury","Ricardo J. G. B. Campello","G. C. Livingston Jr","Mark Goldsworthy","Lachlan O'Neil"],"pdf_url":"https://arxiv.org/pdf/2404.10351v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07938v2","updated":"2024-04-16T07:39:05Z","published":"2024-02-07T21:08:49Z","title":"Large Language User Interfaces: Voice Interactive User Interfaces\n  powered by LLMs","summary":"  The evolution of Large Language Models (LLMs) has showcased remarkable\ncapacities for logical reasoning and natural language comprehension. These\ncapabilities can be leveraged in solutions that semantically and textually\nmodel complex problems. In this paper, we present our efforts toward\nconstructing a framework that can serve as an intermediary between a user and\ntheir user interface (UI), enabling dynamic and real-time interactions. We\nemploy a system that stands upon textual semantic mappings of UI components, in\nthe form of annotations. These mappings are stored, parsed, and scaled in a\ncustom data structure, supplementary to an agent-based prompting backend\nengine. Employing textual semantic mappings allows each component to not only\nexplain its role to the engine but also provide expectations. By comprehending\nthe needs of both the user and the components, our LLM engine can classify the\nmost appropriate application, extract relevant parameters, and subsequently\nexecute precise predictions of the user's expected actions. Such an integration\nevolves static user interfaces into highly dynamic and adaptable solutions,\nintroducing a new frontier of intelligent and responsive user experiences.\n","authors":["Syed Mekael Wasti","Ken Q. Pu","Ali Neshati"],"pdf_url":"https://arxiv.org/pdf/2402.07938v2.pdf","comment":"Accepted as peer-reviewed publication"},{"id":"http://arxiv.org/abs/2404.02138v2","updated":"2024-04-16T07:28:05Z","published":"2024-04-02T17:49:40Z","title":"Topic-based Watermarks for LLM-Generated Text","summary":"  Recent advancements of large language models (LLMs) have resulted in\nindistinguishable text outputs comparable to human-generated text. Watermarking\nalgorithms are potential tools that offer a way to differentiate between LLM-\nand human-generated text by embedding detectable signatures within\nLLM-generated output. However, current watermarking schemes lack robustness\nagainst known attacks against watermarking algorithms. In addition, they are\nimpractical considering an LLM generates tens of thousands of text outputs per\nday and the watermarking algorithm needs to memorize each output it generates\nfor the detection to work. In this work, focusing on the limitations of current\nwatermarking schemes, we propose the concept of a \"topic-based watermarking\nalgorithm\" for LLMs. The proposed algorithm determines how to generate tokens\nfor the watermarked LLM output based on extracted topics of an input prompt or\nthe output of a non-watermarked LLM. Inspired from previous work, we propose\nusing a pair of lists (that are generated based on the specified extracted\ntopic(s)) that specify certain tokens to be included or excluded while\ngenerating the watermarked output of the LLM. Using the proposed watermarking\nalgorithm, we show the practicality of a watermark detection algorithm.\nFurthermore, we discuss a wide range of attacks that can emerge against\nwatermarking algorithms for LLMs and the benefit of the proposed watermarking\nscheme for the feasibility of modeling a potential attacker considering its\nbenefit vs. loss.\n","authors":["Alexander Nemecek","Yuzhou Jiang","Erman Ayday"],"pdf_url":"https://arxiv.org/pdf/2404.02138v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2404.08801v2","updated":"2024-04-16T07:27:58Z","published":"2024-04-12T20:28:14Z","title":"Megalodon: Efficient LLM Pretraining and Inference with Unlimited\n  Context Length","summary":"  The quadratic complexity and weak length extrapolation of Transformers limits\ntheir ability to scale to long sequences, and while sub-quadratic solutions\nlike linear attention and state space models exist, they empirically\nunderperform Transformers in pretraining efficiency and downstream task\naccuracy. We introduce Megalodon, a neural architecture for efficient sequence\nmodeling with unlimited context length. Megalodon inherits the architecture of\nMega (exponential moving average with gated attention), and further introduces\nmultiple technical components to improve its capability and stability,\nincluding complex exponential moving average (CEMA), timestep normalization\nlayer, normalized attention mechanism and pre-norm with two-hop residual\nconfiguration. In a controlled head-to-head comparison with Llama2, Megalodon\nachieves better efficiency than Transformer in the scale of 7 billion\nparameters and 2 trillion training tokens. Megalodon reaches a training loss of\n1.70, landing mid-way between Llama2-7B (1.75) and 13B (1.67). Code:\nhttps://github.com/XuezheMax/megalodon\n","authors":["Xuezhe Ma","Xiaomeng Yang","Wenhan Xiong","Beidi Chen","Lili Yu","Hao Zhang","Jonathan May","Luke Zettlemoyer","Omer Levy","Chunting Zhou"],"pdf_url":"https://arxiv.org/pdf/2404.08801v2.pdf","comment":"9 pages, 6 figures and 8 tables"},{"id":"http://arxiv.org/abs/2404.10341v1","updated":"2024-04-16T07:24:54Z","published":"2024-04-16T07:24:54Z","title":"Asset management, condition monitoring and Digital Twins: damage\n  detection and virtual inspection on a reinforced concrete bridge","summary":"  In April 2021 Stava bridge, a main bridge on E6 in Norway, was abruptly\nclosed for traffic. A structural defect had seriously compromised the bridge\nstructural integrity. The Norwegian Public Roads Administration (NPRA) closed\nit, made a temporary solution and reopened with severe traffic restrictions.\nThe incident was alerted through what constitutes the bridge Digital Twin\nprocessing data from Internet of Things sensors. The solution was crucial in\nonline and offline diagnostics, the case demonstrating the value of\ntechnologies to tackle emerging dangerous situations as well as acting\npreventively. A critical and rapidly developing damage was detected in time to\nstop the development, but not in time to avoid the incident altogether. The\npaper puts risk in a broader perspective for an organization responsible for\nhighway infrastructure. It positions online monitoring and Digital Twins in the\ncontext of Risk- and Condition-Based Maintenance. The situation that arose at\nStava bridge, and how it was detected, analyzed, and diagnosed during virtual\ninspection, is described. The case demonstrates how combining physics-based\nmethods with Machine Learning can facilitate damage detection and diagnostics.\nA summary of lessons learnt, both from technical and organizational\nperspectives, as well as plans of future work, is presented.\n","authors":["Arnulf Hagen","Trond Michael Andersen"],"pdf_url":"https://arxiv.org/pdf/2404.10341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10324v1","updated":"2024-04-16T07:08:04Z","published":"2024-04-16T07:08:04Z","title":"Graph neural network-based surrogate modelling for real-time hydraulic\n  prediction of urban drainage networks","summary":"  Physics-based models are computationally time-consuming and infeasible for\nreal-time scenarios of urban drainage networks, and a surrogate model is needed\nto accelerate the online predictive modelling. Fully-connected neural networks\n(NNs) are potential surrogate models, but may suffer from low interpretability\nand efficiency in fitting complex targets. Owing to the state-of-the-art\nmodelling power of graph neural networks (GNNs) and their match with urban\ndrainage networks in the graph structure, this work proposes a GNN-based\nsurrogate of the flow routing model for the hydraulic prediction problem of\ndrainage networks, which regards recent hydraulic states as initial conditions,\nand future runoff and control policy as boundary conditions. To incorporate\nhydraulic constraints and physical relationships into drainage modelling,\nphysics-guided mechanisms are designed on top of the surrogate model to\nrestrict the prediction variables with flow balance and flooding occurrence\nconstraints. According to case results in a stormwater network, the GNN-based\nmodel is more cost-effective with better hydraulic prediction accuracy than the\nNN-based model after equal training epochs, and the designed mechanisms further\nlimit prediction errors with interpretable domain knowledge. As the model\nstructure adheres to the flow routing mechanisms and hydraulic constraints in\nurban drainage networks, it provides an interpretable and effective solution\nfor data-driven surrogate modelling. Simultaneously, the surrogate model\naccelerates the predictive modelling of urban drainage networks for real-time\nuse compared with the physics-based model.\n","authors":["Zhiyu Zhang","Chenkaixiang Lu","Wenchong Tian","Zhenliang Liao","Zhiguo Yuan"],"pdf_url":"https://arxiv.org/pdf/2404.10324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08193v2","updated":"2024-04-16T07:05:13Z","published":"2024-02-13T03:31:36Z","title":"Gaussian Ensemble Belief Propagation for Efficient Inference in\n  High-Dimensional Systems","summary":"  Efficient inference in high-dimensional models remains a central challenge in\nmachine learning. This paper introduces the Gaussian Ensemble Belief\nPropagation (GEnBP) algorithm, a fusion of the Ensemble Kalman filter and\nGaussian belief propagation (GaBP) methods. GEnBP updates ensembles by passing\nlow-rank local messages in a graphical model structure. This combination\ninherits favourable qualities from each method. Ensemble techniques allow GEnBP\nto handle high-dimensional states, parameters and intricate, noisy, black-box\ngeneration processes. The use of local messages in a graphical model structure\nensures that the approach is suited to distributed computing and can\nefficiently handle complex dependence structures. GEnBP is particularly\nadvantageous when the ensemble size is considerably smaller than the inference\ndimension. This scenario often arises in fields such as spatiotemporal\nmodelling, image processing and physical model inversion. GEnBP can be applied\nto general problem structures, including jointly learning system parameters,\nobservation parameters, and latent state variables.\n","authors":["Dan MacKinlay","Russell Tsuchida","Dan Pagendam","Petra Kuhnert"],"pdf_url":"https://arxiv.org/pdf/2402.08193v2.pdf","comment":"Under conference submission"},{"id":"http://arxiv.org/abs/2404.10320v1","updated":"2024-04-16T07:02:40Z","published":"2024-04-16T07:02:40Z","title":"CARE to Compare: A real-world dataset for anomaly detection in wind\n  turbine data","summary":"  Anomaly detection plays a crucial role in the field of predictive maintenance\nfor wind turbines, yet the comparison of different algorithms poses a difficult\ntask because domain specific public datasets are scarce. Many comparisons of\ndifferent approaches either use benchmarks composed of data from many different\ndomains, inaccessible data or one of the few publicly available datasets which\nlack detailed information about the faults. Moreover, many publications\nhighlight a couple of case studies where fault detection was successful. With\nthis paper we publish a high quality dataset that contains data from 36 wind\nturbines across 3 different wind farms as well as the most detailed fault\ninformation of any public wind turbine dataset as far as we know. The new\ndataset contains 89 years worth of real-world operating data of wind turbines,\ndistributed across 44 labeled time frames for anomalies that led up to faults,\nas well as 51 time series representing normal behavior. Additionally, the\nquality of training data is ensured by turbine-status-based labels for each\ndata point. Furthermore, we propose a new scoring method, called CARE\n(Coverage, Accuracy, Reliability and Earliness), which takes advantage of the\ninformation depth that is present in the dataset to identify a good all-around\nanomaly detection model. This score considers the anomaly detection\nperformance, the ability to recognize normal behavior properly and the\ncapability to raise as few false alarms as possible while simultaneously\ndetecting anomalies early.\n","authors":["Christian G√ºck","Cyriana M. A. Roelofs","Stefan Faulstich"],"pdf_url":"https://arxiv.org/pdf/2404.10320v1.pdf","comment":"28 pages, 3 figures"},{"id":"http://arxiv.org/abs/2404.10319v1","updated":"2024-04-16T06:59:26Z","published":"2024-04-16T06:59:26Z","title":"Application of Deep Learning Methods to Processing of Noisy Medical\n  Video Data","summary":"  Cells count become a challenging problem when the cells move in a continuous\nstream, and their boundaries are difficult for visual detection. To resolve\nthis problem we modified the training and decision making processes using\ncurriculum learning and multi-view predictions techniques, respectively.\n","authors":["Danil Afonchikov","Elena Kornaeva","Irina Makovik","Alexey Kornaev"],"pdf_url":"https://arxiv.org/pdf/2404.10319v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10314v1","updated":"2024-04-16T06:40:51Z","published":"2024-04-16T06:40:51Z","title":"Awareness of uncertainty in classification using a multivariate model\n  and multi-views","summary":"  One of the ways to make artificial intelligence more natural is to give it\nsome room for doubt. Two main questions should be resolved in that way. First,\nhow to train a model to estimate uncertainties of its own predictions? And\nthen, what to do with the uncertain predictions if they appear? First, we\nproposed an uncertainty-aware negative log-likelihood loss for the case of\nN-dimensional multivariate normal distribution with spherical variance matrix\nto the solution of N-classes classification tasks. The loss is similar to the\nheteroscedastic regression loss. The proposed model regularizes uncertain\npredictions, and trains to calculate both the predictions and their uncertainty\nestimations. The model fits well with the label smoothing technique. Second, we\nexpanded the limits of data augmentation at the training and test stages, and\nmade the trained model to give multiple predictions for a given number of\naugmented versions of each test sample. Given the multi-view predictions\ntogether with their uncertainties and confidences, we proposed several methods\nto calculate final predictions, including mode values and bin counts with soft\nand hard weights. For the latter method, we formalized the model tuning task in\nthe form of multimodal optimization with non-differentiable criteria of maximum\naccuracy, and applied particle swarm optimization to solve the tuning task. The\nproposed methodology was tested using CIFAR-10 dataset with clean and noisy\nlabels and demonstrated good results in comparison with other uncertainty\nestimation methods related to sample selection, co-teaching, and label\nsmoothing.\n","authors":["Alexey Kornaev","Elena Kornaeva","Oleg Ivanov","Ilya Pershin","Danis Alukaev"],"pdf_url":"https://arxiv.org/pdf/2404.10314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10310v1","updated":"2024-04-16T06:37:19Z","published":"2024-04-16T06:37:19Z","title":"Wireless Earphone-based Real-Time Monitoring of Breathing Exercises: A\n  Deep Learning Approach","summary":"  Several therapy routines require deep breathing exercises as a key component\nand patients undergoing such therapies must perform these exercises regularly.\nAssessing the outcome of a therapy and tailoring its course necessitates\nmonitoring a patient's compliance with the therapy. While therapy compliance\nmonitoring is routine in a clinical environment, it is challenging to do in an\nat-home setting. This is so because a home setting lacks access to specialized\nequipment and skilled professionals needed to effectively monitor the\nperformance of a therapy routine by a patient. For some types of therapies,\nthese challenges can be addressed with the use of consumer-grade hardware, such\nas earphones and smartphones, as practical solutions. To accurately monitor\nbreathing exercises using wireless earphones, this paper proposes a framework\nthat has the potential for assessing a patient's compliance with an at-home\ntherapy. The proposed system performs real-time detection of breathing phases\nand channels with high accuracy by processing a $\\mathbf{500}$ ms audio signal\nthrough two convolutional neural networks. The first network, called a channel\nclassifier, distinguishes between nasal and oral breathing, and a pause. The\nsecond network, called a phase classifier, determines whether the audio segment\nis from inhalation or exhalation. According to $k$-fold cross-validation, the\nchannel and phase classifiers achieved a maximum F1 score of $\\mathbf{97.99\\%}$\nand $\\mathbf{89.46\\%}$, respectively. The results demonstrate the potential of\nusing commodity earphones for real-time breathing channel and phase detection\nfor breathing therapy compliance monitoring.\n","authors":["Hassam Khan Wazir","Zaid Waghoo","Vikram Kapila"],"pdf_url":"https://arxiv.org/pdf/2404.10310v1.pdf","comment":"4 pages, 2 figures. Paper accepted at IEEE International Conference\n  on Engineering in Medicine & Biology Society, 2024"},{"id":"http://arxiv.org/abs/2404.10308v1","updated":"2024-04-16T06:34:08Z","published":"2024-04-16T06:34:08Z","title":"Hierarchical Context Merging: Better Long Context Understanding for\n  Pre-trained LLMs","summary":"  Large language models (LLMs) have shown remarkable performance in various\nnatural language processing tasks. However, a primary constraint they face is\nthe context limit, i.e., the maximum number of tokens they can process.\nPrevious works have explored architectural changes and modifications in\npositional encoding to relax the constraint, but they often require expensive\ntraining or do not address the computational demands of self-attention. In this\npaper, we present Hierarchical cOntext MERging (HOMER), a new training-free\nscheme designed to overcome the limitations. HOMER uses a divide-and-conquer\nalgorithm, dividing long inputs into manageable chunks. Each chunk is then\nprocessed collectively, employing a hierarchical strategy that merges adjacent\nchunks at progressive transformer layers. A token reduction technique precedes\neach merging, ensuring memory usage efficiency. We also propose an optimized\ncomputational order reducing the memory requirement to logarithmically scale\nwith respect to input length, making it especially favorable for environments\nwith tight memory restrictions. Our experiments demonstrate the proposed\nmethod's superior performance and memory efficiency, enabling the broader use\nof LLMs in contexts requiring extended context. Code is available at\nhttps://github.com/alinlab/HOMER.\n","authors":["Woomin Song","Seunghyuk Oh","Sangwoo Mo","Jaehyung Kim","Sukmin Yun","Jung-Woo Ha","Jinwoo Shin"],"pdf_url":"https://arxiv.org/pdf/2404.10308v1.pdf","comment":"Accepted to ICLR 2024. The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2404.10304v1","updated":"2024-04-16T06:20:06Z","published":"2024-04-16T06:20:06Z","title":"LLM-Powered Test Case Generation for Detecting Tricky Bugs","summary":"  Conventional automated test generation tools struggle to generate test\noracles and tricky bug-revealing test inputs. Large Language Models (LLMs) can\nbe prompted to produce test inputs and oracles for a program directly, but the\nprecision of the tests can be very low for complex scenarios (only 6.3% based\non our experiments). To fill this gap, this paper proposes AID, which combines\nLLMs with differential testing to generate fault-revealing test inputs and\noracles targeting plausibly correct programs (i.e., programs that have passed\nall the existing tests). In particular, AID selects test inputs that yield\ndiverse outputs on a set of program variants generated by LLMs, then constructs\nthe test oracle based on the outputs. We evaluate AID on two large-scale\ndatasets with tricky bugs: TrickyBugs and EvalPlus, and compare it with three\nstate-of-the-art baselines. The evaluation results show that the recall,\nprecision, and F1 score of AID outperform the state-of-the-art by up to 1.80x,\n2.65x, and 1.66x, respectively.\n","authors":["Kaibo Liu","Yiyang Liu","Zhenpeng Chen","Jie M. Zhang","Yudong Han","Yun Ma","Ge Li","Gang Huang"],"pdf_url":"https://arxiv.org/pdf/2404.10304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10301v1","updated":"2024-04-16T06:09:33Z","published":"2024-04-16T06:09:33Z","title":"Long-form music generation with latent diffusion","summary":"  Audio-based generative models for music have seen great strides recently, but\nso far have not managed to produce full-length music tracks with coherent\nmusical structure. We show that by training a generative model on long temporal\ncontexts it is possible to produce long-form music of up to 4m45s. Our model\nconsists of a diffusion-transformer operating on a highly downsampled\ncontinuous latent representation (latent rate of 21.5Hz). It obtains\nstate-of-the-art generations according to metrics on audio quality and prompt\nalignment, and subjective tests reveal that it produces full-length music with\ncoherent structure.\n","authors":["Zach Evans","Julian D. Parker","CJ Carr","Zack Zukowski","Josiah Taylor","Jordi Pons"],"pdf_url":"https://arxiv.org/pdf/2404.10301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.19102v3","updated":"2024-04-16T06:08:05Z","published":"2023-10-29T18:33:05Z","title":"Atom: Low-bit Quantization for Efficient and Accurate LLM Serving","summary":"  The growing demand for Large Language Models (LLMs) in applications such as\ncontent generation, intelligent chatbots, and sentiment analysis poses\nconsiderable challenges for LLM service providers. To efficiently use GPU\nresources and boost throughput, batching multiple requests has emerged as a\npopular paradigm; to further speed up batching, LLM quantization techniques\nreduce memory consumption and increase computing capacity. However, prevalent\nquantization schemes (e.g., 8-bit weight-activation quantization) cannot fully\nleverage the capabilities of modern GPUs, such as 4-bit integer operators,\nresulting in sub-optimal performance.\n  To maximize LLMs' serving throughput, we introduce Atom, a low-bit\nquantization method that achieves high throughput improvements with negligible\naccuracy loss. Atom significantly boosts serving throughput by using low-bit\noperators and considerably reduces memory consumption via low-bit quantization.\nIt attains high accuracy by applying a novel mixed-precision and fine-grained\nquantization process. We evaluate Atom on 4-bit weight-activation quantization\nin the serving context. Atom improves end-to-end throughput (token/s) by up to\n$7.7\\times$ compared to the FP16 and by $2.5\\times$ compared to INT8\nquantization, while maintaining the same latency target.\n","authors":["Yilong Zhao","Chien-Yu Lin","Kan Zhu","Zihao Ye","Lequn Chen","Size Zheng","Luis Ceze","Arvind Krishnamurthy","Tianqi Chen","Baris Kasikci"],"pdf_url":"https://arxiv.org/pdf/2310.19102v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09406v2","updated":"2024-04-16T05:58:39Z","published":"2024-04-15T01:47:44Z","title":"Human-in-the-Loop Segmentation of Multi-species Coral Imagery","summary":"  Broad-scale marine surveys performed by underwater vehicles significantly\nincrease the availability of coral reef imagery, however it is costly and\ntime-consuming for domain experts to label images. Point label propagation is\nan approach used to leverage existing image data labeled with sparse point\nlabels. The resulting augmented ground truth generated is then used to train a\nsemantic segmentation model. Here, we first demonstrate that recent advances in\nfoundation models enable generation of multi-species coral augmented ground\ntruth masks using denoised DINOv2 features and K-Nearest Neighbors (KNN),\nwithout the need for any pre-training or custom-designed algorithms. For\nextremely sparsely labeled images, we propose a labeling regime based on\nhuman-in-the-loop principles, resulting in significant improvement in\nannotation efficiency: If only 5 point labels per image are available, our\nproposed human-in-the-loop approach improves on the state-of-the-art by 17.3%\nfor pixel accuracy and 22.6% for mIoU; and by 10.6% and 19.1% when 10 point\nlabels per image are available. Even if the human-in-the-loop labeling regime\nis not used, the denoised DINOv2 features with a KNN outperforms the prior\nstate-of-the-art by 3.5% for pixel accuracy and 5.7% for mIoU (5 grid points).\nWe also provide a detailed analysis of how point labeling style and the\nquantity of points per image affects the point label propagation quality and\nprovide general recommendations on maximizing point label efficiency.\n","authors":["Scarlett Raine","Ross Marchant","Brano Kusy","Frederic Maire","Niko Suenderhauf","Tobias Fischer"],"pdf_url":"https://arxiv.org/pdf/2404.09406v2.pdf","comment":"Accepted at the CVPR2024 3rd Workshop on Learning with Limited\n  Labelled Data for Image and Video Understanding (L3D-IVU), 10 pages, 6\n  figures, an additional 4 pages of supplementary material"},{"id":"http://arxiv.org/abs/2404.10299v1","updated":"2024-04-16T05:56:41Z","published":"2024-04-16T05:56:41Z","title":"Clustering and Data Augmentation to Improve Accuracy of Sleep Assessment\n  and Sleep Individuality Analysis","summary":"  Recently, growing health awareness, novel methods allow individuals to\nmonitor sleep at home. Utilizing sleep sounds offers advantages over\nconventional methods like smartwatches, being non-intrusive, and capable of\ndetecting various physiological activities. This study aims to construct a\nmachine learning-based sleep assessment model providing evidence-based\nassessments, such as poor sleep due to frequent movement during sleep onset.\nExtracting sleep sound events, deriving latent representations using VAE,\nclustering with GMM, and training LSTM for subjective sleep assessment achieved\na high accuracy of 94.8% in distinguishing sleep satisfaction. Moreover,\nTimeSHAP revealed differences in impactful sound event types and timings for\ndifferent individuals.\n","authors":["Shintaro Tamai","Masayuki Numao","Ken-ichi Fukui"],"pdf_url":"https://arxiv.org/pdf/2404.10299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10296v1","updated":"2024-04-16T05:40:30Z","published":"2024-04-16T05:40:30Z","title":"Engineering software 2.0 by interpolating neural networks: unifying\n  training, solving, and calibration","summary":"  The evolution of artificial intelligence (AI) and neural network theories has\nrevolutionized the way software is programmed, shifting from a hard-coded\nseries of codes to a vast neural network. However, this transition in\nengineering software has faced challenges such as data scarcity, multi-modality\nof data, low model accuracy, and slow inference. Here, we propose a new network\nbased on interpolation theories and tensor decomposition, the interpolating\nneural network (INN). Instead of interpolating training data, a common notion\nin computer science, INN interpolates interpolation points in the physical\nspace whose coordinates and values are trainable. It can also extrapolate if\nthe interpolation points reside outside of the range of training data and the\ninterpolation functions have a larger support domain. INN features orders of\nmagnitude fewer trainable parameters, faster training, a smaller memory\nfootprint, and higher model accuracy compared to feed-forward neural networks\n(FFNN) or physics-informed neural networks (PINN). INN is poised to usher in\nEngineering Software 2.0, a unified neural network that spans various domains\nof space, time, parameters, and initial/boundary conditions. This has\npreviously been computationally prohibitive due to the exponentially growing\nnumber of trainable parameters, easily exceeding the parameter size of ChatGPT,\nwhich is over 1 trillion. INN addresses this challenge by leveraging tensor\ndecomposition and tensor product, with adaptable network architecture.\n","authors":["Chanwook Park","Sourav Saha","Jiachen Guo","Xiaoyu Xie","Satyajit Mojumder","Miguel A. Bessa","Dong Qian","Wei Chen","Gregory J. Wagner","Jian Cao","Wing Kam Liu"],"pdf_url":"https://arxiv.org/pdf/2404.10296v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2404.10282v1","updated":"2024-04-16T04:52:41Z","published":"2024-04-16T04:52:41Z","title":"Tripod: Three Complementary Inductive Biases for Disentangled\n  Representation Learning","summary":"  Inductive biases are crucial in disentangled representation learning for\nnarrowing down an underspecified solution set. In this work, we consider\nendowing a neural network autoencoder with three select inductive biases from\nthe literature: data compression into a grid-like latent space via\nquantization, collective independence amongst latents, and minimal functional\ninfluence of any latent on how other latents determine data generation. In\nprinciple, these inductive biases are deeply complementary: they most directly\nspecify properties of the latent space, encoder, and decoder, respectively. In\npractice, however, naively combining existing techniques instantiating these\ninductive biases fails to yield significant benefits. To address this, we\npropose adaptations to the three techniques that simplify the learning problem,\nequip key regularization terms with stabilizing invariances, and quash\ndegenerate incentives. The resulting model, Tripod, achieves state-of-the-art\nresults on a suite of four image disentanglement benchmarks. We also verify\nthat Tripod significantly improves upon its naive incarnation and that all\nthree of its \"legs\" are necessary for best performance.\n","authors":["Kyle Hsu","Jubayer Ibn Hamid","Kaylee Burns","Chelsea Finn","Jiajun Wu"],"pdf_url":"https://arxiv.org/pdf/2404.10282v1.pdf","comment":"22 pages, 10 figures, code available at\n  https://github.com/kylehkhsu/tripod"},{"id":"http://arxiv.org/abs/2310.07641v2","updated":"2024-04-16T04:50:08Z","published":"2023-10-11T16:38:11Z","title":"Evaluating Large Language Models at Evaluating Instruction Following","summary":"  As research in large language models (LLMs) continues to accelerate,\nLLM-based evaluation has emerged as a scalable and cost-effective alternative\nto human evaluations for comparing the ever increasing list of models. This\npaper investigates the efficacy of these ``LLM evaluators'', particularly in\nusing them to assess instruction following, a metric that gauges how closely\ngenerated text adheres to the given instruction. We introduce a challenging\nmeta-evaluation benchmark, LLMBar, designed to test the ability of an LLM\nevaluator in discerning instruction-following outputs. The authors manually\ncurated 419 pairs of outputs, one adhering to instructions while the other\ndiverging, yet may possess deceptive qualities that mislead an LLM evaluator,\ne.g., a more engaging tone. Contrary to existing meta-evaluation, we discover\nthat different evaluators (i.e., combinations of LLMs and prompts) exhibit\ndistinct performance on LLMBar and even the highest-scoring ones have\nsubstantial room for improvement. We also present a novel suite of prompting\nstrategies that further close the gap between LLM and human evaluators. With\nLLMBar, we hope to offer more insight into LLM evaluators and foster future\nresearch in developing better instruction-following models.\n","authors":["Zhiyuan Zeng","Jiatong Yu","Tianyu Gao","Yu Meng","Tanya Goyal","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2310.07641v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2404.00477v3","updated":"2024-04-16T04:47:23Z","published":"2024-03-30T21:54:01Z","title":"DE-HNN: An effective neural model for Circuit Netlist representation","summary":"  The run-time for optimization tools used in chip design has grown with the\ncomplexity of designs to the point where it can take several days to go through\none design cycle which has become a bottleneck. Designers want fast tools that\ncan quickly give feedback on a design. Using the input and output data of the\ntools from past designs, one can attempt to build a machine learning model that\npredicts the outcome of a design in significantly shorter time than running the\ntool. The accuracy of such models is affected by the representation of the\ndesign data, which is usually a netlist that describes the elements of the\ndigital circuit and how they are connected. Graph representations for the\nnetlist together with graph neural networks have been investigated for such\nmodels. However, the characteristics of netlists pose several challenges for\nexisting graph learning frameworks, due to the large number of nodes and the\nimportance of long-range interactions between nodes. To address these\nchallenges, we represent the netlist as a directed hypergraph and propose a\nDirectional Equivariant Hypergraph Neural Network (DE-HNN) for the effective\nlearning of (directed) hypergraphs. Theoretically, we show that our DE-HNN can\nuniversally approximate any node or hyperedge based function that satisfies\ncertain permutation equivariant and invariant properties natural for directed\nhypergraphs. We compare the proposed DE-HNN with several State-of-the-art\n(SOTA) machine learning models for (hyper)graphs and netlists, and show that\nthe DE-HNN significantly outperforms them in predicting the outcome of\noptimized place-and-route tools directly from the input netlists. Our source\ncode and the netlists data used are publicly available at\nhttps://github.com/YusuLab/chips.git\n","authors":["Zhishang Luo","Truong Son Hy","Puoya Tabaghi","Donghyeon Koh","Michael Defferrard","Elahe Rezaei","Ryan Carey","Rhett Davis","Rajeev Jain","Yusu Wang"],"pdf_url":"https://arxiv.org/pdf/2404.00477v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03597v3","updated":"2024-04-16T04:36:18Z","published":"2024-01-07T22:47:38Z","title":"Few-Shot Causal Representation Learning for Out-of-Distribution\n  Generalization on Heterogeneous Graphs","summary":"  Heterogeneous graph few-shot learning (HGFL) has been developed to address\nthe label sparsity issue in heterogeneous graphs (HGs), which consist of\nvarious types of nodes and edges. The core concept of HGFL is to extract\nknowledge from rich-labeled classes in a source HG, transfer this knowledge to\na target HG to facilitate learning new classes with few-labeled training data,\nand finally make predictions on unlabeled testing data. Existing methods\ntypically assume that the source HG, training data, and testing data all share\nthe same distribution. However, in practice, distribution shifts among these\nthree types of data are inevitable due to two reasons: (1) the limited\navailability of the source HG that matches the target HG distribution, and (2)\nthe unpredictable data generation mechanism of the target HG. Such distribution\nshifts result in ineffective knowledge transfer and poor learning performance\nin existing methods, thereby leading to a novel problem of out-of-distribution\n(OOD) generalization in HGFL. To address this challenging problem, we propose a\nnovel Causal OOD Heterogeneous graph Few-shot learning model, namely COHF. In\nCOHF, we first characterize distribution shifts in HGs with a structural causal\nmodel, establishing an invariance principle for OOD generalization in HGFL.\nThen, following this invariance principle, we propose a new variational\nautoencoder-based heterogeneous graph neural network to mitigate the impact of\ndistribution shifts. Finally, by integrating this network with a novel\nmeta-learning framework, COHF effectively transfers knowledge to the target HG\nto predict new classes with few-labeled data. Extensive experiments on seven\nreal-world datasets have demonstrated the superior performance of COHF over the\nstate-of-the-art methods.\n","authors":["Pengfei Ding","Yan Wang","Guanfeng Liu","Nan Wang","Xiaofang Zhou"],"pdf_url":"https://arxiv.org/pdf/2401.03597v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10275v1","updated":"2024-04-16T04:21:59Z","published":"2024-04-16T04:21:59Z","title":"OptiGrad: A Fair and more Efficient Price Elasticity Optimization via a\n  Gradient Based Learning","summary":"  This paper presents a novel approach to optimizing profit margins in non-life\ninsurance markets through a gradient descent-based method, targeting three key\nobjectives: 1) maximizing profit margins, 2) ensuring conversion rates, and 3)\nenforcing fairness criteria such as demographic parity (DP). Traditional\npricing optimization, which heavily lean on linear and semi definite\nprogramming, encounter challenges in balancing profitability and fairness.\nThese challenges become especially pronounced in situations that necessitate\ncontinuous rate adjustments and the incorporation of fairness criteria.\nSpecifically, indirect Ratebook optimization, a widely-used method for new\nbusiness price setting, relies on predictor models such as XGBoost or GLMs/GAMs\nto estimate on downstream individually optimized prices. However, this strategy\nis prone to sequential errors and struggles to effectively manage optimizations\nfor continuous rate scenarios. In practice, to save time actuaries frequently\nopt for optimization within discrete intervals (e.g., range of [-20\\%, +20\\%]\nwith fix increments) leading to approximate estimations. Moreover, to\ncircumvent infeasible solutions they often use relaxed constraints leading to\nsuboptimal pricing strategies. The reverse-engineered nature of traditional\nmodels complicates the enforcement of fairness and can lead to biased outcomes.\nOur method addresses these challenges by employing a direct optimization\nstrategy in the continuous space of rates and by embedding fairness through an\nadversarial predictor model. This innovation not only reduces sequential errors\nand simplifies the complexities found in traditional models but also directly\nintegrates fairness measures into the commercial premium calculation. We\ndemonstrate improved margin performance and stronger enforcement of fairness\nhighlighting the critical need to evolve existing pricing strategies.\n","authors":["Vincent Grari","Marcin Detyniecki"],"pdf_url":"https://arxiv.org/pdf/2404.10275v1.pdf","comment":"17 pages, 5 figures"},{"id":"http://arxiv.org/abs/2404.08838v2","updated":"2024-04-16T04:20:32Z","published":"2024-04-12T22:53:41Z","title":"Predicting Traffic Congestion at Urban Intersections Using Data-Driven\n  Modeling","summary":"  Traffic congestion at intersections is a significant issue in urban areas,\nleading to increased commute times, safety hazards, and operational\ninefficiencies. This study aims to develop a predictive model for congestion at\nintersections in major U.S. cities, utilizing a dataset of trip-logging metrics\nfrom commercial vehicles across 4,800 intersections. The dataset encompasses 27\nfeatures, including intersection coordinates, street names, time of day, and\ntraffic metrics (Kashyap et al., 2019). Additional features, such as\nrainfall/snowfall percentage, distance from downtown and outskirts, and road\ntypes, were incorporated to enhance the model's predictive power. The\nmethodology involves data exploration, feature transformation, and handling\nmissing values through low-rank models and label encoding. The proposed model\nhas the potential to assist city planners and governments in anticipating\ntraffic hot spots, optimizing operations, and identifying infrastructure\nchallenges.\n","authors":["Tara Kelly","Jessica Gupta"],"pdf_url":"https://arxiv.org/pdf/2404.08838v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10274v1","updated":"2024-04-16T04:17:17Z","published":"2024-04-16T04:17:17Z","title":"Sparse Attention Regression Network Based Soil Fertility Prediction With\n  Ummaso","summary":"  The challenge of imbalanced soil nutrient datasets significantly hampers\naccurate predictions of soil fertility. To tackle this, a new method is\nsuggested in this research, combining Uniform Manifold Approximation and\nProjection (UMAP) with Least Absolute Shrinkage and Selection Operator (LASSO).\nThe main aim is to counter the impact of uneven data distribution and improve\nsoil fertility models' predictive precision. The model introduced uses Sparse\nAttention Regression, effectively incorporating pertinent features from the\nimbalanced dataset. UMAP is utilized initially to reduce data complexity,\nunveiling hidden structures and important patterns. Following this, LASSO is\napplied to refine features and enhance the model's interpretability. The\nexperimental outcomes highlight the effectiveness of the UMAP and LASSO hybrid\napproach. The proposed model achieves outstanding performance metrics, reaching\na predictive accuracy of 98%, demonstrating its capability in accurate soil\nfertility predictions. Additionally, it showcases a Precision of 91.25%,\nindicating its adeptness in identifying fertile soil instances accurately. The\nRecall metric stands at 90.90%, emphasizing the model's ability to capture true\npositive cases effectively.\n","authors":["R V Raghavendra Rao","U Srinivasulu Reddy"],"pdf_url":"https://arxiv.org/pdf/2404.10274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07475v2","updated":"2024-04-16T04:07:42Z","published":"2024-04-11T05:09:03Z","title":"Laissez-Faire Harms: Algorithmic Biases in Generative Language Models","summary":"  The rapid deployment of generative language models (LMs) has raised concerns\nabout social biases affecting the well-being of diverse consumers. The extant\nliterature on generative LMs has primarily examined bias via explicit identity\nprompting. However, prior research on bias in earlier language-based technology\nplatforms, including search engines, has shown that discrimination can occur\neven when identity terms are not specified explicitly. Studies of bias in LM\nresponses to open-ended prompts (where identity classifications are left\nunspecified) are lacking and have not yet been grounded in end-consumer harms.\nHere, we advance studies of generative LM bias by considering a broader set of\nnatural use cases via open-ended prompting. In this \"laissez-faire\" setting, we\nfind that synthetically generated texts from five of the most pervasive LMs\n(ChatGPT3.5, ChatGPT4, Claude2.0, Llama2, and PaLM2) perpetuate harms of\nomission, subordination, and stereotyping for minoritized individuals with\nintersectional race, gender, and/or sexual orientation identities (AI/AN,\nAsian, Black, Latine, MENA, NH/PI, Female, Non-binary, Queer). We find\nwidespread evidence of bias to an extent that such individuals are hundreds to\nthousands of times more likely to encounter LM-generated outputs that portray\ntheir identities in a subordinated manner compared to representative or\nempowering portrayals. We also document a prevalence of stereotypes (e.g.\nperpetual foreigner) in LM-generated outputs that are known to trigger\npsychological harms that disproportionately affect minoritized individuals.\nThese include stereotype threat, which leads to impaired cognitive performance\nand increased negative self-perception. Our findings highlight the urgent need\nto protect consumers from discriminatory harms caused by language models and\ninvest in critical AI education programs tailored towards empowering diverse\nconsumers.\n","authors":["Evan Shieh","Faye-Marie Vassel","Cassidy Sugimoto","Thema Monroe-White"],"pdf_url":"https://arxiv.org/pdf/2404.07475v2.pdf","comment":"16 pages (43 if including supplementals), 8 figures (23 if including\n  supplementals)"},{"id":"http://arxiv.org/abs/2404.10271v1","updated":"2024-04-16T03:59:33Z","published":"2024-04-16T03:59:33Z","title":"Social Choice for AI Alignment: Dealing with Diverse Human Feedback","summary":"  Foundation models such as GPT-4 are fine-tuned to avoid unsafe or otherwise\nproblematic behavior, so that, for example, they refuse to comply with requests\nfor help with committing crimes or with producing racist text. One approach to\nfine-tuning, called reinforcement learning from human feedback, learns from\nhumans' expressed preferences over multiple outputs. Another approach is\nconstitutional AI, in which the input from humans is a list of high-level\nprinciples. But how do we deal with potentially diverging input from humans?\nHow can we aggregate the input into consistent data about ''collective''\npreferences or otherwise use it to make collective choices about model\nbehavior? In this paper, we argue that the field of social choice is well\npositioned to address these questions, and we discuss ways forward for this\nagenda, drawing on discussions in a recent workshop on Social Choice for AI\nEthics and Safety held in Berkeley, CA, USA in December 2023.\n","authors":["Vincent Conitzer","Rachel Freedman","Jobst Heitzig","Wesley H. Holliday","Bob M. Jacobs","Nathan Lambert","Milan Moss√©","Eric Pacuit","Stuart Russell","Hailey Schoelkopf","Emanuel Tewolde","William S. Zwicker"],"pdf_url":"https://arxiv.org/pdf/2404.10271v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2404.05184v3","updated":"2024-04-16T03:48:27Z","published":"2024-04-08T04:18:54Z","title":"Predicting the Geothermal Gradient in Colombia: a Machine Learning\n  Approach","summary":"  Accurate determination of the geothermal gradient is critical for assessing\nthe geothermal energy potential of a given region. Of particular interest is\nthe case of Colombia, a country with abundant geothermal resources. A history\nof active oil and gas exploration and production has left drilled boreholes in\ndifferent geological settings, providing direct measurements of the geothermal\ngradient. Unfortunately, large regions of the country where geothermal\nresources might exist lack such measurements. Indirect geophysical measurements\nare costly and difficult to perform at regional scales. Computational thermal\nmodels could be constructed, but they require very detailed knowledge of the\nunderlying geology and uniform sampling of subsurface temperatures to be\nwell-constrained. We present an alternative approach that leverages recent\nadvances in supervised machine learning and available direct measurements to\npredict the geothermal gradient in regions where only global-scale geophysical\ndatasets and course geological knowledge are available. We find that a Gradient\nBoosted Regression Tree algorithm yields optimal predictions and extensively\nvalidate the trained model. We show that predictions of our model are within\n12\\% accuracy and that independent measurements performed by other authors\nagree well with our model. Finnally, we present a geothermal gradient map for\nColombia that highlights regions where futher exploration and data collection\nshould be performed.\n","authors":["Juan C. Mej√≠a-Fragoso","Manuel A. Florez","Roc√≠o Bernal-Olaya"],"pdf_url":"https://arxiv.org/pdf/2404.05184v3.pdf","comment":"This is the version we re-submitted to the journal after addressing\n  all the peer review requirements"},{"id":"http://arxiv.org/abs/2402.12876v2","updated":"2024-04-16T03:48:17Z","published":"2024-02-20T10:13:44Z","title":"Federated Multi-Task Learning on Non-IID Data Silos: An Experimental\n  Study","summary":"  The innovative Federated Multi-Task Learning (FMTL) approach consolidates the\nbenefits of Federated Learning (FL) and Multi-Task Learning (MTL), enabling\ncollaborative model training on multi-task learning datasets. However, a\ncomprehensive evaluation method, integrating the unique features of both FL and\nMTL, is currently absent in the field. This paper fills this void by\nintroducing a novel framework, FMTL-Bench, for systematic evaluation of the\nFMTL paradigm. This benchmark covers various aspects at the data, model, and\noptimization algorithm levels, and comprises seven sets of comparative\nexperiments, encapsulating a wide array of non-independent and identically\ndistributed (Non-IID) data partitioning scenarios. We propose a systematic\nprocess for comparing baselines of diverse indicators and conduct a case study\non communication expenditure, time, and energy consumption. Through our\nexhaustive experiments, we aim to provide valuable insights into the strengths\nand limitations of existing baseline methods, contributing to the ongoing\ndiscourse on optimal FMTL application in practical scenarios. The source code\ncan be found on https://github.com/youngfish42/FMTL-Benchmark .\n","authors":["Yuwen Yang","Yuxiang Lu","Suizhi Huang","Shalayiding Sirejiding","Hongtao Lu","Yue Ding"],"pdf_url":"https://arxiv.org/pdf/2402.12876v2.pdf","comment":"Accepted by ICMR'24"},{"id":"http://arxiv.org/abs/2310.07023v2","updated":"2024-04-16T03:42:43Z","published":"2023-10-10T21:23:47Z","title":"Automatic Macro Mining from Interaction Traces at Scale","summary":"  Macros are building block tasks of our everyday smartphone activity (e.g.,\n\"login\", or \"booking a flight\"). Effectively extracting macros is important for\nunderstanding mobile interaction and enabling task automation. These macros are\nhowever difficult to extract at scale as they can be comprised of multiple\nsteps yet hidden within programmatic components of mobile apps. In this paper,\nwe introduce a novel approach based on Large Language Models (LLMs) to\nautomatically extract semantically meaningful macros from both random and\nuser-curated mobile interaction traces. The macros produced by our approach are\nautomatically tagged with natural language descriptions and are fully\nexecutable. We conduct multiple studies to validate the quality of extracted\nmacros, including user evaluation, comparative analysis against human-curated\ntasks, and automatic execution of these macros. These experiments and analyses\nshow the effectiveness of our approach and the usefulness of extracted macros\nin various downstream applications.\n","authors":["Forrest Huang","Gang Li","Tao Li","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2310.07023v2.pdf","comment":"Accepted to CHI 2024"},{"id":"http://arxiv.org/abs/2404.00231v2","updated":"2024-04-16T03:38:31Z","published":"2024-03-30T03:23:52Z","title":"Attention-based Shape-Deformation Networks for Artifact-Free Geometry\n  Reconstruction of Lumbar Spine from MR Images","summary":"  Lumbar disc degeneration, a progressive structural wear and tear of lumbar\nintervertebral disc, is regarded as an essential role on low back pain, a\nsignificant global health concern. Automated lumbar spine geometry\nreconstruction from MR images will enable fast measurement of medical\nparameters to evaluate the lumbar status, in order to determine a suitable\ntreatment. Existing image segmentation-based techniques often generate\nerroneous segments or unstructured point clouds, unsuitable for medical\nparameter measurement. In this work, we present TransDeformer: a novel\nattention-based deep learning approach that reconstructs the geometry of the\nlumbar spine with high spatial accuracy and mesh correspondence across\npatients, and we also present a variant of TransDeformer for error estimation.\nSpecially, we devise new attention modules with a new attention formula, which\nintegrate image features and tokenized contour features to predict the\ndisplacements of the points on a shape template without the need for image\nsegmentation. The deformed template reveals the lumbar spine geometry in an\nimage. Experiment results show that our TransDeformer generates artifact-free\ngeometry outputs, and its variant predicts the error of a reconstructed\ngeometry. Our code is available at\nhttps://github.com/linchenq/TransDeformer-Mesh.\n","authors":["Linchen Qian","Jiasong Chen","Linhai Ma","Timur Urakov","Weiyong Gu","Liang Liang"],"pdf_url":"https://arxiv.org/pdf/2404.00231v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10261v1","updated":"2024-04-16T03:31:28Z","published":"2024-04-16T03:31:28Z","title":"Lighter, Better, Faster Multi-Source Domain Adaptation with Gaussian\n  Mixture Models and Optimal Transport","summary":"  In this paper, we tackle Multi-Source Domain Adaptation (MSDA), a task in\ntransfer learning where one adapts multiple heterogeneous, labeled source\nprobability measures towards a different, unlabeled target measure. We propose\na novel framework for MSDA, based on Optimal Transport (OT) and Gaussian\nMixture Models (GMMs). Our framework has two key advantages. First, OT between\nGMMs can be solved efficiently via linear programming. Second, it provides a\nconvenient model for supervised learning, especially classification, as\ncomponents in the GMM can be associated with existing classes. Based on the\nGMM-OT problem, we propose a novel technique for calculating barycenters of\nGMMs. Based on this novel algorithm, we propose two new strategies for MSDA:\nGMM-WBT and GMM-DaDiL. We empirically evaluate our proposed methods on four\nbenchmarks in image classification and fault diagnosis, showing that we improve\nover the prior art while being faster and involving fewer parameters.\n","authors":["Eduardo Fernandes Montesuma","Fred Ngol√® Mboula","Antoine Souloumiac"],"pdf_url":"https://arxiv.org/pdf/2404.10261v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2404.10259v1","updated":"2024-04-16T03:26:43Z","published":"2024-04-16T03:26:43Z","title":"Uncovering Latent Arguments in Social Media Messaging by Employing\n  LLMs-in-the-Loop Strategy","summary":"  The widespread use of social media has led to a surge in popularity for\nautomated methods of analyzing public opinion. Supervised methods are adept at\ntext categorization, yet the dynamic nature of social media discussions poses a\ncontinual challenge for these techniques due to the constant shifting of the\nfocus. On the other hand, traditional unsupervised methods for extracting\nthemes from public discourse, such as topic modeling, often reveal overarching\npatterns that might not capture specific nuances. Consequently, a significant\nportion of research into social media discourse still depends on\nlabor-intensive manual coding techniques and a human-in-the-loop approach,\nwhich are both time-consuming and costly. In this work, we study the problem of\ndiscovering arguments associated with a specific theme. We propose a generic\nLLMs-in-the-Loop strategy that leverages the advanced capabilities of Large\nLanguage Models (LLMs) to extract latent arguments from social media messaging.\nTo demonstrate our approach, we apply our framework to contentious topics. We\nuse two publicly available datasets: (1) the climate campaigns dataset of 14k\nFacebook ads with 25 themes and (2) the COVID-19 vaccine campaigns dataset of\n9k Facebook ads with 14 themes. Furthermore, we analyze demographic targeting\nand the adaptation of messaging based on real-world events.\n","authors":["Tunazzina Islam","Dan Goldwasser"],"pdf_url":"https://arxiv.org/pdf/2404.10259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.15196v2","updated":"2024-04-16T03:25:54Z","published":"2023-07-27T21:01:26Z","title":"The Marginal Value of Momentum for Small Learning Rate SGD","summary":"  Momentum is known to accelerate the convergence of gradient descent in\nstrongly convex settings without stochastic gradient noise. In stochastic\noptimization, such as training neural networks, folklore suggests that momentum\nmay help deep learning optimization by reducing the variance of the stochastic\ngradient update, but previous theoretical analyses do not find momentum to\noffer any provable acceleration. Theoretical results in this paper clarify the\nrole of momentum in stochastic settings where the learning rate is small and\ngradient noise is the dominant source of instability, suggesting that SGD with\nand without momentum behave similarly in the short and long time horizons.\nExperiments show that momentum indeed has limited benefits for both\noptimization and generalization in practical training regimes where the optimal\nlearning rate is not very large, including small- to medium-batch training from\nscratch on ImageNet and fine-tuning language models on downstream tasks.\n","authors":["Runzhe Wang","Sadhika Malladi","Tianhao Wang","Kaifeng Lyu","Zhiyuan Li"],"pdf_url":"https://arxiv.org/pdf/2307.15196v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14866v2","updated":"2024-04-16T03:18:38Z","published":"2024-02-21T07:45:22Z","title":"APTQ: Attention-aware Post-Training Mixed-Precision Quantization for\n  Large Language Models","summary":"  Large Language Models (LLMs) have greatly advanced the natural language\nprocessing paradigm. However, the high computational load and huge model sizes\npose a grand challenge for deployment on edge devices. To this end, we propose\nAPTQ (Attention-aware Post-Training Mixed-Precision Quantization) for LLMs,\nwhich considers not only the second-order information of each layer's weights,\nbut also, for the first time, the nonlinear effect of attention outputs on the\nentire model. We leverage the Hessian trace as a sensitivity metric for\nmixed-precision quantization, ensuring an informed precision reduction that\nretains model performance. Experiments show APTQ surpasses previous\nquantization methods, achieving an average of 4 bit width a 5.22 perplexity\nnearly equivalent to full precision in the C4 dataset. In addition, APTQ\nattains state-of-the-art zero-shot accuracy of 68.24\\% and 70.48\\% at an\naverage bitwidth of 3.8 in LLaMa-7B and LLaMa-13B, respectively, demonstrating\nits effectiveness to produce high-quality quantized LLMs.\n","authors":["Ziyi Guan","Hantao Huang","Yupeng Su","Hong Huang","Ngai Wong","Hao Yu"],"pdf_url":"https://arxiv.org/pdf/2402.14866v2.pdf","comment":"6 pages, 2 figures, published to DAC 2024: 61st IEEE/ACM Design\n  Automation Conference. (DAC'24)"},{"id":"http://arxiv.org/abs/2404.10255v1","updated":"2024-04-16T03:18:27Z","published":"2024-04-16T03:18:27Z","title":"Privacy-Preserving Training-as-a-Service for On-Device Intelligence:\n  Concept, Architectural Scheme, and Open Problems","summary":"  On-device intelligence (ODI) enables artificial intelligence (AI)\napplications to run on end devices, providing real-time and customized AI\nservices without relying on remote servers. However, training models for\non-device deployment face significant challenges due to the decentralized and\nprivacy-sensitive nature of users' data, along with end-side constraints\nrelated to network connectivity, computation efficiency, etc. Existing training\nparadigms, such as cloud-based training, federated learning, and transfer\nlearning, fail to sufficiently address these practical constraints that are\nprevalent for devices. To overcome these challenges, we propose\nPrivacy-Preserving Training-as-a-Service (PTaaS), a novel service computing\nparadigm that provides privacy-friendly, customized AI model training for end\ndevices. PTaaS outsources the core training process to remote and powerful\ncloud or edge servers, efficiently developing customized on-device models based\non uploaded anonymous queries, ensuring data privacy while reducing the\ncomputation load on individual devices. We explore the definition, goals, and\ndesign principles of PTaaS, alongside emerging technologies that support the\nPTaaS paradigm. An architectural scheme for PTaaS is also presented, followed\nby a series of open problems that set the stage for future research directions\nin the field of PTaaS.\n","authors":["Zhiyuan Wu","Sheng Sun","Yuwei Wang","Min Liu","Bo Gao","Tianliu He","Wen Wang"],"pdf_url":"https://arxiv.org/pdf/2404.10255v1.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2401.17541v3","updated":"2024-04-16T03:15:45Z","published":"2024-01-31T02:08:43Z","title":"Towards Understanding Variants of Invariant Risk Minimization through\n  the Lens of Calibration","summary":"  Machine learning models traditionally assume that training and test data are\nindependently and identically distributed. However, in real-world applications,\nthe test distribution often differs from training. This problem, known as\nout-of-distribution generalization, challenges conventional models. Invariant\nRisk Minimization (IRM) emerges as a solution, aiming to identify features\ninvariant across different environments to enhance out-of-distribution\nrobustness. However, IRM's complexity, particularly its bi-level optimization,\nhas led to the development of various approximate methods. Our study\ninvestigates these approximate IRM techniques, employing the Expected\nCalibration Error (ECE) as a key metric. ECE, which measures the reliability of\nmodel prediction, serves as an indicator of whether models effectively capture\nenvironment-invariant features. Through a comparative analysis of datasets with\ndistributional shifts, we observe that Information Bottleneck-based IRM, which\ncondenses representational information, achieves a balance in improving ECE\nwhile preserving accuracy relatively. This finding is pivotal, as it\ndemonstrates a feasible path to maintaining robustness without compromising\naccuracy. Nonetheless, our experiments also caution against\nover-regularization, which can diminish accuracy. This underscores the\nnecessity for a systematic approach in evaluating out-of-distribution\ngeneralization metrics, one that beyond mere accuracy to address the nuanced\ninterplay between accuracy and calibration.\n","authors":["Kotaro Yoshida","Hiroki Naganuma"],"pdf_url":"https://arxiv.org/pdf/2401.17541v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06079v2","updated":"2024-04-16T03:07:02Z","published":"2024-03-10T03:51:59Z","title":"Generalization of Graph Neural Networks through the Lens of Homomorphism","summary":"  Despite the celebrated popularity of Graph Neural Networks (GNNs) across\nnumerous applications, the ability of GNNs to generalize remains less explored.\nIn this work, we propose to study the generalization of GNNs through a novel\nperspective - analyzing the entropy of graph homomorphism. By linking graph\nhomomorphism with information-theoretic measures, we derive generalization\nbounds for both graph and node classifications. These bounds are capable of\ncapturing subtleties inherent in various graph structures, including but not\nlimited to paths, cycles and cliques. This enables a data-dependent\ngeneralization analysis with robust theoretical guarantees. To shed light on\nthe generality of of our proposed bounds, we present a unifying framework that\ncan characterize a broad spectrum of GNN models through the lens of graph\nhomomorphism. We validate the practical applicability of our theoretical\nfindings by showing the alignment between the proposed bounds and the\nempirically observed generalization gaps over both real-world and synthetic\ndatasets.\n","authors":["Shouheng Li","Dongwoo Kim","Qing Wang"],"pdf_url":"https://arxiv.org/pdf/2403.06079v2.pdf","comment":"17 pages, 3 figures"},{"id":"http://arxiv.org/abs/2402.15650v2","updated":"2024-04-16T03:00:51Z","published":"2024-02-23T23:22:06Z","title":"Multi-Constraint Safe RL with Objective Suppression for Safety-Critical\n  Applications","summary":"  Safe reinforcement learning tasks with multiple constraints are a challenging\ndomain despite being very common in the real world. In safety-critical domains,\nproperly handling the constraints becomes even more important. To address this\nchallenge, we first describe the multi-constraint problem with a stronger\nUniformly Constrained MDP (UCMDP) model; we then propose Objective Suppression,\na novel method that adaptively suppresses the task reward maximizing objectives\naccording to a safety critic, as a solution to the Lagrangian dual of a UCMDP.\nWe benchmark Objective Suppression in two multi-constraint safety domains,\nincluding an autonomous driving domain where any incorrect behavior can lead to\ndisastrous consequences. Empirically, we demonstrate that our proposed method,\nwhen combined with existing safe RL algorithms, can match the task reward\nachieved by our baselines with significantly fewer constraint violations.\n","authors":["Zihan Zhou","Jonathan Booher","Khashayar Rohanimanesh","Wei Liu","Aleksandr Petiushko","Animesh Garg"],"pdf_url":"https://arxiv.org/pdf/2402.15650v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.08360v3","updated":"2024-04-16T02:58:34Z","published":"2023-07-17T09:55:35Z","title":"Universal Online Learning with Gradient Variations: A Multi-layer Online\n  Ensemble Approach","summary":"  In this paper, we propose an online convex optimization approach with two\ndifferent levels of adaptivity. On a higher level, our approach is agnostic to\nthe unknown types and curvatures of the online functions, while at a lower\nlevel, it can exploit the unknown niceness of the environments and attain\nproblem-dependent guarantees. Specifically, we obtain $\\mathcal{O}(\\log V_T)$,\n$\\mathcal{O}(d \\log V_T)$ and $\\hat{\\mathcal{O}}(\\sqrt{V_T})$ regret bounds for\nstrongly convex, exp-concave and convex loss functions, respectively, where $d$\nis the dimension, $V_T$ denotes problem-dependent gradient variations and the\n$\\hat{\\mathcal{O}}(\\cdot)$-notation omits $\\log V_T$ factors. Our result not\nonly safeguards the worst-case guarantees but also directly implies the\nsmall-loss bounds in analysis. Moreover, when applied to adversarial/stochastic\nconvex optimization and game theory problems, our result enhances the existing\nuniversal guarantees. Our approach is based on a multi-layer online ensemble\nframework incorporating novel ingredients, including a carefully designed\noptimism for unifying diverse function types and cascaded corrections for\nalgorithmic stability. Notably, despite its multi-layer structure, our\nalgorithm necessitates only one gradient query per round, making it favorable\nwhen the gradient evaluation is time-consuming. This is facilitated by a novel\nregret decomposition equipped with carefully designed surrogate losses.\n","authors":["Yu-Hu Yan","Peng Zhao","Zhi-Hua Zhou"],"pdf_url":"https://arxiv.org/pdf/2307.08360v3.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2312.15685v2","updated":"2024-04-16T02:46:58Z","published":"2023-12-25T10:29:28Z","title":"What Makes Good Data for Alignment? A Comprehensive Study of Automatic\n  Data Selection in Instruction Tuning","summary":"  Instruction tuning is a standard technique employed to align large language\nmodels to end tasks and user preferences after the initial pretraining phase.\nRecent research indicates the critical role of data engineering in instruction\ntuning -- when appropriately selected, only limited data is necessary to\nachieve superior performance. However, we still lack a principled understanding\nof what makes good instruction tuning data for alignment, and how we should\nselect data automatically and effectively. In this work, we delve deeply into\nautomatic data selection strategies for alignment. We start with controlled\nstudies to measure data across three dimensions: complexity, quality, and\ndiversity, along which we examine existing methods and introduce novel\ntechniques for enhanced data measurement. Subsequently, we propose a simple\nstrategy to select data samples based on the measurement. We present deita\n(short for Data-Efficient Instruction Tuning for Alignment), a series of models\nfine-tuned from LLaMA and Mistral models using data samples automatically\nselected with our proposed approach. Empirically, deita performs better or on\npar with the state-of-the-art open-source alignment models with only 6K SFT\ntraining data samples -- over 10x less than the data used in the baselines.\nWhen further trained with direct preference optimization (DPO),\ndeita-Mistral-7B + DPO trained with 6K SFT and 10K DPO samples achieve 7.55\nMT-Bench and 90.06% AlpacaEval scores. We anticipate this work to provide tools\non automatic data selection, facilitating data-efficient alignment. We release\nour models as well as the selected datasets for future researches to\neffectively align models more efficiently.\n","authors":["Wei Liu","Weihao Zeng","Keqing He","Yong Jiang","Junxian He"],"pdf_url":"https://arxiv.org/pdf/2312.15685v2.pdf","comment":"ICLR2024 Camera Ready. Data and model checkpoints are available at\n  https://github.com/hkust-nlp/deita"},{"id":"http://arxiv.org/abs/2404.10957v1","updated":"2024-04-16T23:47:23Z","published":"2024-04-16T23:47:23Z","title":"Personalized Federated Learning via Stacking","summary":"  Traditional Federated Learning (FL) methods typically train a single global\nmodel collaboratively without exchanging raw data. In contrast, Personalized\nFederated Learning (PFL) techniques aim to create multiple models that are\nbetter tailored to individual clients' data. We present a novel personalization\napproach based on stacked generalization where clients directly send each other\nprivacy-preserving models to be used as base models to train a meta-model on\nprivate data. Our approach is flexible, accommodating various\nprivacy-preserving techniques and model types, and can be applied in\nhorizontal, hybrid, and vertically partitioned federations. Additionally, it\noffers a natural mechanism for assessing each client's contribution to the\nfederation. Through comprehensive evaluations across diverse simulated data\nheterogeneity scenarios, we showcase the effectiveness of our method.\n","authors":["Emilio Cantu-Cervini"],"pdf_url":"https://arxiv.org/pdf/2404.10957v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11651v3","updated":"2024-04-16T23:33:50Z","published":"2023-09-20T21:32:58Z","title":"Drift Control of High-Dimensional RBM: A Computational Method Based on\n  Neural Networks","summary":"  Motivated by applications in queueing theory, we consider a stochastic\ncontrol problem whose state space is the $d$-dimensional positive orthant. The\ncontrolled process $Z$ evolves as a reflected Brownian motion whose covariance\nmatrix is exogenously specified, as are its directions of reflection from the\northant's boundary surfaces. A system manager chooses a drift vector\n$\\theta(t)$ at each time $t$ based on the history of $Z$, and the cost rate at\ntime $t$ depends on both $Z(t)$ and $\\theta(t)$. In our initial problem\nformulation, the objective is to minimize expected discounted cost over an\ninfinite planning horizon, after which we treat the corresponding ergodic\ncontrol problem. Extending earlier work by Han et al. (Proceedings of the\nNational Academy of Sciences, 2018, 8505-8510), we develop and illustrate a\nsimulation-based computational method that relies heavily on deep neural\nnetwork technology. For test problems studied thus far, our method is accurate\nto within a fraction of one percent, and is computationally feasible in\ndimensions up to at least $d=30$.\n","authors":["Baris Ata","J. Michael Harrison","Nian Si"],"pdf_url":"https://arxiv.org/pdf/2309.11651v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10949v1","updated":"2024-04-16T23:17:04Z","published":"2024-04-16T23:17:04Z","title":"Human-Algorithm Collaborative Bayesian Optimization for Engineering\n  Systems","summary":"  Bayesian optimization has been successfully applied throughout Chemical\nEngineering for the optimization of functions that are expensive-to-evaluate,\nor where gradients are not easily obtainable. However, domain experts often\npossess valuable physical insights that are overlooked in fully automated\ndecision-making approaches, necessitating the inclusion of human input. In this\narticle we re-introduce the human back into the data-driven decision making\nloop by outlining an approach for collaborative Bayesian optimization. Our\nmethodology exploits the hypothesis that humans are more efficient at making\ndiscrete choices rather than continuous ones and enables experts to influence\ncritical early decisions. We apply high-throughput (batch) Bayesian\noptimization alongside discrete decision theory to enable domain experts to\ninfluence the selection of experiments. At every iteration we apply a\nmulti-objective approach that results in a set of alternate solutions that have\nboth high utility and are reasonably distinct. The expert then selects the\ndesired solution for evaluation from this set, allowing for the inclusion of\nexpert knowledge and improving accountability, whilst maintaining the\nadvantages of Bayesian optimization. We demonstrate our approach across a\nnumber of applied and numerical case studies including bioprocess optimization\nand reactor geometry design, demonstrating that even in the case of an\nuninformed practitioner our algorithm recovers the regret of standard Bayesian\noptimization. Through the inclusion of continuous expert opinion, our approach\nenables faster convergence, and improved accountability for Bayesian\noptimization in engineering systems.\n","authors":["Tom Savage","Ehecatl Antonio del Rio Chanona"],"pdf_url":"https://arxiv.org/pdf/2404.10949v1.pdf","comment":"31 pages with appendix and references"},{"id":"http://arxiv.org/abs/2201.00087v6","updated":"2024-04-16T23:11:52Z","published":"2022-01-01T01:39:05Z","title":"Persistent Homological State-Space Estimation of Functional Human Brain\n  Networks at Rest","summary":"  We introduce an innovative, data-driven topological data analysis (TDA)\ntechnique for estimating the state spaces of dynamically changing functional\nhuman brain networks at rest. Our method utilizes the Wasserstein distance to\nmeasure topological differences, enabling the clustering of brain networks into\ndistinct topological states. This technique outperforms the commonly used\nk-means clustering in identifying brain network state spaces by effectively\nincorporating the temporal dynamics of the data without the need for explicit\nmodel specification. We further investigate the genetic underpinnings of these\ntopological features using a twin study design, examining the heritability of\nsuch state changes. Our findings suggest that the topology of brain networks,\nparticularly in their dynamic state changes, may hold significant hidden\ngenetic information. MATLAB code for the method is available at\nhttps://github.com/laplcebeltrami/PH-STAT.\n","authors":["Moo K. Chung","Shih-Gu Huang","Ian C. Carroll","Vince D. Calhoun","H. Hill Goldsmith"],"pdf_url":"https://arxiv.org/pdf/2201.00087v6.pdf","comment":"To be published in PLOS Computational Biology"},{"id":"http://arxiv.org/abs/2306.01187v3","updated":"2024-04-16T23:01:43Z","published":"2023-06-01T22:56:30Z","title":"Training neural operators to preserve invariant measures of chaotic\n  attractors","summary":"  Chaotic systems make long-horizon forecasts difficult because small\nperturbations in initial conditions cause trajectories to diverge at an\nexponential rate. In this setting, neural operators trained to minimize squared\nerror losses, while capable of accurate short-term forecasts, often fail to\nreproduce statistical or structural properties of the dynamics over longer time\nhorizons and can yield degenerate results. In this paper, we propose an\nalternative framework designed to preserve invariant measures of chaotic\nattractors that characterize the time-invariant statistical properties of the\ndynamics. Specifically, in the multi-environment setting (where each sample\ntrajectory is governed by slightly different dynamics), we consider two novel\napproaches to training with noisy data. First, we propose a loss based on the\noptimal transport distance between the observed dynamics and the neural\noperator outputs. This approach requires expert knowledge of the underlying\nphysics to determine what statistical features should be included in the\noptimal transport loss. Second, we show that a contrastive learning framework,\nwhich does not require any specialized prior knowledge, can preserve\nstatistical properties of the dynamics nearly as well as the optimal transport\napproach. On a variety of chaotic systems, our method is shown empirically to\npreserve invariant measures of chaotic attractors.\n","authors":["Ruoxi Jiang","Peter Y. Lu","Elena Orlova","Rebecca Willett"],"pdf_url":"https://arxiv.org/pdf/2306.01187v3.pdf","comment":"Accepted at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2210.00483v2","updated":"2024-04-16T22:52:21Z","published":"2022-10-02T10:37:04Z","title":"Learning Algorithm Generalization Error Bounds via Auxiliary\n  Distributions","summary":"  Generalization error bounds are essential for comprehending how well machine\nlearning models work. In this work, we suggest a novel method, i.e., the\nAuxiliary Distribution Method, that leads to new upper bounds on expected\ngeneralization errors that are appropriate for supervised learning scenarios.\nWe show that our general upper bounds can be specialized under some conditions\nto new bounds involving the $\\alpha$-Jensen-Shannon, $\\alpha$-R\\'enyi ($0<\n\\alpha < 1$) information between a random variable modeling the set of training\nsamples and another random variable modeling the set of hypotheses. Our upper\nbounds based on $\\alpha$-Jensen-Shannon information are also finite.\nAdditionally, we demonstrate how our auxiliary distribution method can be used\nto derive the upper bounds on excess risk of some learning algorithms in the\nsupervised learning context {\\blue and the generalization error under the\ndistribution mismatch scenario in supervised learning algorithms, where the\ndistribution mismatch is modeled as $\\alpha$-Jensen-Shannon or $\\alpha$-R\\'enyi\ndivergence between the distribution of test and training data samples\ndistributions.} We also outline the conditions for which our proposed upper\nbounds might be tighter than other earlier upper bounds.\n","authors":["Gholamali Aminian","Saeed Masiha","Laura Toni","Miguel R. D. Rodrigues"],"pdf_url":"https://arxiv.org/pdf/2210.00483v2.pdf","comment":"Accepted in IEEE Journal on Selected Areas in Information Theory"},{"id":"http://arxiv.org/abs/2404.10942v1","updated":"2024-04-16T22:47:59Z","published":"2024-04-16T22:47:59Z","title":"What Hides behind Unfairness? Exploring Dynamics Fairness in\n  Reinforcement Learning","summary":"  In sequential decision-making problems involving sensitive attributes like\nrace and gender, reinforcement learning (RL) agents must carefully consider\nlong-term fairness while maximizing returns. Recent works have proposed many\ndifferent types of fairness notions, but how unfairness arises in RL problems\nremains unclear. In this paper, we address this gap in the literature by\ninvestigating the sources of inequality through a causal lens. We first analyse\nthe causal relationships governing the data generation process and decompose\nthe effect of sensitive attributes on long-term well-being into distinct\ncomponents. We then introduce a novel notion called dynamics fairness, which\nexplicitly captures the inequality stemming from environmental dynamics,\ndistinguishing it from those induced by decision-making or inherited from the\npast. This notion requires evaluating the expected changes in the next state\nand the reward induced by changing the value of the sensitive attribute while\nholding everything else constant. To quantitatively evaluate this\ncounterfactual concept, we derive identification formulas that allow us to\nobtain reliable estimations from data. Extensive experiments demonstrate the\neffectiveness of the proposed techniques in explaining, detecting, and reducing\ninequality in reinforcement learning.\n","authors":["Zhihong Deng","Jing Jiang","Guodong Long","Chengqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.10942v1.pdf","comment":"13 pages, 9 figures, accepted by IJCAI 2024"},{"id":"http://arxiv.org/abs/2311.17121v2","updated":"2024-04-16T22:41:26Z","published":"2023-11-28T13:44:33Z","title":"ScribbleGen: Generative Data Augmentation Improves Scribble-supervised\n  Semantic Segmentation","summary":"  Recent advances in generative models, such as diffusion models, have made\ngenerating high-quality synthetic images widely accessible. Prior works have\nshown that training on synthetic images improves many perception tasks, such as\nimage classification, object detection, and semantic segmentation. We are the\nfirst to explore generative data augmentations for scribble-supervised semantic\nsegmentation. We propose ScribbleGen, a generative data augmentation method\nthat leverages a ControlNet diffusion model conditioned on semantic scribbles\nto produce high-quality training data. However, naive implementations of\ngenerative data augmentations may inadvertently harm the performance of the\ndownstream segmentor rather than improve it. We leverage classifier-free\ndiffusion guidance to enforce class consistency and introduce encode ratios to\ntrade off data diversity for data realism. Using the guidance scale and encode\nratio, we can generate a spectrum of high-quality training images. We propose\nmultiple augmentation schemes and find that these schemes significantly impact\nmodel performance, especially in the low-data regime. Our framework further\nreduces the gap between the performance of scribble-supervised segmentation and\nthat of fully-supervised segmentation. We also show that our framework\nsignificantly improves segmentation performance on small datasets, even\nsurpassing fully-supervised segmentation. The code is available at\nhttps://github.com/mengtang-lab/scribblegen.\n","authors":["Jacob Schnell","Jieke Wang","Lu Qi","Vincent Tao Hu","Meng Tang"],"pdf_url":"https://arxiv.org/pdf/2311.17121v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.10060v4","updated":"2024-04-16T22:40:40Z","published":"2023-06-14T13:06:04Z","title":"MUBen: Benchmarking the Uncertainty of Molecular Representation Models","summary":"  Large molecular representation models pre-trained on massive unlabeled data\nhave shown great success in predicting molecular properties. However, these\nmodels may tend to overfit the fine-tuning data, resulting in over-confident\npredictions on test data that fall outside of the training distribution. To\naddress this issue, uncertainty quantification (UQ) methods can be used to\nimprove the models' calibration of predictions. Although many UQ approaches\nexist, not all of them lead to improved performance. While some studies have\nincluded UQ to improve molecular pre-trained models, the process of selecting\nsuitable backbone and UQ methods for reliable molecular uncertainty estimation\nremains underexplored. To address this gap, we present MUBen, which evaluates\ndifferent UQ methods for state-of-the-art backbone molecular representation\nmodels to investigate their capabilities. By fine-tuning various backbones\nusing different molecular descriptors as inputs with UQ methods from different\ncategories, we assess the influence of architectural decisions and training\nstrategies. Our study offers insights for selecting UQ for backbone models,\nwhich can facilitate research on uncertainty-critical applications in fields\nsuch as materials science and drug discovery.\n","authors":["Yinghao Li","Lingkai Kong","Yuanqi Du","Yue Yu","Yuchen Zhuang","Wenhao Mu","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2306.10060v4.pdf","comment":"58 pages, 10 figures, 39 tables, in TMLR"},{"id":"http://arxiv.org/abs/2404.07315v2","updated":"2024-04-16T22:32:34Z","published":"2024-04-10T19:25:51Z","title":"Structured Reinforcement Learning for Media Streaming at the Wireless\n  Edge","summary":"  Media streaming is the dominant application over wireless edge (access)\nnetworks. The increasing softwarization of such networks has led to efforts at\nintelligent control, wherein application-specific actions may be dynamically\ntaken to enhance the user experience. The goal of this work is to develop and\ndemonstrate learning-based policies for optimal decision making to determine\nwhich clients to dynamically prioritize in a video streaming setting. We\nformulate the policy design question as a constrained Markov decision problem\n(CMDP), and observe that by using a Lagrangian relaxation we can decompose it\ninto single-client problems. Further, the optimal policy takes a threshold form\nin the video buffer length, which enables us to design an efficient constrained\nreinforcement learning (CRL) algorithm to learn it. Specifically, we show that\na natural policy gradient (NPG) based algorithm that is derived using the\nstructure of our problem converges to the globally optimal policy. We then\ndevelop a simulation environment for training, and a real-world intelligent\ncontroller attached to a WiFi access point for evaluation. We empirically show\nthat the structured learning approach enables fast learning. Furthermore, such\na structured policy can be easily deployed due to low computational complexity,\nleading to policy execution taking only about 15$\\mu$s. Using YouTube streaming\nexperiments in a resource constrained scenario, we demonstrate that the CRL\napproach can increase quality of experience (QOE) by over 30\\%.\n","authors":["Archana Bura","Sarat Chandra Bobbili","Shreyas Rameshkumar","Desik Rengarajan","Dileep Kalathil","Srinivas Shakkottai"],"pdf_url":"https://arxiv.org/pdf/2404.07315v2.pdf","comment":"15 pages, 14 figures"},{"id":"http://arxiv.org/abs/2307.13938v2","updated":"2024-04-16T22:29:17Z","published":"2023-07-26T03:30:28Z","title":"Improving Semi-Supervised Semantic Segmentation with Dual-Level Siamese\n  Structure Network","summary":"  Semi-supervised semantic segmentation (SSS) is an important task that\nutilizes both labeled and unlabeled data to reduce expenses on labeling\ntraining examples. However, the effectiveness of SSS algorithms is limited by\nthe difficulty of fully exploiting the potential of unlabeled data. To address\nthis, we propose a dual-level Siamese structure network (DSSN) for pixel-wise\ncontrastive learning. By aligning positive pairs with a pixel-wise contrastive\nloss using strong augmented views in both low-level image space and high-level\nfeature space, the proposed DSSN is designed to maximize the utilization of\navailable unlabeled data. Additionally, we introduce a novel class-aware\npseudo-label selection strategy for weak-to-strong supervision, which addresses\nthe limitations of most existing methods that do not perform selection or apply\na predefined threshold for all classes. Specifically, our strategy selects the\ntop high-confidence prediction of the weak view for each class to generate\npseudo labels that supervise the strong augmented views. This strategy is\ncapable of taking into account the class imbalance and improving the\nperformance of long-tailed classes. Our proposed method achieves\nstate-of-the-art results on two datasets, PASCAL VOC 2012 and Cityscapes,\noutperforming other SSS algorithms by a significant margin. The source code is\navailable at https://github.com/kunzhan/DSSN.\n","authors":["Zhibo Tain","Xiaolin Zhang","Peng Zhang","Kun Zhan"],"pdf_url":"https://arxiv.org/pdf/2307.13938v2.pdf","comment":"ACM MM 2023"}],"Multimedia":[{"id":"http://arxiv.org/abs/2404.10702v1","updated":"2024-04-16T16:19:22Z","published":"2024-04-16T16:19:22Z","title":"Retrieval Augmented Verification : Unveiling Disinformation with\n  Structured Representations for Zero-Shot Real-Time Evidence-guided\n  Fact-Checking of Multi-modal Social media posts","summary":"  Social Media posts, where real images are unscrupulously reused along with\nprovocative text to promote a particular idea, have been one of the major\nsources of disinformation. By design, these claims are without editorial\noversight and accessible to a vast population who otherwise may not have access\nto multiple information sources. This implies the need to fact-check these\nposts and clearly explain which parts of the posts are fake. In the supervised\nlearning setup, this is often reduced to a binary classification problem,\nneglecting all intermediate stages. Further, these claims often involve recent\nevents on which systems trained on historical data are prone to fail. In this\nwork, we propose a zero-shot approach by retrieving real-time web-scraped\nevidence from multiple news websites and matching them with the claim text and\nimage using pretrained language vision systems. We propose a graph structured\nrepresentation, which a) allows us to gather evidence automatically and b)\nhelps generate interpretable results by explicitly pointing out which parts of\nthe claim can not be verified. Our zero-shot method, with improved\ninterpretability, generates competitive results against the state-of-the-art\nmethods\n","authors":["Arka Ujjal Dey","Artemis Llabr√©s","Ernest Valveny","Dimosthenis Karatzas"],"pdf_url":"https://arxiv.org/pdf/2404.10702v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.16215v6","updated":"2024-04-16T13:32:25Z","published":"2023-08-30T16:44:38Z","title":"Deep Video Codec Control for Vision Models","summary":"  Standardized lossy video coding is at the core of almost all real-world video\nprocessing pipelines. Rate control is used to enable standard codecs to adapt\nto different network bandwidth conditions or storage constraints. However,\nstandard video codecs (e.g., H.264) and their rate control modules aim to\nminimize video distortion w.r.t. human quality assessment. We demonstrate\nempirically that standard-coded videos vastly deteriorate the performance of\ndeep vision models. To overcome the deterioration of vision performance, this\npaper presents the first end-to-end learnable deep video codec control that\nconsiders both bandwidth constraints and downstream deep vision performance,\nwhile adhering to existing standardization. We demonstrate that our approach\nbetter preserves downstream deep vision performance than traditional standard\nvideo coding.\n","authors":["Christoph Reich","Biplob Debnath","Deep Patel","Tim Prangemeier","Daniel Cremers","Srimat Chakradhar"],"pdf_url":"https://arxiv.org/pdf/2308.16215v6.pdf","comment":"Accepted at CVPR 2024 Workshop on AI for Streaming (AIS)"},{"id":"http://arxiv.org/abs/2404.10528v1","updated":"2024-04-16T12:55:57Z","published":"2024-04-16T12:55:57Z","title":"AllTheDocks road safety dataset: A cyclist's perspective and experience","summary":"  Active travel is an essential component in intelligent transportation\nsystems. Cycling, as a form of active travel, shares the road space with\nmotorised traffic which often affects the cyclists' safety and comfort and\ntherefore peoples' propensity to uptake cycling instead of driving. This paper\npresents a unique dataset, collected by cyclists across London, that includes\nvideo footage, accelerometer, GPS, and gyroscope data. The dataset is then\nlabelled by an independent group of London cyclists to rank the safety level of\neach frame and to identify objects in the cyclist's field of vision that might\naffect their experience. Furthermore, in this dataset, the quality of the road\nis measured by the international roughness index of the surface, which\nindicates the comfort of cycling on the road. The dataset will be made\navailable for open access in the hope of motivating more research in this area\nto underpin the requirements for cyclists' safety and comfort and encourage\nmore people to replace vehicle travel with cycling.\n","authors":["Chia-Yen Chiang","Ruikang Zhong","Jennifer Ding","Joseph Wood","Stephen Bee","Mona Jaber"],"pdf_url":"https://arxiv.org/pdf/2404.10528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10342v1","updated":"2024-04-16T07:25:17Z","published":"2024-04-16T07:25:17Z","title":"Referring Flexible Image Restoration","summary":"  In reality, images often exhibit multiple degradations, such as rain and fog\nat night (triple degradations). However, in many cases, individuals may not\nwant to remove all degradations, for instance, a blurry lens revealing a\nbeautiful snowy landscape (double degradations). In such scenarios, people may\nonly desire to deblur. These situations and requirements shed light on a new\nchallenge in image restoration, where a model must perceive and remove specific\ndegradation types specified by human commands in images with multiple\ndegradations. We term this task Referring Flexible Image Restoration (RFIR). To\naddress this, we first construct a large-scale synthetic dataset called RFIR,\ncomprising 153,423 samples with the degraded image, text prompt for specific\ndegradation removal and restored image. RFIR consists of five basic degradation\ntypes: blur, rain, haze, low light and snow while six main sub-categories are\nincluded for varying degrees of degradation removal. To tackle the challenge,\nwe propose a novel transformer-based multi-task model named TransRFIR, which\nsimultaneously perceives degradation types in the degraded image and removes\nspecific degradation upon text prompt. TransRFIR is based on two devised\nattention modules, Multi-Head Agent Self-Attention (MHASA) and Multi-Head Agent\nCross Attention (MHACA), where MHASA and MHACA introduce the agent token and\nreach the linear complexity, achieving lower computation cost than vanilla\nself-attention and cross-attention and obtaining competitive performances. Our\nTransRFIR achieves state-of-the-art performances compared with other\ncounterparts and is proven as an effective architecture for image restoration.\nWe release our project at https://github.com/GuanRunwei/FIR-CP.\n","authors":["Runwei Guan","Rongsheng Hu","Zhuhao Zhou","Tianlang Xue","Ka Lok Man","Jeremy Smith","Eng Gee Lim","Weiping Ding","Yutao Yue"],"pdf_url":"https://arxiv.org/pdf/2404.10342v1.pdf","comment":"15 pages, 19 figures"},{"id":"http://arxiv.org/abs/2404.10292v1","updated":"2024-04-16T05:29:14Z","published":"2024-04-16T05:29:14Z","title":"From Data Deluge to Data Curation: A Filtering-WoRA Paradigm for\n  Efficient Text-based Person Search","summary":"  In text-based person search endeavors, data generation has emerged as a\nprevailing practice, addressing concerns over privacy preservation and the\narduous task of manual annotation. Although the number of synthesized data can\nbe infinite in theory, the scientific conundrum persists that how much\ngenerated data optimally fuels subsequent model training. We observe that only\na subset of the data in these constructed datasets plays a decisive role.\nTherefore, we introduce a new Filtering-WoRA paradigm, which contains a\nfiltering algorithm to identify this crucial data subset and WoRA (Weighted\nLow-Rank Adaptation) learning strategy for light fine-tuning. The filtering\nalgorithm is based on the cross-modality relevance to remove the lots of coarse\nmatching synthesis pairs. As the number of data decreases, we do not need to\nfine-tune the entire model. Therefore, we propose a WoRA learning strategy to\nefficiently update a minimal portion of model parameters. WoRA streamlines the\nlearning process, enabling heightened efficiency in extracting knowledge from\nfewer, yet potent, data instances. Extensive experimentation validates the\nefficacy of pretraining, where our model achieves advanced and efficient\nretrieval performance on challenging real-world benchmarks. Notably, on the\nCUHK-PEDES dataset, we have achieved a competitive mAP of 67.02% while reducing\nmodel training time by 19.82%.\n","authors":["Jintao Sun","Zhedong Zheng","Gangyi Ding"],"pdf_url":"https://arxiv.org/pdf/2404.10292v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10250v1","updated":"2024-04-16T02:59:52Z","published":"2024-04-16T02:59:52Z","title":"AniFrame: A Programming Language for 2D Drawing and Frame-Based\n  Animation","summary":"  Creative coding is an experimentation-heavy activity that requires\ntranslating high-level visual ideas into code. However, most languages and\nlibraries for creative coding may not be adequately intuitive for beginners. In\nthis paper, we present AniFrame, a domain-specific language for drawing and\nanimation. Designed for novice programmers, it (i) features animation-specific\ndata types, operations, and built-in functions to simplify the creation and\nanimation of composite objects, (ii) allows for fine-grained control over\nanimation sequences through explicit specification of the target object and the\nstart and end frames, (iii) reduces the learning curve through a Python-like\nsyntax, type inferencing, and a minimal set of control structures and keywords\nthat map closely to their semantic intent, and (iv) promotes computational\nexpressivity through support for common mathematical operations, built-in\ntrigonometric functions, and user-defined recursion. Our usability test\ndemonstrates AniFrame's potential to enhance readability and writability for\nmultiple creative coding use cases. AniFrame is open-source, and its\nimplementation and reference are available at\nhttps://github.com/memgonzales/aniframe-language.\n","authors":["Mark Edward M. Gonzales","Hans Oswald A. Ibrahim","Elyssia Barrie H. Ong","Ryan Austin Fernandez"],"pdf_url":"https://arxiv.org/pdf/2404.10250v1.pdf","comment":"Accepted for paper presentation at the 24th Philippine Computing\n  Science Congress (PCSC 2024), held in Laguna, Philippines"},{"id":"http://arxiv.org/abs/2404.10838v1","updated":"2024-04-16T18:22:49Z","published":"2024-04-16T18:22:49Z","title":"Dynamic Self-adaptive Multiscale Distillation from Pre-trained\n  Multimodal Large Model for Efficient Cross-modal Representation Learning","summary":"  In recent years, pre-trained multimodal large models have attracted\nwidespread attention due to their outstanding performance in various multimodal\napplications. Nonetheless, the extensive computational resources and vast\ndatasets required for their training present significant hurdles for deployment\nin environments with limited computational resources. To address this\nchallenge, we propose a novel dynamic self-adaptive multiscale distillation\nfrom pre-trained multimodal large model for efficient cross-modal\nrepresentation learning for the first time. Unlike existing distillation\nmethods, our strategy employs a multiscale perspective, enabling the extraction\nstructural knowledge across from the pre-trained multimodal large model.\nEnsuring that the student model inherits a comprehensive and nuanced\nunderstanding of the teacher knowledge. To optimize each distillation loss in a\nbalanced and efficient manner, we propose a dynamic self-adaptive distillation\nloss balancer, a novel component eliminating the need for manual loss weight\nadjustments and dynamically balances each loss item during the distillation\nprocess. Our methodology streamlines pre-trained multimodal large models using\nonly their output features and original image-level information, requiring\nminimal computational resources. This efficient approach is suited for various\napplications and allows the deployment of advanced multimodal technologies even\nin resource-limited settings. Extensive experiments has demonstrated that our\nmethod maintains high performance while significantly reducing model complexity\nand training costs. Moreover, our distilled student model utilizes only\nimage-level information to achieve state-of-the-art performance on cross-modal\nretrieval tasks, surpassing previous methods that relied on region-level\ninformation.\n","authors":["Zhengyang Liang","Meiyu Liang","Wei Huang","Yawen Li","Zhe Xue"],"pdf_url":"https://arxiv.org/pdf/2404.10838v1.pdf","comment":"10 pages"}]},"2024-04-17T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2403.02333v2","updated":"2024-04-17T17:58:39Z","published":"2024-03-04T18:58:30Z","title":"Key-Point-Driven Data Synthesis with its Enhancement on Mathematical\n  Reasoning","summary":"  Large language models (LLMs) have shown great potential in complex reasoning\ntasks, yet their performance is often hampered by the scarcity of high-quality\nand reasoning-focused training datasets. Addressing this challenge, we propose\nKey-Point-Driven Data Synthesis (KPDDS), a novel data synthesis framework that\nsynthesizes question-answer pairs by leveraging key points and exemplar\npractices from authentic data sources. KPDDS ensures the generation of novel\nquestions with rigorous quality control and substantial scalability. As a\nresult, we present KPMath, an extensive synthetic dataset tailored for\nmathematical reasoning, comprising over 800K question-answer pairs. Utilizing\nKPMath and augmenting it with additional reasoning-intensive corpora, we create\nthe comprehensive KPMath-Plus dataset. The fine-tuned DeepSeekMath model on\nKPMath-Plus achieves zero-shot PASS@1 accuracies of 83.9% on GSM8K and 48.8% on\nMATH, and also reaches promising performance on other math reasoning datasets,\noutperforming competitors in the 7B to 70B range.\n","authors":["Yiming Huang","Xiao Liu","Yeyun Gong","Zhibin Gou","Yelong Shen","Nan Duan","Weizhu Chen"],"pdf_url":"https://arxiv.org/pdf/2403.02333v2.pdf","comment":"In progress"},{"id":"http://arxiv.org/abs/2307.16877v2","updated":"2024-04-17T17:52:18Z","published":"2023-07-31T17:41:00Z","title":"Evaluating Correctness and Faithfulness of Instruction-Following Models\n  for Question Answering","summary":"  Retriever-augmented instruction-following models are attractive alternatives\nto fine-tuned approaches for information-seeking tasks such as question\nanswering (QA). By simply prepending retrieved documents in its input along\nwith an instruction, these models can be adapted to various information domains\nand tasks without additional fine-tuning. While the model responses tend to be\nnatural and fluent, the additional verbosity makes traditional QA evaluation\nmetrics such as exact match (EM) and F1 unreliable for accurately quantifying\nmodel performance.\n  In this work, we investigate the performance of instruction-following models\nacross three information-seeking QA tasks. We use both automatic and human\nevaluation to evaluate these models along two dimensions: 1) how well they\nsatisfy the user's information need (correctness), and 2) whether they produce\na response based on the provided knowledge (faithfulness). Guided by human\nevaluation and analysis, we highlight the shortcomings of traditional metrics\nfor both correctness and faithfulness. We then propose simple token-overlap\nbased and model-based metrics that reflect the true performance of these\nmodels. Our analysis reveals that instruction-following models are competitive,\nand sometimes even outperform fine-tuned models for correctness. However, these\nmodels struggle to stick to the provided knowledge and often hallucinate in\ntheir responses. We hope our work encourages a more holistic evaluation of\ninstruction-following models for QA. Our code and data is available at\nhttps://github.com/McGill-NLP/instruct-qa\n","authors":["Vaibhav Adlakha","Parishad BehnamGhader","Xing Han Lu","Nicholas Meade","Siva Reddy"],"pdf_url":"https://arxiv.org/pdf/2307.16877v2.pdf","comment":"accepted at TACL"},{"id":"http://arxiv.org/abs/2404.11588v1","updated":"2024-04-17T17:37:30Z","published":"2024-04-17T17:37:30Z","title":"Related Work and Citation Text Generation: A Survey","summary":"  To convince readers of the novelty of their research paper, authors must\nperform a literature review and compose a coherent story that connects and\nrelates prior works to the current work. This challenging nature of literature\nreview writing makes automatic related work generation (RWG) academically and\ncomputationally interesting, and also makes it an excellent test bed for\nexamining the capability of SOTA natural language processing (NLP) models.\nSince the initial proposal of the RWG task, its popularity has waxed and waned,\nfollowing the capabilities of mainstream NLP approaches. In this work, we\nsurvey the zoo of RWG historical works, summarizing the key approaches and task\ndefinitions and discussing the ongoing challenges of RWG.\n","authors":["Xiangci Li","Jessica Ouyang"],"pdf_url":"https://arxiv.org/pdf/2404.11588v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11584v1","updated":"2024-04-17T17:32:41Z","published":"2024-04-17T17:32:41Z","title":"The Landscape of Emerging AI Agent Architectures for Reasoning,\n  Planning, and Tool Calling: A Survey","summary":"  This survey paper examines the recent advancements in AI agent\nimplementations, with a focus on their ability to achieve complex goals that\nrequire enhanced reasoning, planning, and tool execution capabilities. The\nprimary objectives of this work are to a) communicate the current capabilities\nand limitations of existing AI agent implementations, b) share insights gained\nfrom our observations of these systems in action, and c) suggest important\nconsiderations for future developments in AI agent design. We achieve this by\nproviding overviews of single-agent and multi-agent architectures, identifying\nkey patterns and divergences in design choices, and evaluating their overall\nimpact on accomplishing a provided goal. Our contribution outlines key themes\nwhen selecting an agentic architecture, the impact of leadership on agent\nsystems, agent communication styles, and key phases for planning, execution,\nand reflection that enable robust AI agent systems.\n","authors":["Tula Masterman","Sandi Besen","Mason Sawtell","Alex Chao"],"pdf_url":"https://arxiv.org/pdf/2404.11584v1.pdf","comment":"13 pages,6 figures,38 references"},{"id":"http://arxiv.org/abs/2311.09835v2","updated":"2024-04-17T17:13:03Z","published":"2023-11-16T12:03:21Z","title":"ML-Bench: Evaluating Large Language Models for Code Generation in\n  Repository-Level Machine Learning Tasks","summary":"  While Large Language Models (LLMs) have demonstrated proficiency in code\ngeneration benchmarks, translating these results into practical development\nscenarios - where leveraging existing repository-level libraries is the norm -\nremains challenging. To bridge the gap between lab-scale benchmarks and\nreal-world coding practices, we introduce ML-Bench: a novel benchmark designed\nto assess LLMs' ability to integrate and utilize repository-level open-source\nlibraries to complete machine learning tasks. ML-Bench comprises a diverse set\nof 9,641 samples across 169 distinct tasks derived from 18 GitHub repositories.\nOur findings reveal that while GPT-4 outshines other LLMs, it successfully\naddresses only 33.82% of the tasks, highlighting the complexity of the\nchallenge. Complementarily, we introduce a baseline agent, ML-Agent, capable of\nskillful codebase navigation and precise generation of functional code\nsegments. This groundwork aims at catalyzing the development of more\nsophisticated LLM agents that can handle the intricacies of real-world\nprogramming. Our code, data, and models are available at\nhttps://github.com/gersteinlab/ML-bench.\n","authors":["Yuliang Liu","Xiangru Tang","Zefan Cai","Junjie Lu","Yichi Zhang","Yanjun Shao","Zexuan Deng","Helan Hu","Kaikai An","Ruijun Huang","Shuzheng Si","Sheng Chen","Haozhe Zhao","Liang Chen","Yan Wang","Tianyu Liu","Zhiwei Jiang","Baobao Chang","Yujia Qin","Wangchunshu Zhou","Yilun Zhao","Arman Cohan","Mark Gerstein"],"pdf_url":"https://arxiv.org/pdf/2311.09835v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09906v2","updated":"2024-04-17T17:12:05Z","published":"2024-02-15T12:12:19Z","title":"Generative Representational Instruction Tuning","summary":"  All text-based language problems can be reduced to either generation or\nembedding. Current models only perform well at one or the other. We introduce\ngenerative representational instruction tuning (GRIT) whereby a large language\nmodel is trained to handle both generative and embedding tasks by\ndistinguishing between them through instructions. Compared to other open\nmodels, our resulting GritLM 7B sets a new state of the art on the Massive Text\nEmbedding Benchmark (MTEB) and outperforms all models up to its size on a range\nof generative tasks. By scaling up further, GritLM 8x7B outperforms all open\ngenerative language models that we tried while still being among the best\nembedding models. Notably, we find that GRIT matches training on only\ngenerative or embedding data, thus we can unify both at no performance loss.\nAmong other benefits, the unification via GRIT speeds up Retrieval-Augmented\nGeneration (RAG) by > 60% for long documents, by no longer requiring separate\nretrieval and generation models. Models, code, etc. are freely available at\nhttps://github.com/ContextualAI/gritlm.\n","authors":["Niklas Muennighoff","Hongjin Su","Liang Wang","Nan Yang","Furu Wei","Tao Yu","Amanpreet Singh","Douwe Kiela"],"pdf_url":"https://arxiv.org/pdf/2402.09906v2.pdf","comment":"66 pages (16 main), 25 figures, 34 tables"},{"id":"http://arxiv.org/abs/2401.12798v3","updated":"2024-04-17T17:00:34Z","published":"2024-01-23T14:31:12Z","title":"Gradient Flow of Energy: A General and Efficient Approach for Entity\n  Alignment Decoding","summary":"  Entity alignment (EA), a pivotal process in integrating multi-source\nKnowledge Graphs (KGs), seeks to identify equivalent entity pairs across these\ngraphs. Most existing approaches regard EA as a graph representation learning\ntask, concentrating on enhancing graph encoders. However, the decoding process\nin EA - essential for effective operation and alignment accuracy - has received\nlimited attention and remains tailored to specific datasets and model\narchitectures, necessitating both entity and additional explicit relation\nembeddings. This specificity limits its applicability, particularly in\nGNN-based models. To address this gap, we introduce a novel, generalized, and\nefficient decoding approach for EA, relying solely on entity embeddings. Our\nmethod optimizes the decoding process by minimizing Dirichlet energy, leading\nto the gradient flow within the graph, to maximize graph homophily. The\ndiscretization of the gradient flow produces a fast and scalable approach,\ntermed Triple Feature Propagation (TFP). TFP innovatively generalizes adjacency\nmatrices to multi-views matrices:entity-to-entity, entity-to-relation,\nrelation-to-entity, and relation-to-triple. The gradient flow through\ngeneralized matrices enables TFP to harness the multi-view structural\ninformation of KGs. Rigorous experimentation on diverse public datasets\ndemonstrates that our approach significantly enhances various EA methods.\nNotably, the approach achieves these advancements with less than 6 seconds of\nadditional computational time, establishing a new benchmark in efficiency and\nadaptability for future EA methods.\n","authors":["Yuanyi Wang","Haifeng Sun","Jingyu Wang","Qi Qi","Shaoling Sun","Jianxin Liao"],"pdf_url":"https://arxiv.org/pdf/2401.12798v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11456v2","updated":"2024-04-17T16:59:35Z","published":"2024-03-18T04:12:35Z","title":"HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive\n  Speech Detection via Large Language Models","summary":"  The ubiquitousness of social media has led to the need for reliable and\nefficient detection of offensive content to limit harmful effects. This has led\nto a proliferation of datasets and models related to detecting offensive\ncontent. While sophisticated models have attained strong performance on\nindividual datasets, these models often do not generalize due to differences\nbetween how \"offensive content\" is conceptualized, and the resulting\ndifferences in how these datasets are labeled. In this paper, we introduce\nHateCOT, a dataset of 52,000 samples drawn from diverse existing sources with\nexplanations generated by GPT-3.5-Turbo and human-curated. We show that\npre-training models for the detection of offensive content on HateCOT\nsignificantly boots open-sourced Language Models on three benchmark datasets in\nboth zero and few-shot settings, despite differences in domain and task.} We\nfurther find that HateCOT enables effective K-shot fine-tuning in the\nlow-resource settings.\n","authors":["Huy Nghiem","Hal Daum√© III"],"pdf_url":"https://arxiv.org/pdf/2403.11456v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2402.16472v2","updated":"2024-04-17T16:59:30Z","published":"2024-02-26T10:33:36Z","title":"mEdIT: Multilingual Text Editing via Instruction Tuning","summary":"  We introduce mEdIT, a multi-lingual extension to CoEdIT -- the recent\nstate-of-the-art text editing models for writing assistance. mEdIT models are\ntrained by fine-tuning multi-lingual large, pre-trained language models (LLMs)\nvia instruction tuning. They are designed to take instructions from the user\nspecifying the attributes of the desired text in the form of natural language\ninstructions, such as Grammatik korrigieren (German) or Parafrasee la oraci\\'on\n(Spanish). We build mEdIT by curating data from multiple publicly available\nhuman-annotated text editing datasets for three text editing tasks (Grammatical\nError Correction (GEC), Text Simplification, and Paraphrasing) across diverse\nlanguages belonging to six different language families. We detail the design\nand training of mEdIT models and demonstrate their strong performance on many\nmulti-lingual text editing benchmarks against other multilingual LLMs. We also\nfind that mEdIT generalizes effectively to new languages over multilingual\nbaselines. We publicly release our data, code, and trained models at\nhttps://github.com/vipulraheja/medit.\n","authors":["Vipul Raheja","Dimitris Alikaniotis","Vivek Kulkarni","Bashar Alhafni","Dhruv Kumar"],"pdf_url":"https://arxiv.org/pdf/2402.16472v2.pdf","comment":"Accepted to NAACL 2024 (Main). 23 pages, 8 tables, 11 figures"},{"id":"http://arxiv.org/abs/2404.11553v1","updated":"2024-04-17T16:53:16Z","published":"2024-04-17T16:53:16Z","title":"Quantifying Multilingual Performance of Large Language Models Across\n  Languages","summary":"  The training process of Large Language Models (LLMs) requires extensive text\ncorpus. However, these data are often unevenly distributed in different\nlanguages. As a result, LLMs perform well on common languages, such as English,\nGerman, and French, but perform poorly on low-resource languages. However,\ncurrently there is no work to quantitatively measure the performance of LLMs in\nlow-resource languages. To fill this gap, we proposed the Language Ranker that\naims to benchmark and rank different languages according to the performance of\nLLMs on those languages. We employ the LLM's performance on the English corpus\nas a baseline to compare the performances of different languages and English.\nWe have the following three findings: 1. The performance rankings of different\nLLMs in all languages are roughly the same. 2. LLMs with different sizes have\nthe same partial order of performance. 3. There is a strong correlation between\nLlaMa2's performance in different languages and the proportion of the\npre-training corpus. These findings illustrate that the Language Ranker can be\nused as an indicator to measure the language performance of LLMs.\n","authors":["Zihao Li","Yucheng Shi","Zirui Liu","Fan Yang","Ninghao Liu","Mengnan Du"],"pdf_url":"https://arxiv.org/pdf/2404.11553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11539v1","updated":"2024-04-17T16:33:22Z","published":"2024-04-17T16:33:22Z","title":"Evaluating Span Extraction in Generative Paradigm: A Reflection on\n  Aspect-Based Sentiment Analysis","summary":"  In the era of rapid evolution of generative language models within the realm\nof natural language processing, there is an imperative call to revisit and\nreformulate evaluation methodologies, especially in the domain of aspect-based\nsentiment analysis (ABSA). This paper addresses the emerging challenges\nintroduced by the generative paradigm, which has moderately blurred traditional\nboundaries between understanding and generation tasks. Building upon prevailing\npractices in the field, we analyze the advantages and shortcomings associated\nwith the prevalent ABSA evaluation paradigms. Through an in-depth examination,\nsupplemented by illustrative examples, we highlight the intricacies involved in\naligning generative outputs with other evaluative metrics, specifically those\nderived from other tasks, including question answering. While we steer clear of\nadvocating for a singular and definitive metric, our contribution lies in\npaving the path for a comprehensive guideline tailored for ABSA evaluations in\nthis generative paradigm. In this position paper, we aim to provide\npractitioners with profound reflections, offering insights and directions that\ncan aid in navigating this evolving landscape, ensuring evaluations that are\nboth accurate and reflective of generative capabilities.\n","authors":["Soyoung Yang","Won Ik Cho"],"pdf_url":"https://arxiv.org/pdf/2404.11539v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2404.11538v1","updated":"2024-04-17T16:32:13Z","published":"2024-04-17T16:32:13Z","title":"GenFighter: A Generative and Evolutive Textual Attack Removal","summary":"  Adversarial attacks pose significant challenges to deep neural networks\n(DNNs) such as Transformer models in natural language processing (NLP). This\npaper introduces a novel defense strategy, called GenFighter, which enhances\nadversarial robustness by learning and reasoning on the training classification\ndistribution. GenFighter identifies potentially malicious instances deviating\nfrom the distribution, transforms them into semantically equivalent instances\naligned with the training data, and employs ensemble techniques for a unified\nand robust response. By conducting extensive experiments, we show that\nGenFighter outperforms state-of-the-art defenses in accuracy under attack and\nattack success rate metrics. Additionally, it requires a high number of queries\nper attack, making the attack more challenging in real scenarios. The ablation\nstudy shows that our approach integrates transfer learning, a\ngenerative/evolutive procedure, and an ensemble method, providing an effective\ndefense against NLP adversarial attacks.\n","authors":["Md Athikul Islam","Edoardo Serra","Sushil Jajodia"],"pdf_url":"https://arxiv.org/pdf/2404.11538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10636v2","updated":"2024-04-17T16:27:37Z","published":"2024-03-27T18:12:02Z","title":"What are human values, and how do we align AI to them?","summary":"  There is an emerging consensus that we need to align AI systems with human\nvalues (Gabriel, 2020; Ji et al., 2024), but it remains unclear how to apply\nthis to language models in practice. We split the problem of \"aligning to human\nvalues\" into three parts: first, eliciting values from people; second,\nreconciling those values into an alignment target for training ML models; and\nthird, actually training the model. In this paper, we focus on the first two\nparts, and ask the question: what are \"good\" ways to synthesize diverse human\ninputs about values into a target for aligning language models? To answer this\nquestion, we first define a set of 6 criteria that we believe must be satisfied\nfor an alignment target to shape model behavior in accordance with human\nvalues. We then propose a process for eliciting and reconciling values called\nMoral Graph Elicitation (MGE), which uses a large language model to interview\nparticipants about their values in particular contexts; our approach is\ninspired by the philosophy of values advanced by Taylor (1977), Chang (2004),\nand others. We trial MGE with a representative sample of 500 Americans, on 3\nintentionally divisive prompts (e.g. advice about abortion). Our results\ndemonstrate that MGE is promising for improving model alignment across all 6\ncriteria. For example, almost all participants (89.1%) felt well represented by\nthe process, and (89%) thought the final moral graph was fair, even if their\nvalue wasn't voted as the wisest. Our process often results in \"expert\" values\n(e.g. values from women who have solicited abortion advice) rising to the top\nof the moral graph, without defining who is considered an expert in advance.\n","authors":["Oliver Klingefjord","Ryan Lowe","Joe Edelman"],"pdf_url":"https://arxiv.org/pdf/2404.10636v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11532v1","updated":"2024-04-17T16:25:19Z","published":"2024-04-17T16:25:19Z","title":"Select and Reorder: A Novel Approach for Neural Sign Language Production","summary":"  Sign languages, often categorised as low-resource languages, face significant\nchallenges in achieving accurate translation due to the scarcity of parallel\nannotated datasets. This paper introduces Select and Reorder (S&R), a novel\napproach that addresses data scarcity by breaking down the translation process\ninto two distinct steps: Gloss Selection (GS) and Gloss Reordering (GR). Our\nmethod leverages large spoken language models and the substantial lexical\noverlap between source spoken languages and target sign languages to establish\nan initial alignment. Both steps make use of Non-AutoRegressive (NAR) decoding\nfor reduced computation and faster inference speeds. Through this\ndisentanglement of tasks, we achieve state-of-the-art BLEU and Rouge scores on\nthe Meine DGS Annotated (mDGS) dataset, demonstrating a substantial BLUE-1\nimprovement of 37.88% in Text to Gloss (T2G) Translation. This innovative\napproach paves the way for more effective translation models for sign\nlanguages, even in resource-constrained settings.\n","authors":["Harry Walsh","Ben Saunders","Richard Bowden"],"pdf_url":"https://arxiv.org/pdf/2404.11532v1.pdf","comment":"8 Pages, 5 Figures, 7 Tables, LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2404.11531v1","updated":"2024-04-17T16:24:07Z","published":"2024-04-17T16:24:07Z","title":"Pack of LLMs: Model Fusion at Test-Time via Perplexity Optimization","summary":"  Fusing knowledge from multiple Large Language Models (LLMs) can combine their\ndiverse strengths to achieve improved performance on a given task. However,\ncurrent fusion approaches either rely on learning-based fusers that do not\ngeneralize to new LLMs, or do not take into account how well each LLM\nunderstands the input. In this work, we study LLM fusion at test-time, which\nenables leveraging knowledge from arbitrary user-specified LLMs during\ninference. We introduce Pack of LLMs (PackLLM), an effective method for\ntest-time fusion that leverages each LLM's expertise, given an input prompt.\nPackLLM performs model fusion by solving an optimization problem for\ndetermining each LLM's importance, so that perplexity over the input prompt is\nminimized. First, our simple PackLLM-sim variant validates that perplexity is a\ngood indicator for measuring each LLM's expertise. Second, our PackLLM-opt\nvariant approximately solves the perplexity minimization problem via a greedy\nalgorithm. The derived importance weights are used to combine the LLMs during\ninference. We conduct experiments with over 100 total LLMs on a diverse set of\ntasks. Experimental results show that (i) perplexity is a reliable measure for\nLLM fusion, (ii) PackLLM outperforms test-time fusion baselines by 1.89%\naccuracy points, and (iii) PackLLM can leverage new LLMs to improve performance\nover learning-based fusion approaches by 3.92-11.94% accuracy points.\n","authors":["Costas Mavromatis","Petros Karypis","George Karypis"],"pdf_url":"https://arxiv.org/pdf/2404.11531v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11502v1","updated":"2024-04-17T15:57:50Z","published":"2024-04-17T15:57:50Z","title":"Towards Coarse-to-Fine Evaluation of Inference Efficiency for Large\n  Language Models","summary":"  In real world, large language models (LLMs) can serve as the assistant to\nhelp users accomplish their jobs, and also support the development of advanced\napplications. For the wide application of LLMs, the inference efficiency is an\nessential concern, which has been widely studied in existing work, and numerous\noptimization algorithms and code libraries have been proposed to improve it.\nNonetheless, users still find it challenging to compare the effectiveness of\nall the above methods and understand the underlying mechanisms. In this work,\nwe perform a detailed coarse-to-fine analysis of the inference performance of\nvarious code libraries. To evaluate the overall effectiveness, we examine four\nusage scenarios within two practical applications. We further provide both\ntheoretical and empirical fine-grained analyses of each module in the\nTransformer architecture. Our experiments yield comprehensive results that are\ninvaluable for researchers to evaluate code libraries and improve inference\nstrategies.\n","authors":["Yushuo Chen","Tianyi Tang","Erge Xiang","Linjiang Li","Wayne Xin Zhao","Jing Wang","Yunpeng Chai","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2404.11502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11500v1","updated":"2024-04-17T15:53:49Z","published":"2024-04-17T15:53:49Z","title":"Paraphrase and Solve: Exploring and Exploiting the Impact of Surface\n  Form on Mathematical Reasoning in Large Language Models","summary":"  This paper studies the relationship between the surface form of a\nmathematical problem and its solvability by large language models. We find that\nsubtle alterations in the surface form can significantly impact the answer\ndistribution and the solve rate, exposing the language model's lack of\nrobustness and sensitivity to the surface form in reasoning through complex\nproblems. To improve mathematical reasoning performance, we propose\nSelf-Consistency-over-Paraphrases (SCoP), which diversifies reasoning paths\nfrom specific surface forms of the problem. We evaluate our approach on four\nmathematics reasoning benchmarks over three large language models and show that\nSCoP improves mathematical reasoning performance over vanilla self-consistency,\nparticularly for problems initially deemed unsolvable. Finally, we provide\nadditional experiments and discussion regarding problem difficulty and surface\nforms, including cross-model difficulty agreement and paraphrasing\ntransferability, and Variance of Variations (VOV) for language model\nevaluation.\n","authors":["Yue Zhou","Yada Zhu","Diego Antognini","Yoon Kim","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.11500v1.pdf","comment":"Accepted to the main conference of NAACL (2024)"},{"id":"http://arxiv.org/abs/2404.11499v1","updated":"2024-04-17T15:52:38Z","published":"2024-04-17T15:52:38Z","title":"A Data-Driven Representation for Sign Language Production","summary":"  Phonetic representations are used when recording spoken languages, but no\nequivalent exists for recording signed languages. As a result, linguists have\nproposed several annotation systems that operate on the gloss or sub-unit\nlevel; however, these resources are notably irregular and scarce.\n  Sign Language Production (SLP) aims to automatically translate spoken\nlanguage sentences into continuous sequences of sign language. However, current\nstate-of-the-art approaches rely on scarce linguistic resources to work. This\nhas limited progress in the field. This paper introduces an innovative solution\nby transforming the continuous pose generation problem into a discrete sequence\ngeneration problem. Thus, overcoming the need for costly annotation. Although,\nif available, we leverage the additional information to enhance our approach.\n  By applying Vector Quantisation (VQ) to sign language data, we first learn a\ncodebook of short motions that can be combined to create a natural sequence of\nsign. Where each token in the codebook can be thought of as the lexicon of our\nrepresentation. Then using a transformer we perform a translation from spoken\nlanguage text to a sequence of codebook tokens. Each token can be directly\nmapped to a sequence of poses allowing the translation to be performed by a\nsingle network. Furthermore, we present a sign stitching method to effectively\njoin tokens together. We evaluate on the RWTH-PHOENIX-Weather-2014T\n(PHOENIX14T) and the more challenging Meine DGS Annotated (mDGS) datasets. An\nextensive evaluation shows our approach outperforms previous methods,\nincreasing the BLEU-1 back translation score by up to 72%.\n","authors":["Harry Walsh","Abolfazl Ravanshad","Mariam Rahmani","Richard Bowden"],"pdf_url":"https://arxiv.org/pdf/2404.11499v1.pdf","comment":"8 Pages, 3 Figures, 7 Tables, 18th IEEE International Conference on\n  Automatic Face and Gesture Recognition 2024"},{"id":"http://arxiv.org/abs/2303.18062v2","updated":"2024-04-17T15:23:12Z","published":"2023-03-30T12:36:46Z","title":"Solving morphological analogies: from retrieval to generation","summary":"  Analogical inference is a remarkable capability of human reasoning, and has\nbeen used to solve hard reasoning tasks. Analogy based reasoning (AR) has\ngained increasing interest from the artificial intelligence community and has\nshown its potential in multiple machine learning tasks such as classification,\ndecision making and recommendation with competitive results. We propose a deep\nlearning (DL) framework to address and tackle two key tasks in AR: analogy\ndetection and solving. The framework is thoroughly tested on the Siganalogies\ndataset of morphological analogical proportions (APs) between words, and shown\nto outperform symbolic approaches in many languages. Previous work have\nexplored the behavior of the Analogy Neural Network for classification (ANNc)\non analogy detection and of the Analogy Neural Network for retrieval (ANNr) on\nanalogy solving by retrieval, as well as the potential of an autoencoder (AE)\nfor analogy solving by generating the solution word. In this article we\nsummarize these findings and we extend them by combining ANNr and the AE\nembedding model, and checking the performance of ANNc as an retrieval method.\nThe combination of ANNr and AE outperforms the other approaches in almost all\ncases, and ANNc as a retrieval method achieves competitive or better\nperformance than 3CosMul. We conclude with general guidelines on using our\nframework to tackle APs with DL.\n","authors":["Esteban Marquer","Miguel Couceiro"],"pdf_url":"https://arxiv.org/pdf/2303.18062v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11470v1","updated":"2024-04-17T15:23:12Z","published":"2024-04-17T15:23:12Z","title":"A Federated Learning Approach to Privacy Preserving Offensive Language\n  Identification","summary":"  The spread of various forms of offensive speech online is an important\nconcern in social media. While platforms have been investing heavily in ways of\ncoping with this problem, the question of privacy remains largely unaddressed.\nModels trained to detect offensive language on social media are trained and/or\nfine-tuned using large amounts of data often stored in centralized servers.\nSince most social media data originates from end users, we propose a privacy\npreserving decentralized architecture for identifying offensive language online\nby introducing Federated Learning (FL) in the context of offensive language\nidentification. FL is a decentralized architecture that allows multiple models\nto be trained locally without the need for data sharing hence preserving users'\nprivacy. We propose a model fusion approach to perform FL. We trained multiple\ndeep learning models on four publicly available English benchmark datasets\n(AHSD, HASOC, HateXplain, OLID) and evaluated their performance in detail. We\nalso present initial cross-lingual experiments in English and Spanish. We show\nthat the proposed model fusion approach outperforms baselines in all the\ndatasets while preserving privacy.\n","authors":["Marcos Zampieri","Damith Premasiri","Tharindu Ranasinghe"],"pdf_url":"https://arxiv.org/pdf/2404.11470v1.pdf","comment":"Accepted to TRAC 2024 (Fourth Workshop on Threat, Aggression and\n  Cyberbullying) at LREC-COLING 2024 (The 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation)"},{"id":"http://arxiv.org/abs/2310.09089v2","updated":"2024-04-17T15:18:54Z","published":"2023-10-13T13:17:03Z","title":"Qilin-Med: Multi-stage Knowledge Injection Advanced Medical Large\n  Language Model","summary":"  Integrating large language models (LLMs) into healthcare holds great\npotential but faces challenges. Pre-training LLMs from scratch for domains like\nmedicine is resource-heavy and often unfeasible. On the other hand, sole\nreliance on Supervised Fine-tuning (SFT) can result in overconfident\npredictions and may not tap into domain-specific insights. In response, we\npresent a multi-stage training method combining Domain-specific Continued\nPre-training (DCPT), SFT, and Direct Preference Optimization (DPO). In\naddition, we publish a 3Gb Chinese Medicine (ChiMed) dataset, encompassing\nmedical question answering, plain texts, knowledge graphs, and dialogues,\nsegmented into three training stages. The medical LLM trained with our\npipeline, Qilin-Med, shows substantial performance improvement. In the CPT and\nSFT phases, Qilin-Med achieved 38.4% and 40.0% accuracy on the CMExam test set,\nrespectively. It outperformed the basemodel Baichuan-7B (accuracy: 33.5%), by\n7.5%. In the DPO phase, it scored 16.66 in BLEU-1 and 27.44 in ROUGE-1 on the\nHuatuo-26M test set, bringing further improvement to the SFT phase (12.69 in\nBLEU-1 and 24.21 in ROUGE-1). Additionally, we have further enhanced the\nmodel's performance through the Retrieval Augmented Generation (RAG) approach.\nExperiments demonstrate that Qilin-Med-RAG achieves an accuracy rate of 42.8%\non CMExam. These results highlight the contribution of our novel training\napproach in building LLMs for medical applications.\n","authors":["Qichen Ye","Junling Liu","Dading Chong","Peilin Zhou","Yining Hua","Fenglin Liu","Meng Cao","Ziming Wang","Xuxin Cheng","Zhu Lei","Zhenhua Guo"],"pdf_url":"https://arxiv.org/pdf/2310.09089v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11459v1","updated":"2024-04-17T15:07:06Z","published":"2024-04-17T15:07:06Z","title":"Octopus v3: Technical Report for On-device Sub-billion Multimodal AI\n  Agent","summary":"  A multimodal AI agent is characterized by its ability to process and learn\nfrom various types of data, including natural language, visual, and audio\ninputs, to inform its actions. Despite advancements in large language models\nthat incorporate visual data, such as GPT-4V, effectively translating\nimage-based data into actionable outcomes for AI agents continues to be\nchallenging. In this paper, we introduce a multimodal model that incorporates\nthe concept of functional token specifically designed for AI agent\napplications. To ensure compatibility with edge devices, our model is optimized\nto a compact size of less than 1B parameters. Like GPT-4, our model can process\nboth English and Chinese. We demonstrate that this model is capable of\noperating efficiently on a wide range of edge devices, including as constrained\nas a Raspberry Pi.\n","authors":["Wei Chen","Zhiyuan Li"],"pdf_url":"https://arxiv.org/pdf/2404.11459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11457v1","updated":"2024-04-17T15:05:03Z","published":"2024-04-17T15:05:03Z","title":"Unifying Bias and Unfairness in Information Retrieval: A Survey of\n  Challenges and Opportunities with Large Language Models","summary":"  With the rapid advancement of large language models (LLMs), information\nretrieval (IR) systems, such as search engines and recommender systems, have\nundergone a significant paradigm shift. This evolution, while heralding new\nopportunities, introduces emerging challenges, particularly in terms of biases\nand unfairness, which may threaten the information ecosystem. In this paper, we\npresent a comprehensive survey of existing works on emerging and pressing bias\nand unfairness issues in IR systems when the integration of LLMs. We first\nunify bias and unfairness issues as distribution mismatch problems, providing a\ngroundwork for categorizing various mitigation strategies through distribution\nalignment. Subsequently, we systematically delve into the specific bias and\nunfairness issues arising from three critical stages of LLMs integration into\nIR systems: data collection, model development, and result evaluation. In doing\nso, we meticulously review and analyze recent literature, focusing on the\ndefinitions, characteristics, and corresponding mitigation strategies\nassociated with these issues. Finally, we identify and highlight some open\nproblems and challenges for future work, aiming to inspire researchers and\nstakeholders in the IR field and beyond to better understand and mitigate bias\nand unfairness issues of IR in this LLM era. We also consistently maintain a\nGitHub repository for the relevant papers and resources in this rising\ndirection at https://github.com/KID-22/LLM-IR-Bias-Fairness-Survey.\n","authors":["Sunhao Dai","Chen Xu","Shicheng Xu","Liang Pang","Zhenhua Dong","Jun Xu"],"pdf_url":"https://arxiv.org/pdf/2404.11457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12219v2","updated":"2024-04-17T15:03:19Z","published":"2024-02-19T15:21:58Z","title":"Reformatted Alignment","summary":"  The quality of finetuning data is crucial for aligning large language models\n(LLMs) with human values. Current methods to improve data quality are either\nlabor-intensive or prone to factual errors caused by LLM hallucinations. This\npaper explores elevating the quality of existing instruction data to better\nalign with human values, introducing a simple and effective approach named\nReAlign, which reformats the responses of instruction data into a format that\nbetter aligns with pre-established criteria and the collated evidence. This\napproach minimizes human annotation, hallucination, and the difficulty in\nscaling, remaining orthogonal to existing alignment techniques. Experimentally,\nReAlign significantly boosts the general alignment ability, math reasoning,\nfactuality, and readability of the LLMs.\n  Encouragingly, without introducing any additional data or advanced training\ntechniques, and merely by reformatting the response, LLaMA-2-13B's mathematical\nreasoning ability on GSM8K can be improved from 46.77% to 56.63% in accuracy.\nAdditionally, a mere 5% of ReAlign data yields a 67% boost in general alignment\nability measured by the Alpaca dataset. This work highlights the need for\nfurther research into the science and mechanistic interpretability of LLMs. We\nhave made the associated code and data publicly accessible to support future\nstudies at https://github.com/GAIR-NLP/ReAlign.\n","authors":["Run-Ze Fan","Xuefeng Li","Haoyang Zou","Junlong Li","Shwai He","Ethan Chern","Jiewen Hu","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2402.12219v2.pdf","comment":"Homepage: https://gair-nlp.github.io/ReAlign/"},{"id":"http://arxiv.org/abs/2404.11449v1","updated":"2024-04-17T14:55:27Z","published":"2024-04-17T14:55:27Z","title":"AI-Enhanced Cognitive Behavioral Therapy: Deep Learning and Large\n  Language Models for Extracting Cognitive Pathways from Social Media Texts","summary":"  Cognitive Behavioral Therapy (CBT) is an effective technique for addressing\nthe irrational thoughts stemming from mental illnesses, but it necessitates\nprecise identification of cognitive pathways to be successfully implemented in\npatient care. In current society, individuals frequently express negative\nemotions on social media on specific topics, often exhibiting cognitive\ndistortions, including suicidal behaviors in extreme cases. Yet, there is a\nnotable absence of methodologies for analyzing cognitive pathways that could\naid psychotherapists in conducting effective interventions online. In this\nstudy, we gathered data from social media and established the task of\nextracting cognitive pathways, annotating the data based on a cognitive\ntheoretical framework. We initially categorized the task of extracting\ncognitive pathways as a hierarchical text classification with four main\ncategories and nineteen subcategories. Following this, we structured a text\nsummarization task to help psychotherapists quickly grasp the essential\ninformation. Our experiments evaluate the performance of deep learning and\nlarge language models (LLMs) on these tasks. The results demonstrate that our\ndeep learning method achieved a micro-F1 score of 62.34% in the hierarchical\ntext classification task. Meanwhile, in the text summarization task, GPT-4\nattained a Rouge-1 score of 54.92 and a Rouge-2 score of 30.86, surpassing the\nexperimental deep learning model's performance. However, it may suffer from an\nissue of hallucination. We have made all models and codes publicly available to\nsupport further research in this field.\n","authors":["Meng Jiang","Yi Jing Yu","Qing Zhao","Jianqiang Li","Changwei Song","Hongzhi Qi","Wei Zhai","Dan Luo","Xiaoqin Wang","Guanghui Fu","Bing Xiang Yang"],"pdf_url":"https://arxiv.org/pdf/2404.11449v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11447v1","updated":"2024-04-17T14:55:03Z","published":"2024-04-17T14:55:03Z","title":"Research on emotionally intelligent dialogue generation based on\n  automatic dialogue system","summary":"  Automated dialogue systems are important applications of artificial\nintelligence, and traditional systems struggle to understand user emotions and\nprovide empathetic feedback. This study integrates emotional intelligence\ntechnology into automated dialogue systems and creates a dialogue generation\nmodel with emotional intelligence through deep learning and natural language\nprocessing techniques. The model can detect and understand a wide range of\nemotions and specific pain signals in real time, enabling the system to provide\nempathetic interaction. By integrating the results of the study \"Can artificial\nintelligence detect pain and express pain empathy?\", the model's ability to\nunderstand the subtle elements of pain empathy has been enhanced, setting\nhigher standards for emotional intelligence dialogue systems. The project aims\nto provide theoretical understanding and practical suggestions to integrate\nadvanced emotional intelligence capabilities into dialogue systems, thereby\nimproving user experience and interaction quality.\n","authors":["Jin Wang","JinFei Wang","Shuying Dai","Jiqiang Yu","Keqin Li"],"pdf_url":"https://arxiv.org/pdf/2404.11447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11446v1","updated":"2024-04-17T14:54:58Z","published":"2024-04-17T14:54:58Z","title":"Open-Ended Wargames with Large Language Models","summary":"  Wargames are a powerful tool for understanding and rehearsing real-world\ndecision making. Automated play of wargames using artificial intelligence (AI)\nenables possibilities beyond those of human-conducted games, such as playing\nthe game many times over to see a range of possible outcomes. There are two\ncategories of wargames: quantitative games, with discrete types of moves, and\nqualitative games, which revolve around open-ended responses. Historically,\nautomation efforts have focused on quantitative games, but large language\nmodels (LLMs) make it possible to automate qualitative wargames. We introduce\n\"Snow Globe,\" an LLM-powered multi-agent system for playing qualitative\nwargames. With Snow Globe, every stage of a text-based qualitative wargame from\nscenario preparation to post-game analysis can be optionally carried out by AI,\nhumans, or a combination thereof. We describe its software architecture\nconceptually and release an open-source implementation alongside this\npublication. As case studies, we simulate a tabletop exercise about an AI\nincident response and a political wargame about a geopolitical crisis. We\ndiscuss potential applications of the approach and how it fits into the broader\nwargaming ecosystem.\n","authors":["Daniel P. Hogan","Andrea Brennen"],"pdf_url":"https://arxiv.org/pdf/2404.11446v1.pdf","comment":"15 pages, 2 figures"},{"id":"http://arxiv.org/abs/2402.10689v2","updated":"2024-04-17T14:12:07Z","published":"2024-02-16T13:46:38Z","title":"Multi-Cultural Commonsense Knowledge Distillation","summary":"  Despite recent progress, large language models (LLMs) still face the\nchallenge of appropriately reacting to the intricacies of social and cultural\nconventions. This paper presents MANGO, a methodology for distilling\nhigh-accuracy, high-recall assertions of cultural knowledge. We judiciously and\niteratively prompt LLMs for this purpose from two entry points, concepts and\ncultures. Outputs are consolidated via clustering and generative summarization.\nRunning the MANGO method with GPT-3.5 as underlying LLM yields 167K\nhigh-accuracy assertions for 30K concepts and 11K cultures, surpassing prior\nresources by a large margin. For extrinsic evaluation, we explore augmenting\ndialogue systems with cultural knowledge assertions. We find that adding\nknowledge from MANGO improves the overall quality, specificity, and cultural\nsensitivity of dialogue responses, as judged by human annotators. Data and code\nare available for download.\n","authors":["Tuan-Phong Nguyen","Simon Razniewski","Gerhard Weikum"],"pdf_url":"https://arxiv.org/pdf/2402.10689v2.pdf","comment":"20 pages, 5 figures, 13 tables"},{"id":"http://arxiv.org/abs/2404.02340v2","updated":"2024-04-17T14:02:03Z","published":"2024-04-02T22:27:24Z","title":"Corpus Considerations for Annotator Modeling and Scaling","summary":"  Recent trends in natural language processing research and annotation tasks\naffirm a paradigm shift from the traditional reliance on a single ground truth\nto a focus on individual perspectives, particularly in subjective tasks. In\nscenarios where annotation tasks are meant to encompass diversity, models that\nsolely rely on the majority class labels may inadvertently disregard valuable\nminority perspectives. This oversight could result in the omission of crucial\ninformation and, in a broader context, risk disrupting the balance within\nlarger ecosystems. As the landscape of annotator modeling unfolds with diverse\nrepresentation techniques, it becomes imperative to investigate their\neffectiveness with the fine-grained features of the datasets in view. This\nstudy systematically explores various annotator modeling techniques and\ncompares their performance across seven corpora.\n  From our findings, we show that the commonly used user token model\nconsistently outperforms more complex models. We introduce a composite\nembedding approach and show distinct differences in which model performs best\nas a function of the agreement with a given dataset. Our findings shed light on\nthe relationship between corpus statistics and annotator modeling performance,\nwhich informs future work on corpus construction and perspectivist NLP.\n","authors":["Olufunke O. Sarumi","B√©la Neuendorf","Joan Plepi","Lucie Flek","J√∂rg Schl√∂tterer","Charles Welch"],"pdf_url":"https://arxiv.org/pdf/2404.02340v2.pdf","comment":"Accepted at NAACL 2024"},{"id":"http://arxiv.org/abs/2312.06722v2","updated":"2024-04-17T13:56:06Z","published":"2023-12-11T03:35:58Z","title":"EgoPlan-Bench: Benchmarking Egocentric Embodied Planning with Multimodal\n  Large Language Models","summary":"  Multimodal Large Language Models, combining the remarkable reasoning and\ngeneralization capabilities of Large Language Models (LLMs) with the ability to\ncomprehend visual inputs, have opened up new avenues for embodied task\nplanning. Given diverse environmental inputs, including real-time task\nprogress, visual observations, and open-form language instructions, a\nproficient task planner is expected to predict feasible actions, which is a\nfeat inherently achievable by Multimodal Large Language Models (MLLMs). In this\npaper, we aim to quantitatively investigate the potential of MLLMs as embodied\ntask planners in real-world scenarios by introducing a benchmark with human\nannotations named EgoPlan-Bench. Our benchmark is distinguished by realistic\ntasks derived from real-world videos, a diverse set of actions involving\ninteractions with hundreds of different objects, and complex visual\nobservations from varied scenes. We evaluate a wide range of MLLMs, revealing\nthat these models have not yet evolved into embodied planning generalists (even\nGPT-4V). We further construct an instruction-tuning dataset EgoPlan-IT from\nvideos with human-object interactions, to facilitate the learning of high-level\ntask planning in intricate real-world situations. The experiment results\ndemonstrate that the model tuned on EgoPlan-IT not only significantly improves\nperformance on our benchmark, but can also be applied as a task planner for\nguiding embodied agents in simulations.\n","authors":["Yi Chen","Yuying Ge","Yixiao Ge","Mingyu Ding","Bohao Li","Rui Wang","Ruifeng Xu","Ying Shan","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2312.06722v2.pdf","comment":"Project released at: https://github.com/ChenYi99/EgoPlan"},{"id":"http://arxiv.org/abs/2404.11384v1","updated":"2024-04-17T13:44:29Z","published":"2024-04-17T13:44:29Z","title":"Exploring Key Point Analysis with Pairwise Generation and Graph\n  Partitioning","summary":"  Key Point Analysis (KPA), the summarization of multiple arguments into a\nconcise collection of key points, continues to be a significant and unresolved\nissue within the field of argument mining. Existing models adapt a two-stage\npipeline of clustering arguments or generating key points for argument\nclusters. This approach rely on semantic similarity instead of measuring the\nexistence of shared key points among arguments. Additionally, it only models\nthe intra-cluster relationship among arguments, disregarding the inter-cluster\nrelationship between arguments that do not share key points. To address these\nlimitations, we propose a novel approach for KPA with pairwise generation and\ngraph partitioning. Our objective is to train a generative model that can\nsimultaneously provide a score indicating the presence of shared key point\nbetween a pair of arguments and generate the shared key point. Subsequently, to\nmap generated redundant key points to a concise set of key points, we proceed\nto construct an arguments graph by considering the arguments as vertices, the\ngenerated key points as edges, and the scores as edge weights. We then propose\na graph partitioning algorithm to partition all arguments sharing the same key\npoints to the same subgraph. Notably, our experimental findings demonstrate\nthat our proposed model surpasses previous models when evaluated on both the\nArgKP and QAM datasets.\n","authors":["Xiao Li","Yong Jiang","Shen Huang","Pengjun Xie","Gong Cheng","Fei Huang"],"pdf_url":"https://arxiv.org/pdf/2404.11384v1.pdf","comment":"11 pages, 4 figures, 4 tables. Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2310.03262v3","updated":"2024-04-17T13:43:59Z","published":"2023-10-05T02:35:00Z","title":"Predicting Emergent Abilities with Infinite Resolution Evaluation","summary":"  The scientific scale-up of large language models (LLMs) necessitates a\ncomprehensive understanding of their scaling properties. However, the existing\nliterature on the scaling properties only yields an incomplete answer:\noptimization loss decreases predictably as the model size increases, in line\nwith established scaling law; yet no scaling law for task has been established\nand the task performances are far from predictable during scaling. Task\nperformances typically show minor gains on small models until they improve\ndramatically once models exceed a size threshold, exemplifying the ``emergent\nabilities''. In this study, we discover that small models, although they\nexhibit minor performance, demonstrate critical and consistent task performance\nimprovements that are not captured by conventional evaluation strategies due to\ninsufficient measurement resolution. To measure such improvements, we introduce\nPassUntil, an evaluation strategy with theoretically infinite resolution,\nthrough massive sampling in the decoding phase. With PassUntil, we conduct a\nquantitative investigation into the scaling law of task performance. The\ninvestigation contains two parts. Firstly, a strict task scaling law that is\nnot conventionally known to exist, is identified, enhancing the predictability\nof task performances. Remarkably, we are able to predict the performance of the\n2.4B model on code generation with merely 0.05\\% deviation before training\nstarts, which is the first systematic attempt to verify predictable scaling\nproposed by GPT-4's report. Secondly, we are able to study emergent abilities\nquantitatively. We identify a kind of accelerated emergence whose scaling curve\ncannot be fitted by standard scaling law function and has a increasing speed.\nWe then examine two hypothesis and imply that the ``multiple circuits\nhypothesis'' might be responsible for the accelerated emergence.\n","authors":["Shengding Hu","Xin Liu","Xu Han","Xinrong Zhang","Chaoqun He","Weilin Zhao","Yankai Lin","Ning Ding","Zebin Ou","Guoyang Zeng","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2310.03262v3.pdf","comment":"After revision"},{"id":"http://arxiv.org/abs/2404.11349v1","updated":"2024-04-17T13:07:56Z","published":"2024-04-17T13:07:56Z","title":"TeClass: A Human-Annotated Relevance-based Headline Classification and\n  Generation Dataset for Telugu","summary":"  News headline generation is a crucial task in increasing productivity for\nboth the readers and producers of news. This task can easily be aided by\nautomated News headline-generation models. However, the presence of irrelevant\nheadlines in scraped news articles results in sub-optimal performance of\ngeneration models. We propose that relevance-based headline classification can\ngreatly aid the task of generating relevant headlines. Relevance-based headline\nclassification involves categorizing news headlines based on their relevance to\nthe corresponding news articles. While this task is well-established in\nEnglish, it remains under-explored in low-resource languages like Telugu due to\na lack of annotated data. To address this gap, we present TeClass, the\nfirst-ever human-annotated Telugu news headline classification dataset,\ncontaining 78,534 annotations across 26,178 article-headline pairs. We\nexperiment with various baseline models and provide a comprehensive analysis of\ntheir results. We further demonstrate the impact of this work by fine-tuning\nvarious headline generation models using TeClass dataset. The headlines\ngenerated by the models fine-tuned on highly relevant article-headline pairs,\nshowed about a 5 point increment in the ROUGE-L scores. To encourage future\nresearch, the annotated dataset as well as the annotation guidelines will be\nmade publicly available.\n","authors":["Gopichand Kanumolu","Lokesh Madasu","Nirmal Surange","Manish Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2404.11349v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2404.11315v1","updated":"2024-04-17T12:26:52Z","published":"2024-04-17T12:26:52Z","title":"To Drop or Not to Drop? Predicting Argument Ellipsis Judgments: A Case\n  Study in Japanese","summary":"  Speakers sometimes omit certain arguments of a predicate in a sentence; such\nomission is especially frequent in pro-drop languages. This study addresses a\nquestion about ellipsis -- what can explain the native speakers' ellipsis\ndecisions? -- motivated by the interest in human discourse processing and\nwriting assistance for this choice. To this end, we first collect large-scale\nhuman annotations of whether and why a particular argument should be omitted\nacross over 2,000 data points in the balanced corpus of Japanese, a\nprototypical pro-drop language. The data indicate that native speakers overall\nshare common criteria for such judgments and further clarify their quantitative\ncharacteristics, e.g., the distribution of related linguistic factors in the\nbalanced corpus. Furthermore, the performance of the language model-based\nargument ellipsis judgment model is examined, and the gap between the systems'\nprediction and human judgments in specific linguistic aspects is revealed. We\nhope our fundamental resource encourages further studies on natural human\nellipsis judgment.\n","authors":["Yukiko Ishizuki","Tatsuki Kuribayashi","Yuichiroh Matsubayashi","Ryohei Sasano","Kentaro Inui"],"pdf_url":"https://arxiv.org/pdf/2404.11315v1.pdf","comment":"13 pages; accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2310.18463v5","updated":"2024-04-17T12:03:27Z","published":"2023-10-27T20:15:23Z","title":"Benchingmaking Large Langage Models in Biomedical Triple Extraction","summary":"  Biomedical triple extraction systems aim to automatically extract biomedical\nentities and relations between entities. The exploration of applying large\nlanguage models (LLM) to triple extraction is still relatively unexplored. In\nthis work, we mainly focus on sentence-level biomedical triple extraction.\nFurthermore, the absence of a high-quality biomedical triple extraction dataset\nimpedes the progress in developing robust triple extraction systems. To address\nthese challenges, initially, we compare the performance of various large\nlanguage models. Additionally, we present GIT, an expert-annotated biomedical\ntriple extraction dataset that covers a wider range of relation types.\n","authors":["Mingchen Li","Huixue Zhou","Rui Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.18463v5.pdf","comment":"this is the onging work"},{"id":"http://arxiv.org/abs/2206.04317v4","updated":"2024-04-17T11:55:14Z","published":"2022-06-09T07:28:16Z","title":"Topic-Controllable Summarization: Topic-Aware Evaluation and Transformer\n  Methods","summary":"  Topic-controllable summarization is an emerging research area with a wide\nrange of potential applications. However, existing approaches suffer from\nsignificant limitations. For example, the majority of existing methods built\nupon recurrent architectures, which can significantly limit their performance\ncompared to more recent Transformer-based architectures, while they also\nrequire modifications to the model's architecture for controlling the topic. At\nthe same time, there is currently no established evaluation metric designed\nspecifically for topic-controllable summarization. This work proposes a new\ntopic-oriented evaluation measure to automatically evaluate the generated\nsummaries based on the topic affinity between the generated summary and the\ndesired topic. The reliability of the proposed measure is demonstrated through\nappropriately designed human evaluation. In addition, we adapt topic embeddings\nto work with powerful Transformer architectures and propose a novel and\nefficient approach for guiding the summary generation through control tokens.\nExperimental results reveal that control tokens can achieve better performance\ncompared to more complicated embedding-based approaches while also being\nsignificantly faster.\n","authors":["Tatiana Passali","Grigorios Tsoumakas"],"pdf_url":"https://arxiv.org/pdf/2206.04317v4.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2404.11288v1","updated":"2024-04-17T11:52:47Z","published":"2024-04-17T11:52:47Z","title":"A Preference-driven Paradigm for Enhanced Translation with Large\n  Language Models","summary":"  Recent research has shown that large language models (LLMs) can achieve\nremarkable translation performance through supervised fine-tuning (SFT) using\nonly a small amount of parallel data. However, SFT simply instructs the model\nto imitate the reference translations at the token level, making it vulnerable\nto the noise present in the references. Hence, the assistance from SFT often\nreaches a plateau once the LLMs have achieved a certain level of translation\ncapability, and further increasing the size of parallel data does not provide\nadditional benefits. To overcome this plateau associated with imitation-based\nSFT, we propose a preference-based approach built upon the Plackett-Luce model.\nThe objective is to steer LLMs towards a more nuanced understanding of\ntranslation preferences from a holistic view, while also being more resilient\nin the absence of gold translations. We further build a dataset named MAPLE to\nverify the effectiveness of our approach, which includes multiple translations\nof varying quality for each source sentence. Extensive experiments demonstrate\nthe superiority of our approach in \"breaking the plateau\" across diverse LLMs\nand test settings. Our in-depth analysis underscores the pivotal role of\ndiverse translations and accurate preference scores in the success of our\napproach.\n","authors":["Dawei Zhu","Sony Trenous","Xiaoyu Shen","Dietrich Klakow","Bill Byrne","Eva Hasler"],"pdf_url":"https://arxiv.org/pdf/2404.11288v1.pdf","comment":"Accepted to NAACL 2024 (long, main)"},{"id":"http://arxiv.org/abs/2310.00347v3","updated":"2024-04-17T11:48:11Z","published":"2023-09-30T12:06:04Z","title":"Unlocking Bias Detection: Leveraging Transformer-Based Models for\n  Content Analysis","summary":"  Bias detection in text is crucial for combating the spread of negative\nstereotypes, misinformation, and biased decision-making. Traditional language\nmodels frequently face challenges in generalizing beyond their training data\nand are typically designed for a single task, often focusing on bias detection\nat the sentence level. To address this, we present the Contextualized\nBi-Directional Dual Transformer (CBDT) \\textcolor{green}{\\faLeaf} classifier.\nThis model combines two complementary transformer networks: the Context\nTransformer and the Entity Transformer, with a focus on improving bias\ndetection capabilities. We have prepared a dataset specifically for training\nthese models to identify and locate biases in texts. Our evaluations across\nvarious datasets demonstrate CBDT \\textcolor{green} effectiveness in\ndistinguishing biased narratives from neutral ones and identifying specific\nbiased terms. This work paves the way for applying the CBDT \\textcolor{green}\nmodel in various linguistic and cultural contexts, enhancing its utility in\nbias detection efforts. We also make the annotated dataset available for\nresearch purposes.\n","authors":["Shaina Raza","Oluwanifemi Bamgbose","Veronica Chatrath","Shardul Ghuge","Yan Sidyakin","Abdullah Y Muaad"],"pdf_url":"https://arxiv.org/pdf/2310.00347v3.pdf","comment":"Accepted in IEEE Transactions on Computational Social Systems"},{"id":"http://arxiv.org/abs/2402.11176v2","updated":"2024-04-17T11:45:00Z","published":"2024-02-17T02:54:32Z","title":"KnowTuning: Knowledge-aware Fine-tuning for Large Language Models","summary":"  Despite their success at many natural language processing (NLP) tasks, large\nlanguage models still struggle to effectively leverage knowledge for\nknowledge-intensive tasks, manifesting limitations such as generating\nincomplete, non-factual, or illogical answers. These limitations stem from\ninadequate knowledge awareness of LLMs during vanilla fine-tuning. To address\nthese problems, we propose a knowledge-aware fine-tuning (KnowTuning) method to\nimprove fine-grained and coarse-grained knowledge awareness of LLMs. We devise\na fine-grained knowledge augmentation stage to train LLMs to identify difficult\nfine-grained knowledge in answers. We also propose a coarse-grained knowledge\ncomparison stage to train LLMs to distinguish between reliable and unreliable\nknowledge, in three aspects: completeness, factuality, and logicality.\nExtensive experiments on both generic and medical question answering (QA)\ndatasets confirm the effectiveness of KnowTuning, through automatic and human\nevaluations, across various sizes of LLMs. We further verify that KnowTuning\ngenerates more facts with less factual error rate under fine-grained facts\nevaluation.\n","authors":["Yougang Lyu","Lingyong Yan","Shuaiqiang Wang","Haibo Shi","Dawei Yin","Pengjie Ren","Zhumin Chen","Maarten de Rijke","Zhaochun Ren"],"pdf_url":"https://arxiv.org/pdf/2402.11176v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.16533v2","updated":"2024-04-17T11:38:12Z","published":"2023-06-28T20:06:36Z","title":"ICSVR: Investigating Compositional and Syntactic Understanding in Video\n  Retrieval Models","summary":"  Video retrieval (VR) involves retrieving the ground truth video from the\nvideo database given a text caption or vice-versa. The two important components\nof compositionality: objects & attributes and actions are joined using correct\nsyntax to form a proper text query. These components (objects & attributes,\nactions and syntax) each play an important role to help distinguish among\nvideos and retrieve the correct ground truth video. However, it is unclear what\nis the effect of these components on the video retrieval performance. We\ntherefore, conduct a systematic study to evaluate the compositional and\nsyntactic understanding of video retrieval models on standard benchmarks such\nas MSRVTT, MSVD and DIDEMO. The study is performed on two categories of video\nretrieval models: (i) which are pre-trained on video-text pairs and fine-tuned\non downstream video retrieval datasets (Eg. Frozen-in-Time, Violet, MCQ etc.)\n(ii) which adapt pre-trained image-text representations like CLIP for video\nretrieval (Eg. CLIP4Clip, XCLIP, CLIP2Video etc.). Our experiments reveal that\nactions and syntax play a minor role compared to objects & attributes in video\nunderstanding. Moreover, video retrieval models that use pre-trained image-text\nrepresentations (CLIP) have better syntactic and compositional understanding as\ncompared to models pre-trained on video-text data. The code is available at\nhttps://github.com/IntelLabs/multimodal_cognitive_ai/tree/main/ICSVR\n","authors":["Avinash Madasu","Vasudev Lal"],"pdf_url":"https://arxiv.org/pdf/2306.16533v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11262v1","updated":"2024-04-17T11:12:59Z","published":"2024-04-17T11:12:59Z","title":"Sampling-based Pseudo-Likelihood for Membership Inference Attacks","summary":"  Large Language Models (LLMs) are trained on large-scale web data, which makes\nit difficult to grasp the contribution of each text. This poses the risk of\nleaking inappropriate data such as benchmarks, personal information, and\ncopyrighted texts in the training data. Membership Inference Attacks (MIA),\nwhich determine whether a given text is included in the model's training data,\nhave been attracting attention. Previous studies of MIAs revealed that\nlikelihood-based classification is effective for detecting leaks in LLMs.\nHowever, the existing methods cannot be applied to some proprietary models like\nChatGPT or Claude 3 because the likelihood is unavailable to the user. In this\nstudy, we propose a Sampling-based Pseudo-Likelihood (\\textbf{SPL}) method for\nMIA (\\textbf{SaMIA}) that calculates SPL using only the text generated by an\nLLM to detect leaks. The SaMIA treats the target text as the reference text and\nmultiple outputs from the LLM as text samples, calculates the degree of\n$n$-gram match as SPL, and determines the membership of the text in the\ntraining data. Even without likelihoods, SaMIA performed on par with existing\nlikelihood-based methods.\n","authors":["Masahiro Kaneko","Youmi Ma","Yuki Wata","Naoaki Okazaki"],"pdf_url":"https://arxiv.org/pdf/2404.11262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02761v3","updated":"2024-04-17T10:56:48Z","published":"2024-04-03T14:07:02Z","title":"AQuA -- Combining Experts' and Non-Experts' Views To Assess Deliberation\n  Quality in Online Discussions Using LLMs","summary":"  Measuring the quality of contributions in political online discussions is\ncrucial in deliberation research and computer science. Research has identified\nvarious indicators to assess online discussion quality, and with deep learning\nadvancements, automating these measures has become feasible. While some studies\nfocus on analyzing specific quality indicators, a comprehensive quality score\nincorporating various deliberative aspects is often preferred. In this work, we\nintroduce AQuA, an additive score that calculates a unified deliberative\nquality score from multiple indices for each discussion post. Unlike other\nsingular scores, AQuA preserves information on the deliberative aspects present\nin comments, enhancing model transparency. We develop adapter models for 20\ndeliberative indices, and calculate correlation coefficients between experts'\nannotations and the perceived deliberativeness by non-experts to weigh the\nindividual indices into a single deliberative score. We demonstrate that the\nAQuA score can be computed easily from pre-trained adapters and aligns well\nwith annotations on other datasets that have not be seen during training. The\nanalysis of experts' vs. non-experts' annotations confirms theoretical findings\nin the social science literature.\n","authors":["Maike Behrendt","Stefan Sylvius Wagner","Marc Ziegele","Lena Wilms","Anke Stoll","Dominique Heinbach","Stefan Harmeling"],"pdf_url":"https://arxiv.org/pdf/2404.02761v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01805v2","updated":"2024-04-17T10:50:04Z","published":"2024-02-02T09:45:33Z","title":"Can LLMs perform structured graph reasoning?","summary":"  Pretrained Large Language Models (LLMs) have demonstrated various reasoning\ncapabilities through language-based prompts alone, particularly in unstructured\ntask settings (tasks purely based on language semantics). However, LLMs often\nstruggle with structured tasks, because of the inherent incompatibility of\ninput representation. Reducing structured tasks to uni-dimensional language\nsemantics often renders the problem trivial. Keeping the trade-off between LLM\ncompatibility and structure complexity in mind, we design various graph\nreasoning tasks as a proxy to semi-structured tasks in this paper, in order to\ntest the ability to navigate through representations beyond plain text in\nvarious LLMs. Particularly, we design 10 distinct problems of graph traversal,\neach representing increasing levels of complexity, and benchmark 5 different\ninstruct-finetuned LLMs (GPT-4, GPT-3.5, Claude-2, Llama-2 and Palm-2) on the\naforementioned tasks. Further, we analyse the performance of models across\nvarious settings such as varying sizes of graphs as well as different forms of\nk-shot prompting. We highlight various limitations, biases and properties of\nLLMs through this benchmarking process, such as an inverse relation to the\naverage degrees of freedom of traversal per node in graphs, the overall\nnegative impact of k-shot prompting on graph reasoning tasks, and a positive\nresponse bias which prevents LLMs from identifying the absence of a valid\nsolution. Finally, we introduce a new prompting technique specially designed\nfor graph traversal tasks (PathCompare), which demonstrates a notable increase\nin the performance of LLMs in comparison to standard prompting techniques such\nas Chain-of-Thought (CoT).\n","authors":["Palaash Agrawal","Shavak Vasania","Cheston Tan"],"pdf_url":"https://arxiv.org/pdf/2402.01805v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11225v1","updated":"2024-04-17T10:19:15Z","published":"2024-04-17T10:19:15Z","title":"In-Context Learning State Vector with Inner and Momentum Optimization","summary":"  Large Language Models (LLMs) have exhibited an impressive ability to perform\nIn-Context Learning (ICL) from only a few examples. Recent works have indicated\nthat the functions learned by ICL can be represented through compressed vectors\nderived from the transformer. However, the working mechanisms and optimization\nof these vectors are yet to be thoroughly explored. In this paper, we address\nthis gap by presenting a comprehensive analysis of these compressed vectors,\ndrawing parallels to the parameters trained with gradient descent, and\nintroduce the concept of state vector. Inspired by the works on model soup and\nmomentum-based gradient descent, we propose inner and momentum optimization\nmethods that are applied to refine the state vector progressively as test-time\nadaptation. Moreover, we simulate state vector aggregation in the multiple\nexample setting, where demonstrations comprising numerous examples are usually\ntoo lengthy for regular ICL, and further propose a divide-and-conquer\naggregation method to address this challenge. We conduct extensive experiments\nusing Llama-2 and GPT-J in both zero-shot setting and few-shot setting. The\nexperimental results show that our optimization method effectively enhances the\nstate vector and achieves the state-of-the-art performance on diverse tasks.\nCode is available at https://github.com/HITsz-TMG/ICL-State-Vector\n","authors":["Dongfang Li","Zhenyu Liu","Xinshuo Hu","Zetian Sun","Baotian Hu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.11225v1.pdf","comment":"17 pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2404.11216v1","updated":"2024-04-17T10:00:56Z","published":"2024-04-17T10:00:56Z","title":"Position Engineering: Boosting Large Language Models through Positional\n  Information Manipulation","summary":"  The performance of large language models (LLMs) is significantly influenced\nby the quality of the prompts provided. In response, researchers have developed\nenormous prompt engineering strategies aimed at modifying the prompt text to\nenhance task performance. In this paper, we introduce a novel technique termed\nposition engineering, which offers a more efficient way to guide large language\nmodels. Unlike prompt engineering, which requires substantial effort to modify\nthe text provided to LLMs, position engineering merely involves altering the\npositional information in the prompt without modifying the text itself. We have\nevaluated position engineering in two widely-used LLM scenarios:\nretrieval-augmented generation (RAG) and in-context learning (ICL). Our\nfindings show that position engineering substantially improves upon the\nbaseline in both cases. Position engineering thus represents a promising new\nstrategy for exploiting the capabilities of large language models.\n","authors":["Zhiyuan He","Huiqiang Jiang","Zilong Wang","Yuqing Yang","Luna Qiu","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2404.11216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10669v3","updated":"2024-04-17T09:56:26Z","published":"2024-02-16T13:21:06Z","title":"Humans or LLMs as the Judge? A Study on Judgement Biases","summary":"  Adopting human and large language models (LLM) as judges (\\textit{a.k.a}\nhuman- and LLM-as-a-judge) for evaluating the performance of LLMs has recently\ngained attention. Nonetheless, this approach concurrently introduces potential\nbiases from human and LLM judges, questioning the reliability of the evaluation\nresults. In this paper, we propose a novel framework that is free from\nreferencing groundtruth annotations for investigating Fallacy Oversight Bias,\nAuthority Bias and Beauty Bias on LLM and human judges. We curate a dataset\nreferring to the revised Bloom's Taxonomy and conduct thousands of human and\nLLM evaluations. Results show that human and LLM judges are vulnerable to\nperturbations to various degrees, and that even the cutting-edge judges possess\nconsiderable biases. We further exploit their weakness and conduct attacks on\nLLM judges. We hope that our work can notify the community of the vulnerability\nof human- and LLM-as-a-judge against perturbations, as well as the urgency of\ndeveloping robust evaluation systems.\n","authors":["Guiming Hardy Chen","Shunian Chen","Ziche Liu","Feng Jiang","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2402.10669v3.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2404.11206v1","updated":"2024-04-17T09:39:02Z","published":"2024-04-17T09:39:02Z","title":"Prompt-tuning for Clickbait Detection via Text Summarization","summary":"  Clickbaits are surprising social posts or deceptive news headlines that\nattempt to lure users for more clicks, which have posted at unprecedented rates\nfor more profit or commercial revenue. The spread of clickbait has significant\nnegative impacts on the users, which brings users misleading or even\nclick-jacking attacks. Different from fake news, the crucial problem in\nclickbait detection is determining whether the headline matches the\ncorresponding content. Most existing methods compute the semantic similarity\nbetween the headlines and contents for detecting clickbait. However, due to\nsignificant differences in length and semantic features between headlines and\ncontents, directly calculating semantic similarity is often difficult to\nsummarize the relationship between them. To address this problem, we propose a\nprompt-tuning method for clickbait detection via text summarization in this\npaper, text summarization is introduced to summarize the contents, and\nclickbait detection is performed based on the similarity between the generated\nsummary and the contents. Specifically, we first introduce a two-stage text\nsummarization model to produce high-quality news summaries based on pre-trained\nlanguage models, and then both the headlines and new generated summaries are\nincorporated as the inputs for prompt-tuning. Additionally, a variety of\nstrategies are conducted to incorporate external knowledge for improving the\nperformance of clickbait detection. The extensive experiments on well-known\nclickbait detection datasets demonstrate that our method achieved\nstate-of-the-art performance.\n","authors":["Haoxiang Deng","Yi Zhu","Ye Wang","Jipeng Qiang","Yunhao Yuan","Yun Li","Runmei Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.11206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11205v1","updated":"2024-04-17T09:37:25Z","published":"2024-04-17T09:37:25Z","title":"Kathakali Hand Gesture Recognition With Minimal Data","summary":"  The Indian classical dance-drama Kathakali has a set of hand gestures called\nMudras, which form the fundamental units of all its dance moves and postures.\nRecognizing the depicted mudra becomes one of the first steps in its digital\nprocessing. The work treats the problem as a 24-class classification task and\nproposes a vector-similarity-based approach using pose estimation, eliminating\nthe need for further training or fine-tuning. This approach overcomes the\nchallenge of data scarcity that limits the application of AI in similar\ndomains. The method attains 92% accuracy which is a similar or better\nperformance as other model-training-based works existing in the domain, with\nthe added advantage that the method can still work with data sizes as small as\n1 or 5 samples with a slightly reduced performance. Working with images,\nvideos, and even real-time streams is possible. The system can work with\nhand-cropped or full-body images alike. We have developed and made public a\ndataset for the Kathakali Mudra Recognition as part of this work.\n","authors":["Kavitha Raju","Nandini J. Warrier"],"pdf_url":"https://arxiv.org/pdf/2404.11205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11201v1","updated":"2024-04-17T09:33:19Z","published":"2024-04-17T09:33:19Z","title":"Neuron Specialization: Leveraging intrinsic task modularity for\n  multilingual machine translation","summary":"  Training a unified multilingual model promotes knowledge transfer but\ninevitably introduces negative interference. Language-specific modeling methods\nshow promise in reducing interference. However, they often rely on heuristics\nto distribute capacity and struggle to foster cross-lingual transfer via\nisolated modules. In this paper, we explore intrinsic task modularity within\nmultilingual networks and leverage these observations to circumvent\ninterference under multilingual translation. We show that neurons in the\nfeed-forward layers tend to be activated in a language-specific manner.\nMeanwhile, these specialized neurons exhibit structural overlaps that reflect\nlanguage proximity, which progress across layers. Based on these findings, we\npropose Neuron Specialization, an approach that identifies specialized neurons\nto modularize feed-forward layers and then continuously updates them through\nsparse networks. Extensive experiments show that our approach achieves\nconsistent performance gains over strong baselines with additional analyses\ndemonstrating reduced interference and increased knowledge transfer.\n","authors":["Shaomu Tan","Di Wu","Christof Monz"],"pdf_url":"https://arxiv.org/pdf/2404.11201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02977v2","updated":"2024-04-17T09:09:17Z","published":"2023-10-04T17:12:18Z","title":"T$^3$Bench: Benchmarking Current Progress in Text-to-3D Generation","summary":"  Recent methods in text-to-3D leverage powerful pretrained diffusion models to\noptimize NeRF. Notably, these methods are able to produce high-quality 3D\nscenes without training on 3D data. Due to the open-ended nature of the task,\nmost studies evaluate their results with subjective case studies and user\nexperiments, thereby presenting a challenge in quantitatively addressing the\nquestion: How has current progress in Text-to-3D gone so far? In this paper, we\nintroduce T$^3$Bench, the first comprehensive text-to-3D benchmark containing\ndiverse text prompts of three increasing complexity levels that are specially\ndesigned for 3D generation. To assess both the subjective quality and the text\nalignment, we propose two automatic metrics based on multi-view images produced\nby the 3D contents. The quality metric combines multi-view text-image scores\nand regional convolution to detect quality and view inconsistency. The\nalignment metric uses multi-view captioning and GPT-4 evaluation to measure\ntext-3D consistency. Both metrics closely correlate with different dimensions\nof human judgments, providing a paradigm for efficiently evaluating text-to-3D\nmodels. The benchmarking results, shown in Fig. 1, reveal performance\ndifferences among an extensive 10 prevalent text-to-3D methods. Our analysis\nfurther highlights the common struggles for current methods on generating\nsurroundings and multi-object scenes, as well as the bottleneck of leveraging\n2D guidance for 3D generation. Our project page is available at:\nhttps://t3bench.com.\n","authors":["Yuze He","Yushi Bai","Matthieu Lin","Wang Zhao","Yubin Hu","Jenny Sheng","Ran Yi","Juanzi Li","Yong-Jin Liu"],"pdf_url":"https://arxiv.org/pdf/2310.02977v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2310.03031v2","updated":"2024-04-17T09:04:28Z","published":"2023-09-21T07:54:25Z","title":"How Prevalent is Gender Bias in ChatGPT? -- Exploring German and English\n  ChatGPT Responses","summary":"  With the introduction of ChatGPT, OpenAI made large language models (LLM)\naccessible to users with limited IT expertise. However, users with no\nbackground in natural language processing (NLP) might lack a proper\nunderstanding of LLMs. Thus the awareness of their inherent limitations, and\ntherefore will take the systems' output at face value. In this paper, we\nsystematically analyse prompts and the generated responses to identify possible\nproblematic issues with a special focus on gender biases, which users need to\nbe aware of when processing the system's output. We explore how ChatGPT reacts\nin English and German if prompted to answer from a female, male, or neutral\nperspective. In an in-depth investigation, we examine selected prompts and\nanalyse to what extent responses differ if the system is prompted several times\nin an identical way. On this basis, we show that ChatGPT is indeed useful for\nhelping non-IT users draft texts for their daily work. However, it is\nabsolutely crucial to thoroughly check the system's responses for biases as\nwell as for syntactic and grammatical mistakes.\n","authors":["Stefanie Urchs","Veronika Thurner","Matthias A√üenmacher","Christian Heumann","Stephanie Thiemichen"],"pdf_url":"https://arxiv.org/pdf/2310.03031v2.pdf","comment":"Accepted @ \"1st Workshop on Biased Data in Conversational Agents\"\n  (co-located with ECML PKDD 2023). This is the author's version of the work.\n  The definite version of record will be published in the proceedings"},{"id":"http://arxiv.org/abs/2404.11184v1","updated":"2024-04-17T09:01:02Z","published":"2024-04-17T09:01:02Z","title":"FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out\n  Document","summary":"  Through the advent of pre-trained language models, there have been notable\nadvancements in abstractive summarization systems. Simultaneously, a\nconsiderable number of novel methods for evaluating factual consistency in\nabstractive summarization systems has been developed. But these evaluation\napproaches incorporate substantial limitations, especially on refinement and\ninterpretability. In this work, we propose highly effective and interpretable\nfactual inconsistency detection method metric Factual Inconsistency Detection\nby Zoom-in Summary and Zoom-out Document for abstractive summarization systems\nthat is based on fine-grained atomic facts decomposition. Moreover, we align\natomic facts decomposed from the summary with the source document through\nadaptive granularity expansion. These atomic facts represent a more\nfine-grained unit of information, facilitating detailed understanding and\ninterpretability of the summary's factual inconsistency. Experimental results\ndemonstrate that our proposed factual consistency checking system significantly\noutperforms existing systems. We release the code at\nhttps://github.com/plm3332/FIZZ.\n","authors":["Joonho Yang","Seunghyun Yoon","Byeongjeong Kim","Hwanhee Lee"],"pdf_url":"https://arxiv.org/pdf/2404.11184v1.pdf","comment":"Submitted to ACL ARR on April 17th 2024"},{"id":"http://arxiv.org/abs/2404.10710v2","updated":"2024-04-17T08:44:30Z","published":"2024-04-16T16:36:50Z","title":"Dual Modalities of Text: Visual and Textual Generative Pre-training","summary":"  Harnessing visual texts represents a burgeoning frontier in the evolution of\nlanguage modeling. In this paper, we introduce a novel pre-training framework\nfor a suite of pixel-based autoregressive language models, pre-training on a\ncorpus of over 400 million documents rendered as RGB images. Our approach is\ncharacterized by a dual-modality training regimen, engaging both visual data\nthrough next patch prediction with a regression head and textual data via next\ntoken prediction with a classification head. This study is particularly focused\non investigating the synergistic interplay between visual and textual\nmodalities of language. Our comprehensive evaluation across a diverse array of\nbenchmarks reveals that the confluence of visual and textual data substantially\naugments the efficacy of pixel-based language models. Notably, our findings\nshow that a unidirectional pixel-based model, devoid of textual data during\ntraining, can match the performance levels of advanced bidirectional\npixel-based models on various language understanding benchmarks. This work\nhighlights the considerable untapped potential of integrating visual and\ntextual information for language modeling purposes. We will release our code,\ndata, and checkpoints to inspire further research advancement.\n","authors":["Yekun Chai","Qingyi Liu","Jingwu Xiao","Shuohuan Wang","Yu Sun","Hua Wu"],"pdf_url":"https://arxiv.org/pdf/2404.10710v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.02046v4","updated":"2024-04-17T08:36:26Z","published":"2023-07-05T06:03:40Z","title":"Recommender Systems in the Era of Large Language Models (LLMs)","summary":"  With the prosperity of e-commerce and web applications, Recommender Systems\n(RecSys) have become an important component of our daily life, providing\npersonalized suggestions that cater to user preferences. While Deep Neural\nNetworks (DNNs) have made significant advancements in enhancing recommender\nsystems by modeling user-item interactions and incorporating textual side\ninformation, DNN-based methods still face limitations, such as difficulties in\nunderstanding users' interests and capturing textual side information,\ninabilities in generalizing to various recommendation scenarios and reasoning\non their predictions, etc. Meanwhile, the emergence of Large Language Models\n(LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural\nLanguage Processing (NLP) and Artificial Intelligence (AI), due to their\nremarkable abilities in fundamental responsibilities of language understanding\nand generation, as well as impressive generalization and reasoning\ncapabilities. As a result, recent studies have attempted to harness the power\nof LLMs to enhance recommender systems. Given the rapid evolution of this\nresearch direction in recommender systems, there is a pressing need for a\nsystematic overview that summarizes existing LLM-empowered recommender systems,\nto provide researchers in relevant fields with an in-depth understanding.\nTherefore, in this paper, we conduct a comprehensive review of LLM-empowered\nrecommender systems from various aspects including Pre-training, Fine-tuning,\nand Prompting. More specifically, we first introduce representative methods to\nharness the power of LLMs (as a feature encoder) for learning representations\nof users and items. Then, we review recent techniques of LLMs for enhancing\nrecommender systems from three paradigms, namely pre-training, fine-tuning, and\nprompting. Finally, we comprehensively discuss future directions in this\nemerging field.\n","authors":["Zihuai Zhao","Wenqi Fan","Jiatong Li","Yunqing Liu","Xiaowei Mei","Yiqi Wang","Zhen Wen","Fei Wang","Xiangyu Zhao","Jiliang Tang","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2307.02046v4.pdf","comment":"Accepted by IEEE TKDE"},{"id":"http://arxiv.org/abs/2404.11141v1","updated":"2024-04-17T07:36:40Z","published":"2024-04-17T07:36:40Z","title":"Context-Aware Siamese Networks for Efficient Emotion Recognition in\n  Conversation","summary":"  The advent of deep learning models has made a considerable contribution to\nthe achievement of Emotion Recognition in Conversation (ERC). However, this\ntask still remains an important challenge due to the plurality and subjectivity\nof human emotions. Previous work on ERC provides predictive models using mostly\ngraph-based conversation representations. In this work, we propose a way to\nmodel the conversational context that we incorporate into a metric learning\ntraining strategy, with a two-step process. This allows us to perform ERC in a\nflexible classification scenario and to end up with a lightweight yet efficient\nmodel. Using metric learning through a Siamese Network architecture, we achieve\n57.71 in macro F1 score for emotion classification in conversation on\nDailyDialog dataset, which outperforms the related work. This state-of-the-art\nresult is promising regarding the use of metric learning for emotion\nrecognition, yet perfectible compared to the microF1 score obtained.\n","authors":["Barbara Gendron","Ga√´l Guibon"],"pdf_url":"https://arxiv.org/pdf/2404.11141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04329v2","updated":"2024-04-17T07:31:01Z","published":"2023-11-07T20:21:42Z","title":"Formal Aspects of Language Modeling","summary":"  Large language models have become one of the most commonly deployed NLP\ninventions. In the past half-decade, their integration into core natural\nlanguage processing tools has dramatically increased the performance of such\ntools, and they have entered the public discourse surrounding artificial\nintelligence. Consequently, it is important for both developers and researchers\nalike to understand the mathematical foundations of large language models, as\nwell as how to implement them. These notes are the accompaniment to the\ntheoretical portion of the ETH Z\\\"urich course on large language models,\ncovering what constitutes a language model from a formal, theoretical\nperspective.\n","authors":["Ryan Cotterell","Anej Svete","Clara Meister","Tianyu Liu","Li Du"],"pdf_url":"https://arxiv.org/pdf/2311.04329v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11132v1","updated":"2024-04-17T07:26:23Z","published":"2024-04-17T07:26:23Z","title":"A Novel ICD Coding Framework Based on Associated and Hierarchical Code\n  Description Distillation","summary":"  ICD(International Classification of Diseases) coding involves assigning ICD\ncodes to patients visit based on their medical notes. ICD coding is a\nchallenging multilabel text classification problem due to noisy medical\ndocument inputs. Recent advancements in automated ICD coding have enhanced\nperformance by integrating additional data and knowledge bases with the\nencoding of medical notes and codes. However, most of them ignore the code\nhierarchy, leading to improper code assignments. To address these problems, we\npropose a novel framework based on associated and hierarchical code description\ndistillation (AHDD) for better code representation learning and avoidance of\nimproper code assignment.we utilize the code description and the hierarchical\nstructure inherent to the ICD codes. Therefore, in this paper, we leverage the\ncode description and the hierarchical structure inherent to the ICD codes. The\ncode description is also applied to aware the attention layer and output layer.\nExperimental results on the benchmark dataset show the superiority of the\nproposed framework over several state-of-the-art baselines.\n","authors":["Bin Zhang","Junli Wang"],"pdf_url":"https://arxiv.org/pdf/2404.11132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05330v2","updated":"2024-04-17T07:23:35Z","published":"2024-03-08T14:07:44Z","title":"Consecutive Model Editing with Batch alongside HooK Layers","summary":"  As the typical retraining paradigm is unacceptably time- and\nresource-consuming, researchers are turning to model editing in order to seek\nan effective, consecutive, and batch-supportive way to edit the model behavior\ndirectly. Despite all these practical expectations, existing model editing\nmethods fail to realize all of them. Furthermore, the memory demands for such\nsuccession-supportive model editing approaches tend to be prohibitive,\nfrequently necessitating an external memory that grows incrementally over time.\nTo cope with these challenges, we propose COMEBA-HK, a model editing method\nthat is both consecutive and batch-supportive. COMEBA-HK is memory-friendly as\nit only needs a small amount of it to store several hook layers with updated\nweights. Experimental results demonstrate the superiority of our method over\nother batch-supportive model editing methods under both single-round and\nconsecutive batch editing scenarios. Extensive analyses of COMEBA-HK have been\nconducted to verify the stability of our method over 1) the number of\nconsecutive steps and 2) the number of editing instance.\n","authors":["Shuaiyi Li","Yang Deng","Deng Cai","Hongyuan Lu","Liang Chen","Wai Lam"],"pdf_url":"https://arxiv.org/pdf/2403.05330v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2404.11124v1","updated":"2024-04-17T07:15:07Z","published":"2024-04-17T07:15:07Z","title":"What's under the hood: Investigating Automatic Metrics on Meeting\n  Summarization","summary":"  Meeting summarization has become a critical task considering the increase in\nonline interactions. While new techniques are introduced regularly, their\nevaluation uses metrics not designed to capture meeting-specific errors,\nundermining effective evaluation. This paper investigates what the frequently\nused automatic metrics capture and which errors they mask by correlating\nautomatic metric scores with human evaluations across a broad error taxonomy.\nWe commence with a comprehensive literature review on English meeting\nsummarization to define key challenges like speaker dynamics and contextual\nturn-taking and error types such as missing information and linguistic\ninaccuracy, concepts previously loosely defined in the field. We examine the\nrelationship between characteristic challenges and errors by using annotated\ntranscripts and summaries from Transformer-based sequence-to-sequence and\nautoregressive models from the general summary QMSum dataset. Through\nexperimental validation, we find that different model architectures respond\nvariably to challenges in meeting transcripts, resulting in different\npronounced links between challenges and errors. Current default-used metrics\nstruggle to capture observable errors, showing weak to mid-correlations, while\na third of the correlations show trends of error masking. Only a subset reacts\naccurately to specific errors, while most correlations show either\nunresponsiveness or failure to reflect the error's impact on summary quality.\n","authors":["Frederic Kirstein","Jan Philip Wahle","Terry Ruas","Bela Gipp"],"pdf_url":"https://arxiv.org/pdf/2404.11124v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05904v2","updated":"2024-04-17T07:03:17Z","published":"2024-04-08T23:16:22Z","title":"The Hallucinations Leaderboard -- An Open Effort to Measure\n  Hallucinations in Large Language Models","summary":"  Large Language Models (LLMs) have transformed the Natural Language Processing\n(NLP) landscape with their remarkable ability to understand and generate\nhuman-like text. However, these models are prone to ``hallucinations'' --\noutputs that do not align with factual reality or the input context. This paper\nintroduces the Hallucinations Leaderboard, an open initiative to quantitatively\nmeasure and compare the tendency of each model to produce hallucinations. The\nleaderboard uses a comprehensive set of benchmarks focusing on different\naspects of hallucinations, such as factuality and faithfulness, across various\ntasks, including question-answering, summarisation, and reading comprehension.\nOur analysis provides insights into the performance of different models,\nguiding researchers and practitioners in choosing the most reliable models for\ntheir applications.\n","authors":["Giwon Hong","Aryo Pradipta Gema","Rohit Saxena","Xiaotang Du","Ping Nie","Yu Zhao","Laura Perez-Beltrachini","Max Ryabinin","Xuanli He","Cl√©mentine Fourrier","Pasquale Minervini"],"pdf_url":"https://arxiv.org/pdf/2404.05904v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11109v1","updated":"2024-04-17T06:49:14Z","published":"2024-04-17T06:49:14Z","title":"Consistency Training by Synthetic Question Generation for Conversational\n  Question Answering","summary":"  Efficiently modeling historical information is a critical component in\naddressing user queries within a conversational question-answering (QA)\ncontext, as historical context plays a vital role in clarifying the user's\nquestions. However, irrelevant history induces noise in the reasoning process,\nespecially for those questions with a considerable historical context. In our\nnovel model-agnostic approach, referred to as CoTaH (Consistency-Trained\naugmented History), we augment the historical information with synthetic\nquestions and subsequently employ consistency training to train a model that\nutilizes both real and augmented historical data to implicitly make the\nreasoning robust to irrelevant history. To the best of our knowledge, this is\nthe first instance of research using question generation as a form of data\naugmentation to model conversational QA settings. By citing a common modeling\nerror prevalent in previous research, we introduce a new baseline model and\ncompare our model's performance against it, demonstrating an improvement in\nresults, particularly when dealing with questions that include a substantial\namount of historical context. The source code can be found on our GitHub page.\n","authors":["Hamed Hematian Hemati","Hamid Beigy"],"pdf_url":"https://arxiv.org/pdf/2404.11109v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11095v1","updated":"2024-04-17T06:26:32Z","published":"2024-04-17T06:26:32Z","title":"Inductive-Deductive Strategy Reuse for Multi-Turn Instructional\n  Dialogues","summary":"  Aligning large language models (LLMs) with human expectations requires\nhigh-quality instructional dialogues, which can be achieved by raising diverse,\nin-depth, and insightful instructions that deepen interactions. Existing\nmethods target instructions from real instruction dialogues as a learning goal\nand fine-tune a user simulator for posing instructions. However, the user\nsimulator struggles to implicitly model complex dialogue flows and pose\nhigh-quality instructions. In this paper, we take inspiration from the\ncognitive abilities inherent in human learning and propose the explicit\nmodeling of complex dialogue flows through instructional strategy reuse.\nSpecifically, we first induce high-level strategies from various real\ninstruction dialogues. These strategies are applied to new dialogue scenarios\ndeductively, where the instructional strategies facilitate high-quality\ninstructions. Experimental results show that our method can generate diverse,\nin-depth, and insightful instructions for a given dialogue history. The\nconstructed multi-turn instructional dialogues can outperform competitive\nbaselines on the downstream chat model.\n","authors":["Jiao Ou","Jiayu Wu","Che Liu","Fuzheng Zhang","Di Zhang","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2404.11095v1.pdf","comment":"27 pages, 3 figures, 12 tables"},{"id":"http://arxiv.org/abs/2311.09569v2","updated":"2024-04-17T06:00:51Z","published":"2023-11-16T05:08:33Z","title":"Strings from the Library of Babel: Random Sampling as a Strong Baseline\n  for Prompt Optimisation","summary":"  Recent prompt optimisation approaches use the generative nature of language\nmodels to produce prompts -- even rivaling the performance of human-curated\nprompts. In this paper, we demonstrate that randomly sampling tokens from the\nmodel vocabulary as ``separators'' can be as effective as language models for\nprompt-style text classification. Our experiments show that random separators\nare competitive baselines, having less than a 1% difference compared to\nprevious self-optimisation methods and showing a 12% average relative\nimprovement over strong human baselines across nine text classification tasks\nand eight language models. We further analyse this phenomenon in detail using\nthree different random generation strategies, establishing that the language\nspace is rich with potentially good separators, with a greater than 40% average\nchance that a randomly drawn separator performs better than human-curated\nseparators. These observations challenge the common assumption that an\neffective prompt should be human readable or task relevant and establish a\nstrong baseline for prompt optimisation research.\n","authors":["Yao Lu","Jiayi Wang","Raphael Tang","Sebastian Riedel","Pontus Stenetorp"],"pdf_url":"https://arxiv.org/pdf/2311.09569v2.pdf","comment":"Accepted to NAACL 2024. The code is publicly available at\n  https://github.com/yaolu/random-prompt"},{"id":"http://arxiv.org/abs/2404.11086v1","updated":"2024-04-17T05:57:17Z","published":"2024-04-17T05:57:17Z","title":"ViLLM-Eval: A Comprehensive Evaluation Suite for Vietnamese Large\n  Language Models","summary":"  The rapid advancement of large language models (LLMs) necessitates the\ndevelopment of new benchmarks to accurately assess their capabilities. To\naddress this need for Vietnamese, this work aims to introduce ViLLM-Eval, the\ncomprehensive evaluation suite designed to measure the advanced knowledge and\nreasoning abilities of foundation models within a Vietnamese context.\nViLLM-Eval consists of multiple-choice questions and predict next word tasks\nspanning various difficulty levels and diverse disciplines, ranging from\nhumanities to science and engineering. A thorough evaluation of the most\nadvanced LLMs on ViLLM-Eval revealed that even the best performing models have\nsignificant room for improvement in understanding and responding to Vietnamese\nlanguage tasks. ViLLM-Eval is believed to be instrumental in identifying key\nstrengths and weaknesses of foundation models, ultimately promoting their\ndevelopment and enhancing their performance for Vietnamese users.\n","authors":["Trong-Hieu Nguyen","Anh-Cuong Le","Viet-Cuong Nguyen"],"pdf_url":"https://arxiv.org/pdf/2404.11086v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2305.08322 by other authors"},{"id":"http://arxiv.org/abs/2402.13919v2","updated":"2024-04-17T05:27:25Z","published":"2024-02-21T16:33:22Z","title":"SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in\n  Clinical Summarization","summary":"  Large Language Models (LLMs) such as GPT & Llama have demonstrated\nsignificant achievements in summarization tasks but struggle with factual\ninaccuracies, a critical issue in clinical NLP applications where errors could\nlead to serious consequences. To counter the high costs and limited\navailability of expert-annotated data for factual alignment, this study\nintroduces an innovative pipeline that utilizes >100B parameter GPT variants\nlike GPT-3.5 & GPT-4 to act as synthetic experts to generate high-quality\nsynthetics feedback aimed at enhancing factual consistency in clinical note\nsummarization. Our research primarily focuses on edit feedback generated by\nthese synthetic feedback experts without additional human annotations,\nmirroring and optimizing the practical scenario in which medical professionals\nrefine AI system outputs. Although such 100B+ parameter GPT variants have\nproven to demonstrate expertise in various clinical NLP tasks, such as the\nMedical Licensing Examination, there is scant research on their capacity to act\nas synthetic feedback experts and deliver expert-level edit feedback for\nimproving the generation quality of weaker (<10B parameter) LLMs like GPT-2\n(1.5B) & Llama 2 (7B) in clinical domain. So in this work, we leverage 100B+\nGPT variants to act as synthetic feedback experts offering expert-level edit\nfeedback, that is used to reduce hallucinations and align weaker (<10B\nparameter) LLMs with medical facts using two distinct alignment algorithms (DPO\n& SALT), endeavoring to narrow the divide between AI-generated content and\nfactual accuracy. This highlights the substantial potential of LLM-based\nsynthetic edits in enhancing the alignment of clinical factuality.\n","authors":["Prakamya Mishra","Zonghai Yao","Parth Vashisht","Feiyun Ouyang","Beining Wang","Vidhi Dhaval Mody","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2402.13919v2.pdf","comment":"Equal contribution for the first two authors"},{"id":"http://arxiv.org/abs/2404.11061v1","updated":"2024-04-17T04:37:58Z","published":"2024-04-17T04:37:58Z","title":"Unified Examination of Entity Linking in Absence of Candidate Sets","summary":"  Despite remarkable strides made in the development of entity linking systems\nin recent years, a comprehensive comparative analysis of these systems using a\nunified framework is notably absent. This paper addresses this oversight by\nintroducing a new black-box benchmark and conducting a comprehensive evaluation\nof all state-of-the-art entity linking methods. We use an ablation study to\ninvestigate the impact of candidate sets on the performance of entity linking.\nOur findings uncover exactly how much such entity linking systems depend on\ncandidate sets, and how much this limits the general applicability of each\nsystem. We present an alternative approach to candidate sets, demonstrating\nthat leveraging the entire in-domain candidate set can serve as a viable\nsubstitute for certain models. We show the trade-off between less restrictive\ncandidate sets, increased inference time and memory footprint for some models.\n","authors":["Nicolas Ong","Hassan Shavarani","Anoop Sarkar"],"pdf_url":"https://arxiv.org/pdf/2404.11061v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.05836v3","updated":"2024-04-17T04:27:10Z","published":"2023-06-09T12:09:15Z","title":"Can Large Language Models Infer Causation from Correlation?","summary":"  Causal inference is one of the hallmarks of human intelligence. While the\nfield of CausalNLP has attracted much interest in the recent years, existing\ncausal inference datasets in NLP primarily rely on discovering causality from\nempirical knowledge (e.g., commonsense knowledge). In this work, we propose the\nfirst benchmark dataset to test the pure causal inference skills of large\nlanguage models (LLMs). Specifically, we formulate a novel task Corr2Cause,\nwhich takes a set of correlational statements and determines the causal\nrelationship between the variables. We curate a large-scale dataset of more\nthan 200K samples, on which we evaluate seventeen existing LLMs. Through our\nexperiments, we identify a key shortcoming of LLMs in terms of their causal\ninference skills, and show that these models achieve almost close to random\nperformance on the task. This shortcoming is somewhat mitigated when we try to\nre-purpose LLMs for this skill via finetuning, but we find that these models\nstill fail to generalize -- they can only perform causal inference in\nin-distribution settings when variable names and textual expressions used in\nthe queries are similar to those in the training set, but fail in\nout-of-distribution settings generated by perturbing these queries. Corr2Cause\nis a challenging task for LLMs, and would be helpful in guiding future research\non improving LLMs' pure reasoning skills and generalizability. Our data is at\nhttps://huggingface.co/datasets/causalnlp/corr2cause. Our code is at\nhttps://github.com/causalNLP/corr2cause.\n","authors":["Zhijing Jin","Jiarui Liu","Zhiheng Lyu","Spencer Poff","Mrinmaya Sachan","Rada Mihalcea","Mona Diab","Bernhard Sch√∂lkopf"],"pdf_url":"https://arxiv.org/pdf/2306.05836v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2309.15098v2","updated":"2024-04-17T04:25:21Z","published":"2023-09-26T17:48:55Z","title":"Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of\n  Language Models","summary":"  We investigate the internal behavior of Transformer-based Large Language\nModels (LLMs) when they generate factually incorrect text. We propose modeling\nfactual queries as constraint satisfaction problems and use this framework to\ninvestigate how the LLM interacts internally with factual constraints. We find\na strong positive relationship between the LLM's attention to constraint tokens\nand the factual accuracy of generations. We curate a suite of 10 datasets\ncontaining over 40,000 prompts to study the task of predicting factual errors\nwith the Llama-2 family across all scales (7B, 13B, 70B). We propose SAT Probe,\na method probing attention patterns, that can predict factual errors and\nfine-grained constraint satisfaction, and allow early error identification. The\napproach and findings take another step towards using the mechanistic\nunderstanding of LLMs to enhance their reliability.\n","authors":["Mert Yuksekgonul","Varun Chandrasekaran","Erik Jones","Suriya Gunasekar","Ranjita Naik","Hamid Palangi","Ece Kamar","Besmira Nushi"],"pdf_url":"https://arxiv.org/pdf/2309.15098v2.pdf","comment":"Published at ICLR 2024"},{"id":"http://arxiv.org/abs/2404.05893v2","updated":"2024-04-17T04:17:12Z","published":"2024-04-08T22:29:53Z","title":"Use of a Structured Knowledge Base Enhances Metadata Curation by Large\n  Language Models","summary":"  Metadata play a crucial role in ensuring the findability, accessibility,\ninteroperability, and reusability of datasets. This paper investigates the\npotential of large language models (LLMs), specifically GPT-4, to improve\nadherence to metadata standards. We conducted experiments on 200 random data\nrecords describing human samples relating to lung cancer from the NCBI\nBioSample repository, evaluating GPT-4's ability to suggest edits for adherence\nto metadata standards. We computed the adherence accuracy of field name-field\nvalue pairs through a peer review process, and we observed a marginal average\nimprovement in adherence to the standard data dictionary from 79% to 80%\n(p<0.01). We then prompted GPT-4 with domain information in the form of the\ntextual descriptions of CEDAR templates and recorded a significant improvement\nto 97% from 79% (p<0.01). These results indicate that, while LLMs may not be\nable to correct legacy metadata to ensure satisfactory adherence to standards\nwhen unaided, they do show promise for use in automated metadata curation when\nintegrated with a structured knowledge base.\n","authors":["Sowmya S. Sundaram","Benjamin Solomon","Avani Khatri","Anisha Laumas","Purvesh Khatri","Mark A. Musen"],"pdf_url":"https://arxiv.org/pdf/2404.05893v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.11752v5","updated":"2024-04-17T04:09:25Z","published":"2023-02-23T02:38:39Z","title":"EVJVQA Challenge: Multilingual Visual Question Answering","summary":"  Visual Question Answering (VQA) is a challenging task of natural language\nprocessing (NLP) and computer vision (CV), attracting significant attention\nfrom researchers. English is a resource-rich language that has witnessed\nvarious developments in datasets and models for visual question answering.\nVisual question answering in other languages also would be developed for\nresources and models. In addition, there is no multilingual dataset targeting\nthe visual content of a particular country with its own objects and cultural\ncharacteristics. To address the weakness, we provide the research community\nwith a benchmark dataset named EVJVQA, including 33,000+ pairs of\nquestion-answer over three languages: Vietnamese, English, and Japanese, on\napproximately 5,000 images taken from Vietnam for evaluating multilingual VQA\nsystems or models. EVJVQA is used as a benchmark dataset for the challenge of\nmultilingual visual question answering at the 9th Workshop on Vietnamese\nLanguage and Speech Processing (VLSP 2022). This task attracted 62 participant\nteams from various universities and organizations. In this article, we present\ndetails of the organization of the challenge, an overview of the methods\nemployed by shared-task participants, and the results. The highest performances\nare 0.4392 in F1-score and 0.4009 in BLUE on the private test set. The\nmultilingual QA systems proposed by the top 2 teams use ViT for the pre-trained\nvision model and mT5 for the pre-trained language model, a powerful pre-trained\nlanguage model based on the transformer architecture. EVJVQA is a challenging\ndataset that motivates NLP and CV researchers to further explore the\nmultilingual models or systems for visual question answering systems. We\nreleased the challenge on the Codalab evaluation system for further research.\n","authors":["Ngan Luu-Thuy Nguyen","Nghia Hieu Nguyen","Duong T. D Vo","Khanh Quoc Tran","Kiet Van Nguyen"],"pdf_url":"https://arxiv.org/pdf/2302.11752v5.pdf","comment":"VLSP2022 EVJVQA challenge"},{"id":"http://arxiv.org/abs/2404.11055v1","updated":"2024-04-17T04:04:34Z","published":"2024-04-17T04:04:34Z","title":"On the Causal Nature of Sentiment Analysis","summary":"  Sentiment analysis (SA) aims to identify the sentiment expressed in a text,\nsuch as a product review. Given a review and the sentiment associated with it,\nthis paper formulates SA as a combination of two tasks: (1) a causal discovery\ntask that distinguishes whether a review \"primes\" the sentiment (Causal\nHypothesis C1), or the sentiment \"primes\" the review (Causal Hypothesis C2);\nand (2) the traditional prediction task to model the sentiment using the review\nas input. Using the peak-end rule in psychology, we classify a sample as C1 if\nits overall sentiment score approximates an average of all the sentence-level\nsentiments in the review, and C2 if the overall sentiment score approximates an\naverage of the peak and end sentiments. For the prediction task, we use the\ndiscovered causal mechanisms behind the samples to improve the performance of\nLLMs by proposing causal prompts that give the models an inductive bias of the\nunderlying causal graph, leading to substantial improvements by up to 32.13 F1\npoints on zero-shot five-class SA. Our code is at\nhttps://github.com/cogito233/causal-sa\n","authors":["Zhiheng Lyu","Zhijing Jin","Fernando Gonzalez","Rada Mihalcea","Bernhard Schoelkopf","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2404.11055v1.pdf","comment":"An enhanced version of our previous exploration in arXiv:2305.01764"},{"id":"http://arxiv.org/abs/2404.11049v1","updated":"2024-04-17T03:44:58Z","published":"2024-04-17T03:44:58Z","title":"Stepwise Alignment for Constrained Language Model Policy Optimization","summary":"  Safety and trustworthiness are indispensable requirements for applying AI\nsystems based on large language models (LLMs) in real-world applications. This\npaper formulates a human value alignment as a language model policy\noptimization problem to maximize reward under a safety constraint and then\nproposes an algorithm called Stepwise Alignment for Constrained Policy\nOptimization (SACPO). A key idea behind SACPO, supported by theory, is that the\noptimal policy incorporating both reward and safety can be directly obtained\nfrom a reward-aligned policy. Based on this key idea, SACPO aligns the LLMs\nwith each metric step-wise while leveraging simple yet powerful alignment\nalgorithms such as direct preference optimization (DPO). SACPO provides many\nbenefits such as simplicity, stability, computational efficiency, and\nflexibility regarding algorithms and dataset selection. Under mild assumption,\nour theoretical analysis provides the upper bounds regarding near-optimality\nand safety constraint violation. Our experimental results show that SACPO can\nfine-tune Alpaca-7B better than the state-of-the-art method in terms of both\nhelpfulness and harmlessness\n","authors":["Akifumi Wachi","Thien Q Tran","Rei Sato","Takumi Tanabe","Yohei Akimoto"],"pdf_url":"https://arxiv.org/pdf/2404.11049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11045v1","updated":"2024-04-17T03:39:51Z","published":"2024-04-17T03:39:51Z","title":"Offset Unlearning for Large Language Models","summary":"  Despite the strong capabilities of Large Language Models (LLMs) to acquire\nknowledge from their training corpora, the memorization of sensitive\ninformation in the corpora such as copyrighted, harmful, and private content\nhas led to ethical and legal concerns. In response to these challenges,\nunlearning has emerged as a potential remedy for LLMs affected by problematic\ntraining data. However, previous unlearning techniques are either not\napplicable to black-box LLMs due to required access to model internal weights,\nor violate data protection principles by retaining sensitive data for\ninference-time correction. We propose $\\delta$-unlearning, an offset unlearning\nframework for black-box LLMs. Instead of tuning the black-box LLM itself,\n$\\delta$-unlearning learns the logit offset needed for unlearning by\ncontrasting the logits from a pair of smaller models. Experiments demonstrate\nthat $\\delta$-unlearning can effectively unlearn target data while maintaining\nsimilar or even stronger performance on general out-of-forget-scope tasks.\n$\\delta$-unlearning also effectively incorporates different unlearning\nalgorithms, making our approach a versatile solution to adapting various\nexisting unlearning algorithms to black-box LLMs.\n","authors":["James Y. Huang","Wenxuan Zhou","Fei Wang","Fred Morstatter","Sheng Zhang","Hoifung Poon","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2404.11045v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11036v1","updated":"2024-04-17T03:25:54Z","published":"2024-04-17T03:25:54Z","title":"Cross-Platform Hate Speech Detection with Weakly Supervised Causal\n  Disentanglement","summary":"  Content moderation faces a challenging task as social media's ability to\nspread hate speech contrasts with its role in promoting global connectivity.\nWith rapidly evolving slang and hate speech, the adaptability of conventional\ndeep learning to the fluid landscape of online dialogue remains limited. In\nresponse, causality inspired disentanglement has shown promise by segregating\nplatform specific peculiarities from universal hate indicators. However, its\ndependency on available ground truth target labels for discerning these nuances\nfaces practical hurdles with the incessant evolution of platforms and the\nmutable nature of hate speech. Using confidence based reweighting and\ncontrastive regularization, this study presents HATE WATCH, a novel framework\nof weakly supervised causal disentanglement that circumvents the need for\nexplicit target labeling and effectively disentangles input features into\ninvariant representations of hate. Empirical validation across platforms two\nwith target labels and two without positions HATE WATCH as a novel method in\ncross platform hate speech detection with superior performance. HATE WATCH\nadvances scalable content moderation techniques towards developing safer online\ncommunities.\n","authors":["Paras Sheth","Tharindu Kumarage","Raha Moraffah","Aman Chadha","Huan Liu"],"pdf_url":"https://arxiv.org/pdf/2404.11036v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07922v4","updated":"2024-04-17T03:23:33Z","published":"2024-04-11T17:09:28Z","title":"LaVy: Vietnamese Multimodal Large Language Model","summary":"  Large Language Models (LLMs) and Multimodal Large language models (MLLMs)\nhave taken the world by storm with impressive abilities in complex reasoning\nand linguistic comprehension. Meanwhile there are plethora of works related to\nVietnamese Large Language Models, the lack of high-quality resources in\nmultimodality limits the progress of Vietnamese MLLMs. In this paper, we\npioneer in address this by introducing LaVy, a state-of-the-art Vietnamese\nMLLM, and we also introduce LaVy-Bench benchmark designated for evaluating\nMLLMs's understanding on Vietnamese visual language tasks. Our project is\npublic at https://github.com/baochi0212/LaVy\n","authors":["Chi Tran","Huong Le Thanh"],"pdf_url":"https://arxiv.org/pdf/2404.07922v4.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2402.17564v2","updated":"2024-04-17T03:17:58Z","published":"2024-02-27T15:05:32Z","title":"Unleashing the Potential of Large Language Models as Prompt Optimizers:\n  An Analogical Analysis with Gradient-based Model Optimizers","summary":"  Automatic prompt optimization is an important approach to improving the\nperformance of large language models (LLMs). Recent research demonstrates the\npotential of using LLMs as prompt optimizers, which can generate improved task\nprompts via iterative refinement. In this paper, we propose a novel perspective\nto investigate the design of LLM-based prompt optimizers, by drawing an analogy\nwith gradient-based model optimizers. To connect these two approaches, we\nidentify two pivotal factors in model parameter learning: update direction and\nupdate method. Focused on the two aspects, we borrow the theoretical framework\nand learning methods from gradient-based optimization to design improved\nstrategies for LLM-based prompt optimizers. By systematically analyzing a rich\nset of improvement strategies, we further develop a capable Gradient-inspired\nLLM-based Prompt Optimizer called GPO. At each step, it first retrieves\nrelevant prompts from the optimization trajectory as the update direction.\nThen, it utilizes the generation-based refinement strategy to perform the\nupdate, while controlling the edit distance through a cosine-based decay\nstrategy. Extensive experiments demonstrate the effectiveness and efficiency of\nGPO. In particular, GPO brings an additional improvement of up to 56.8% on\nBig-Bench Hard and 55.3% on MMLU compared to baseline methods.\n","authors":["Xinyu Tang","Xiaolei Wang","Wayne Xin Zhao","Siyuan Lu","Yaliang Li","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2402.17564v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11023v1","updated":"2024-04-17T02:57:42Z","published":"2024-04-17T02:57:42Z","title":"Advancing Social Intelligence in AI Agents: Technical Challenges and\n  Open Questions","summary":"  Building socially-intelligent AI agents (Social-AI) is a multidisciplinary,\nmultimodal research goal that involves creating agents that can sense,\nperceive, reason about, learn from, and respond to affect, behavior, and\ncognition of other agents (human or artificial). Progress towards Social-AI has\naccelerated in the past decade across several computing communities, including\nnatural language processing, machine learning, robotics, human-machine\ninteraction, computer vision, and speech. Natural language processing, in\nparticular, has been prominent in Social-AI research, as language plays a key\nrole in constructing the social world. In this position paper, we identify a\nset of underlying technical challenges and open questions for researchers\nacross computing communities to advance Social-AI. We anchor our discussion in\nthe context of social intelligence concepts and prior progress in Social-AI\nresearch.\n","authors":["Leena Mathur","Paul Pu Liang","Louis-Philippe Morency"],"pdf_url":"https://arxiv.org/pdf/2404.11023v1.pdf","comment":"Position Paper, Under Review, 19 pages, 2 figures"},{"id":"http://arxiv.org/abs/2404.11018v1","updated":"2024-04-17T02:49:26Z","published":"2024-04-17T02:49:26Z","title":"Many-Shot In-Context Learning","summary":"  Large language models (LLMs) excel at few-shot in-context learning (ICL) --\nlearning from a few examples provided in context at inference, without any\nweight updates. Newly expanded context windows allow us to investigate ICL with\nhundreds or thousands of examples -- the many-shot regime. Going from few-shot\nto many-shot, we observe significant performance gains across a wide variety of\ngenerative and discriminative tasks. While promising, many-shot ICL can be\nbottlenecked by the available amount of human-generated examples. To mitigate\nthis limitation, we explore two new settings: Reinforced and Unsupervised ICL.\nReinforced ICL uses model-generated chain-of-thought rationales in place of\nhuman examples. Unsupervised ICL removes rationales from the prompt altogether,\nand prompts the model only with domain-specific questions. We find that both\nReinforced and Unsupervised ICL can be quite effective in the many-shot regime,\nparticularly on complex reasoning tasks. Finally, we demonstrate that, unlike\nfew-shot learning, many-shot learning is effective at overriding pretraining\nbiases and can learn high-dimensional functions with numerical inputs. Our\nanalysis also reveals the limitations of next-token prediction loss as an\nindicator of downstream ICL performance.\n","authors":["Rishabh Agarwal","Avi Singh","Lei M. Zhang","Bernd Bohnet","Stephanie Chan","Ankesh Anand","Zaheer Abbas","Azade Nova","John D. Co-Reyes","Eric Chu","Feryal Behbahani","Aleksandra Faust","Hugo Larochelle"],"pdf_url":"https://arxiv.org/pdf/2404.11018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14891v4","updated":"2024-04-17T02:23:29Z","published":"2024-02-22T12:36:31Z","title":"LLMBind: A Unified Modality-Task Integration Framework","summary":"  In the multi-modal domain, the dependence of various models on specific input\nformats leads to user confusion and hinders progress. To address this\nchallenge, we introduce \\textbf{LLMBind}, a novel framework designed to unify a\ndiverse array of multi-modal tasks. By harnessing a Mixture-of-Experts (MoE)\nLarge Language Model (LLM), LLMBind processes multi-modal inputs and generates\ntask-specific tokens, enabling the invocation of corresponding models to\naccomplish tasks. This unique approach empowers LLMBind to interpret inputs and\ngenerate outputs across various modalities, including image, text, video, and\naudio. Furthermore, we have constructed an interaction dataset comprising 400k\ninstructions, which unlocks the ability of LLMBind for interactive visual\ngeneration and editing tasks. Extensive experimentation demonstrates that\nLLMBind achieves very superior performance across diverse tasks and outperforms\nexisting models in user evaluations conducted in real-world scenarios.\nMoreover, the adaptability of LLMBind allows for seamless integration with the\nlatest models and extension to new modality tasks, highlighting its potential\nto serve as a unified AI agent for modeling universal modalities.\n","authors":["Bin Zhu","Munan Ning","Peng Jin","Bin Lin","Jinfa Huang","Qi Song","Junwu Zhang","Zhenyu Tang","Mingjun Pan","Xing Zhou","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2402.14891v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10981v1","updated":"2024-04-17T01:27:42Z","published":"2024-04-17T01:27:42Z","title":"A Survey on Retrieval-Augmented Text Generation for Large Language\n  Models","summary":"  Retrieval-Augmented Generation (RAG) merges retrieval methods with deep\nlearning advancements to address the static limitations of large language\nmodels (LLMs) by enabling the dynamic integration of up-to-date external\ninformation. This methodology, focusing primarily on the text domain, provides\na cost-effective solution to the generation of plausible but incorrect\nresponses by LLMs, thereby enhancing the accuracy and reliability of their\noutputs through the use of real-world data. As RAG grows in complexity and\nincorporates multiple concepts that can influence its performance, this paper\norganizes the RAG paradigm into four categories: pre-retrieval, retrieval,\npost-retrieval, and generation, offering a detailed perspective from the\nretrieval viewpoint. It outlines RAG's evolution and discusses the field's\nprogression through the analysis of significant studies. Additionally, the\npaper introduces evaluation methods for RAG, addressing the challenges faced\nand proposing future research directions. By offering an organized framework\nand categorization, the study aims to consolidate existing research on RAG,\nclarify its technological underpinnings, and highlight its potential to broaden\nthe adaptability and applications of LLMs.\n","authors":["Yizheng Huang","Jimmy Huang"],"pdf_url":"https://arxiv.org/pdf/2404.10981v1.pdf","comment":"Ongoing work"},{"id":"http://arxiv.org/abs/2404.10975v1","updated":"2024-04-17T01:13:04Z","published":"2024-04-17T01:13:04Z","title":"Procedural Dilemma Generation for Evaluating Moral Reasoning in Humans\n  and Language Models","summary":"  As AI systems like language models are increasingly integrated into\ndecision-making processes affecting people's lives, it's critical to ensure\nthat these systems have sound moral reasoning. To test whether they do, we need\nto develop systematic evaluations. We provide a framework that uses a language\nmodel to translate causal graphs that capture key aspects of moral dilemmas\ninto prompt templates. With this framework, we procedurally generated a large\nand diverse set of moral dilemmas -- the OffTheRails benchmark -- consisting of\n50 scenarios and 400 unique test items. We collected moral permissibility and\nintention judgments from human participants for a subset of our items and\ncompared these judgments to those from two language models (GPT-4 and Claude-2)\nacross eight conditions. We find that moral dilemmas in which the harm is a\nnecessary means (as compared to a side effect) resulted in lower permissibility\nand higher intention ratings for both participants and language models. The\nsame pattern was observed for evitable versus inevitable harmful outcomes.\nHowever, there was no clear effect of whether the harm resulted from an agent's\naction versus from having omitted to act. We discuss limitations of our prompt\ngeneration pipeline and opportunities for improving scenarios to increase the\nstrength of experimental effects.\n","authors":["Jan-Philipp Fr√§nken","Kanishk Gandhi","Tori Qiu","Ayesha Khawaja","Noah D. Goodman","Tobias Gerstenberg"],"pdf_url":"https://arxiv.org/pdf/2404.10975v1.pdf","comment":"CogSci 2024"},{"id":"http://arxiv.org/abs/2402.04437v3","updated":"2024-04-17T00:24:57Z","published":"2024-02-06T22:15:09Z","title":"Structured Entity Extraction Using Large Language Models","summary":"  Recent advances in machine learning have significantly impacted the field of\ninformation extraction, with Large Language Models (LLMs) playing a pivotal\nrole in extracting structured information from unstructured text. Prior works\ntypically represent information extraction as triplet-centric and use classical\nmetrics such as precision and recall for evaluation. We reformulate the task to\nbe entity-centric, enabling the use of diverse metrics that can provide more\ninsights from various perspectives. We contribute to the field by introducing\nStructured Entity Extraction (SEE) and proposing the Approximate Entity Set\nOverlaP (AESOP) metric, designed to appropriately assess model performance.\nLater, we introduce a new model that harnesses the power of LLMs for enhanced\neffectiveness and efficiency by decomposing the extraction task into multiple\nstages. Quantitative and human side-by-side evaluations confirm that our model\noutperforms baselines, offering promising directions for future advancements in\nstructured entity extraction.\n","authors":["Haolun Wu","Ye Yuan","Liana Mikaelyan","Alexander Meulemans","Xue Liu","James Hensman","Bhaskar Mitra"],"pdf_url":"https://arxiv.org/pdf/2402.04437v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11793v1","updated":"2024-04-17T23:00:29Z","published":"2024-04-17T23:00:29Z","title":"Enhancing Argument Summarization: Prioritizing Exhaustiveness in Key\n  Point Generation and Introducing an Automatic Coverage Evaluation Metric","summary":"  The proliferation of social media platforms has given rise to the amount of\nonline debates and arguments. Consequently, the need for automatic\nsummarization methods for such debates is imperative, however this area of\nsummarization is rather understudied. The Key Point Analysis (KPA) task\nformulates argument summarization as representing the summary of a large\ncollection of arguments in the form of concise sentences in bullet-style\nformat, called key points. A sub-task of KPA, called Key Point Generation\n(KPG), focuses on generating these key points given the arguments. This paper\nintroduces a novel extractive approach for key point generation, that\noutperforms previous state-of-the-art methods for the task. Our method utilizes\nan extractive clustering based approach that offers concise, high quality\ngenerated key points with higher coverage of reference summaries, and less\nredundant outputs. In addition, we show that the existing evaluation metrics\nfor summarization such as ROUGE are incapable of differentiating between\ngenerated key points of different qualities. To this end, we propose a new\nevaluation metric for assessing the generated key points by their coverage. Our\ncode can be accessed online.\n","authors":["Mohammad Khosravani","Chenyang Huang","Amine Trabelsi"],"pdf_url":"https://arxiv.org/pdf/2404.11793v1.pdf","comment":"NAACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2404.11782v1","updated":"2024-04-17T22:12:41Z","published":"2024-04-17T22:12:41Z","title":"REQUAL-LM: Reliability and Equity through Aggregation in Large Language\n  Models","summary":"  The extensive scope of large language models (LLMs) across various domains\nunderscores the critical importance of responsibility in their application,\nbeyond natural language processing. In particular, the randomized nature of\nLLMs, coupled with inherent biases and historical stereotypes in data, raises\ncritical concerns regarding reliability and equity. Addressing these challenges\nare necessary before using LLMs for applications with societal impact. Towards\naddressing this gap, we introduce REQUAL-LM, a novel method for finding\nreliable and equitable LLM outputs through aggregation. Specifically, we\ndevelop a Monte Carlo method based on repeated sampling to find a reliable\noutput close to the mean of the underlying distribution of possible outputs. We\nformally define the terms such as reliability and bias, and design an\nequity-aware aggregation to minimize harmful bias while finding a highly\nreliable output. REQUAL-LM does not require specialized hardware, does not\nimpose a significant computing load, and uses LLMs as a blackbox. This design\nchoice enables seamless scalability alongside the rapid advancement of LLM\ntechnologies. Our system does not require retraining the LLMs, which makes it\ndeployment ready and easy to adapt. Our comprehensive experiments using various\ntasks and datasets demonstrate that REQUAL- LM effectively mitigates bias and\nselects a more equitable response, specifically the outputs that properly\nrepresents minority groups.\n","authors":["Sana Ebrahimi","Nima Shahbazi","Abolfazl Asudeh"],"pdf_url":"https://arxiv.org/pdf/2404.11782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11757v1","updated":"2024-04-17T21:27:33Z","published":"2024-04-17T21:27:33Z","title":"Language Models Still Struggle to Zero-shot Reason about Time Series","summary":"  Time series are critical for decision-making in fields like finance and\nhealthcare. Their importance has driven a recent influx of works passing time\nseries into language models, leading to non-trivial forecasting on some\ndatasets. But it remains unknown whether non-trivial forecasting implies that\nlanguage models can reason about time series. To address this gap, we generate\na first-of-its-kind evaluation framework for time series reasoning, including\nformal tasks and a corresponding dataset of multi-scale time series paired with\ntext captions across ten domains. Using these data, we probe whether language\nmodels achieve three forms of reasoning: (1) Etiological Reasoning - given an\ninput time series, can the language model identify the scenario that most\nlikely created it? (2) Question Answering - can a language model answer factual\nquestions about time series? (3) Context-Aided Forecasting - does highly\nrelevant textual context improve a language model's time series forecasts?\n  We find that otherwise highly-capable language models demonstrate\nsurprisingly limited time series reasoning: they score marginally above random\non etiological and question answering tasks (up to 30 percentage points worse\nthan humans) and show modest success in using context to improve forecasting.\nThese weakness showcase that time series reasoning is an impactful, yet deeply\nunderdeveloped direction for language model research. We also make our datasets\nand code public at to support further research in this direction at\nhttps://github.com/behavioral-data/TSandLanguage\n","authors":["Mike A. Merrill","Mingtian Tan","Vinayak Gupta","Tom Hartvigsen","Tim Althoff"],"pdf_url":"https://arxiv.org/pdf/2404.11757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11752v1","updated":"2024-04-17T21:09:13Z","published":"2024-04-17T21:09:13Z","title":"Mapping Violence: Developing an Extensive Framework to Build a Bangla\n  Sectarian Expression Dataset from Social Media Interactions","summary":"  Communal violence in online forums has become extremely prevalent in South\nAsia, where many communities of different cultures coexist and share resources.\nThese societies exhibit a phenomenon characterized by strong bonds within their\nown groups and animosity towards others, leading to conflicts that frequently\nescalate into violent confrontations. To address this issue, we have developed\nthe first comprehensive framework for the automatic detection of communal\nviolence markers in online Bangla content accompanying the largest collection\n(13K raw sentences) of social media interactions that fall under the definition\nof four major violence class and their 16 coarse expressions. Our workflow\nintroduces a 7-step expert annotation process incorporating insights from\nsocial scientists, linguists, and psychologists. By presenting data statistics\nand benchmarking performance using this dataset, we have determined that, aside\nfrom the category of Non-communal violence, Religio-communal violence is\nparticularly pervasive in Bangla text. Moreover, we have substantiated the\neffectiveness of fine-tuning language models in identifying violent comments by\nconducting preliminary benchmarking on the state-of-the-art Bangla deep\nlearning model.\n","authors":["Nazia Tasnim","Sujan Sen Gupta","Md. Istiak Hossain Shihab","Fatiha Islam Juee","Arunima Tahsin","Pritom Ghum","Kanij Fatema","Marshia Haque","Wasema Farzana","Prionti Nasir","Ashique KhudaBukhsh","Farig Sadeque","Asif Sushmit"],"pdf_url":"https://arxiv.org/pdf/2404.11752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11730v1","updated":"2024-04-17T20:31:05Z","published":"2024-04-17T20:31:05Z","title":"Missed Connections: Lateral Thinking Puzzles for Large Language Models","summary":"  The Connections puzzle published each day by the New York Times tasks players\nwith dividing a bank of sixteen words into four groups of four words that each\nrelate to a common theme. Solving the puzzle requires both common linguistic\nknowledge (i.e. definitions and typical usage) as well as, in many cases,\nlateral or abstract thinking. This is because the four categories ascend in\ncomplexity, with the most challenging category often requiring thinking about\nwords in uncommon ways or as parts of larger phrases. We investigate the\ncapacity for automated AI systems to play Connections and explore the game's\npotential as an automated benchmark for abstract reasoning and a way to measure\nthe semantic information encoded by data-driven linguistic systems. In\nparticular, we study both a sentence-embedding baseline and modern large\nlanguage models (LLMs). We report their accuracy on the task, measure the\nimpacts of chain-of-thought prompting, and discuss their failure modes.\nOverall, we find that the Connections task is challenging yet feasible, and a\nstrong test-bed for future work.\n","authors":["Graham Todd","Tim Merino","Sam Earle","Julian Togelius"],"pdf_url":"https://arxiv.org/pdf/2404.11730v1.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2404.11726v1","updated":"2024-04-17T20:24:41Z","published":"2024-04-17T20:24:41Z","title":"Investigating Gender Bias in Turkish Language Models","summary":"  Language models are trained mostly on Web data, which often contains social\nstereotypes and biases that the models can inherit. This has potentially\nnegative consequences, as models can amplify these biases in downstream tasks\nor applications. However, prior research has primarily focused on the English\nlanguage, especially in the context of gender bias. In particular,\ngrammatically gender-neutral languages such as Turkish are underexplored\ndespite representing different linguistic properties to language models with\npossibly different effects on biases. In this paper, we fill this research gap\nand investigate the significance of gender bias in Turkish language models. We\nbuild upon existing bias evaluation frameworks and extend them to the Turkish\nlanguage by translating existing English tests and creating new ones designed\nto measure gender bias in the context of T\\\"urkiye. Specifically, we also\nevaluate Turkish language models for their embedded ethnic bias toward Kurdish\npeople. Based on the experimental results, we attribute possible biases to\ndifferent model characteristics such as the model size, their multilingualism,\nand the training corpora. We make the Turkish gender bias dataset publicly\navailable.\n","authors":["Orhun Caglidil","Malte Ostendorff","Georg Rehm"],"pdf_url":"https://arxiv.org/pdf/2404.11726v1.pdf","comment":"arXiv admin note: text overlap with arXiv:1903.10561 by other authors"},{"id":"http://arxiv.org/abs/2404.11717v1","updated":"2024-04-17T20:11:32Z","published":"2024-04-17T20:11:32Z","title":"How often are errors in natural language reasoning due to paraphrastic\n  variability?","summary":"  Large language models have been shown to behave inconsistently in response to\nmeaning-preserving paraphrastic inputs. At the same time, researchers evaluate\nthe knowledge and reasoning abilities of these models with test evaluations\nthat do not disaggregate the effect of paraphrastic variability on performance.\nWe propose a metric for evaluating the paraphrastic consistency of natural\nlanguage reasoning models based on the probability of a model achieving the\nsame correctness on two paraphrases of the same problem. We mathematically\nconnect this metric to the proportion of a model's variance in correctness\nattributable to paraphrasing. To estimate paraphrastic consistency, we collect\nParaNLU, a dataset of 7,782 human-written and validated paraphrased reasoning\nproblems constructed on top of existing benchmark datasets for defeasible and\nabductive natural language inference. Using ParaNLU, we measure the\nparaphrastic consistency of several model classes and show that consistency\ndramatically increases with pretraining but not finetuning. All models tested\nexhibited room for improvement in paraphrastic consistency.\n","authors":["Neha Srikanth","Marine Carpuat","Rachel Rudinger"],"pdf_url":"https://arxiv.org/pdf/2404.11717v1.pdf","comment":"accepted to TACL 2024 (pre-MIT Press publication version)"},{"id":"http://arxiv.org/abs/2403.18933v5","updated":"2024-04-17T19:49:12Z","published":"2024-03-27T18:30:26Z","title":"SemEval-2024 Task 1: Semantic Textual Relatedness for African and Asian\n  Languages","summary":"  We present the first shared task on Semantic Textual Relatedness (STR). While\nearlier shared tasks primarily focused on semantic similarity, we instead\ninvestigate the broader phenomenon of semantic relatedness across 14 languages:\nAfrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian,\nKinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic, Punjabi,\nSpanish, and Telugu. These languages originate from five distinct language\nfamilies and are predominantly spoken in Africa and Asia -- regions\ncharacterised by the relatively limited availability of NLP resources. Each\ninstance in the datasets is a sentence pair associated with a score that\nrepresents the degree of semantic textual relatedness between the two\nsentences. Participating systems were asked to rank sentence pairs by their\ncloseness in meaning (i.e., their degree of semantic relatedness) in the 14\nlanguages in three main tracks: (a) supervised, (b) unsupervised, and (c)\ncrosslingual. The task attracted 163 participants. We received 70 submissions\nin total (across all tasks) from 51 different teams, and 38 system description\npapers. We report on the best-performing systems as well as the most common and\nthe most effective approaches for the three different tracks.\n","authors":["Nedjma Ousidhoum","Shamsuddeen Hassan Muhammad","Mohamed Abdalla","Idris Abdulmumin","Ibrahim Said Ahmad","Sanchit Ahuja","Alham Fikri Aji","Vladimir Araujo","Meriem Beloucif","Christine De Kock","Oumaima Hourrane","Manish Shrivastava","Thamar Solorio","Nirmal Surange","Krishnapriya Vishnubhotla","Seid Muhie Yimam","Saif M. Mohammad"],"pdf_url":"https://arxiv.org/pdf/2403.18933v5.pdf","comment":"SemEval 2024 Task Description Paper. arXiv admin note: text overlap\n  with arXiv:2402.08638"},{"id":"http://arxiv.org/abs/2403.00199v2","updated":"2024-04-17T18:53:55Z","published":"2024-03-01T00:08:20Z","title":"Improving Socratic Question Generation using Data Augmentation and\n  Preference Optimization","summary":"  The Socratic method is a way of guiding students toward solving a problem\nindependently without directly revealing the solution to the problem. Although\nthis method has been shown to significantly improve student learning outcomes,\nit remains a complex labor-intensive task for instructors. Large language\nmodels (LLMs) can be used to augment human effort by automatically generating\nSocratic questions for students. However, existing methods that involve\nprompting these LLMs sometimes produce invalid outputs, e.g., those that\ndirectly reveal the solution to the problem or provide irrelevant or premature\nquestions. To alleviate this problem, inspired by reinforcement learning with\nAI feedback (RLAIF), we first propose a data augmentation method to enrich\nexisting Socratic questioning datasets with questions that are invalid in\nspecific ways. Next, we propose a method to optimize open-source LLMs such as\nLLama 2 to prefer ground-truth questions over generated invalid ones, using\ndirect preference optimization (DPO). Our experiments on a Socratic questions\ndataset for student code debugging show that a DPO-optimized 7B LLama 2 model\ncan effectively avoid generating invalid questions, and as a result,\noutperforms existing state-of-the-art prompting methods.\n","authors":["Nischal Ashok Kumar","Andrew Lan"],"pdf_url":"https://arxiv.org/pdf/2403.00199v2.pdf","comment":"To appear at the 19th BEA Workshop co-located with NAACL-2024"},{"id":"http://arxiv.org/abs/2404.11691v1","updated":"2024-04-17T18:42:36Z","published":"2024-04-17T18:42:36Z","title":"Improvement in Semantic Address Matching using Natural Language\n  Processing","summary":"  Address matching is an important task for many businesses especially delivery\nand take out companies which help them to take out a certain address from their\ndata warehouse. Existing solution uses similarity of strings, and edit distance\nalgorithms to find out the similar addresses from the address database, but\nthese algorithms could not work effectively with redundant, unstructured, or\nincomplete address data. This paper discuss semantic Address matching\ntechnique, by which we can find out a particular address from a list of\npossible addresses. We have also reviewed existing practices and their\nshortcoming. Semantic address matching is an essentially NLP task in the field\nof deep learning. Through this technique We have the ability to triumph the\ndrawbacks of existing methods like redundant or abbreviated data problems. The\nsolution uses the OCR on invoices to extract the address and create the data\npool of addresses. Then this data is fed to the algorithm BM-25 for scoring the\nbest matching entries. Then to observe the best result, this will pass through\nBERT for giving the best possible result from the similar queries. Our\ninvestigation exhibits that our methodology enormously improves both accuracy\nand review of cutting-edge technology existing techniques.\n","authors":["Vansh Gupta","Mohit Gupta","Jai Garg","Nitesh Garg"],"pdf_url":"https://arxiv.org/pdf/2404.11691v1.pdf","comment":"5 pages, 7 tables, 2021 2nd International Conference for Emerging\n  Technology (INCET)"},{"id":"http://arxiv.org/abs/2404.11682v1","updated":"2024-04-17T18:27:59Z","published":"2024-04-17T18:27:59Z","title":"How Well Can You Articulate that Idea? Insights from Automated Formative\n  Assessment","summary":"  Automated methods are becoming increasingly integrated into studies of\nformative feedback on students' science explanation writing. Most of this work,\nhowever, addresses students' responses to short answer questions. We\ninvestigate automated feedback on students' science explanation essays, where\nstudents must articulate multiple ideas. Feedback is based on a rubric that\nidentifies the main ideas students are prompted to include in explanatory\nessays about the physics of energy and mass, given their experiments with a\nsimulated roller coaster. We have found that students generally improve on\nrevised versions of their essays. Here, however, we focus on two factors that\naffect the accuracy of the automated feedback. First, we find that the main\nideas in the rubric differ with respect to how much freedom they afford in\nexplanations of the idea, thus explanation of a natural law is relatively\nconstrained. Students have more freedom in how they explain complex relations\nthey observe in their roller coasters, such as transfer of different forms of\nenergy. Second, by tracing the automated decision process, we can diagnose when\na student's statement lacks sufficient clarity for the automated tool to\nassociate it more strongly with one of the main ideas above all others. This in\nturn provides an opportunity for teachers and peers to help students reflect on\nhow to state their ideas more clearly.\n","authors":["Mahsa Sheikhi Karizaki","Dana Gnesdilow","Sadhana Puntambekar","Rebecca J. Passonneau"],"pdf_url":"https://arxiv.org/pdf/2404.11682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10688v4","updated":"2024-04-17T18:24:45Z","published":"2023-10-14T17:01:37Z","title":"A decoder-only foundation model for time-series forecasting","summary":"  Motivated by recent advances in large language models for Natural Language\nProcessing (NLP), we design a time-series foundation model for forecasting\nwhose out-of-the-box zero-shot performance on a variety of public datasets\ncomes close to the accuracy of state-of-the-art supervised forecasting models\nfor each individual dataset. Our model is based on pretraining a\npatched-decoder style attention model on a large time-series corpus, and can\nwork well across different forecasting history lengths, prediction lengths and\ntemporal granularities.\n","authors":["Abhimanyu Das","Weihao Kong","Rajat Sen","Yichen Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.10688v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.07877v2","updated":"2024-04-17T18:15:22Z","published":"2021-09-16T11:16:43Z","title":"MFE-NER: Multi-feature Fusion Embedding for Chinese Named Entity\n  Recognition","summary":"  In Chinese Named Entity Recognition, character substitution is a complicated\nlinguistic phenomenon. Some Chinese characters are quite similar as they share\nthe same components or have similar pronunciations. People replace characters\nin a named entity with similar characters to generate a new collocation but\nreferring to the same object. As a result, it always leads to unrecognizable or\nmislabeling errors in the NER task. In this paper, we propose a lightweight\nmethod, MFE-NER, which fuses glyph and phonetic features, to help pre-trained\nlanguage models handle the character substitution problem in the NER task with\nlimited extra cost. Basically, in the glyph domain, we disassemble Chinese\ncharacters into Five-Stroke components to represent structure features. In the\nphonetic domain, an improved phonetic system is proposed in our work, making it\nreasonable to describe phonetic similarity among Chinese characters.\nExperiments demonstrate that our method performs especially well in detecting\ncharacter substitutions while slightly improving the overall performance of\nChinese NER.\n","authors":["Jiatong Li","Kui Meng"],"pdf_url":"https://arxiv.org/pdf/2109.07877v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11672v1","updated":"2024-04-17T18:13:16Z","published":"2024-04-17T18:13:16Z","title":"MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory","summary":"  While current large language models (LLMs) demonstrate some capabilities in\nknowledge-intensive tasks, they are limited by relying on their parameters as\nan implicit storage mechanism. As a result, they struggle with infrequent\nknowledge and temporal degradation. In addition, the uninterpretable nature of\nparametric memorization makes it challenging to understand and prevent\nhallucination. Parametric memory pools and model editing are only partial\nsolutions. Retrieval Augmented Generation (RAG) $\\unicode{x2013}$ though\nnon-parametric $\\unicode{x2013}$ has its own limitations: it lacks structure,\ncomplicates interpretability and makes it hard to effectively manage stored\nknowledge. In this paper, we introduce MemLLM, a novel method of enhancing LLMs\nby integrating a structured and explicit read-and-write memory module. MemLLM\ntackles the aforementioned challenges by enabling dynamic interaction with the\nmemory and improving the LLM's capabilities in using stored knowledge. Our\nexperiments indicate that MemLLM enhances the LLM's performance and\ninterpretability, in language modeling in general and knowledge-intensive tasks\nin particular. We see MemLLM as an important step towards making LLMs more\ngrounded and factual through memory augmentation.\n","authors":["Ali Modarressi","Abdullatif K√∂ksal","Ayyoob Imani","Mohsen Fayyaz","Hinrich Sch√ºtze"],"pdf_url":"https://arxiv.org/pdf/2404.11672v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2404.11615v1","updated":"2024-04-17T17:59:59Z","published":"2024-04-17T17:59:59Z","title":"Factorized Diffusion: Perceptual Illusions by Noise Decomposition","summary":"  Given a factorization of an image into a sum of linear components, we present\na zero-shot method to control each individual component through diffusion model\nsampling. For example, we can decompose an image into low and high spatial\nfrequencies and condition these components on different text prompts. This\nproduces hybrid images, which change appearance depending on viewing distance.\nBy decomposing an image into three frequency subbands, we can generate hybrid\nimages with three prompts. We also use a decomposition into grayscale and color\ncomponents to produce images whose appearance changes when they are viewed in\ngrayscale, a phenomena that naturally occurs under dim lighting. And we explore\na decomposition by a motion blur kernel, which produces images that change\nappearance under motion blurring. Our method works by denoising with a\ncomposite noise estimate, built from the components of noise estimates\nconditioned on different prompts. We also show that for certain decompositions,\nour method recovers prior approaches to compositional generation and spatial\ncontrol. Finally, we show that we can extend our approach to generate hybrid\nimages from real images. We do this by holding one component fixed and\ngenerating the remaining components, effectively solving an inverse problem.\n","authors":["Daniel Geng","Inbum Park","Andrew Owens"],"pdf_url":"https://arxiv.org/pdf/2404.11615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11614v1","updated":"2024-04-17T17:59:55Z","published":"2024-04-17T17:59:55Z","title":"Dynamic Typography: Bringing Words to Life","summary":"  Text animation serves as an expressive medium, transforming static\ncommunication into dynamic experiences by infusing words with motion to evoke\nemotions, emphasize meanings, and construct compelling narratives. Crafting\nanimations that are semantically aware poses significant challenges, demanding\nexpertise in graphic design and animation. We present an automated text\nanimation scheme, termed \"Dynamic Typography\", which combines two challenging\ntasks. It deforms letters to convey semantic meaning and infuses them with\nvibrant movements based on user prompts. Our technique harnesses vector\ngraphics representations and an end-to-end optimization-based framework. This\nframework employs neural displacement fields to convert letters into base\nshapes and applies per-frame motion, encouraging coherence with the intended\ntextual concept. Shape preservation techniques and perceptual loss\nregularization are employed to maintain legibility and structural integrity\nthroughout the animation process. We demonstrate the generalizability of our\napproach across various text-to-video models and highlight the superiority of\nour end-to-end methodology over baseline methods, which might comprise separate\ntasks. Through quantitative and qualitative evaluations, we demonstrate the\neffectiveness of our framework in generating coherent text animations that\nfaithfully interpret user prompts while maintaining readability. Our code is\navailable at: https://animate-your-word.github.io/demo/.\n","authors":["Zichen Liu","Yihao Meng","Hao Ouyang","Yue Yu","Bolin Zhao","Daniel Cohen-Or","Huamin Qu"],"pdf_url":"https://arxiv.org/pdf/2404.11614v1.pdf","comment":"Our demo page is available at:\n  https://animate-your-word.github.io/demo/"},{"id":"http://arxiv.org/abs/2404.11613v1","updated":"2024-04-17T17:59:53Z","published":"2024-04-17T17:59:53Z","title":"InFusion: Inpainting 3D Gaussians via Learning Depth Completion from\n  Diffusion Prior","summary":"  3D Gaussians have recently emerged as an efficient representation for novel\nview synthesis. This work studies its editability with a particular focus on\nthe inpainting task, which aims to supplement an incomplete set of 3D Gaussians\nwith additional points for visually harmonious rendering. Compared to 2D\ninpainting, the crux of inpainting 3D Gaussians is to figure out the\nrendering-relevant properties of the introduced points, whose optimization\nlargely benefits from their initial 3D positions. To this end, we propose to\nguide the point initialization with an image-conditioned depth completion\nmodel, which learns to directly restore the depth map based on the observed\nimage. Such a design allows our model to fill in depth values at an aligned\nscale with the original depth, and also to harness strong generalizability from\nlargescale diffusion prior. Thanks to the more accurate depth completion, our\napproach, dubbed InFusion, surpasses existing alternatives with sufficiently\nbetter fidelity and efficiency under various complex scenarios. We further\ndemonstrate the effectiveness of InFusion with several practical applications,\nsuch as inpainting with user-specific texture or with novel object insertion.\n","authors":["Zhiheng Liu","Hao Ouyang","Qiuyu Wang","Ka Leong Cheng","Jie Xiao","Kai Zhu","Nan Xue","Yu Liu","Yujun Shen","Yang Cao"],"pdf_url":"https://arxiv.org/pdf/2404.11613v1.pdf","comment":"Project page: https://johanan528.github.io/Infusion"},{"id":"http://arxiv.org/abs/2311.16278v3","updated":"2024-04-17T17:58:59Z","published":"2023-11-27T19:34:04Z","title":"VehicleGAN: Pair-flexible Pose Guided Image Synthesis for Vehicle\n  Re-identification","summary":"  Vehicle Re-identification (Re-ID) has been broadly studied in the last\ndecade; however, the different camera view angle leading to confused\ndiscrimination in the feature subspace for the vehicles of various poses, is\nstill challenging for the Vehicle Re-ID models in the real world. To promote\nthe Vehicle Re-ID models, this paper proposes to synthesize a large number of\nvehicle images in the target pose, whose idea is to project the vehicles of\ndiverse poses into the unified target pose so as to enhance feature\ndiscrimination. Considering that the paired data of the same vehicles in\ndifferent traffic surveillance cameras might be not available in the real\nworld, we propose the first Pair-flexible Pose Guided Image Synthesis method\nfor Vehicle Re-ID, named as VehicleGAN in this paper, which works for both\nsupervised and unsupervised settings without the knowledge of geometric 3D\nmodels. Because of the feature distribution difference between real and\nsynthetic data, simply training a traditional metric learning based Re-ID model\nwith data-level fusion (i.e., data augmentation) is not satisfactory, therefore\nwe propose a new Joint Metric Learning (JML) via effective feature-level fusion\nfrom both real and synthetic data. Intensive experimental results on the public\nVeRi-776 and VehicleID datasets prove the accuracy and effectiveness of our\nproposed VehicleGAN and JML.\n","authors":["Baolu Li","Ping Liu","Lan Fu","Jinlong Li","Jianwu Fang","Zhigang Xu","Hongkai Yu"],"pdf_url":"https://arxiv.org/pdf/2311.16278v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11605v1","updated":"2024-04-17T17:54:49Z","published":"2024-04-17T17:54:49Z","title":"VG4D: Vision-Language Model Goes 4D Video Recognition","summary":"  Understanding the real world through point cloud video is a crucial aspect of\nrobotics and autonomous driving systems. However, prevailing methods for 4D\npoint cloud recognition have limitations due to sensor resolution, which leads\nto a lack of detailed information. Recent advances have shown that\nVision-Language Models (VLM) pre-trained on web-scale text-image datasets can\nlearn fine-grained visual concepts that can be transferred to various\ndownstream tasks. However, effectively integrating VLM into the domain of 4D\npoint clouds remains an unresolved problem. In this work, we propose the\nVision-Language Models Goes 4D (VG4D) framework to transfer VLM knowledge from\nvisual-text pre-trained models to a 4D point cloud network. Our approach\ninvolves aligning the 4D encoder's representation with a VLM to learn a shared\nvisual and text space from training on large-scale image-text pairs. By\ntransferring the knowledge of the VLM to the 4D encoder and combining the VLM,\nour VG4D achieves improved recognition performance. To enhance the 4D encoder,\nwe modernize the classic dynamic point cloud backbone and propose an improved\nversion of PSTNet, im-PSTNet, which can efficiently model point cloud videos.\nExperiments demonstrate that our method achieves state-of-the-art performance\nfor action recognition on both the NTU RGB+D 60 dataset and the NTU RGB+D 120\ndataset. Code is available at \\url{https://github.com/Shark0-0/VG4D}.\n","authors":["Zhichao Deng","Xiangtai Li","Xia Li","Yunhai Tong","Shen Zhao","Mengyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2404.11605v1.pdf","comment":"ICRA 2024"},{"id":"http://arxiv.org/abs/2112.06979v2","updated":"2024-04-17T17:50:54Z","published":"2021-12-13T19:25:16Z","title":"The Brain Tumor Sequence Registration (BraTS-Reg) Challenge:\n  Establishing Correspondence Between Pre-Operative and Follow-up MRI Scans of\n  Diffuse Glioma Patients","summary":"  Registration of longitudinal brain MRI scans containing pathologies is\nchallenging due to dramatic changes in tissue appearance. Although there has\nbeen progress in developing general-purpose medical image registration\ntechniques, they have not yet attained the requisite precision and reliability\nfor this task, highlighting its inherent complexity. Here we describe the Brain\nTumor Sequence Registration (BraTS-Reg) challenge, as the first public\nbenchmark environment for deformable registration algorithms focusing on\nestimating correspondences between pre-operative and follow-up scans of the\nsame patient diagnosed with a diffuse brain glioma. The BraTS-Reg data comprise\nde-identified multi-institutional multi-parametric MRI (mpMRI) scans, curated\nfor size and resolution according to a canonical anatomical template, and\ndivided into training, validation, and testing sets. Clinical experts annotated\nground truth (GT) landmark points of anatomical locations distinct across the\ntemporal domain. Quantitative evaluation and ranking were based on the Median\nEuclidean Error (MEE), Robustness, and the determinant of the Jacobian of the\ndisplacement field. The top-ranked methodologies yielded similar performance\nacross all evaluation metrics and shared several methodological commonalities,\nincluding pre-alignment, deep neural networks, inverse consistency analysis,\nand test-time instance optimization per-case basis as a post-processing step.\nThe top-ranked method attained the MEE at or below that of the inter-rater\nvariability for approximately 60% of the evaluated landmarks, underscoring the\nscope for further accuracy and robustness improvements, especially relative to\nhuman experts. The aim of BraTS-Reg is to continue to serve as an active\nresource for research, with the data and online evaluation tools accessible at\nhttps://bratsreg.github.io/.\n","authors":["Bhakti Baheti","Satrajit Chakrabarty","Hamed Akbari","Michel Bilello","Benedikt Wiestler","Julian Schwarting","Evan Calabrese","Jeffrey Rudie","Syed Abidi","Mina Mousa","Javier Villanueva-Meyer","Brandon K. K. Fields","Florian Kofler","Russell Takeshi Shinohara","Juan Eugenio Iglesias","Tony C. W. Mok","Albert C. S. Chung","Marek Wodzinski","Artur Jurgas","Niccolo Marini","Manfredo Atzori","Henning Muller","Christoph Grobroehmer","Hanna Siebert","Lasse Hansen","Mattias P. Heinrich","Luca Canalini","Jan Klein","Annika Gerken","Stefan Heldmann","Alessa Hering","Horst K. Hahn","Mingyuan Meng","Lei Bi","Dagan Feng","Jinman Kim","Ramy A. Zeineldin","Mohamed E. Karar","Franziska Mathis-Ullrich","Oliver Burgert","Javid Abderezaei","Aymeric Pionteck","Agamdeep Chopra","Mehmet Kurt","Kewei Yan","Yonghong Yan","Zhe Tang","Jianqiang Ma","Sahar Almahfouz Nasser","Nikhil Cherian Kurian","Mohit Meena","Saqib Shamsi","Amit Sethi","Nicholas J. Tustison","Brian B. Avants","Philip Cook","James C. Gee","Lin Tian","Hastings Greer","Marc Niethammer","Andrew Hoopes","Malte Hoffmann","Adrian V. Dalca","Stergios Christodoulidis","Theo Estiene","Maria Vakalopoulou","Nikos Paragios","Daniel S. Marcus","Christos Davatzikos","Aristeidis Sotiras","Bjoern Menze","Spyridon Bakas","Diana Waldmannstetter"],"pdf_url":"https://arxiv.org/pdf/2112.06979v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11599v1","updated":"2024-04-17T17:50:24Z","published":"2024-04-17T17:50:24Z","title":"Variational Bayesian Last Layers","summary":"  We introduce a deterministic variational formulation for training Bayesian\nlast layer neural networks. This yields a sampling-free, single-pass model and\nloss that effectively improves uncertainty estimation. Our variational Bayesian\nlast layer (VBLL) can be trained and evaluated with only quadratic complexity\nin last layer width, and is thus (nearly) computationally free to add to\nstandard architectures. We experimentally investigate VBLLs, and show that they\nimprove predictive accuracy, calibration, and out of distribution detection\nover baselines across both regression and classification. Finally, we\ninvestigate combining VBLL layers with variational Bayesian feature learning,\nyielding a lower variance collapsed variational inference method for Bayesian\nneural networks.\n","authors":["James Harrison","John Willes","Jasper Snoek"],"pdf_url":"https://arxiv.org/pdf/2404.11599v1.pdf","comment":"International Conference on Learning Representations (ICLR) 2024"},{"id":"http://arxiv.org/abs/2404.11593v1","updated":"2024-04-17T17:45:08Z","published":"2024-04-17T17:45:08Z","title":"IntrinsicAnything: Learning Diffusion Priors for Inverse Rendering Under\n  Unknown Illumination","summary":"  This paper aims to recover object materials from posed images captured under\nan unknown static lighting condition. Recent methods solve this task by\noptimizing material parameters through differentiable physically based\nrendering. However, due to the coupling between object geometry, materials, and\nenvironment lighting, there is inherent ambiguity during the inverse rendering\nprocess, preventing previous methods from obtaining accurate results. To\novercome this ill-posed problem, our key idea is to learn the material prior\nwith a generative model for regularizing the optimization process. We observe\nthat the general rendering equation can be split into diffuse and specular\nshading terms, and thus formulate the material prior as diffusion models of\nalbedo and specular. Thanks to this design, our model can be trained using the\nexisting abundant 3D object data, and naturally acts as a versatile tool to\nresolve the ambiguity when recovering material representations from RGB images.\nIn addition, we develop a coarse-to-fine training strategy that leverages\nestimated materials to guide diffusion models to satisfy multi-view consistent\nconstraints, leading to more stable and accurate results. Extensive experiments\non real-world and synthetic datasets demonstrate that our approach achieves\nstate-of-the-art performance on material recovery. The code will be available\nat https://zju3dv.github.io/IntrinsicAnything.\n","authors":["Xi Chen","Sida Peng","Dongchen Yang","Yuan Liu","Bowen Pan","Chengfei Lv","Xiaowei Zhou"],"pdf_url":"https://arxiv.org/pdf/2404.11593v1.pdf","comment":"Project page: https://zju3dv.github.io/IntrinsicAnything"},{"id":"http://arxiv.org/abs/2312.02255v2","updated":"2024-04-17T17:44:44Z","published":"2023-12-04T18:56:08Z","title":"Re-Nerfing: Improving Novel Views Synthesis through Novel Views\n  Synthesis","summary":"  Neural Radiance Fields (NeRFs) have shown remarkable novel view synthesis\ncapabilities even in large-scale, unbounded scenes, albeit requiring hundreds\nof views or introducing artifacts in sparser settings. Their optimization\nsuffers from shape-radiance ambiguities wherever only a small visual overlap is\navailable. This leads to erroneous scene geometry and artifacts. In this paper,\nwe propose Re-Nerfing, a simple and general multi-stage data augmentation\napproach that leverages NeRF's own view synthesis ability to address these\nlimitations. With Re-Nerfing, we enhance the geometric consistency of novel\nviews as follows: First, we train a NeRF with the available views. Then, we use\nthe optimized NeRF to synthesize pseudo-views around the original ones with a\nview selection strategy to improve coverage and preserve view quality. Finally,\nwe train a second NeRF with both the original images and the pseudo views\nmasking out uncertain regions. Extensive experiments applying Re-Nerfing on\nvarious pipelines on the mip-NeRF 360 dataset, including Gaussian Splatting,\nprovide valuable insights into the improvements achievable without external\ndata or supervision, on denser and sparser input scenarios. Project page:\nhttps://renerfing.github.io\n","authors":["Felix Tristram","Stefano Gasperini","Nassir Navab","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2312.02255v2.pdf","comment":"Code will be released upon acceptance"},{"id":"http://arxiv.org/abs/2310.01040v3","updated":"2024-04-17T17:44:24Z","published":"2023-10-02T09:33:54Z","title":"Segmenting the motion components of a video: A long-term unsupervised\n  model","summary":"  Human beings have the ability to continuously analyze a video and immediately\nextract the motion components. We want to adopt this paradigm to provide a\ncoherent and stable motion segmentation over the video sequence. In this\nperspective, we propose a novel long-term spatio-temporal model operating in a\ntotally unsupervised way. It takes as input the volume of consecutive optical\nflow (OF) fields, and delivers a volume of segments of coherent motion over the\nvideo. More specifically, we have designed a transformer-based network, where\nwe leverage a mathematically well-founded framework, the Evidence Lower Bound\n(ELBO), to derive the loss function. The loss function combines a flow\nreconstruction term involving spatio-temporal parametric motion models\ncombining, in a novel way, polynomial (quadratic) motion models for the spatial\ndimensions and B-splines for the time dimension of the video sequence, and a\nregularization term enforcing temporal consistency on the segments. We report\nexperiments on four VOS benchmarks, demonstrating competitive quantitative\nresults, while performing motion segmentation on a whole sequence in one go. We\nalso highlight through visual results the key contributions on temporal\nconsistency brought by our method.\n","authors":["Etienne Meunier","Patrick Bouthemy"],"pdf_url":"https://arxiv.org/pdf/2310.01040v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11590v1","updated":"2024-04-17T17:39:59Z","published":"2024-04-17T17:39:59Z","title":"A Subspace-Constrained Tyler's Estimator and its Applications to\n  Structure from Motion","summary":"  We present the subspace-constrained Tyler's estimator (STE) designed for\nrecovering a low-dimensional subspace within a dataset that may be highly\ncorrupted with outliers. STE is a fusion of the Tyler's M-estimator (TME) and a\nvariant of the fast median subspace. Our theoretical analysis suggests that,\nunder a common inlier-outlier model, STE can effectively recover the underlying\nsubspace, even when it contains a smaller fraction of inliers relative to other\nmethods in the field of robust subspace recovery. We apply STE in the context\nof Structure from Motion (SfM) in two ways: for robust estimation of the\nfundamental matrix and for the removal of outlying cameras, enhancing the\nrobustness of the SfM pipeline. Numerical experiments confirm the\nstate-of-the-art performance of our method in these applications. This research\nmakes significant contributions to the field of robust subspace recovery,\nparticularly in the context of computer vision and 3D reconstruction.\n","authors":["Feng Yu","Teng Zhang","Gilad Lerman"],"pdf_url":"https://arxiv.org/pdf/2404.11590v1.pdf","comment":"23 pages, accepted by CVPR 24"},{"id":"http://arxiv.org/abs/2404.11589v1","updated":"2024-04-17T17:38:56Z","published":"2024-04-17T17:38:56Z","title":"Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept\n  Understanding","summary":"  The rapid evolution of text-to-image diffusion models has opened the door of\ngenerative AI, enabling the translation of textual descriptions into visually\ncompelling images with remarkable quality. However, a persistent challenge\nwithin this domain is the optimization of prompts to effectively convey\nabstract concepts into concrete objects. For example, text encoders can hardly\nexpress \"peace\", while can easily illustrate olive branches and white doves.\nThis paper introduces a novel approach named Prompt Optimizer for Abstract\nConcepts (POAC) specifically designed to enhance the performance of\ntext-to-image diffusion models in interpreting and generating images from\nabstract concepts. We propose a Prompt Language Model (PLM), which is\ninitialized from a pre-trained language model, and then fine-tuned with a\ncurated dataset of abstract concept prompts. The dataset is created with GPT-4\nto extend the abstract concept to a scene and concrete objects. Our framework\nemploys a Reinforcement Learning (RL)-based optimization strategy, focusing on\nthe alignment between the generated images by a stable diffusion model and\noptimized prompts. Through extensive experiments, we demonstrate that our\nproposed POAC significantly improves the accuracy and aesthetic quality of\ngenerated images, particularly in the description of abstract concepts and\nalignment with optimized prompts. We also present a comprehensive analysis of\nour model's performance across diffusion models under different settings,\nshowcasing its versatility and effectiveness in enhancing abstract concept\nrepresentation.\n","authors":["Zezhong Fan","Xiaohan Li","Chenhao Fang","Topojoy Biswas","Kaushiki Nag","Jianpeng Xu","Kannan Achan"],"pdf_url":"https://arxiv.org/pdf/2404.11589v1.pdf","comment":"WWW 2024 Companion"},{"id":"http://arxiv.org/abs/2206.10177v3","updated":"2024-04-17T17:36:19Z","published":"2022-06-21T08:16:08Z","title":"TCJA-SNN: Temporal-Channel Joint Attention for Spiking Neural Networks","summary":"  Spiking Neural Networks (SNNs) are attracting widespread interest due to\ntheir biological plausibility, energy efficiency, and powerful spatio-temporal\ninformation representation ability. Given the critical role of attention\nmechanisms in enhancing neural network performance, the integration of SNNs and\nattention mechanisms exhibits potential to deliver energy-efficient and\nhigh-performance computing paradigms. We present a novel Temporal-Channel Joint\nAttention mechanism for SNNs, referred to as TCJA-SNN. The proposed TCJA-SNN\nframework can effectively assess the significance of spike sequence from both\nspatial and temporal dimensions. More specifically, our essential technical\ncontribution lies on: 1) We employ the squeeze operation to compress the spike\nstream into an average matrix. Then, we leverage two local attention mechanisms\nbased on efficient 1D convolutions to facilitate comprehensive feature\nextraction at the temporal and channel levels independently. 2) We introduce\nthe Cross Convolutional Fusion (CCF) layer as a novel approach to model the\ninter-dependencies between the temporal and channel scopes. This layer breaks\nthe independence of these two dimensions and enables the interaction between\nfeatures. Experimental results demonstrate that the proposed TCJA-SNN\noutperforms SOTA by up to 15.7% accuracy on standard static and neuromorphic\ndatasets, including Fashion-MNIST, CIFAR10-DVS, N-Caltech 101, and DVS128\nGesture. Furthermore, we apply the TCJA-SNN framework to image generation tasks\nby leveraging a variation autoencoder. To the best of our knowledge, this study\nis the first instance where the SNN-attention mechanism has been employed for\nimage classification and generation tasks. Notably, our approach has achieved\nSOTA performance in both domains, establishing a significant advancement in the\nfield. Codes are available at https://github.com/ridgerchu/TCJA.\n","authors":["Rui-Jie Zhu","Malu Zhang","Qihang Zhao","Haoyu Deng","Yule Duan","Liang-Jian Deng"],"pdf_url":"https://arxiv.org/pdf/2206.10177v3.pdf","comment":"Accepted by IEEE Transactions on Neural Networks and Learning Systems"},{"id":"http://arxiv.org/abs/2404.11576v1","updated":"2024-04-17T17:19:48Z","published":"2024-04-17T17:19:48Z","title":"State-space Decomposition Model for Video Prediction Considering\n  Long-term Motion Trend","summary":"  Stochastic video prediction enables the consideration of uncertainty in\nfuture motion, thereby providing a better reflection of the dynamic nature of\nthe environment. Stochastic video prediction methods based on image\nauto-regressive recurrent models need to feed their predictions back into the\nlatent space. Conversely, the state-space models, which decouple frame\nsynthesis and temporal prediction, proves to be more efficient. However,\ninferring long-term temporal information about motion and generalizing to\ndynamic scenarios under non-stationary assumptions remains an unresolved\nchallenge. In this paper, we propose a state-space decomposition stochastic\nvideo prediction model that decomposes the overall video frame generation into\ndeterministic appearance prediction and stochastic motion prediction. Through\nadaptive decomposition, the model's generalization capability to dynamic\nscenarios is enhanced. In the context of motion prediction, obtaining a prior\non the long-term trend of future motion is crucial. Thus, in the stochastic\nmotion prediction branch, we infer the long-term motion trend from conditional\nframes to guide the generation of future frames that exhibit high consistency\nwith the conditional frames. Experimental results demonstrate that our model\noutperforms baselines on multiple datasets.\n","authors":["Fei Cui","Jiaojiao Fang","Xiaojiang Wu","Zelong Lai","Mengke Yang","Menghan Jia","Guizhong Liu"],"pdf_url":"https://arxiv.org/pdf/2404.11576v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11569v1","updated":"2024-04-17T17:11:47Z","published":"2024-04-17T17:11:47Z","title":"Simple Image Signal Processing using Global Context Guidance","summary":"  In modern smartphone cameras, the Image Signal Processor (ISP) is the core\nelement that converts the RAW readings from the sensor into perceptually\npleasant RGB images for the end users. The ISP is typically proprietary and\nhandcrafted and consists of several blocks such as white balance, color\ncorrection, and tone mapping. Deep learning-based ISPs aim to transform RAW\nimages into DSLR-like RGB images using deep neural networks. However, most\nlearned ISPs are trained using patches (small regions) due to computational\nlimitations. Such methods lack global context, which limits their efficacy on\nfull-resolution images and harms their ability to capture global properties\nsuch as color constancy or illumination. First, we propose a novel module that\ncan be integrated into any neural ISP to capture the global context information\nfrom the full RAW images. Second, we propose an efficient and simple neural ISP\nthat utilizes our proposed module. Our model achieves state-of-the-art results\non different benchmarks using diverse and real smartphone images.\n","authors":["Omar Elezabi","Marcos V. Conde","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2404.11569v1.pdf","comment":"Preprint under review"},{"id":"http://arxiv.org/abs/2404.11565v1","updated":"2024-04-17T17:08:05Z","published":"2024-04-17T17:08:05Z","title":"MoA: Mixture-of-Attention for Subject-Context Disentanglement in\n  Personalized Image Generation","summary":"  We introduce a new architecture for personalization of text-to-image\ndiffusion models, coined Mixture-of-Attention (MoA). Inspired by the\nMixture-of-Experts mechanism utilized in large language models (LLMs), MoA\ndistributes the generation workload between two attention pathways: a\npersonalized branch and a non-personalized prior branch. MoA is designed to\nretain the original model's prior by fixing its attention layers in the prior\nbranch, while minimally intervening in the generation process with the\npersonalized branch that learns to embed subjects in the layout and context\ngenerated by the prior branch. A novel routing mechanism manages the\ndistribution of pixels in each layer across these branches to optimize the\nblend of personalized and generic content creation. Once trained, MoA\nfacilitates the creation of high-quality, personalized images featuring\nmultiple subjects with compositions and interactions as diverse as those\ngenerated by the original model. Crucially, MoA enhances the distinction\nbetween the model's pre-existing capability and the newly augmented\npersonalized intervention, thereby offering a more disentangled subject-context\ncontrol that was previously unattainable. Project page:\nhttps://snap-research.github.io/mixture-of-attention\n","authors":[" Kuan-Chieh"," Wang","Daniil Ostashev","Yuwei Fang","Sergey Tulyakov","Kfir Aberman"],"pdf_url":"https://arxiv.org/pdf/2404.11565v1.pdf","comment":"Project Website: https://snap-research.github.io/mixture-of-attention"},{"id":"http://arxiv.org/abs/2404.11554v1","updated":"2024-04-17T16:56:31Z","published":"2024-04-17T16:56:31Z","title":"Predicting Long-horizon Futures by Conditioning on Geometry and Time","summary":"  Our work explores the task of generating future sensor observations\nconditioned on the past. We are motivated by `predictive coding' concepts from\nneuroscience as well as robotic applications such as self-driving vehicles.\nPredictive video modeling is challenging because the future may be multi-modal\nand learning at scale remains computationally expensive for video processing.\nTo address both challenges, our key insight is to leverage the large-scale\npretraining of image diffusion models which can handle multi-modality. We\nrepurpose image models for video prediction by conditioning on new frame\ntimestamps. Such models can be trained with videos of both static and dynamic\nscenes. To allow them to be trained with modestly-sized datasets, we introduce\ninvariances by factoring out illumination and texture by forcing the model to\npredict (pseudo) depth, readily obtained for in-the-wild videos via\noff-the-shelf monocular depth networks. In fact, we show that simply modifying\nnetworks to predict grayscale pixels already improves the accuracy of video\nprediction. Given the extra controllability with timestamp conditioning, we\npropose sampling schedules that work better than the traditional autoregressive\nand hierarchical sampling strategies. Motivated by probabilistic metrics from\nthe object forecasting literature, we create a benchmark for video prediction\non a diverse set of videos spanning indoor and outdoor scenes and a large\nvocabulary of objects. Our experiments illustrate the effectiveness of learning\nto condition on timestamps, and show the importance of predicting the future\nwith invariant modalities.\n","authors":["Tarasha Khurana","Deva Ramanan"],"pdf_url":"https://arxiv.org/pdf/2404.11554v1.pdf","comment":"Project page: http://www.cs.cmu.edu/~tkhurana/depthforecasting/"},{"id":"http://arxiv.org/abs/2403.11376v4","updated":"2024-04-17T16:46:02Z","published":"2024-03-18T00:03:48Z","title":"ShapeFormer: Shape Prior Visible-to-Amodal Transformer-based Amodal\n  Instance Segmentation","summary":"  Amodal Instance Segmentation (AIS) presents a challenging task as it involves\npredicting both visible and occluded parts of objects within images. Existing\nAIS methods rely on a bidirectional approach, encompassing both the transition\nfrom amodal features to visible features (amodal-to-visible) and from visible\nfeatures to amodal features (visible-to-amodal). Our observation shows that the\nutilization of amodal features through the amodal-to-visible can confuse the\nvisible features due to the extra information of occluded/hidden segments not\npresented in visible display. Consequently, this compromised quality of visible\nfeatures during the subsequent visible-to-amodal transition. To tackle this\nissue, we introduce ShapeFormer, a decoupled Transformer-based model with a\nvisible-to-amodal transition. It facilitates the explicit relationship between\noutput segmentations and avoids the need for amodal-to-visible transitions.\nShapeFormer comprises three key modules: (i) Visible-Occluding Mask Head for\npredicting visible segmentation with occlusion awareness, (ii) Shape-Prior\nAmodal Mask Head for predicting amodal and occluded masks, and (iii)\nCategory-Specific Shape Prior Retriever aims to provide shape prior knowledge.\nComprehensive experiments and extensive ablation studies across various AIS\nbenchmarks demonstrate the effectiveness of our ShapeFormer. The code is\navailable at: \\url{https://github.com/UARK-AICV/ShapeFormer}\n","authors":["Minh Tran","Winston Bounsavy","Khoa Vo","Anh Nguyen","Tri Nguyen","Ngan Le"],"pdf_url":"https://arxiv.org/pdf/2403.11376v4.pdf","comment":"Accepted to IJCNN2024"},{"id":"http://arxiv.org/abs/2312.03678v2","updated":"2024-04-17T16:37:44Z","published":"2023-12-06T18:41:01Z","title":"Hybrid Functional Maps for Crease-Aware Non-Isometric Shape Matching","summary":"  Non-isometric shape correspondence remains a fundamental challenge in\ncomputer vision. Traditional methods using Laplace-Beltrami operator (LBO)\neigenmodes face limitations in characterizing high-frequency extrinsic shape\nchanges like bending and creases. We propose a novel approach of combining the\nnon-orthogonal extrinsic basis of eigenfunctions of the elastic thin-shell\nhessian with the intrinsic ones of the LBO, creating a hybrid spectral space in\nwhich we construct functional maps. To this end, we present a theoretical\nframework to effectively integrate non-orthogonal basis functions into\ndescriptor- and learning-based functional map methods. Our approach can be\nincorporated easily into existing functional map pipelines across varying\napplications and is able to handle complex deformations beyond isometries. We\nshow extensive evaluations across various supervised and unsupervised settings\nand demonstrate significant improvements. Notably, our approach achieves up to\n15% better mean geodesic error for non-isometric correspondence settings and up\nto 45% improvement in scenarios with topological noise.\n","authors":["Lennart Bastian","Yizheng Xie","Nassir Navab","Zorah L√§hner"],"pdf_url":"https://arxiv.org/pdf/2312.03678v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2404.11537v1","updated":"2024-04-17T16:30:56Z","published":"2024-04-17T16:30:56Z","title":"SSDiff: Spatial-spectral Integrated Diffusion Model for Remote Sensing\n  Pansharpening","summary":"  Pansharpening is a significant image fusion technique that merges the spatial\ncontent and spectral characteristics of remote sensing images to generate\nhigh-resolution multispectral images. Recently, denoising diffusion\nprobabilistic models have been gradually applied to visual tasks, enhancing\ncontrollable image generation through low-rank adaptation (LoRA). In this\npaper, we introduce a spatial-spectral integrated diffusion model for the\nremote sensing pansharpening task, called SSDiff, which considers the\npansharpening process as the fusion process of spatial and spectral components\nfrom the perspective of subspace decomposition. Specifically, SSDiff utilizes\nspatial and spectral branches to learn spatial details and spectral features\nseparately, then employs a designed alternating projection fusion module (APFM)\nto accomplish the fusion. Furthermore, we propose a frequency modulation\ninter-branch module (FMIM) to modulate the frequency distribution between\nbranches. The two components of SSDiff can perform favorably against the APFM\nwhen utilizing a LoRA-like branch-wise alternative fine-tuning method. It\nrefines SSDiff to capture component-discriminating features more sufficiently.\nFinally, extensive experiments on four commonly used datasets, i.e.,\nWorldView-3, WorldView-2, GaoFen-2, and QuickBird, demonstrate the superiority\nof SSDiff both visually and quantitatively. The code will be made open source\nafter possible acceptance.\n","authors":["Yu Zhong","Xiao Wu","Liang-Jian Deng","Zihan Cao"],"pdf_url":"https://arxiv.org/pdf/2404.11537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11525v1","updated":"2024-04-17T16:16:12Z","published":"2024-04-17T16:16:12Z","title":"JointViT: Modeling Oxygen Saturation Levels with Joint Supervision on\n  Long-Tailed OCTA","summary":"  The oxygen saturation level in the blood (SaO2) is crucial for health,\nparticularly in relation to sleep-related breathing disorders. However,\ncontinuous monitoring of SaO2 is time-consuming and highly variable depending\non patients' conditions. Recently, optical coherence tomography angiography\n(OCTA) has shown promising development in rapidly and effectively screening\neye-related lesions, offering the potential for diagnosing sleep-related\ndisorders. To bridge this gap, our paper presents three key contributions.\nFirstly, we propose JointViT, a novel model based on the Vision Transformer\narchitecture, incorporating a joint loss function for supervision. Secondly, we\nintroduce a balancing augmentation technique during data preprocessing to\nimprove the model's performance, particularly on the long-tail distribution\nwithin the OCTA dataset. Lastly, through comprehensive experiments on the OCTA\ndataset, our proposed method significantly outperforms other state-of-the-art\nmethods, achieving improvements of up to 12.28% in overall accuracy. This\nadvancement lays the groundwork for the future utilization of OCTA in\ndiagnosing sleep-related disorders. See project website\nhttps://steve-zeyu-zhang.github.io/JointViT\n","authors":["Zeyu Zhang","Xuyin Qi","Mingxi Chen","Guangxi Li","Ryan Pham","Ayub Zuhair","Ella Berry","Zhibin Liao","Owen Siggs","Robert Mclaughlin","Jamie Craig","Minh-Son To"],"pdf_url":"https://arxiv.org/pdf/2404.11525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05889v2","updated":"2024-04-17T16:13:22Z","published":"2023-12-10T13:44:03Z","title":"SuperPrimitive: Scene Reconstruction at a Primitive Level","summary":"  Joint camera pose and dense geometry estimation from a set of images or a\nmonocular video remains a challenging problem due to its computational\ncomplexity and inherent visual ambiguities. Most dense incremental\nreconstruction systems operate directly on image pixels and solve for their 3D\npositions using multi-view geometry cues. Such pixel-level approaches suffer\nfrom ambiguities or violations of multi-view consistency (e.g. caused by\ntextureless or specular surfaces).\n  We address this issue with a new image representation which we call a\nSuperPrimitive. SuperPrimitives are obtained by splitting images into\nsemantically correlated local regions and enhancing them with estimated surface\nnormal directions, both of which are predicted by state-of-the-art single image\nneural networks. This provides a local geometry estimate per SuperPrimitive,\nwhile their relative positions are adjusted based on multi-view observations.\n  We demonstrate the versatility of our new representation by addressing three\n3D reconstruction tasks: depth completion, few-view structure from motion, and\nmonocular dense visual odometry.\n","authors":["Kirill Mazur","Gwangbin Bae","Andrew J. Davison"],"pdf_url":"https://arxiv.org/pdf/2312.05889v2.pdf","comment":"CVPR2024. Project Page: https://makezur.github.io/SuperPrimitive/"},{"id":"http://arxiv.org/abs/2404.11511v1","updated":"2024-04-17T16:06:29Z","published":"2024-04-17T16:06:29Z","title":"Event Cameras Meet SPADs for High-Speed, Low-Bandwidth Imaging","summary":"  Traditional cameras face a trade-off between low-light performance and\nhigh-speed imaging: longer exposure times to capture sufficient light results\nin motion blur, whereas shorter exposures result in Poisson-corrupted noisy\nimages. While burst photography techniques help mitigate this tradeoff,\nconventional cameras are fundamentally limited in their sensor noise\ncharacteristics. Event cameras and single-photon avalanche diode (SPAD) sensors\nhave emerged as promising alternatives to conventional cameras due to their\ndesirable properties. SPADs are capable of single-photon sensitivity with\nmicrosecond temporal resolution, and event cameras can measure brightness\nchanges up to 1 MHz with low bandwidth requirements. We show that these\nproperties are complementary, and can help achieve low-light, high-speed image\nreconstruction with low bandwidth requirements. We introduce a sensor fusion\nframework to combine SPADs with event cameras to improves the reconstruction of\nhigh-speed, low-light scenes while reducing the high bandwidth cost associated\nwith using every SPAD frame. Our evaluation, on both synthetic and real sensor\ndata, demonstrates significant enhancements ( > 5 dB PSNR) in reconstructing\nlow-light scenes at high temporal resolution (100 kHz) compared to conventional\ncameras. Event-SPAD fusion shows great promise for real-world applications,\nsuch as robotics or medical imaging.\n","authors":["Manasi Muglikar","Siddharth Somasundaram","Akshat Dave","Edoardo Charbon","Ramesh Raskar","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2404.11511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06567v3","updated":"2024-04-17T15:58:36Z","published":"2024-03-11T10:06:45Z","title":"Leveraging Foundation Models for Content-Based Medical Image Retrieval\n  in Radiology","summary":"  Content-based image retrieval (CBIR) has the potential to significantly\nimprove diagnostic aid and medical research in radiology. Current CBIR systems\nface limitations due to their specialization to certain pathologies, limiting\ntheir utility. In response, we propose using vision foundation models as\npowerful and versatile off-the-shelf feature extractors for content-based\nmedical image retrieval. By benchmarking these models on a comprehensive\ndataset of 1.6 million 2D radiological images spanning four modalities and 161\npathologies, we identify weakly-supervised models as superior, achieving a P@1\nof up to 0.594. This performance not only competes with a specialized model but\ndoes so without the need for fine-tuning. Our analysis further explores the\nchallenges in retrieving pathological versus anatomical structures, indicating\nthat accurate retrieval of pathological features presents greater difficulty.\nDespite these challenges, our research underscores the vast potential of\nfoundation models for CBIR in radiology, proposing a shift towards versatile,\ngeneral-purpose medical image retrieval systems that do not require specific\ntuning.\n","authors":["Stefan Denner","David Zimmerer","Dimitrios Bounias","Markus Bujotzek","Shuhan Xiao","Lisa Kausch","Philipp Schader","Tobias Penzkofer","Paul F. J√§ger","Klaus Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2403.06567v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11492v1","updated":"2024-04-17T15:47:26Z","published":"2024-04-17T15:47:26Z","title":"arcjetCV: an open-source software to analyze material ablation","summary":"  arcjetCV is an open-source Python software designed to automate time-resolved\nmeasurements of heatshield material recession and recession rates from arcjet\ntest video footage. This new automated and accessible capability greatly\nexceeds previous manual extraction methods, enabling rapid and detailed\ncharacterization of material recession for any sample with a profile video.\narcjetCV automates the video segmentation process using machine learning\nmodels, including a one-dimensional (1D) Convolutional Neural Network (CNN) to\ninfer the time-window of interest, a two-dimensional (2D) CNN for image and\nedge segmentation, and a Local Outlier Factor (LOF) for outlier filtering. A\ngraphical user interface (GUI) simplifies the user experience and an\napplication programming interface (API) allows users to call the core functions\nfrom scripts, enabling video batch processing. arcjetCV's capability to measure\ntime-resolved recession in turn enables characterization of non-linear\nprocesses (shrinkage, swelling, melt flows, etc.), contributing to higher\nfidelity validation and improved modeling of heatshield material performance.\nThe source code associated with this article can be found at\nhttps://github.com/magnus-haw/arcjetCV.\n","authors":["Alexandre Quintart","Magnus Haw","Federico Semeraro"],"pdf_url":"https://arxiv.org/pdf/2404.11492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11488v1","updated":"2024-04-17T15:45:49Z","published":"2024-04-17T15:45:49Z","title":"Multi-resolution Rescored ByteTrack for Video Object Detection on\n  Ultra-low-power Embedded Systems","summary":"  This paper introduces Multi-Resolution Rescored Byte-Track (MR2-ByteTrack), a\nnovel video object detection framework for ultra-low-power embedded processors.\nThis method reduces the average compute load of an off-the-shelf Deep Neural\nNetwork (DNN) based object detector by up to 2.25$\\times$ by alternating the\nprocessing of high-resolution images (320$\\times$320 pixels) with multiple\ndown-sized frames (192$\\times$192 pixels). To tackle the accuracy degradation\ndue to the reduced image input size, MR2-ByteTrack correlates the output\ndetections over time using the ByteTrack tracker and corrects potential\nmisclassification using a novel probabilistic Rescore algorithm. By\ninterleaving two down-sized images for every high-resolution one as the input\nof different state-of-the-art DNN object detectors with our MR2-ByteTrack, we\ndemonstrate an average accuracy increase of 2.16% and a latency reduction of\n43% on the GAP9 microcontroller compared to a baseline frame-by-frame inference\nscheme using exclusively full-resolution images. Code available at:\nhttps://github.com/Bomps4/Multi_Resolution_Rescored_ByteTrack\n","authors":["Luca Bompani","Manuele Rusci","Daniele Palossi","Francesco Conti","Luca Benini"],"pdf_url":"https://arxiv.org/pdf/2404.11488v1.pdf","comment":"9 pages, 3 figures Accepted for publication at the Embedded Vision\n  Workshop of the Computer Vision and Pattern Recognition conference, Seattle,\n  2024"},{"id":"http://arxiv.org/abs/2404.11475v1","updated":"2024-04-17T15:31:06Z","published":"2024-04-17T15:31:06Z","title":"AdaIR: Exploiting Underlying Similarities of Image Restoration Tasks\n  with Adapters","summary":"  Existing image restoration approaches typically employ extensive networks\nspecifically trained for designated degradations. Despite being effective, such\nmethods inevitably entail considerable storage costs and computational\noverheads due to the reliance on task-specific networks. In this work, we go\nbeyond this well-established framework and exploit the inherent commonalities\namong image restoration tasks. The primary objective is to identify components\nthat are shareable across restoration tasks and augment the shared components\nwith modules specifically trained for individual tasks. Towards this goal, we\npropose AdaIR, a novel framework that enables low storage cost and efficient\ntraining without sacrificing performance. Specifically, a generic restoration\nnetwork is first constructed through self-supervised pre-training using\nsynthetic degradations. Subsequent to the pre-training phase, adapters are\ntrained to adapt the pre-trained network to specific degradations. AdaIR\nrequires solely the training of lightweight, task-specific modules, ensuring a\nmore efficient storage and training regimen. We have conducted extensive\nexperiments to validate the effectiveness of AdaIR and analyze the influence of\nthe pre-training strategy on discovering shareable components. Extensive\nexperimental results show that AdaIR achieves outstanding results on multi-task\nrestoration while utilizing significantly fewer parameters (1.9 MB) and less\ntraining time (7 hours) for each restoration task. The source codes and trained\nmodels will be released.\n","authors":["Hao-Wei Chen","Yu-Syuan Xu","Kelvin C. K. Chan","Hsien-Kai Kuo","Chun-Yi Lee","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2404.11475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11474v1","updated":"2024-04-17T15:28:53Z","published":"2024-04-17T15:28:53Z","title":"Towards Highly Realistic Artistic Style Transfer via Stable Diffusion\n  with Step-aware and Layer-aware Prompt","summary":"  Artistic style transfer aims to transfer the learned artistic style onto an\narbitrary content image, generating artistic stylized images. Existing\ngenerative adversarial network-based methods fail to generate highly realistic\nstylized images and always introduce obvious artifacts and disharmonious\npatterns. Recently, large-scale pre-trained diffusion models opened up a new\nway for generating highly realistic artistic stylized images. However,\ndiffusion model-based methods generally fail to preserve the content structure\nof input content images well, introducing some undesired content structure and\nstyle patterns. To address the above problems, we propose a novel pre-trained\ndiffusion-based artistic style transfer method, called LSAST, which can\ngenerate highly realistic artistic stylized images while preserving the content\nstructure of input content images well, without bringing obvious artifacts and\ndisharmonious style patterns. Specifically, we introduce a Step-aware and\nLayer-aware Prompt Space, a set of learnable prompts, which can learn the style\ninformation from the collection of artworks and dynamically adjusts the input\nimages' content structure and style pattern. To train our prompt space, we\npropose a novel inversion method, called Step-ware and Layer-aware Prompt\nInversion, which allows the prompt space to learn the style information of the\nartworks collection. In addition, we inject a pre-trained conditional branch of\nControlNet into our LSAST, which further improved our framework's ability to\nmaintain content structure. Extensive experiments demonstrate that our proposed\nmethod can generate more highly realistic artistic stylized images than the\nstate-of-the-art artistic style transfer methods.\n","authors":["Zhanjie Zhang","Quanwei Zhang","Huaizhong Lin","Wei Xing","Juncheng Mo","Shuaicheng Huang","Jinheng Xie","Guangyuan Li","Junsheng Luan","Lei Zhao","Dalong Zhang","Lixia Chen"],"pdf_url":"https://arxiv.org/pdf/2404.11474v1.pdf","comment":"Accepted by IJCAI2024"},{"id":"http://arxiv.org/abs/2303.12054v5","updated":"2024-04-17T15:12:29Z","published":"2023-03-21T17:45:38Z","title":"Influencer Backdoor Attack on Semantic Segmentation","summary":"  When a small number of poisoned samples are injected into the training\ndataset of a deep neural network, the network can be induced to exhibit\nmalicious behavior during inferences, which poses potential threats to\nreal-world applications. While they have been intensively studied in\nclassification, backdoor attacks on semantic segmentation have been largely\noverlooked. Unlike classification, semantic segmentation aims to classify every\npixel within a given image. In this work, we explore backdoor attacks on\nsegmentation models to misclassify all pixels of a victim class by injecting a\nspecific trigger on non-victim pixels during inferences, which is dubbed\nInfluencer Backdoor Attack (IBA). IBA is expected to maintain the\nclassification accuracy of non-victim pixels and mislead classifications of all\nvictim pixels in every single inference and could be easily applied to\nreal-world scenes. Based on the context aggregation ability of segmentation\nmodels, we proposed a simple, yet effective, Nearest-Neighbor trigger injection\nstrategy. We also introduce an innovative Pixel Random Labeling strategy which\nmaintains optimal performance even when the trigger is placed far from the\nvictim pixels. Our extensive experiments reveal that current segmentation\nmodels do suffer from backdoor attacks, demonstrate IBA real-world\napplicability, and show that our proposed techniques can further increase\nattack performance.\n","authors":["Haoheng Lan","Jindong Gu","Philip Torr","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.12054v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11461v1","updated":"2024-04-17T15:09:31Z","published":"2024-04-17T15:09:31Z","title":"Using Game Engines and Machine Learning to Create Synthetic Satellite\n  Imagery for a Tabletop Verification Exercise","summary":"  Satellite imagery is regarded as a great opportunity for citizen-based\nmonitoring of activities of interest. Relevant imagery may however not be\navailable at sufficiently high resolution, quality, or cadence -- let alone be\nuniformly accessible to open-source analysts. This limits an assessment of the\ntrue long-term potential of citizen-based monitoring of nuclear activities\nusing publicly available satellite imagery. In this article, we demonstrate how\nmodern game engines combined with advanced machine-learning techniques can be\nused to generate synthetic imagery of sites of interest with the ability to\nchoose relevant parameters upon request; these include time of day, cloud\ncover, season, or level of activity onsite. At the same time, resolution and\noff-nadir angle can be adjusted to simulate different characteristics of the\nsatellite. While there are several possible use-cases for synthetic imagery,\nhere we focus on its usefulness to support tabletop exercises in which simple\nmonitoring scenarios can be examined to better understand verification\ncapabilities enabled by new satellite constellations and very short revisit\ntimes.\n","authors":["Johannes Hoster","Sara Al-Sayed","Felix Biessmann","Alexander Glaser","Kristian Hildebrand","Igor Moric","Tuong Vy Nguyen"],"pdf_url":"https://arxiv.org/pdf/2404.11461v1.pdf","comment":"Annual Meeting of the Institute of Nuclear Materials Management\n  (INMM), Vienna"},{"id":"http://arxiv.org/abs/2404.11459v1","updated":"2024-04-17T15:07:06Z","published":"2024-04-17T15:07:06Z","title":"Octopus v3: Technical Report for On-device Sub-billion Multimodal AI\n  Agent","summary":"  A multimodal AI agent is characterized by its ability to process and learn\nfrom various types of data, including natural language, visual, and audio\ninputs, to inform its actions. Despite advancements in large language models\nthat incorporate visual data, such as GPT-4V, effectively translating\nimage-based data into actionable outcomes for AI agents continues to be\nchallenging. In this paper, we introduce a multimodal model that incorporates\nthe concept of functional token specifically designed for AI agent\napplications. To ensure compatibility with edge devices, our model is optimized\nto a compact size of less than 1B parameters. Like GPT-4, our model can process\nboth English and Chinese. We demonstrate that this model is capable of\noperating efficiently on a wide range of edge devices, including as constrained\nas a Raspberry Pi.\n","authors":["Wei Chen","Zhiyuan Li"],"pdf_url":"https://arxiv.org/pdf/2404.11459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.05309v2","updated":"2024-04-17T15:04:14Z","published":"2023-02-10T15:12:40Z","title":"The LuViRA Dataset: Measurement Description","summary":"  We present a dataset to evaluate localization algorithms, which utilizes\nvision, audio, and radio sensors: the Lund University Vision, Radio, and Audio\n(LuViRA) Dataset. The dataset includes RGB images, corresponding depth maps,\nIMU readings, channel response between a massive MIMO channel sounder and a\nuser equipment, audio recorded by 12 microphones, and 0.5 mm accurate 6DoF pose\nground truth. We synchronize these sensors to make sure that all data are\nrecorded simultaneously. A camera, speaker, and transmit antenna are placed on\ntop of a slowly moving service robot and 88 trajectories are recorded. Each\ntrajectory includes 20 to 50 seconds of recorded sensor data and ground truth\nlabels. The data from different sensors can be used separately or jointly to\nconduct localization tasks and a motion capture system is used to verify the\nresults obtained by the localization algorithms. The main aim of this dataset\nis to enable research on fusing the most commonly used sensors for localization\ntasks. However, the full dataset or some parts of it can also be used for other\nresearch areas such as channel estimation, image classification, etc. Fusing\nsensor data can lead to increased localization accuracy and reliability, as\nwell as decreased latency and power consumption. The created dataset will be\nmade public at a later date.\n","authors":["Ilayda Yaman","Guoda Tian","Martin Larsson","Patrik Persson","Michiel Sandra","Alexander D√ºrr","Erik Tegler","Nikhil Challa","Henrik Garde","Fredrik Tufvesson","Kalle √Östr√∂m","Ove Edfors","Steffen Malkowsky","Liang Liu"],"pdf_url":"https://arxiv.org/pdf/2302.05309v2.pdf","comment":"7 pages, 7 figures, Accepted to ICRA 2024"},{"id":"http://arxiv.org/abs/2305.15964v5","updated":"2024-04-17T15:01:39Z","published":"2023-05-25T12:03:31Z","title":"ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs","summary":"  The integration of Computer-Aided Diagnosis (CAD) with Large Language Models\n(LLMs) presents a promising frontier in clinical applications, notably in\nautomating diagnostic processes akin to those performed by radiologists and\nproviding consultations similar to a virtual family doctor. Despite the\npromising potential of this integration, current works face at least two\nlimitations: (1) From the perspective of a radiologist, existing studies\ntypically have a restricted scope of applicable imaging domains, failing to\nmeet the diagnostic needs of different patients. Also, the insufficient\ndiagnostic capability of LLMs further undermine the quality and reliability of\nthe generated medical reports. (2) Current LLMs lack the requisite depth in\nmedical expertise, rendering them less effective as virtual family doctors due\nto the potential unreliability of the advice provided during patient\nconsultations. To address these limitations, we introduce ChatCAD+, to be\nuniversal and reliable. Specifically, it is featured by two main modules: (1)\nReliable Report Generation and (2) Reliable Interaction. The Reliable Report\nGeneration module is capable of interpreting medical images from diverse\ndomains and generate high-quality medical reports via our proposed hierarchical\nin-context learning. Concurrently, the interaction module leverages up-to-date\ninformation from reputable medical websites to provide reliable medical advice.\nTogether, these designed modules synergize to closely align with the expertise\nof human medical professionals, offering enhanced consistency and reliability\nfor interpretation and advice. The source code is available at\nhttps://github.com/zhaozh10/ChatCAD.\n","authors":["Zihao Zhao","Sheng Wang","Jinchen Gu","Yitao Zhu","Lanzhuju Mei","Zixu Zhuang","Zhiming Cui","Qian Wang","Dinggang Shen"],"pdf_url":"https://arxiv.org/pdf/2305.15964v5.pdf","comment":"Authors Zihao Zhao, Sheng Wang, Jinchen Gu, Yitao Zhu contributed\n  equally to this work and should be considered co-first authors"},{"id":"http://arxiv.org/abs/2403.18807v4","updated":"2024-04-17T14:59:51Z","published":"2024-03-27T17:53:30Z","title":"ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth\n  Estimation","summary":"  In the absence of parallax cues, a learning-based single image depth\nestimation (SIDE) model relies heavily on shading and contextual cues in the\nimage. While this simplicity is attractive, it is necessary to train such\nmodels on large and varied datasets, which are difficult to capture. It has\nbeen shown that using embeddings from pre-trained foundational models, such as\nCLIP, improves zero shot transfer in several applications. Taking inspiration\nfrom this, in our paper we explore the use of global image priors generated\nfrom a pre-trained ViT model to provide more detailed contextual information.\nWe argue that the embedding vector from a ViT model, pre-trained on a large\ndataset, captures greater relevant information for SIDE than the usual route of\ngenerating pseudo image captions, followed by CLIP based text embeddings. Based\non this idea, we propose a new SIDE model using a diffusion backbone which is\nconditioned on ViT embeddings. Our proposed design establishes a new\nstate-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of\n0.059 (14% improvement) compared to 0.069 by the current SOTA (VPD). And on\nKITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to\n0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model\ntrained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%)\nover NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%,\n18%, 45%, 9%) by ZoeDepth. The project page is available at\nhttps://ecodepth-iitd.github.io\n","authors":["Suraj Patni","Aradhye Agarwal","Chetan Arora"],"pdf_url":"https://arxiv.org/pdf/2403.18807v4.pdf","comment":"IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n  2024"},{"id":"http://arxiv.org/abs/2402.00186v2","updated":"2024-04-17T14:54:56Z","published":"2024-01-31T21:28:40Z","title":"Distance and Collision Probability Estimation from Gaussian Surface\n  Models","summary":"  This paper describes continuous-space methodologies to estimate the collision\nprobability, Euclidean distance and gradient between an ellipsoidal robot model\nand an environment surface modeled as a set of Gaussian distributions.\nContinuous-space collision probability estimation is critical for\nuncertainty-aware motion planning. Most collision detection and avoidance\napproaches assume the robot is modeled as a sphere, but ellipsoidal\nrepresentations provide tighter approximations and enable navigation in\ncluttered and narrow spaces. State-of-the-art methods derive the Euclidean\ndistance and gradient by processing raw point clouds, which is computationally\nexpensive for large workspaces. Recent advances in Gaussian surface modeling\n(e.g. mixture models, splatting) enable compressed and high-fidelity surface\nrepresentations. Few methods exist to estimate continuous-space occupancy from\nsuch models. They require Gaussians to model free space and are unable to\nestimate the collision probability, Euclidean distance and gradient for an\nellipsoidal robot. The proposed methods bridge this gap by extending prior work\nin ellipsoid-to-ellipsoid Euclidean distance and collision probability\nestimation to Gaussian surface models. A geometric blending approach is also\nproposed to improve collision probability estimation. The approaches are\nevaluated with numerical 2D and 3D experiments using real-world point cloud\ndata. Methods for efficient calculation of these quantities are demonstrated to\nexecute within a few microseconds per ellipsoid pair using a single-thread on\nlow-power CPUs of modern embedded computers\n","authors":["Kshitij Goel","Wennie Tabib"],"pdf_url":"https://arxiv.org/pdf/2402.00186v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11429v1","updated":"2024-04-17T14:34:56Z","published":"2024-04-17T14:34:56Z","title":"CarcassFormer: An End-to-end Transformer-based Framework for\n  Simultaneous Localization, Segmentation and Classification of Poultry Carcass\n  Defect","summary":"  In the food industry, assessing the quality of poultry carcasses during\nprocessing is a crucial step. This study proposes an effective approach for\nautomating the assessment of carcass quality without requiring skilled labor or\ninspector involvement. The proposed system is based on machine learning (ML)\nand computer vision (CV) techniques, enabling automated defect detection and\ncarcass quality assessment. To this end, an end-to-end framework called\nCarcassFormer is introduced. It is built upon a Transformer-based architecture\ndesigned to effectively extract visual representations while simultaneously\ndetecting, segmenting, and classifying poultry carcass defects. Our proposed\nframework is capable of analyzing imperfections resulting from production and\ntransport welfare issues, as well as processing plant stunner, scalder, picker,\nand other equipment malfunctions. To benchmark the framework, a dataset of\n7,321 images was initially acquired, which contained both single and multiple\ncarcasses per image. In this study, the performance of the CarcassFormer system\nis compared with other state-of-the-art (SOTA) approaches for both\nclassification, detection, and segmentation tasks. Through extensive\nquantitative experiments, our framework consistently outperforms existing\nmethods, demonstrating remarkable improvements across various evaluation\nmetrics such as AP, AP@50, and AP@75. Furthermore, the qualitative results\nhighlight the strengths of CarcassFormer in capturing fine details, including\nfeathers, and accurately localizing and segmenting carcasses with high\nprecision. To facilitate further research and collaboration, the pre-trained\nmodel and source code of CarcassFormer is available for research purposes at:\n\\url{https://github.com/UARK-AICV/CarcassFormer}.\n","authors":["Minh Tran","Sang Truong","Arthur F. A. Fernandes","Michael T. Kidd","Ngan Le"],"pdf_url":"https://arxiv.org/pdf/2404.11429v1.pdf","comment":"Accepted to Poultry Science Journal"},{"id":"http://arxiv.org/abs/2404.11428v1","updated":"2024-04-17T14:34:35Z","published":"2024-04-17T14:34:35Z","title":"Explainable Lung Disease Classification from Chest X-Ray Images\n  Utilizing Deep Learning and XAI","summary":"  Lung diseases remain a critical global health concern, and it's crucial to\nhave accurate and quick ways to diagnose them. This work focuses on classifying\ndifferent lung diseases into five groups: viral pneumonia, bacterial pneumonia,\nCOVID, tuberculosis, and normal lungs. Employing advanced deep learning\ntechniques, we explore a diverse range of models including CNN, hybrid models,\nensembles, transformers, and Big Transfer. The research encompasses\ncomprehensive methodologies such as hyperparameter tuning, stratified k-fold\ncross-validation, and transfer learning with fine-tuning.Remarkably, our\nfindings reveal that the Xception model, fine-tuned through 5-fold\ncross-validation, achieves the highest accuracy of 96.21\\%. This success shows\nthat our methods work well in accurately identifying different lung diseases.\nThe exploration of explainable artificial intelligence (XAI) methodologies\nfurther enhances our understanding of the decision-making processes employed by\nthese models, contributing to increased trust in their clinical applications.\n","authors":["Tanzina Taher Ifty","Saleh Ahmed Shafin","Shoeb Mohammad Shahriar","Tashfia Towhid"],"pdf_url":"https://arxiv.org/pdf/2404.11428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11426v1","updated":"2024-04-17T14:33:41Z","published":"2024-04-17T14:33:41Z","title":"SPAMming Labels: Efficient Annotations for the Trackers of Tomorrow","summary":"  Increasing the annotation efficiency of trajectory annotations from videos\nhas the potential to enable the next generation of data-hungry tracking\nalgorithms to thrive on large-scale datasets. Despite the importance of this\ntask, there are currently very few works exploring how to efficiently label\ntracking datasets comprehensively. In this work, we introduce SPAM, a tracking\ndata engine that provides high-quality labels with minimal human intervention.\nSPAM is built around two key insights: i) most tracking scenarios can be easily\nresolved. To take advantage of this, we utilize a pre-trained model to generate\nhigh-quality pseudo-labels, reserving human involvement for a smaller subset of\nmore difficult instances; ii) handling the spatiotemporal dependencies of track\nannotations across time can be elegantly and efficiently formulated through\ngraphs. Therefore, we use a unified graph formulation to address the annotation\nof both detections and identity association for tracks across time. Based on\nthese insights, SPAM produces high-quality annotations with a fraction of\nground truth labeling cost. We demonstrate that trackers trained on SPAM labels\nachieve comparable performance to those trained on human annotations while\nrequiring only 3-20% of the human labeling effort. Hence, SPAM paves the way\ntowards highly efficient labeling of large-scale tracking datasets. Our code\nand models will be available upon acceptance.\n","authors":["Orcun Cetintas","Tim Meinhardt","Guillem Bras√≥","Laura Leal-Taix√©"],"pdf_url":"https://arxiv.org/pdf/2404.11426v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11419v1","updated":"2024-04-17T14:23:28Z","published":"2024-04-17T14:23:28Z","title":"SLAIM: Robust Dense Neural SLAM for Online Tracking and Mapping","summary":"  We present SLAIM - Simultaneous Localization and Implicit Mapping. We propose\na novel coarse-to-fine tracking model tailored for Neural Radiance Field SLAM\n(NeRF-SLAM) to achieve state-of-the-art tracking performance. Notably, existing\nNeRF-SLAM systems consistently exhibit inferior tracking performance compared\nto traditional SLAM algorithms. NeRF-SLAM methods solve camera tracking via\nimage alignment and photometric bundle-adjustment. Such optimization processes\nare difficult to optimize due to the narrow basin of attraction of the\noptimization loss in image space (local minima) and the lack of initial\ncorrespondences. We mitigate these limitations by implementing a Gaussian\npyramid filter on top of NeRF, facilitating a coarse-to-fine tracking\noptimization strategy. Furthermore, NeRF systems encounter challenges in\nconverging to the right geometry with limited input views. While prior\napproaches use a Signed-Distance Function (SDF)-based NeRF and directly\nsupervise SDF values by approximating ground truth SDF through depth\nmeasurements, this often results in suboptimal geometry. In contrast, our\nmethod employs a volume density representation and introduces a novel KL\nregularizer on the ray termination distribution, constraining scene geometry to\nconsist of empty space and opaque surfaces. Our solution implements both local\nand global bundle-adjustment to produce a robust (coarse-to-fine) and accurate\n(KL regularizer) SLAM solution. We conduct experiments on multiple datasets\n(ScanNet, TUM, Replica) showing state-of-the-art results in tracking and in\nreconstruction accuracy.\n","authors":["Vincent Cartillier","Grant Schindler","Irfan Essa"],"pdf_url":"https://arxiv.org/pdf/2404.11419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11416v1","updated":"2024-04-17T14:17:05Z","published":"2024-04-17T14:17:05Z","title":"Neural Shr√∂dinger Bridge Matching for Pansharpening","summary":"  Recent diffusion probabilistic models (DPM) in the field of pansharpening\nhave been gradually gaining attention and have achieved state-of-the-art (SOTA)\nperformance. In this paper, we identify shortcomings in directly applying DPMs\nto the task of pansharpening as an inverse problem: 1) initiating sampling\ndirectly from Gaussian noise neglects the low-resolution multispectral image\n(LRMS) as a prior; 2) low sampling efficiency often necessitates a higher\nnumber of sampling steps. We first reformulate pansharpening into the\nstochastic differential equation (SDE) form of an inverse problem. Building\nupon this, we propose a Schr\\\"odinger bridge matching method that addresses\nboth issues.\n  We design an efficient deep neural network architecture tailored for the\nproposed SB matching.\n  In comparison to the well-established DL-regressive-based framework and the\nrecent DPM framework, our method demonstrates SOTA performance with fewer\nsampling steps. Moreover, we discuss the relationship between SB matching and\nother methods based on SDEs and ordinary differential equations (ODEs), as well\nas its connection with optimal transport.\n  Code will be available.\n","authors":["Zihan Cao","Xiao Wu","Liang-Jian Deng"],"pdf_url":"https://arxiv.org/pdf/2404.11416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11401v1","updated":"2024-04-17T14:07:22Z","published":"2024-04-17T14:07:22Z","title":"RainyScape: Unsupervised Rainy Scene Reconstruction using Decoupled\n  Neural Rendering","summary":"  We propose RainyScape, an unsupervised framework for reconstructing clean\nscenes from a collection of multi-view rainy images. RainyScape consists of two\nmain modules: a neural rendering module and a rain-prediction module that\nincorporates a predictor network and a learnable latent embedding that captures\nthe rain characteristics of the scene. Specifically, based on the spectral bias\nproperty of neural networks, we first optimize the neural rendering pipeline to\nobtain a low-frequency scene representation. Subsequently, we jointly optimize\nthe two modules, driven by the proposed adaptive direction-sensitive\ngradient-based reconstruction loss, which encourages the network to distinguish\nbetween scene details and rain streaks, facilitating the propagation of\ngradients to the relevant components. Extensive experiments on both the classic\nneural radiance field and the recently proposed 3D Gaussian splatting\ndemonstrate the superiority of our method in effectively eliminating rain\nstreaks and rendering clean images, achieving state-of-the-art performance. The\nconstructed high-quality dataset and source code will be publicly available.\n","authors":["Xianqiang Lyu","Hui Liu","Junhui Hou"],"pdf_url":"https://arxiv.org/pdf/2404.11401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16749v3","updated":"2024-04-17T14:06:28Z","published":"2024-02-26T17:11:11Z","title":"MISC: Ultra-low Bitrate Image Semantic Compression Driven by Large\n  Multimodal Model","summary":"  With the evolution of storage and communication protocols, ultra-low bitrate\nimage compression has become a highly demanding topic. However, existing\ncompression algorithms must sacrifice either consistency with the ground truth\nor perceptual quality at ultra-low bitrate. In recent years, the rapid\ndevelopment of the Large Multimodal Model (LMM) has made it possible to balance\nthese two goals. To solve this problem, this paper proposes a method called\nMultimodal Image Semantic Compression (MISC), which consists of an LMM encoder\nfor extracting the semantic information of the image, a map encoder to locate\nthe region corresponding to the semantic, an image encoder generates an\nextremely compressed bitstream, and a decoder reconstructs the image based on\nthe above information. Experimental results show that our proposed MISC is\nsuitable for compressing both traditional Natural Sense Images (NSIs) and\nemerging AI-Generated Images (AIGIs) content. It can achieve optimal\nconsistency and perception results while saving 50% bitrate, which has strong\npotential applications in the next generation of storage and communication. The\ncode will be released on https://github.com/lcysyzxdxc/MISC.\n","authors":["Chunyi Li","Guo Lu","Donghui Feng","Haoning Wu","Zicheng Zhang","Xiaohong Liu","Guangtao Zhai","Weisi Lin","Wenjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.16749v3.pdf","comment":"13 page, 11 figures, 4 tables"},{"id":"http://arxiv.org/abs/2312.06722v2","updated":"2024-04-17T13:56:06Z","published":"2023-12-11T03:35:58Z","title":"EgoPlan-Bench: Benchmarking Egocentric Embodied Planning with Multimodal\n  Large Language Models","summary":"  Multimodal Large Language Models, combining the remarkable reasoning and\ngeneralization capabilities of Large Language Models (LLMs) with the ability to\ncomprehend visual inputs, have opened up new avenues for embodied task\nplanning. Given diverse environmental inputs, including real-time task\nprogress, visual observations, and open-form language instructions, a\nproficient task planner is expected to predict feasible actions, which is a\nfeat inherently achievable by Multimodal Large Language Models (MLLMs). In this\npaper, we aim to quantitatively investigate the potential of MLLMs as embodied\ntask planners in real-world scenarios by introducing a benchmark with human\nannotations named EgoPlan-Bench. Our benchmark is distinguished by realistic\ntasks derived from real-world videos, a diverse set of actions involving\ninteractions with hundreds of different objects, and complex visual\nobservations from varied scenes. We evaluate a wide range of MLLMs, revealing\nthat these models have not yet evolved into embodied planning generalists (even\nGPT-4V). We further construct an instruction-tuning dataset EgoPlan-IT from\nvideos with human-object interactions, to facilitate the learning of high-level\ntask planning in intricate real-world situations. The experiment results\ndemonstrate that the model tuned on EgoPlan-IT not only significantly improves\nperformance on our benchmark, but can also be applied as a task planner for\nguiding embodied agents in simulations.\n","authors":["Yi Chen","Yuying Ge","Yixiao Ge","Mingyu Ding","Bohao Li","Rui Wang","Ruifeng Xu","Ying Shan","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2312.06722v2.pdf","comment":"Project released at: https://github.com/ChenYi99/EgoPlan"},{"id":"http://arxiv.org/abs/2310.20621v2","updated":"2024-04-17T13:41:07Z","published":"2023-10-31T16:54:14Z","title":"Deepfake detection by exploiting surface anomalies: the SurFake approach","summary":"  The ever-increasing use of synthetically generated content in different\nsectors of our everyday life, one for all media information, poses a strong\nneed for deepfake detection tools in order to avoid the proliferation of\naltered messages. The process to identify manipulated content, in particular\nimages and videos, is basically performed by looking for the presence of some\ninconsistencies and/or anomalies specifically due to the fake generation\nprocess. Different techniques exist in the scientific literature that exploit\ndiverse ad-hoc features in order to highlight possible modifications. In this\npaper, we propose to investigate how deepfake creation can impact on the\ncharacteristics that the whole scene had at the time of the acquisition. In\nparticular, when an image (video) is captured the overall geometry of the scene\n(e.g. surfaces) and the acquisition process (e.g. illumination) determine a\nunivocal environment that is directly represented by the image pixel values;\nall these intrinsic relations are possibly changed by the deepfake generation\nprocess. By resorting to the analysis of the characteristics of the surfaces\ndepicted in the image it is possible to obtain a descriptor usable to train a\nCNN for deepfake detection: we refer to such an approach as SurFake.\nExperimental results carried out on the FF++ dataset for different kinds of\ndeepfake forgeries and diverse deep learning models confirm that such a feature\ncan be adopted to discriminate between pristine and altered images;\nfurthermore, experiments witness that it can also be combined with visual data\nto provide a certain improvement in terms of detection accuracy.\n","authors":["Andrea Ciamarra","Roberto Caldelli","Federico Becattini","Lorenzo Seidenari","Alberto Del Bimbo"],"pdf_url":"https://arxiv.org/pdf/2310.20621v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11375v1","updated":"2024-04-17T13:33:09Z","published":"2024-04-17T13:33:09Z","title":"Text-controlled Motion Mamba: Text-Instructed Temporal Grounding of\n  Human Motion","summary":"  Human motion understanding is a fundamental task with diverse practical\napplications, facilitated by the availability of large-scale motion capture\ndatasets. Recent studies focus on text-motion tasks, such as text-based motion\ngeneration, editing and question answering. In this study, we introduce the\nnovel task of text-based human motion grounding (THMG), aimed at precisely\nlocalizing temporal segments corresponding to given textual descriptions within\nuntrimmed motion sequences. Capturing global temporal information is crucial\nfor the THMG task. However, transformer-based models that rely on global\ntemporal self-attention face challenges when handling long untrimmed sequences\ndue to the quadratic computational cost. We address these challenges by\nproposing Text-controlled Motion Mamba (TM-Mamba), a unified model that\nintegrates temporal global context, language query control, and spatial graph\ntopology with only linear memory cost. The core of the model is a\ntext-controlled selection mechanism which dynamically incorporates global\ntemporal information based on text query. The model is further enhanced to be\ntopology-aware through the integration of relational embeddings. For\nevaluation, we introduce BABEL-Grounding, the first text-motion dataset that\nprovides detailed textual descriptions of human actions along with their\ncorresponding temporal segments. Extensive evaluations demonstrate the\neffectiveness of TM-Mamba on BABEL-Grounding.\n","authors":["Xinghan Wang","Zixi Kang","Yadong Mu"],"pdf_url":"https://arxiv.org/pdf/2404.11375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13756v3","updated":"2024-04-17T13:32:15Z","published":"2024-02-21T12:34:31Z","title":"High-throughput Visual Nano-drone to Nano-drone Relative Localization\n  using Onboard Fully Convolutional Networks","summary":"  Relative drone-to-drone localization is a fundamental building block for any\nswarm operations. We address this task in the context of miniaturized\nnano-drones, i.e., 10cm in diameter, which show an ever-growing interest due to\nnovel use cases enabled by their reduced form factor. The price for their\nversatility comes with limited onboard resources, i.e., sensors, processing\nunits, and memory, which limits the complexity of the onboard algorithms. A\ntraditional solution to overcome these limitations is represented by\nlightweight deep learning models directly deployed aboard nano-drones. This\nwork tackles the challenging relative pose estimation between nano-drones using\nonly a gray-scale low-resolution camera and an ultra-low-power System-on-Chip\n(SoC) hosted onboard. We present a vertically integrated system based on a\nnovel vision-based fully convolutional neural network (FCNN), which runs at\n39Hz within 101mW onboard a Crazyflie nano-drone extended with the GWT GAP8\nSoC. We compare our FCNN against three State-of-the-Art (SoA) systems.\nConsidering the best-performing SoA approach, our model results in an R-squared\nimprovement from 32 to 47% on the horizontal image coordinate and from 18 to\n55% on the vertical image coordinate, on a real-world dataset of 30k images.\nFinally, our in-field tests show a reduction of the average tracking error of\n37% compared to a previous SoA work and an endurance performance up to the\nentire battery lifetime of 4 minutes.\n","authors":["Luca Crupi","Alessandro Giusti","Daniele Palossi"],"pdf_url":"https://arxiv.org/pdf/2402.13756v3.pdf","comment":"ICRA 2024, IEEE Conference"},{"id":"http://arxiv.org/abs/2401.11470v2","updated":"2024-04-17T13:25:38Z","published":"2024-01-21T11:55:42Z","title":"Exploring Missing Modality in Multimodal Egocentric Datasets","summary":"  Multimodal video understanding is crucial for analyzing egocentric videos,\nwhere integrating multiple sensory signals significantly enhances action\nrecognition and moment localization. However, practical applications often\ngrapple with incomplete modalities due to factors like privacy concerns,\nefficiency demands, or hardware malfunctions. Addressing this, our study delves\ninto the impact of missing modalities on egocentric action recognition,\nparticularly within transformer-based models. We introduce a novel concept\n-Missing Modality Token (MMT)-to maintain performance even when modalities are\nabsent, a strategy that proves effective in the Ego4D, Epic-Kitchens, and\nEpic-Sounds datasets. Our method mitigates the performance loss, reducing it\nfrom its original $\\sim 30\\%$ drop to only $\\sim 10\\%$ when half of the test\nset is modal-incomplete. Through extensive experimentation, we demonstrate the\nadaptability of MMT to different training scenarios and its superiority in\nhandling missing modalities compared to current methods. Our research\ncontributes a comprehensive analysis and an innovative approach, opening\navenues for more resilient multimodal systems in real-world settings.\n","authors":["Merey Ramazanova","Alejandro Pardo","Humam Alwassel","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2401.11470v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10247v2","updated":"2024-04-17T13:25:35Z","published":"2023-03-17T20:54:04Z","title":"Video shutter angle estimation using optical flow and linear blur","summary":"  We present a method for estimating the shutter angle, a.k.a. exposure\nfraction - the ratio of the exposure time and the reciprocal of frame rate - of\nvideoclips containing motion. The approach exploits the relation of the\nexposure fraction, optical flow, and linear motion blur. Robustness is achieved\nby selecting image patches where both the optical flow and blur estimates are\nreliable, checking their consistency. The method was evaluated on the publicly\navailable Beam-Splitter Dataset with a range of exposure fractions from 0.015\nto 0.36. The best achieved mean absolute error of estimates was 0.039. We\nsuccessfully test the suitability of the method for a forensic application of\ndetection of video tampering by frame removal or insertion\n","authors":["David Korcak","Jiri Matas"],"pdf_url":"https://arxiv.org/pdf/2303.10247v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11361v1","updated":"2024-04-17T13:18:39Z","published":"2024-04-17T13:18:39Z","title":"Boosting Medical Image Segmentation Performance with Adaptive\n  Convolution Layer","summary":"  Medical image segmentation plays a vital role in various clinical\napplications, enabling accurate delineation and analysis of anatomical\nstructures or pathological regions. Traditional CNNs have achieved remarkable\nsuccess in this field. However, they often rely on fixed kernel sizes, which\ncan limit their performance and adaptability in medical images where features\nexhibit diverse scales and configurations due to variability in equipment,\ntarget sizes, and expert interpretations.\n  In this paper, we propose an adaptive layer placed ahead of leading\ndeep-learning models such as UCTransNet, which dynamically adjusts the kernel\nsize based on the local context of the input image.\n  By adaptively capturing and fusing features at multiple scales, our approach\nenhances the network's ability to handle diverse anatomical structures and\nsubtle image details, even for recently performing architectures that\ninternally implement intra-scale modules, such as UCTransnet.\n  Extensive experiments are conducted on\n  benchmark medical image datasets to evaluate the effectiveness of our\nproposal. It consistently outperforms traditional \\glspl{CNN} with fixed kernel\nsizes with a similar number of parameters, achieving superior segmentation\nAccuracy, Dice, and IoU in popular datasets such as SegPC2021 and ISIC2018. The\nmodel and data are published in the open-source repository, ensuring\ntransparency and reproducibility of our promising results.\n","authors":["Seyed M. R. Modaresi","Aomar Osmani","Mohammadreza Razzazi","Abdelghani Chibani"],"pdf_url":"https://arxiv.org/pdf/2404.11361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11358v1","updated":"2024-04-17T13:14:52Z","published":"2024-04-17T13:14:52Z","title":"DeblurGS: Gaussian Splatting for Camera Motion Blur","summary":"  Although significant progress has been made in reconstructing sharp 3D scenes\nfrom motion-blurred images, a transition to real-world applications remains\nchallenging. The primary obstacle stems from the severe blur which leads to\ninaccuracies in the acquisition of initial camera poses through\nStructure-from-Motion, a critical aspect often overlooked by previous\napproaches. To address this challenge, we propose DeblurGS, a method to\noptimize sharp 3D Gaussian Splatting from motion-blurred images, even with the\nnoisy camera pose initialization. We restore a fine-grained sharp scene by\nleveraging the remarkable reconstruction capability of 3D Gaussian Splatting.\nOur approach estimates the 6-Degree-of-Freedom camera motion for each blurry\nobservation and synthesizes corresponding blurry renderings for the\noptimization process. Furthermore, we propose Gaussian Densification Annealing\nstrategy to prevent the generation of inaccurate Gaussians at erroneous\nlocations during the early training stages when camera motion is still\nimprecise. Comprehensive experiments demonstrate that our DeblurGS achieves\nstate-of-the-art performance in deblurring and novel view synthesis for\nreal-world and synthetic benchmark datasets, as well as field-captured blurry\nsmartphone videos.\n","authors":["Jeongtaek Oh","Jaeyoung Chung","Dongwoo Lee","Kyoung Mu Lee"],"pdf_url":"https://arxiv.org/pdf/2404.11358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11357v1","updated":"2024-04-17T13:12:14Z","published":"2024-04-17T13:12:14Z","title":"Detector Collapse: Backdooring Object Detection to Catastrophic Overload\n  or Blindness","summary":"  Object detection tasks, crucial in safety-critical systems like autonomous\ndriving, focus on pinpointing object locations. These detectors are known to be\nsusceptible to backdoor attacks. However, existing backdoor techniques have\nprimarily been adapted from classification tasks, overlooking deeper\nvulnerabilities specific to object detection. This paper is dedicated to\nbridging this gap by introducing Detector Collapse} (DC), a brand-new backdoor\nattack paradigm tailored for object detection. DC is designed to instantly\nincapacitate detectors (i.e., severely impairing detector's performance and\nculminating in a denial-of-service). To this end, we develop two innovative\nattack schemes: Sponge for triggering widespread misidentifications and\nBlinding for rendering objects invisible. Remarkably, we introduce a novel\npoisoning strategy exploiting natural objects, enabling DC to act as a\npractical backdoor in real-world environments. Our experiments on different\ndetectors across several benchmarks show a significant improvement\n($\\sim$10\\%-60\\% absolute and $\\sim$2-7$\\times$ relative) in attack efficacy\nover state-of-the-art attacks.\n","authors":["Hangtao Zhang","Shengshan Hu","Yichen Wang","Leo Yu Zhang","Ziqi Zhou","Xianlong Wang","Yanjun Zhang","Chao Chen"],"pdf_url":"https://arxiv.org/pdf/2404.11357v1.pdf","comment":"Accepted by IJCAI-24"},{"id":"http://arxiv.org/abs/2404.11355v1","updated":"2024-04-17T13:09:44Z","published":"2024-04-17T13:09:44Z","title":"Consisaug: A Consistency-based Augmentation for Polyp Detection in\n  Endoscopy Image Analysis","summary":"  Colorectal cancer (CRC), which frequently originates from initially benign\npolyps, remains a significant contributor to global cancer-related mortality.\nEarly and accurate detection of these polyps via colonoscopy is crucial for CRC\nprevention. However, traditional colonoscopy methods depend heavily on the\noperator's experience, leading to suboptimal polyp detection rates. Besides,\nthe public database are limited in polyp size and shape diversity. To enhance\nthe available data for polyp detection, we introduce Consisaug, an innovative\nand effective methodology to augment data that leverages deep learning. We\nutilize the constraint that when the image is flipped the class label should be\nequal and the bonding boxes should be consistent. We implement our Consisaug on\nfive public polyp datasets and at three backbones, and the results show the\neffectiveness of our method.\n","authors":["Ziyu Zhou","Wenyuan Shen","Chang Liu"],"pdf_url":"https://arxiv.org/pdf/2404.11355v1.pdf","comment":"MLMI 2023"},{"id":"http://arxiv.org/abs/2404.11339v1","updated":"2024-04-17T13:00:05Z","published":"2024-04-17T13:00:05Z","title":"Best Practices for a Handwritten Text Recognition System","summary":"  Handwritten text recognition has been developed rapidly in the recent years,\nfollowing the rise of deep learning and its applications. Though deep learning\nmethods provide notable boost in performance concerning text recognition,\nnon-trivial deviation in performance can be detected even when small\npre-processing or architectural/optimization elements are changed. This work\nfollows a ``best practice'' rationale; highlight simple yet effective empirical\npractices that can further help training and provide well-performing\nhandwritten text recognition systems. Specifically, we considered three basic\naspects of a deep HTR system and we proposed simple yet effective solutions: 1)\nretain the aspect ratio of the images in the preprocessing step, 2) use\nmax-pooling for converting the 3D feature map of CNN output into a sequence of\nfeatures and 3) assist the training procedure via an additional CTC loss which\nacts as a shortcut on the max-pooled sequential features. Using these proposed\nsimple modifications, one can attain close to state-of-the-art results, while\nconsidering a basic convolutional-recurrent (CNN+LSTM) architecture, for both\nIAM and RIMES datasets. Code is available at\nhttps://github.com/georgeretsi/HTR-best-practices/.\n","authors":["George Retsinas","Giorgos Sfikas","Basilis Gatos","Christophoros Nikou"],"pdf_url":"https://arxiv.org/pdf/2404.11339v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11336v1","updated":"2024-04-17T12:53:57Z","published":"2024-04-17T12:53:57Z","title":"Vision-based control for landing an aerial vehicle on a marine vessel","summary":"  This work addresses the landing problem of an aerial vehicle, exemplified by\na simple quadrotor, on a moving platform using image-based visual servo\ncontrol. First, the mathematical model of the quadrotor aircraft is introduced,\nfollowed by the design of the inner-loop control. At the second stage, the\nimage features on the textured target plane are exploited to derive a\nvision-based control law. The image of the spherical centroid of a set of\nlandmarks present in the landing target is used as a position measurement,\nwhereas the translational optical flow is used as velocity measurement. The\nkinematics of the vision-based system is expressed in terms of the observable\nfeatures, and the proposed control law guarantees convergence without\nestimating the unknown distance between the vision system and the target, which\nis also guaranteed to remain strictly positive, avoiding undesired collisions.\nThe performance of the proposed control law is evaluated in MATLAB and 3-D\nsimulation software Gazebo. Simulation results for a quadrotor UAV are provided\nfor different velocity profiles of the moving target, showcasing the robustness\nof the proposed controller.\n","authors":["Haohua Dong"],"pdf_url":"https://arxiv.org/pdf/2404.11336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11335v1","updated":"2024-04-17T12:53:45Z","published":"2024-04-17T12:53:45Z","title":"SoccerNet Game State Reconstruction: End-to-End Athlete Tracking and\n  Identification on a Minimap","summary":"  Tracking and identifying athletes on the pitch holds a central role in\ncollecting essential insights from the game, such as estimating the total\ndistance covered by players or understanding team tactics. This tracking and\nidentification process is crucial for reconstructing the game state, defined by\nthe athletes' positions and identities on a 2D top-view of the pitch, (i.e. a\nminimap). However, reconstructing the game state from videos captured by a\nsingle camera is challenging. It requires understanding the position of the\nathletes and the viewpoint of the camera to localize and identify players\nwithin the field. In this work, we formalize the task of Game State\nReconstruction and introduce SoccerNet-GSR, a novel Game State Reconstruction\ndataset focusing on football videos. SoccerNet-GSR is composed of 200 video\nsequences of 30 seconds, annotated with 9.37 million line points for pitch\nlocalization and camera calibration, as well as over 2.36 million athlete\npositions on the pitch with their respective role, team, and jersey number.\nFurthermore, we introduce GS-HOTA, a novel metric to evaluate game state\nreconstruction methods. Finally, we propose and release an end-to-end baseline\nfor game state reconstruction, bootstrapping the research on this task. Our\nexperiments show that GSR is a challenging novel task, which opens the field\nfor future research. Our dataset and codebase are publicly available at\nhttps://github.com/SoccerNet/sn-gamestate.\n","authors":["Vladimir Somers","Victor Joos","Anthony Cioppa","Silvio Giancola","Seyed Abolfazl Ghasemzadeh","Floriane Magera","Baptiste Standaert","Amir Mohammad Mansourian","Xin Zhou","Shohreh Kasaei","Bernard Ghanem","Alexandre Alahi","Marc Van Droogenbroeck","Christophe De Vleeschouwer"],"pdf_url":"https://arxiv.org/pdf/2404.11335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11327v1","updated":"2024-04-17T12:39:48Z","published":"2024-04-17T12:39:48Z","title":"Following the Human Thread in Social Navigation","summary":"  The success of collaboration between humans and robots in shared environments\nrelies on the robot's real-time adaptation to human motion. Specifically, in\nSocial Navigation, the agent should be close enough to assist but ready to back\nup to let the human move freely, avoiding collisions. Human trajectories emerge\nas crucial cues in Social Navigation, but they are partially observable from\nthe robot's egocentric view and computationally complex to process.\n  We propose the first Social Dynamics Adaptation model (SDA) based on the\nrobot's state-action history to infer the social dynamics. We propose a\ntwo-stage Reinforcement Learning framework: the first learns to encode the\nhuman trajectories into social dynamics and learns a motion policy conditioned\non this encoded information, the current status, and the previous action. Here,\nthe trajectories are fully visible, i.e., assumed as privileged information. In\nthe second stage, the trained policy operates without direct access to\ntrajectories. Instead, the model infers the social dynamics solely from the\nhistory of previous actions and statuses in real-time. Tested on the novel\nHabitat 3.0 platform, SDA sets a novel state of the art (SoA) performance in\nfinding and following humans.\n","authors":["Luca Scofano","Alessio Sampieri","Tommaso Campari","Valentino Sacco","Indro Spinelli","Lamberto Ballan","Fabio Galasso"],"pdf_url":"https://arxiv.org/pdf/2404.11327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11326v1","updated":"2024-04-17T12:38:58Z","published":"2024-04-17T12:38:58Z","title":"Single-temporal Supervised Remote Change Detection for Domain\n  Generalization","summary":"  Change detection is widely applied in remote sensing image analysis. Existing\nmethods require training models separately for each dataset, which leads to\npoor domain generalization. Moreover, these methods rely heavily on large\namounts of high-quality pair-labelled data for training, which is expensive and\nimpractical. In this paper, we propose a multimodal contrastive learning\n(ChangeCLIP) based on visual-language pre-training for change detection domain\ngeneralization. Additionally, we propose a dynamic context optimization for\nprompt learning. Meanwhile, to address the data dependency issue of existing\nmethods, we introduce a single-temporal and controllable AI-generated training\nstrategy (SAIN). This allows us to train the model using a large number of\nsingle-temporal images without image pairs in the real world, achieving\nexcellent generalization. Extensive experiments on series of real change\ndetection datasets validate the superiority and strong generalization of\nChangeCLIP, outperforming state-of-the-art change detection methods. Code will\nbe available.\n","authors":["Qiangang Du","Jinlong Peng","Xu Chen","Qingdong He","Qiang Nie","Wenbing Zhu","Mingmin Chi","Yabiao Wang","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2404.11326v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01431v2","updated":"2024-04-17T12:36:06Z","published":"2023-12-03T15:40:10Z","title":"D$^2$ST-Adapter: Disentangled-and-Deformable Spatio-Temporal Adapter for\n  Few-shot Action Recognition","summary":"  Adapting large pre-trained image models to few-shot action recognition has\nproven to be an effective and efficient strategy for learning robust feature\nextractors, which is essential for few-shot learning. Typical fine-tuning based\nadaptation paradigm is prone to overfitting in the few-shot learning scenarios\nand offers little modeling flexibility for learning temporal features in video\ndata. In this work we present the Disentangled-and-Deformable Spatio-Temporal\nAdapter (D$^2$ST-Adapter), which is a novel adapter tuning framework\nwell-suited for few-shot action recognition due to lightweight design and low\nparameter-learning overhead. It is designed in a dual-pathway architecture to\nencode spatial and temporal features in a disentangled manner. In particular,\nwe devise the anisotropic Deformable Spatio-Temporal Attention module as the\ncore component of D$^2$ST-Adapter, which can be tailored with anisotropic\nsampling densities along spatial and temporal domains to learn spatial and\ntemporal features specifically in corresponding pathways, allowing our\nD$^2$ST-Adapter to encode features in a global view in 3D spatio-temporal space\nwhile maintaining a lightweight design. Extensive experiments with\ninstantiations of our method on both pre-trained ResNet and ViT demonstrate the\nsuperiority of our method over state-of-the-art methods for few-shot action\nrecognition. Our method is particularly well-suited to challenging scenarios\nwhere temporal dynamics are critical for action recognition.\n","authors":["Wenjie Pei","Qizhong Tan","Guangming Lu","Jiandong Tian"],"pdf_url":"https://arxiv.org/pdf/2312.01431v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11322v1","updated":"2024-04-17T12:34:49Z","published":"2024-04-17T12:34:49Z","title":"VBR: A Vision Benchmark in Rome","summary":"  This paper presents a vision and perception research dataset collected in\nRome, featuring RGB data, 3D point clouds, IMU, and GPS data. We introduce a\nnew benchmark targeting visual odometry and SLAM, to advance the research in\nautonomous robotics and computer vision. This work complements existing\ndatasets by simultaneously addressing several issues, such as environment\ndiversity, motion patterns, and sensor frequency. It uses up-to-date devices\nand presents effective procedures to accurately calibrate the intrinsic and\nextrinsic of the sensors while addressing temporal synchronization. During\nrecording, we cover multi-floor buildings, gardens, urban and highway\nscenarios. Combining handheld and car-based data collections, our setup can\nsimulate any robot (quadrupeds, quadrotors, autonomous vehicles). The dataset\nincludes an accurate 6-dof ground truth based on a novel methodology that\nrefines the RTK-GPS estimate with LiDAR point clouds through Bundle Adjustment.\nAll sequences divided in training and testing are accessible through our\nwebsite.\n","authors":["Leonardo Brizi","Emanuele Giacomini","Luca Di Giammarino","Simone Ferrari","Omar Salem","Lorenzo De Rebotti","Giorgio Grisetti"],"pdf_url":"https://arxiv.org/pdf/2404.11322v1.pdf","comment":"Accepted at IEEE ICRA 2024 Website:\n  https://rvp-group.net/datasets/slam.html"},{"id":"http://arxiv.org/abs/2404.11318v1","updated":"2024-04-17T12:32:10Z","published":"2024-04-17T12:32:10Z","title":"Leveraging Fine-Grained Information and Noise Decoupling for Remote\n  Sensing Change Detection","summary":"  Change detection aims to identify remote sense object changes by analyzing\ndata between bitemporal image pairs. Due to the large temporal and spatial span\nof data collection in change detection image pairs, there are often a\nsignificant amount of task-specific and task-agnostic noise. Previous effort\nhas focused excessively on denoising, with this goes a great deal of loss of\nfine-grained information. In this paper, we revisit the importance of\nfine-grained features in change detection and propose a series of operations\nfor fine-grained information compensation and noise decoupling (FINO). First,\nthe context is utilized to compensate for the fine-grained information in the\nfeature space. Next, a shape-aware and a brightness-aware module are designed\nto improve the capacity for representation learning. The shape-aware module\nguides the backbone for more precise shape estimation, guiding the backbone\nnetwork in extracting object shape features. The brightness-aware module learns\na overall brightness estimation to improve the model's robustness to\ntask-agnostic noise. Finally, a task-specific noise decoupling structure is\ndesigned as a way to improve the model's ability to separate noise interference\nfrom feature similarity. With these training schemes, our proposed method\nachieves new state-of-the-art (SOTA) results in multiple change detection\nbenchmarks. The code will be made available.\n","authors":["Qiangang Du","Jinlong Peng","Changan Wang","Xu Chen","Qingdong He","Wenbing Zhu","Mingmin Chi","Yabiao Wang","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2404.11318v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11317v1","updated":"2024-04-17T12:30:54Z","published":"2024-04-17T12:30:54Z","title":"Improving Composed Image Retrieval via Contrastive Learning with Scaling\n  Positives and Negatives","summary":"  The Composed Image Retrieval (CIR) task aims to retrieve target images using\na composed query consisting of a reference image and a modified text. Advanced\nmethods often utilize contrastive learning as the optimization objective, which\nbenefits from adequate positive and negative examples. However, the triplet for\nCIR incurs high manual annotation costs, resulting in limited positive\nexamples. Furthermore, existing methods commonly use in-batch negative\nsampling, which reduces the negative number available for the model. To address\nthe problem of lack of positives, we propose a data generation method by\nleveraging a multi-modal large language model to construct triplets for CIR. To\nintroduce more negatives during fine-tuning, we design a two-stage fine-tuning\nframework for CIR, whose second stage introduces plenty of static\nrepresentations of negatives to optimize the representation space rapidly. The\nabove two improvements can be effectively stacked and designed to be\nplug-and-play, easily applied to existing CIR models without changing their\noriginal architectures. Extensive experiments and ablation analysis demonstrate\nthat our method effectively scales positives and negatives and achieves\nstate-of-the-art results on both FashionIQ and CIRR datasets. In addition, our\nmethods also perform well in zero-shot composed image retrieval, providing a\nnew CIR solution for the low-resources scenario.\n","authors":["Zhangchi Feng","Richong Zhang","Zhijie Nie"],"pdf_url":"https://arxiv.org/pdf/2404.11317v1.pdf","comment":"12 pages, 11 figures"},{"id":"http://arxiv.org/abs/2309.11930v2","updated":"2024-04-17T12:27:25Z","published":"2023-09-21T09:44:39Z","title":"Bridging the Gap: Learning Pace Synchronization for Open-World\n  Semi-Supervised Learning","summary":"  In open-world semi-supervised learning, a machine learning model is tasked\nwith uncovering novel categories from unlabeled data while maintaining\nperformance on seen categories from labeled data. The central challenge is the\nsubstantial learning gap between seen and novel categories, as the model learns\nthe former faster due to accurate supervisory information. Moreover, capturing\nthe semantics of unlabeled novel category samples is also challenging due to\nthe missing label information. To address the above issues, we introduce 1) the\nadaptive synchronizing marginal loss which imposes class-specific negative\nmargins to alleviate the model bias towards seen classes, and 2) the\npseudo-label contrastive clustering which exploits pseudo-labels predicted by\nthe model to group unlabeled data from the same category together in the output\nspace. Extensive experiments on benchmark datasets demonstrate that previous\napproaches may significantly hinder novel class learning, whereas our method\nstrikingly balances the learning pace between seen and novel classes, achieving\na remarkable 3% average accuracy increase on the ImageNet dataset. Importantly,\nwe find that fine-tuning the self-supervised pre-trained model significantly\nboosts the performance, which is overlooked in prior literature. Our code is\navailable at https://github.com/yebo0216best/LPS-main.\n","authors":["Bo Ye","Kai Gan","Tong Wei","Min-Ling Zhang"],"pdf_url":"https://arxiv.org/pdf/2309.11930v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11309v1","updated":"2024-04-17T12:21:57Z","published":"2024-04-17T12:21:57Z","title":"Achieving Rotation Invariance in Convolution Operations: Shifting from\n  Data-Driven to Mechanism-Assured","summary":"  Achieving rotation invariance in deep neural networks without relying on data\nhas always been a hot research topic. Intrinsic rotation invariance can enhance\nthe model's feature representation capability, enabling better performance in\ntasks such as multi-orientation object recognition and detection. Based on\nvarious types of non-learnable operators, including gradient, sort, local\nbinary pattern, maximum, etc., this paper designs a set of new convolution\noperations that are natually invariant to arbitrary rotations. Unlike most\nprevious studies, these rotation-invariant convolutions (RIConvs) have the same\nnumber of learnable parameters and a similar computational process as\nconventional convolution operations, allowing them to be interchangeable. Using\nthe MNIST-Rot dataset, we first verify the invariance of these RIConvs under\nvarious rotation angles and compare their performance with previous\nrotation-invariant convolutional neural networks (RI-CNNs). Two types of\nRIConvs based on gradient operators achieve state-of-the-art results.\nSubsequently, we combine RIConvs with different types and depths of classic CNN\nbackbones. Using the OuTex_00012, MTARSI, and NWPU-RESISC-45 datasets, we test\ntheir performance on texture recognition, aircraft type recognition, and remote\nsensing image classification tasks. The results show that RIConvs significantly\nimprove the accuracy of these CNN backbones, especially when the training data\nis limited. Furthermore, we find that even with data augmentation, RIConvs can\nfurther enhance model performance.\n","authors":["Hanlin Mo","Guoying Zhao"],"pdf_url":"https://arxiv.org/pdf/2404.11309v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11302v1","updated":"2024-04-17T12:13:18Z","published":"2024-04-17T12:13:18Z","title":"A Semantic Segmentation-guided Approach for Ground-to-Aerial Image\n  Matching","summary":"  Nowadays the accurate geo-localization of ground-view images has an important\nrole across domains as diverse as journalism, forensics analysis, transports,\nand Earth Observation. This work addresses the problem of matching a query\nground-view image with the corresponding satellite image without GPS data. This\nis done by comparing the features from a ground-view image and a satellite one,\ninnovatively leveraging the corresponding latter's segmentation mask through a\nthree-stream Siamese-like network. The proposed method, Semantic Align Net\n(SAN), focuses on limited Field-of-View (FoV) and ground panorama images\n(images with a FoV of 360{\\deg}). The novelty lies in the fusion of satellite\nimages in combination with their semantic segmentation masks, aimed at ensuring\nthat the model can extract useful features and focus on the significant parts\nof the images. This work shows how SAN through semantic analysis of images\nimproves the performance on the unlabelled CVUSA dataset for all the tested\nFoVs.\n","authors":["Francesco Pro","Nikolaos Dionelis","Luca Maiano","Bertrand Le Saux","Irene Amerini"],"pdf_url":"https://arxiv.org/pdf/2404.11302v1.pdf","comment":"6 pages, 2 figures, 2 tables, Submitted to IGARSS 2024"},{"id":"http://arxiv.org/abs/2404.11299v1","updated":"2024-04-17T12:12:48Z","published":"2024-04-17T12:12:48Z","title":"Learning from Unlabelled Data with Transformers: Domain Adaptation for\n  Semantic Segmentation of High Resolution Aerial Images","summary":"  Data from satellites or aerial vehicles are most of the times unlabelled.\nAnnotating such data accurately is difficult, requires expertise, and is costly\nin terms of time. Even if Earth Observation (EO) data were correctly labelled,\nlabels might change over time. Learning from unlabelled data within a\nsemi-supervised learning framework for segmentation of aerial images is\nchallenging. In this paper, we develop a new model for semantic segmentation of\nunlabelled images, the Non-annotated Earth Observation Semantic Segmentation\n(NEOS) model. NEOS performs domain adaptation as the target domain does not\nhave ground truth semantic segmentation masks. The distribution inconsistencies\nbetween the target and source domains are due to differences in acquisition\nscenes, environment conditions, sensors, and times. Our model aligns the\nlearned representations of the different domains to make them coincide. The\nevaluation results show that NEOS is successful and outperforms other models\nfor semantic segmentation of unlabelled data.\n","authors":["Nikolaos Dionelis","Francesco Pro","Luca Maiano","Irene Amerini","Bertrand Le Saux"],"pdf_url":"https://arxiv.org/pdf/2404.11299v1.pdf","comment":"6 pages, 7 figures, Submitted to IGARSS 2024"},{"id":"http://arxiv.org/abs/2404.10588v2","updated":"2024-04-17T12:09:17Z","published":"2024-04-16T14:13:44Z","title":"Do Counterfactual Examples Complicate Adversarial Training?","summary":"  We leverage diffusion models to study the robustness-performance tradeoff of\nrobust classifiers. Our approach introduces a simple, pretrained diffusion\nmethod to generate low-norm counterfactual examples (CEs): semantically altered\ndata which results in different true class membership. We report that the\nconfidence and accuracy of robust models on their clean training data are\nassociated with the proximity of the data to their CEs. Moreover, robust models\nperform very poorly when evaluated on the CEs directly, as they become\nincreasingly invariant to the low-norm, semantic changes brought by CEs. The\nresults indicate a significant overlap between non-robust and semantic\nfeatures, countering the common assumption that non-robust features are not\ninterpretable.\n","authors":["Eric Yeats","Cameron Darwin","Eduardo Ortega","Frank Liu","Hai Li"],"pdf_url":"https://arxiv.org/pdf/2404.10588v2.pdf","comment":"Accepted as a short paper to the GCV Workshop at CVPR'24"},{"id":"http://arxiv.org/abs/2403.00303v2","updated":"2024-04-17T12:05:28Z","published":"2024-03-01T06:13:53Z","title":"ODM: A Text-Image Further Alignment Pre-training Approach for Scene Text\n  Detection and Spotting","summary":"  In recent years, text-image joint pre-training techniques have shown\npromising results in various tasks. However, in Optical Character Recognition\n(OCR) tasks, aligning text instances with their corresponding text regions in\nimages poses a challenge, as it requires effective alignment between text and\nOCR-Text (referring to the text in images as OCR-Text to distinguish from the\ntext in natural language) rather than a holistic understanding of the overall\nimage content. In this paper, we propose a new pre-training method called\nOCR-Text Destylization Modeling (ODM) that transfers diverse styles of text\nfound in images to a uniform style based on the text prompt. With ODM, we\nachieve better alignment between text and OCR-Text and enable pre-trained\nmodels to adapt to the complex and diverse styles of scene text detection and\nspotting tasks. Additionally, we have designed a new labeling generation method\nspecifically for ODM and combined it with our proposed Text-Controller module\nto address the challenge of annotation costs in OCR tasks, allowing a larger\namount of unlabeled data to participate in pre-training. Extensive experiments\non multiple public datasets demonstrate that our method significantly improves\nperformance and outperforms current pre-training methods in scene text\ndetection and spotting tasks. Code is available at\nhttps://github.com/PriNing/ODM.\n","authors":["Chen Duan","Pei Fu","Shan Guo","Qianyi Jiang","Xiaoming Wei"],"pdf_url":"https://arxiv.org/pdf/2403.00303v2.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2404.11291v1","updated":"2024-04-17T11:55:45Z","published":"2024-04-17T11:55:45Z","title":"Closely Interactive Human Reconstruction with Proxemics and\n  Physics-Guided Adaption","summary":"  Existing multi-person human reconstruction approaches mainly focus on\nrecovering accurate poses or avoiding penetration, but overlook the modeling of\nclose interactions. In this work, we tackle the task of reconstructing closely\ninteractive humans from a monocular video. The main challenge of this task\ncomes from insufficient visual information caused by depth ambiguity and severe\ninter-person occlusion. In view of this, we propose to leverage knowledge from\nproxemic behavior and physics to compensate the lack of visual information.\nThis is based on the observation that human interaction has specific patterns\nfollowing the social proxemics. Specifically, we first design a latent\nrepresentation based on Vector Quantised-Variational AutoEncoder (VQ-VAE) to\nmodel human interaction. A proxemics and physics guided diffusion model is then\nintroduced to denoise the initial distribution. We design the diffusion model\nas dual branch with each branch representing one individual such that the\ninteraction can be modeled via cross attention. With the learned priors of\nVQ-VAE and physical constraint as the additional information, our proposed\napproach is capable of estimating accurate poses that are also proxemics and\nphysics plausible. Experimental results on Hi4D, 3DPW, and CHI3D demonstrate\nthat our method outperforms existing approaches. The code is available at\n\\url{https://github.com/boycehbz/HumanInteraction}.\n","authors":["Buzhen Huang","Chen Li","Chongyang Xu","Liang Pan","Yangang Wang","Gim Hee Lee"],"pdf_url":"https://arxiv.org/pdf/2404.11291v1.pdf","comment":"CVPR2024"},{"id":"http://arxiv.org/abs/2306.16533v2","updated":"2024-04-17T11:38:12Z","published":"2023-06-28T20:06:36Z","title":"ICSVR: Investigating Compositional and Syntactic Understanding in Video\n  Retrieval Models","summary":"  Video retrieval (VR) involves retrieving the ground truth video from the\nvideo database given a text caption or vice-versa. The two important components\nof compositionality: objects & attributes and actions are joined using correct\nsyntax to form a proper text query. These components (objects & attributes,\nactions and syntax) each play an important role to help distinguish among\nvideos and retrieve the correct ground truth video. However, it is unclear what\nis the effect of these components on the video retrieval performance. We\ntherefore, conduct a systematic study to evaluate the compositional and\nsyntactic understanding of video retrieval models on standard benchmarks such\nas MSRVTT, MSVD and DIDEMO. The study is performed on two categories of video\nretrieval models: (i) which are pre-trained on video-text pairs and fine-tuned\non downstream video retrieval datasets (Eg. Frozen-in-Time, Violet, MCQ etc.)\n(ii) which adapt pre-trained image-text representations like CLIP for video\nretrieval (Eg. CLIP4Clip, XCLIP, CLIP2Video etc.). Our experiments reveal that\nactions and syntax play a minor role compared to objects & attributes in video\nunderstanding. Moreover, video retrieval models that use pre-trained image-text\nrepresentations (CLIP) have better syntactic and compositional understanding as\ncompared to models pre-trained on video-text data. The code is available at\nhttps://github.com/IntelLabs/multimodal_cognitive_ai/tree/main/ICSVR\n","authors":["Avinash Madasu","Vasudev Lal"],"pdf_url":"https://arxiv.org/pdf/2306.16533v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11273v1","updated":"2024-04-17T11:25:19Z","published":"2024-04-17T11:25:19Z","title":"Training Transformer Models by Wavelet Losses Improves Quantitative and\n  Visual Performance in Single Image Super-Resolution","summary":"  Transformer-based models have achieved remarkable results in low-level vision\ntasks including image super-resolution (SR). However, early Transformer-based\napproaches that rely on self-attention within non-overlapping windows encounter\nchallenges in acquiring global information. To activate more input pixels\nglobally, hybrid attention models have been proposed. Moreover, training by\nsolely minimizing pixel-wise RGB losses, such as L1, have been found inadequate\nfor capturing essential high-frequency details. This paper presents two\ncontributions: i) We introduce convolutional non-local sparse attention (NLSA)\nblocks to extend the hybrid transformer architecture in order to further\nenhance its receptive field. ii) We employ wavelet losses to train Transformer\nmodels to improve quantitative and subjective performance. While wavelet losses\nhave been explored previously, showing their power in training\nTransformer-based SR models is novel. Our experimental results demonstrate that\nthe proposed model provides state-of-the-art PSNR results as well as superior\nvisual performance across various benchmark datasets.\n","authors":["Cansu Korkmaz","A. Murat Tekalp"],"pdf_url":"https://arxiv.org/pdf/2404.11273v1.pdf","comment":"total of 10 pages including references, 5 tables and 5 figures,\n  accepted for NTIRE 2024 Single Image Super Resolution (x4) challenge"},{"id":"http://arxiv.org/abs/2404.11266v1","updated":"2024-04-17T11:17:12Z","published":"2024-04-17T11:17:12Z","title":"Criteria for Uncertainty-based Corner Cases Detection in Instance\n  Segmentation","summary":"  The operating environment of a highly automated vehicle is subject to change,\ne.g., weather, illumination, or the scenario containing different objects and\nother participants in which the highly automated vehicle has to navigate its\npassengers safely. These situations must be considered when developing and\nvalidating highly automated driving functions. This already poses a problem for\ntraining and evaluating deep learning models because without the costly\nlabeling of thousands of recordings, not knowing whether the data contains\nrelevant, interesting data for further model training, it is a guess under\nwhich conditions and situations the model performs poorly. For this purpose, we\npresent corner case criteria based on the predictive uncertainty. With our\ncorner case criteria, we are able to detect uncertainty-based corner cases of\nan object instance segmentation model without relying on ground truth (GT)\ndata. We evaluated each corner case criterion using the COCO and the NuImages\ndataset to analyze the potential of our approach. We also provide a corner case\ndecision function that allows us to distinguish each object into True Positive\n(TP), localization and/or classification corner case, or False Positive (FP).\nWe also present our first results of an iterative training cycle that\noutperforms the baseline and where the data added to the training dataset is\nselected based on the corner case decision function.\n","authors":["Florian Heidecker","Ahmad El-Khateeb","Maarten Bieshaar","Bernhard Sick"],"pdf_url":"https://arxiv.org/pdf/2404.11266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11265v1","updated":"2024-04-17T11:15:58Z","published":"2024-04-17T11:15:58Z","title":"The Victim and The Beneficiary: Exploiting a Poisoned Model to Train a\n  Clean Model on Poisoned Data","summary":"  Recently, backdoor attacks have posed a serious security threat to the\ntraining process of deep neural networks (DNNs). The attacked model behaves\nnormally on benign samples but outputs a specific result when the trigger is\npresent. However, compared with the rocketing progress of backdoor attacks,\nexisting defenses are difficult to deal with these threats effectively or\nrequire benign samples to work, which may be unavailable in real scenarios. In\nthis paper, we find that the poisoned samples and benign samples can be\ndistinguished with prediction entropy. This inspires us to propose a novel\ndual-network training framework: The Victim and The Beneficiary (V&B), which\nexploits a poisoned model to train a clean model without extra benign samples.\nFirstly, we sacrifice the Victim network to be a powerful poisoned sample\ndetector by training on suspicious samples. Secondly, we train the Beneficiary\nnetwork on the credible samples selected by the Victim to inhibit backdoor\ninjection. Thirdly, a semi-supervised suppression strategy is adopted for\nerasing potential backdoors and improving model performance. Furthermore, to\nbetter inhibit missed poisoned samples, we propose a strong data augmentation\nmethod, AttentionMix, which works well with our proposed V&B framework.\nExtensive experiments on two widely used datasets against 6 state-of-the-art\nattacks demonstrate that our framework is effective in preventing backdoor\ninjection and robust to various attacks while maintaining the performance on\nbenign samples. Our code is available at https://github.com/Zixuan-Zhu/VaB.\n","authors":["Zixuan Zhu","Rui Wang","Cong Zou","Lihua Jing"],"pdf_url":"https://arxiv.org/pdf/2404.11265v1.pdf","comment":"13 pages, 6 figures, published to ICCV"},{"id":"http://arxiv.org/abs/2402.17187v3","updated":"2024-04-17T11:08:02Z","published":"2024-02-27T03:53:27Z","title":"PE-MVCNet: Multi-view and Cross-modal Fusion Network for Pulmonary\n  Embolism Prediction","summary":"  The early detection of a pulmonary embolism (PE) is critical for enhancing\npatient survival rates. Both image-based and non-image-based features are of\nutmost importance in medical classification tasks. In a clinical setting,\nphysicians tend to rely on the contextual information provided by Electronic\nMedical Records (EMR) to interpret medical imaging. However, very few models\neffectively integrate clinical information with imaging data. To address this\nshortcoming, we suggest a multimodal fusion methodology, termed PE-MVCNet,\nwhich capitalizes on Computed Tomography Pulmonary Angiography imaging and EMR\ndata. This method comprises the Image-only module with an integrated multi-view\nblock, the EMR-only module, and the Cross-modal Attention Fusion (CMAF) module.\nThese modules cooperate to extract comprehensive features that subsequently\ngenerate predictions for PE. We conducted experiments using the publicly\naccessible Stanford University Medical Center dataset, achieving an AUROC of\n94.1%, an accuracy rate of 90.2%, and an F1 score of 90.6%. Our proposed model\noutperforms existing methodologies, corroborating that our multimodal fusion\nmodel excels compared to models that use a single data modality. Our source\ncode is available at https://github.com/LeavingStarW/PE-MVCNET.\n","authors":["Zhaoxin Guo","Zhipeng Wang","Ruiquan Ge","Jianxun Yu","Feiwei Qin","Yuan Tian","Yuqing Peng","Yonghong Li","Changmiao Wang"],"pdf_url":"https://arxiv.org/pdf/2402.17187v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11256v1","updated":"2024-04-17T11:06:42Z","published":"2024-04-17T11:06:42Z","title":"MMCBE: Multi-modality Dataset for Crop Biomass Estimation and Beyond","summary":"  Crop biomass, a critical indicator of plant growth, health, and productivity,\nis invaluable for crop breeding programs and agronomic research. However, the\naccurate and scalable quantification of crop biomass remains inaccessible due\nto limitations in existing measurement methods. One of the obstacles impeding\nthe advancement of current crop biomass prediction methodologies is the\nscarcity of publicly available datasets. Addressing this gap, we introduce a\nnew dataset in this domain, i.e. Multi-modality dataset for crop biomass\nestimation (MMCBE). Comprising 216 sets of multi-view drone images, coupled\nwith LiDAR point clouds, and hand-labelled ground truth, MMCBE represents the\nfirst multi-modality one in the field. This dataset aims to establish benchmark\nmethods for crop biomass quantification and foster the development of\nvision-based approaches. We have rigorously evaluated state-of-the-art crop\nbiomass estimation methods using MMCBE and ventured into additional potential\napplications, such as 3D crop reconstruction from drone imagery and novel-view\nrendering. With this publication, we are making our comprehensive dataset\navailable to the broader community.\n","authors":["Xuesong Li","Zeeshan Hayder","Ali Zia","Connor Cassidy","Shiming Liu","Warwick Stiller","Eric Stone","Warren Conaty","Lars Petersson","Vivien Rolland"],"pdf_url":"https://arxiv.org/pdf/2404.11256v1.pdf","comment":"10 pages, 10 figures, 3 tables"},{"id":"http://arxiv.org/abs/2305.10300v5","updated":"2024-04-17T11:04:57Z","published":"2023-05-17T15:37:47Z","title":"One-Prompt to Segment All Medical Images","summary":"  Large foundation models, known for their strong zero-shot generalization,\nhave excelled in visual and language applications. However, applying them to\nmedical image segmentation, a domain with diverse imaging types and target\nlabels, remains an open challenge. Current approaches, such as adapting\ninteractive segmentation models like Segment Anything Model (SAM), require user\nprompts for each sample during inference. Alternatively, transfer learning\nmethods like few/one-shot models demand labeled samples, leading to high costs.\nThis paper introduces a new paradigm toward the universal medical image\nsegmentation, termed 'One-Prompt Segmentation.' One-Prompt Segmentation\ncombines the strengths of one-shot and interactive methods. In the inference\nstage, with just \\textbf{one prompted sample}, it can adeptly handle the unseen\ntask in a single forward pass. We train One-Prompt Model on 64 open-source\nmedical datasets, accompanied by the collection of over 3,000 clinician-labeled\nprompts. Tested on 14 previously unseen datasets, the One-Prompt Model\nshowcases superior zero-shot segmentation capabilities, outperforming a wide\nrange of related methods. The code and data is released as\nhttps://github.com/KidsWithTokens/one-prompt.\n","authors":["Junde Wu","Jiayuan Zhu","Yueming Jin","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2305.10300v5.pdf","comment":"arXiv admin note: text overlap with arXiv:2304.12620"},{"id":"http://arxiv.org/abs/2404.11249v1","updated":"2024-04-17T10:56:06Z","published":"2024-04-17T10:56:06Z","title":"A Progressive Framework of Vision-language Knowledge Distillation and\n  Alignment for Multilingual Scene","summary":"  Pre-trained vision-language (V-L) models such as CLIP have shown excellent\nperformance in many downstream cross-modal tasks. However, most of them are\nonly applicable to the English context. Subsequent research has focused on this\nproblem and proposed improved models, such as CN-CLIP and AltCLIP, to\nfacilitate their applicability to Chinese and even other languages.\nNevertheless, these models suffer from high latency and a large memory\nfootprint in inference, which limits their further deployment on\nresource-constrained edge devices. In this work, we propose a conceptually\nsimple yet effective multilingual CLIP Compression framework and train a\nlightweight multilingual vision-language model, called DC-CLIP, for both\nChinese and English context. In this framework, we collect high-quality Chinese\nand English text-image pairs and design two training stages, including\nmultilingual vision-language feature distillation and alignment. During the\nfirst stage, lightweight image/text student models are designed to learn robust\nvisual/multilingual textual feature representation ability from corresponding\nteacher models, respectively. Subsequently, the multilingual vision-language\nalignment stage enables effective alignment of visual and multilingual textual\nfeatures to further improve the model's multilingual performance. Comprehensive\nexperiments in zero-shot image classification, conducted based on the ELEVATER\nbenchmark, showcase that DC-CLIP achieves superior performance in the English\ncontext and competitive performance in the Chinese context, even with less\ntraining data, when compared to existing models of similar parameter magnitude.\nThe evaluation demonstrates the effectiveness of our designed training\nmechanism.\n","authors":["Wenbo Zhang","Yifan Zhang","Jianfeng Lin","Binqiang Huang","Jinlu Zhang","Wenhao Yu"],"pdf_url":"https://arxiv.org/pdf/2404.11249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11243v1","updated":"2024-04-17T10:49:00Z","published":"2024-04-17T10:49:00Z","title":"Optical Image-to-Image Translation Using Denoising Diffusion Models:\n  Heterogeneous Change Detection as a Use Case","summary":"  We introduce an innovative deep learning-based method that uses a denoising\ndiffusion-based model to translate low-resolution images to high-resolution\nones from different optical sensors while preserving the contents and avoiding\nundesired artifacts. The proposed method is trained and tested on a large and\ndiverse data set of paired Sentinel-II and Planet Dove images. We show that it\ncan solve serious image generation issues observed when the popular\nclassifier-free guided Denoising Diffusion Implicit Model (DDIM) framework is\nused in the task of Image-to-Image Translation of multi-sensor optical remote\nsensing images and that it can generate large images with highly consistent\npatches, both in colors and in features. Moreover, we demonstrate how our\nmethod improves heterogeneous change detection results in two urban areas:\nBeirut, Lebanon, and Austin, USA. Our contributions are: i) a new training and\ntesting algorithm based on denoising diffusion models for optical image\ntranslation; ii) a comprehensive image quality evaluation and ablation study;\niii) a comparison with the classifier-free guided DDIM framework; and iv)\nchange detection experiments on heterogeneous data.\n","authors":["Jo√£o Gabriel Vinholi","Marco Chini","Anis Amziane","Renato Machado","Danilo Silva","Patrick Matgen"],"pdf_url":"https://arxiv.org/pdf/2404.11243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14065v7","updated":"2024-04-17T10:42:06Z","published":"2023-09-25T11:57:16Z","title":"AsymFormer: Asymmetrical Cross-Modal Representation Learning for Mobile\n  Platform Real-Time RGB-D Semantic Segmentation","summary":"  Understanding indoor scenes is crucial for urban studies. Considering the\ndynamic nature of indoor environments, effective semantic segmentation requires\nboth real-time operation and high accuracy.To address this, we propose\nAsymFormer, a novel network that improves real-time semantic segmentation\naccuracy using RGB-D multi-modal information without substantially increasing\nnetwork complexity. AsymFormer uses an asymmetrical backbone for multimodal\nfeature extraction, reducing redundant parameters by optimizing computational\nresource distribution. To fuse asymmetric multimodal features, a Local\nAttention-Guided Feature Selection (LAFS) module is used to selectively fuse\nfeatures from different modalities by leveraging their dependencies.\nSubsequently, a Cross-Modal Attention-Guided Feature Correlation Embedding\n(CMA) module is introduced to further extract cross-modal representations. The\nAsymFormer demonstrates competitive results with 54.1% mIoU on NYUv2 and 49.1%\nmIoU on SUNRGBD. Notably, AsymFormer achieves an inference speed of 65 FPS (79\nFPS after implementing mixed precision quantization) on RTX3090, demonstrating\nthat AsymFormer can strike a balance between high accuracy and efficiency.\n","authors":["Siqi Du","Weixi Wang","Renzhong Guo","Ruisheng Wang","Yibin Tian","Shengjun Tang"],"pdf_url":"https://arxiv.org/pdf/2309.14065v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11236v1","updated":"2024-04-17T10:38:51Z","published":"2024-04-17T10:38:51Z","title":"ONOT: a High-Quality ICAO-compliant Synthetic Mugshot Dataset","summary":"  Nowadays, state-of-the-art AI-based generative models represent a viable\nsolution to overcome privacy issues and biases in the collection of datasets\ncontaining personal information, such as faces. Following this intuition, in\nthis paper we introduce ONOT, a synthetic dataset specifically focused on the\ngeneration of high-quality faces in adherence to the requirements of the\nISO/IEC 39794-5 standards that, following the guidelines of the International\nCivil Aviation Organization (ICAO), defines the interchange formats of face\nimages in electronic Machine-Readable Travel Documents (eMRTD). The strictly\ncontrolled and varied mugshot images included in ONOT are useful in research\nfields related to the analysis of face images in eMRTD, such as Morphing Attack\nDetection and Face Quality Assessment. The dataset is publicly released, in\ncombination with the generation procedure details in order to improve the\nreproducibility and enable future extensions.\n","authors":["Nicol√≤ Di Domenico","Guido Borghi","Annalisa Franco","Davide Maltoni"],"pdf_url":"https://arxiv.org/pdf/2404.11236v1.pdf","comment":"Paper accepted in IEEE FG 2024"},{"id":"http://arxiv.org/abs/2404.11230v1","updated":"2024-04-17T10:26:49Z","published":"2024-04-17T10:26:49Z","title":"Energy-Efficient Uncertainty-Aware Biomass Composition Prediction at the\n  Edge","summary":"  Clover fixates nitrogen from the atmosphere to the ground, making\ngrass-clover mixtures highly desirable to reduce external nitrogen\nfertilization. Herbage containing clover additionally promotes higher food\nintake, resulting in higher milk production. Herbage probing however remains\nlargely unused as it requires a time-intensive manual laboratory analysis.\nWithout this information, farmers are unable to perform localized clover sowing\nor take targeted fertilization decisions. Deep learning algorithms have been\nproposed with the goal to estimate the dry biomass composition from images of\nthe grass directly in the fields. The energy-intensive nature of deep learning\nhowever limits deployment to practical edge devices such as smartphones. This\npaper proposes to fill this gap by applying filter pruning to reduce the energy\nrequirement of existing deep learning solutions. We report that although pruned\nnetworks are accurate on controlled, high-quality images of the grass, they\nstruggle to generalize to real-world smartphone images that are blurry or taken\nfrom challenging angles. We address this challenge by training filter-pruned\nmodels using a variance attenuation loss so they can predict the uncertainty of\ntheir predictions. When the uncertainty exceeds a threshold, we re-infer using\na more accurate unpruned model. This hybrid approach allows us to reduce energy\nconsumption while retaining a high accuracy. We evaluate our algorithm on two\ndatasets: the GrassClover and the Irish clover using an NVIDIA Jetson Nano edge\ndevice. We find that we reduce energy reduction with respect to\nstate-of-the-art solutions by 50% on average with only 4% accuracy loss.\n","authors":["Muhammad Zawish","Paul Albert","Flavio Esposito","Steven Davy","Lizy Abraham"],"pdf_url":"https://arxiv.org/pdf/2404.11230v1.pdf","comment":"The paper has been accepted to CVPR 2024 5th Workshop on Vision for\n  Agriculture"},{"id":"http://arxiv.org/abs/2404.11226v1","updated":"2024-04-17T10:20:16Z","published":"2024-04-17T10:20:16Z","title":"Simple In-place Data Augmentation for Surveillance Object Detection","summary":"  Motivated by the need to improve model performance in traffic monitoring\ntasks with limited labeled samples, we propose a straightforward augmentation\ntechnique tailored for object detection datasets, specifically designed for\nstationary camera-based applications. Our approach focuses on placing objects\nin the same positions as the originals to ensure its effectiveness. By applying\nin-place augmentation on objects from the same camera input image, we address\nthe challenge of overlapping with original and previously selected objects.\nThrough extensive testing on two traffic monitoring datasets, we illustrate the\nefficacy of our augmentation strategy in improving model performance,\nparticularly in scenarios with limited labeled samples and imbalanced class\ndistributions. Notably, our method achieves comparable performance to models\ntrained on the entire dataset while utilizing only 8.5 percent of the original\ndata. Moreover, we report significant improvements, with mAP@.5 increasing from\n0.4798 to 0.5025, and the mAP@.5:.95 rising from 0.29 to 0.3138 on the\nFishEye8K dataset. These results highlight the potential of our augmentation\napproach in enhancing object detection models for traffic monitoring\napplications.\n","authors":["Munkh-Erdene Otgonbold","Ganzorig Batnasan","Munkhjargal Gochoo"],"pdf_url":"https://arxiv.org/pdf/2404.11226v1.pdf","comment":"CVPR Workshop 2024"},{"id":"http://arxiv.org/abs/2404.11214v1","updated":"2024-04-17T09:58:53Z","published":"2024-04-17T09:58:53Z","title":"Feature Corrective Transfer Learning: End-to-End Solutions to Object\n  Detection in Non-Ideal Visual Conditions","summary":"  A significant challenge in the field of object detection lies in the system's\nperformance under non-ideal imaging conditions, such as rain, fog, low\nillumination, or raw Bayer images that lack ISP processing. Our study\nintroduces \"Feature Corrective Transfer Learning\", a novel approach that\nleverages transfer learning and a bespoke loss function to facilitate the\nend-to-end detection of objects in these challenging scenarios without the need\nto convert non-ideal images into their RGB counterparts. In our methodology, we\ninitially train a comprehensive model on a pristine RGB image dataset.\nSubsequently, non-ideal images are processed by comparing their feature maps\nagainst those from the initial ideal RGB model. This comparison employs the\nExtended Area Novel Structural Discrepancy Loss (EANSDL), a novel loss function\ndesigned to quantify similarities and integrate them into the detection loss.\nThis approach refines the model's ability to perform object detection across\nvarying conditions through direct feature map correction, encapsulating the\nessence of Feature Corrective Transfer Learning. Experimental validation on\nvariants of the KITTI dataset demonstrates a significant improvement in mean\nAverage Precision (mAP), resulting in a 3.8-8.1% relative enhancement in\ndetection under non-ideal conditions compared to the baseline model, and a less\nmarginal performance difference within 1.3% of the mAP@[0.5:0.95] achieved\nunder ideal conditions by the standard Faster RCNN algorithm.\n","authors":["Chuheng Wei","Guoyuan Wu","Matthew J. Barth"],"pdf_url":"https://arxiv.org/pdf/2404.11214v1.pdf","comment":"10 pages, 3 figures, accepted by 2024 CVPR UG2 Workshop"},{"id":"http://arxiv.org/abs/2311.10339v2","updated":"2024-04-17T09:50:25Z","published":"2023-11-17T05:49:50Z","title":"A2XP: Towards Private Domain Generalization","summary":"  Deep Neural Networks (DNNs) have become pivotal in various fields, especially\nin computer vision, outperforming previous methodologies. A critical challenge\nin their deployment is the bias inherent in data across different domains, such\nas image style and environmental conditions, leading to domain gaps. This\nnecessitates techniques for learning general representations from biased\ntraining data, known as domain generalization. This paper presents Attend to\neXpert Prompts (A2XP), a novel approach for domain generalization that\npreserves the privacy and integrity of the network architecture. A2XP consists\nof two phases: Expert Adaptation and Domain Generalization. In the first phase,\nprompts for each source domain are optimized to guide the model towards the\noptimal direction. In the second phase, two embedder networks are trained to\neffectively amalgamate these expert prompts, aiming for an optimal output. Our\nextensive experiments demonstrate that A2XP achieves state-of-the-art results\nover existing non-private domain generalization methods. The experimental\nresults validate that the proposed approach not only tackles the domain\ngeneralization challenge in DNNs but also offers a privacy-preserving,\nefficient solution to the broader field of computer vision.\n","authors":["Geunhyeok Yu","Hyoseok Hwang"],"pdf_url":"https://arxiv.org/pdf/2311.10339v2.pdf","comment":"Accepted to CVPR 2024. Our code is available at\n  https://github.com/AIRLABkhu/A2XP"},{"id":"http://arxiv.org/abs/2404.11209v1","updated":"2024-04-17T09:45:43Z","published":"2024-04-17T09:45:43Z","title":"Prompt-Guided Generation of Structured Chest X-Ray Report Using a\n  Pre-trained LLM","summary":"  Medical report generation automates radiology descriptions from images,\neasing the burden on physicians and minimizing errors. However, current methods\nlack structured outputs and physician interactivity for clear, clinically\nrelevant reports. Our method introduces a prompt-guided approach to generate\nstructured chest X-ray reports using a pre-trained large language model (LLM).\nFirst, we identify anatomical regions in chest X-rays to generate focused\nsentences that center on key visual elements, thereby establishing a structured\nreport foundation with anatomy-based sentences. We also convert the detected\nanatomy into textual prompts conveying anatomical comprehension to the LLM.\nAdditionally, the clinical context prompts guide the LLM to emphasize\ninteractivity and clinical requirements. By integrating anatomy-focused\nsentences and anatomy/clinical prompts, the pre-trained LLM can generate\nstructured chest X-ray reports tailored to prompted anatomical regions and\nclinical contexts. We evaluate using language generation and clinical\neffectiveness metrics, demonstrating strong performance.\n","authors":["Hongzhao Li","Hongyu Wang","Xia Sun","Hua He","Jun Feng"],"pdf_url":"https://arxiv.org/pdf/2404.11209v1.pdf","comment":"Accepted by IEEE Conference on Multimedia Expo 2024"},{"id":"http://arxiv.org/abs/2404.11207v1","updated":"2024-04-17T09:39:07Z","published":"2024-04-17T09:39:07Z","title":"Exploring the Transferability of Visual Prompting for Multimodal Large\n  Language Models","summary":"  Although Multimodal Large Language Models (MLLMs) have demonstrated promising\nversatile capabilities, their performance is still inferior to specialized\nmodels on downstream tasks, which makes adaptation necessary to enhance their\nutility. However, fine-tuning methods require independent training for every\nmodel, leading to huge computation and memory overheads. In this paper, we\npropose a novel setting where we aim to improve the performance of diverse\nMLLMs with a group of shared parameters optimized for a downstream task. To\nachieve this, we propose Transferable Visual Prompting (TVP), a simple and\neffective approach to generate visual prompts that can transfer to different\nmodels and improve their performance on downstream tasks after trained on only\none model. We introduce two strategies to address the issue of cross-model\nfeature corruption of existing visual prompting methods and enhance the\ntransferability of the learned prompts, including 1) Feature Consistency\nAlignment: which imposes constraints to the prompted feature changes to\nmaintain task-agnostic knowledge; 2) Task Semantics Enrichment: which\nencourages the prompted images to contain richer task-specific semantics with\nlanguage guidance. We validate the effectiveness of TVP through extensive\nexperiments with 6 modern MLLMs on a wide variety of tasks ranging from object\nrecognition and counting to multimodal reasoning and hallucination correction.\n","authors":["Yichi Zhang","Yinpeng Dong","Siyuan Zhang","Tianzan Min","Hang Su","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2404.11207v1.pdf","comment":"Accepted in CVPR 2024 as Poster (Highlight)"},{"id":"http://arxiv.org/abs/2404.11205v1","updated":"2024-04-17T09:37:25Z","published":"2024-04-17T09:37:25Z","title":"Kathakali Hand Gesture Recognition With Minimal Data","summary":"  The Indian classical dance-drama Kathakali has a set of hand gestures called\nMudras, which form the fundamental units of all its dance moves and postures.\nRecognizing the depicted mudra becomes one of the first steps in its digital\nprocessing. The work treats the problem as a 24-class classification task and\nproposes a vector-similarity-based approach using pose estimation, eliminating\nthe need for further training or fine-tuning. This approach overcomes the\nchallenge of data scarcity that limits the application of AI in similar\ndomains. The method attains 92% accuracy which is a similar or better\nperformance as other model-training-based works existing in the domain, with\nthe added advantage that the method can still work with data sizes as small as\n1 or 5 samples with a slightly reduced performance. Working with images,\nvideos, and even real-time streams is possible. The system can work with\nhand-cropped or full-body images alike. We have developed and made public a\ndataset for the Kathakali Mudra Recognition as part of this work.\n","authors":["Kavitha Raju","Nandini J. Warrier"],"pdf_url":"https://arxiv.org/pdf/2404.11205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11202v1","updated":"2024-04-17T09:33:31Z","published":"2024-04-17T09:33:31Z","title":"GhostNetV3: Exploring the Training Strategies for Compact Models","summary":"  Compact neural networks are specially designed for applications on edge\ndevices with faster inference speed yet modest performance. However, training\nstrategies of compact models are borrowed from that of conventional models at\npresent, which ignores their difference in model capacity and thus may impede\nthe performance of compact models. In this paper, by systematically\ninvestigating the impact of different training ingredients, we introduce a\nstrong training strategy for compact models. We find that the appropriate\ndesigns of re-parameterization and knowledge distillation are crucial for\ntraining high-performance compact models, while some commonly used data\naugmentations for training conventional models, such as Mixup and CutMix, lead\nto worse performance. Our experiments on ImageNet-1K dataset demonstrate that\nour specialized training strategy for compact models is applicable to various\narchitectures, including GhostNetV2, MobileNetV2 and ShuffleNetV2.\nSpecifically, equipped with our strategy, GhostNetV3 1.3$\\times$ achieves a\ntop-1 accuracy of 79.1% with only 269M FLOPs and a latency of 14.46ms on mobile\ndevices, surpassing its ordinarily trained counterpart by a large margin.\nMoreover, our observation can also be extended to object detection scenarios.\nPyTorch code and checkpoints can be found at\nhttps://github.com/huawei-noah/Efficient-AI-Backbones/tree/master/ghostnetv3_pytorch.\n","authors":["Zhenhua Liu","Zhiwei Hao","Kai Han","Yehui Tang","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2404.11202v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02977v2","updated":"2024-04-17T09:09:17Z","published":"2023-10-04T17:12:18Z","title":"T$^3$Bench: Benchmarking Current Progress in Text-to-3D Generation","summary":"  Recent methods in text-to-3D leverage powerful pretrained diffusion models to\noptimize NeRF. Notably, these methods are able to produce high-quality 3D\nscenes without training on 3D data. Due to the open-ended nature of the task,\nmost studies evaluate their results with subjective case studies and user\nexperiments, thereby presenting a challenge in quantitatively addressing the\nquestion: How has current progress in Text-to-3D gone so far? In this paper, we\nintroduce T$^3$Bench, the first comprehensive text-to-3D benchmark containing\ndiverse text prompts of three increasing complexity levels that are specially\ndesigned for 3D generation. To assess both the subjective quality and the text\nalignment, we propose two automatic metrics based on multi-view images produced\nby the 3D contents. The quality metric combines multi-view text-image scores\nand regional convolution to detect quality and view inconsistency. The\nalignment metric uses multi-view captioning and GPT-4 evaluation to measure\ntext-3D consistency. Both metrics closely correlate with different dimensions\nof human judgments, providing a paradigm for efficiently evaluating text-to-3D\nmodels. The benchmarking results, shown in Fig. 1, reveal performance\ndifferences among an extensive 10 prevalent text-to-3D methods. Our analysis\nfurther highlights the common struggles for current methods on generating\nsurroundings and multi-object scenes, as well as the bottleneck of leveraging\n2D guidance for 3D generation. Our project page is available at:\nhttps://t3bench.com.\n","authors":["Yuze He","Yushi Bai","Matthieu Lin","Wang Zhao","Yubin Hu","Jenny Sheng","Ran Yi","Juanzi Li","Yong-Jin Liu"],"pdf_url":"https://arxiv.org/pdf/2310.02977v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2311.18402v2","updated":"2024-04-17T08:57:35Z","published":"2023-11-30T09:51:53Z","title":"MV-CLIP: Multi-View CLIP for Zero-shot 3D Shape Recognition","summary":"  Large-scale pre-trained models have demonstrated impressive performance in\nvision and language tasks within open-world scenarios. Due to the lack of\ncomparable pre-trained models for 3D shapes, recent methods utilize\nlanguage-image pre-training to realize zero-shot 3D shape recognition. However,\ndue to the modality gap, pretrained language-image models are not confident\nenough in the generalization to 3D shape recognition. Consequently, this paper\naims to improve the confidence with view selection and hierarchical prompts.\nLeveraging the CLIP model as an example, we employ view selection on the vision\nside by identifying views with high prediction confidence from multiple\nrendered views of a 3D shape. On the textual side, the strategy of hierarchical\nprompts is proposed for the first time. The first layer prompts several\nclassification candidates with traditional class-level descriptions, while the\nsecond layer refines the prediction based on function-level descriptions or\nfurther distinctions between the candidates. Remarkably, without the need for\nadditional training, our proposed method achieves impressive zero-shot 3D\nclassification accuracies of 84.44%, 91.51%, and 66.17% on ModelNet40,\nModelNet10, and ShapeNet Core55, respectively. Furthermore, we will make the\ncode publicly available to facilitate reproducibility and further research in\nthis area.\n","authors":["Dan Song","Xinwei Fu","Weizhi Nie","Wenhui Li","Lanjun Wang","You Yang","Anan Liu"],"pdf_url":"https://arxiv.org/pdf/2311.18402v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10710v2","updated":"2024-04-17T08:44:30Z","published":"2024-04-16T16:36:50Z","title":"Dual Modalities of Text: Visual and Textual Generative Pre-training","summary":"  Harnessing visual texts represents a burgeoning frontier in the evolution of\nlanguage modeling. In this paper, we introduce a novel pre-training framework\nfor a suite of pixel-based autoregressive language models, pre-training on a\ncorpus of over 400 million documents rendered as RGB images. Our approach is\ncharacterized by a dual-modality training regimen, engaging both visual data\nthrough next patch prediction with a regression head and textual data via next\ntoken prediction with a classification head. This study is particularly focused\non investigating the synergistic interplay between visual and textual\nmodalities of language. Our comprehensive evaluation across a diverse array of\nbenchmarks reveals that the confluence of visual and textual data substantially\naugments the efficacy of pixel-based language models. Notably, our findings\nshow that a unidirectional pixel-based model, devoid of textual data during\ntraining, can match the performance levels of advanced bidirectional\npixel-based models on various language understanding benchmarks. This work\nhighlights the considerable untapped potential of integrating visual and\ntextual information for language modeling purposes. We will release our code,\ndata, and checkpoints to inspire further research advancement.\n","authors":["Yekun Chai","Qingyi Liu","Jingwu Xiao","Shuohuan Wang","Yu Sun","Hua Wu"],"pdf_url":"https://arxiv.org/pdf/2404.10710v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00311v3","updated":"2024-04-17T08:40:57Z","published":"2023-12-01T03:05:21Z","title":"3D Face Reconstruction with the Geometric Guidance of Facial Part\n  Segmentation","summary":"  3D Morphable Models (3DMMs) provide promising 3D face reconstructions in\nvarious applications. However, existing methods struggle to reconstruct faces\nwith extreme expressions due to deficiencies in supervisory signals, such as\nsparse or inaccurate landmarks. Segmentation information contains effective\ngeometric contexts for face reconstruction. Certain attempts intuitively depend\non differentiable renderers to compare the rendered silhouettes of\nreconstruction with segmentation, which is prone to issues like local optima\nand gradient instability. In this paper, we fully utilize the facial part\nsegmentation geometry by introducing Part Re-projection Distance Loss (PRDL).\nSpecifically, PRDL transforms facial part segmentation into 2D points and\nre-projects the reconstruction onto the image plane. Subsequently, by\nintroducing grid anchors and computing different statistical distances from\nthese anchors to the point sets, PRDL establishes geometry descriptors to\noptimize the distribution of the point sets for face reconstruction. PRDL\nexhibits a clear gradient compared to the renderer-based methods and presents\nstate-of-the-art reconstruction performance in extensive quantitative and\nqualitative experiments. Our project is available at\nhttps://github.com/wang-zidu/3DDFA-V3 .\n","authors":["Zidu Wang","Xiangyu Zhu","Tianshuo Zhang","Baiqin Wang","Zhen Lei"],"pdf_url":"https://arxiv.org/pdf/2312.00311v3.pdf","comment":"CVPR2024 (Highlight)"},{"id":"http://arxiv.org/abs/2312.08555v2","updated":"2024-04-17T08:38:54Z","published":"2023-12-13T23:00:48Z","title":"KDAS: Knowledge Distillation via Attention Supervision Framework for\n  Polyp Segmentation","summary":"  Polyp segmentation, a contentious issue in medical imaging, has seen numerous\nproposed methods aimed at improving the quality of segmented masks. While\ncurrent state-of-the-art techniques yield impressive results, the size and\ncomputational cost of these models create challenges for practical industry\napplications. To address this challenge, we present KDAS, a Knowledge\nDistillation framework that incorporates attention supervision, and our\nproposed Symmetrical Guiding Module. This framework is designed to facilitate a\ncompact student model with fewer parameters, allowing it to learn the strengths\nof the teacher model and mitigate the inconsistency between teacher features\nand student features, a common challenge in Knowledge Distillation, via the\nSymmetrical Guiding Module. Through extensive experiments, our compact models\ndemonstrate their strength by achieving competitive results with\nstate-of-the-art methods, offering a promising approach to creating compact\nmodels with high accuracy for polyp segmentation and in the medical imaging\nfield. The implementation is available on https://github.com/huyquoctrinh/KDAS.\n","authors":["Quoc-Huy Trinh","Minh-Van Nguyen","Phuoc-Thao Vo Thi"],"pdf_url":"https://arxiv.org/pdf/2312.08555v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11161v1","updated":"2024-04-17T08:21:02Z","published":"2024-04-17T08:21:02Z","title":"Pre-processing matters: A segment search method for WSI classification","summary":"  Pre-processing for whole slide images can affect classification performance\nboth in the training and inference stages. Our study analyzes the impact of\npre-processing parameters on inference and training across single- and\nmultiple-domain datasets. However, searching for an optimal parameter set is\ntime-consuming. To overcome this, we propose a novel Similarity-based Simulated\nAnnealing approach for fast parameter tuning to enhance inference performance\non single-domain data. Our method demonstrates significant performance\nimprovements in accuracy, which raise accuracy from 0.512 to 0.847 in a single\ndomain. We further extend our insight into training performance in multi-domain\ndata by employing a novel Bayesian optimization to search optimal\npre-processing parameters, resulting in a high AUC of 0.967. We highlight that\nbetter pre-processing for WSI can contribute to further accuracy improvement in\nthe histology area.\n","authors":["Jun Wang","Yufei Cui","Yu Mao","Nan Guan","Chun Jason Xue"],"pdf_url":"https://arxiv.org/pdf/2404.11161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11159v1","updated":"2024-04-17T08:15:25Z","published":"2024-04-17T08:15:25Z","title":"Deep Portrait Quality Assessment. A NTIRE 2024 Challenge Survey","summary":"  This paper reviews the NTIRE 2024 Portrait Quality Assessment Challenge,\nhighlighting the proposed solutions and results. This challenge aims to obtain\nan efficient deep neural network capable of estimating the perceptual quality\nof real portrait photos. The methods must generalize to diverse scenes and\ndiverse lighting conditions (indoor, outdoor, low-light), movement, blur, and\nother challenging conditions. In the challenge, 140 participants registered,\nand 35 submitted results during the challenge period. The performance of the\ntop 5 submissions is reviewed and provided here as a gauge for the current\nstate-of-the-art in Portrait Quality Assessment.\n","authors":["Nicolas Chahine","Marcos V. Conde","Daniela Carfora","Gabriel Pacianotto","Benoit Pochon","Sira Ferradans","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2404.11159v1.pdf","comment":"CVPRW - NTIRE 2024"},{"id":"http://arxiv.org/abs/2404.11156v1","updated":"2024-04-17T08:09:25Z","published":"2024-04-17T08:09:25Z","title":"Learning SO(3)-Invariant Semantic Correspondence via Local Shape\n  Transform","summary":"  Establishing accurate 3D correspondences between shapes stands as a pivotal\nchallenge with profound implications for computer vision and robotics. However,\nexisting self-supervised methods for this problem assume perfect input shape\nalignment, restricting their real-world applicability. In this work, we\nintroduce a novel self-supervised Rotation-Invariant 3D correspondence learner\nwith Local Shape Transform, dubbed RIST, that learns to establish dense\ncorrespondences between shapes even under challenging intra-class variations\nand arbitrary orientations. Specifically, RIST learns to dynamically formulate\nan SO(3)-invariant local shape transform for each point, which maps the\nSO(3)-equivariant global shape descriptor of the input shape to a local shape\ndescriptor. These local shape descriptors are provided as inputs to our decoder\nto facilitate point cloud self- and cross-reconstruction. Our proposed\nself-supervised training pipeline encourages semantically corresponding points\nfrom different shapes to be mapped to similar local shape descriptors, enabling\nRIST to establish dense point-wise correspondences. RIST demonstrates\nstate-of-the-art performances on 3D part label transfer and semantic keypoint\ntransfer given arbitrarily rotated point cloud pairs, outperforming existing\nmethods by significant margins.\n","authors":["Chunghyun Park","Seungwook Sim","Jaesik Park","Minsu Cho"],"pdf_url":"https://arxiv.org/pdf/2404.11156v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2404.11155v1","updated":"2024-04-17T08:08:34Z","published":"2024-04-17T08:08:34Z","title":"HybriMap: Hybrid Clues Utilization for Effective Vectorized HD Map\n  Construction","summary":"  Constructing vectorized high-definition maps from surround-view cameras has\ngarnered significant attention in recent years. However, the commonly employed\nmulti-stage sequential workflow in prevailing approaches often leads to the\nloss of early-stage information, particularly in perspective-view features.\nUsually, such loss is observed as an instance missing or shape mismatching in\nthe final birds-eye-view predictions. To address this concern, we propose a\nnovel approach, namely \\textbf{HybriMap}, which effectively exploits clues from\nhybrid features to ensure the delivery of valuable information. Specifically,\nwe design the Dual Enhancement Module, to enable both explicit integration and\nimplicit modification under the guidance of hybrid features. Additionally, the\nperspective keypoints are utilized as supervision, further directing the\nfeature enhancement process. Extensive experiments conducted on existing\nbenchmarks have demonstrated the state-of-the-art performance of our proposed\napproach.\n","authors":["Chi Zhang","Qi Song","Feifei Li","Yongquan Chen","Rui Huang"],"pdf_url":"https://arxiv.org/pdf/2404.11155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07773v2","updated":"2024-04-17T08:06:51Z","published":"2024-04-11T14:08:45Z","title":"ConsistencyDet: A Robust Object Detector with a Denoising Paradigm of\n  Consistency Model","summary":"  Object detection, a quintessential task in the realm of perceptual computing,\ncan be tackled using a generative methodology. In the present study, we\nintroduce a novel framework designed to articulate object detection as a\ndenoising diffusion process, which operates on the perturbed bounding boxes of\nannotated entities. This framework, termed ConsistencyDet, leverages an\ninnovative denoising concept known as the Consistency Model. The hallmark of\nthis model is its self-consistency feature, which empowers the model to map\ndistorted information from any temporal stage back to its pristine state,\nthereby realizing a \"one-step denoising\" mechanism. Such an attribute markedly\nelevates the operational efficiency of the model, setting it apart from the\nconventional Diffusion Model. Throughout the training phase, ConsistencyDet\ninitiates the diffusion sequence with noise-infused boxes derived from the\nground-truth annotations and conditions the model to perform the denoising\ntask. Subsequently, in the inference stage, the model employs a denoising\nsampling strategy that commences with bounding boxes randomly sampled from a\nnormal distribution. Through iterative refinement, the model transforms an\nassortment of arbitrarily generated boxes into definitive detections.\nComprehensive evaluations employing standard benchmarks, such as MS-COCO and\nLVIS, corroborate that ConsistencyDet surpasses other leading-edge detectors in\nperformance metrics. Our code is available at\nhttps://github.com/Tankowa/ConsistencyDet.\n","authors":["Lifan Jiang","Zhihui Wang","Changmiao Wang","Ming Li","Jiaxu Leng","Xindong Wu"],"pdf_url":"https://arxiv.org/pdf/2404.07773v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11152v1","updated":"2024-04-17T08:05:04Z","published":"2024-04-17T08:05:04Z","title":"Multi-target and multi-stage liver lesion segmentation and detection in\n  multi-phase computed tomography scans","summary":"  Multi-phase computed tomography (CT) scans use contrast agents to highlight\ndifferent anatomical structures within the body to improve the probability of\nidentifying and detecting anatomical structures of interest and abnormalities\nsuch as liver lesions. Yet, detecting these lesions remains a challenging task\nas these lesions vary significantly in their size, shape, texture, and contrast\nwith respect to surrounding tissue. Therefore, radiologists need to have an\nextensive experience to be able to identify and detect these lesions.\nSegmentation-based neural networks can assist radiologists with this task.\nCurrent state-of-the-art lesion segmentation networks use the encoder-decoder\ndesign paradigm based on the UNet architecture where the multi-phase CT scan\nvolume is fed to the network as a multi-channel input. Although this approach\nutilizes information from all the phases and outperform single-phase\nsegmentation networks, we demonstrate that their performance is not optimal and\ncan be further improved by incorporating the learning from models trained on\neach single-phase individually. Our approach comprises three stages. The first\nstage identifies the regions within the liver where there might be lesions at\nthree different scales (4, 8, and 16 mm). The second stage includes the main\nsegmentation model trained using all the phases as well as a segmentation model\ntrained on each of the phases individually. The third stage uses the\nmulti-phase CT volumes together with the predictions from each of the\nsegmentation models to generate the final segmentation map. Overall, our\napproach improves relative liver lesion segmentation performance by 1.6% while\nreducing performance variability across subjects by 8% when compared to the\ncurrent state-of-the-art models.\n","authors":["Abdullah F. Al-Battal","Soan T. M. Duong","Van Ha Tang","Quang Duc Tran","Steven Q. H. Truong","Chien Phan","Truong Q. Nguyen","Cheolhong An"],"pdf_url":"https://arxiv.org/pdf/2404.11152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11151v1","updated":"2024-04-17T08:01:55Z","published":"2024-04-17T08:01:55Z","title":"REACTO: Reconstructing Articulated Objects from a Single Video","summary":"  In this paper, we address the challenge of reconstructing general articulated\n3D objects from a single video. Existing works employing dynamic neural\nradiance fields have advanced the modeling of articulated objects like humans\nand animals from videos, but face challenges with piece-wise rigid general\narticulated objects due to limitations in their deformation models. To tackle\nthis, we propose Quasi-Rigid Blend Skinning, a novel deformation model that\nenhances the rigidity of each part while maintaining flexible deformation of\nthe joints. Our primary insight combines three distinct approaches: 1) an\nenhanced bone rigging system for improved component modeling, 2) the use of\nquasi-sparse skinning weights to boost part rigidity and reconstruction\nfidelity, and 3) the application of geodesic point assignment for precise\nmotion and seamless deformation. Our method outperforms previous works in\nproducing higher-fidelity 3D reconstructions of general articulated objects, as\ndemonstrated on both real and synthetic datasets. Project page:\nhttps://chaoyuesong.github.io/REACTO.\n","authors":["Chaoyue Song","Jiacheng Wei","Chuan-Sheng Foo","Guosheng Lin","Fayao Liu"],"pdf_url":"https://arxiv.org/pdf/2404.11151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09326v2","updated":"2024-04-17T07:46:28Z","published":"2024-04-14T18:57:38Z","title":"Weight Copy and Low-Rank Adaptation for Few-Shot Distillation of Vision\n  Transformers","summary":"  Few-shot knowledge distillation recently emerged as a viable approach to\nharness the knowledge of large-scale pre-trained models, using limited data and\ncomputational resources. In this paper, we propose a novel few-shot feature\ndistillation approach for vision transformers. Our approach is based on two key\nsteps. Leveraging the fact that vision transformers have a consistent\ndepth-wise structure, we first copy the weights from intermittent layers of\nexisting pre-trained vision transformers (teachers) into shallower\narchitectures (students), where the intermittence factor controls the\ncomplexity of the student transformer with respect to its teacher. Next, we\nemploy an enhanced version of Low-Rank Adaptation (LoRA) to distill knowledge\ninto the student in a few-shot scenario, aiming to recover the information\nprocessing carried out by the skipped teacher layers. We present comprehensive\nexperiments with supervised and self-supervised transformers as teachers, on\nfive data sets from various domains, including natural, medical and satellite\nimages. The empirical results confirm the superiority of our approach over\ncompetitive baselines. Moreover, the ablation results demonstrate the\nusefulness of each component of the proposed pipeline.\n","authors":["Diana-Nicoleta Grigore","Mariana-Iuliana Georgescu","Jon Alvarez Justo","Tor Johansen","Andreea Iuliana Ionescu","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2404.09326v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03788v2","updated":"2024-04-17T07:41:48Z","published":"2024-01-08T10:08:48Z","title":"Low-light Image Enhancement via CLIP-Fourier Guided Wavelet Diffusion","summary":"  Low-light image enhancement techniques have significantly progressed, but\nunstable image quality recovery and unsatisfactory visual perception are still\nsignificant challenges. To solve these problems, we propose a novel and robust\nlow-light image enhancement method via CLIP-Fourier Guided Wavelet Diffusion,\nabbreviated as CFWD. Specifically, CFWD leverages multimodal visual-language\ninformation in the frequency domain space created by multiple wavelet\ntransforms to guide the enhancement process. Multi-scale supervision across\ndifferent modalities facilitates the alignment of image features with semantic\nfeatures during the wavelet diffusion process, effectively bridging the gap\nbetween degraded and normal domains. Moreover, to further promote the effective\nrecovery of the image details, we combine the Fourier transform based on the\nwavelet transform and construct a Hybrid High Frequency Perception Module\n(HFPM) with a significant perception of the detailed features. This module\navoids the diversity confusion of the wavelet diffusion process by guiding the\nfine-grained structure recovery of the enhancement results to achieve\nfavourable metric and perceptually oriented enhancement. Extensive quantitative\nand qualitative experiments on publicly available real-world benchmarks show\nthat our approach outperforms existing state-of-the-art methods, achieving\nsignificant progress in image quality and noise suppression. The project code\nis available at https://github.com/hejh8/CFWD.\n","authors":["Minglong Xue","Jinhong He","Wenhai Wang","Mingliang Zhou"],"pdf_url":"https://arxiv.org/pdf/2401.03788v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08926v2","updated":"2024-04-17T07:38:32Z","published":"2024-04-13T08:27:10Z","title":"Diffusion Models Meet Remote Sensing: Principles, Methods, and\n  Perspectives","summary":"  As a newly emerging advance in deep generative models, diffusion models have\nachieved state-of-the-art results in many fields, including computer vision,\nnatural language processing, and molecule design. The remote sensing community\nhas also noticed the powerful ability of diffusion models and quickly applied\nthem to a variety of tasks for image processing. Given the rapid increase in\nresearch on diffusion models in the field of remote sensing, it is necessary to\nconduct a comprehensive review of existing diffusion model-based remote sensing\npapers, to help researchers recognize the potential of diffusion models and\nprovide some directions for further exploration. Specifically, this paper first\nintroduces the theoretical background of diffusion models, and then\nsystematically reviews the applications of diffusion models in remote sensing,\nincluding image generation, enhancement, and interpretation. Finally, the\nlimitations of existing remote sensing diffusion models and worthy research\ndirections for further exploration are discussed and summarized.\n","authors":["Yidan Liu","Jun Yue","Shaobo Xia","Pedram Ghamisi","Weiying Xie","Leyuan Fang"],"pdf_url":"https://arxiv.org/pdf/2404.08926v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11139v1","updated":"2024-04-17T07:34:21Z","published":"2024-04-17T07:34:21Z","title":"GeoReF: Geometric Alignment Across Shape Variation for Category-level\n  Object Pose Refinement","summary":"  Object pose refinement is essential for robust object pose estimation.\nPrevious work has made significant progress towards instance-level object pose\nrefinement. Yet, category-level pose refinement is a more challenging problem\ndue to large shape variations within a category and the discrepancies between\nthe target object and the shape prior. To address these challenges, we\nintroduce a novel architecture for category-level object pose refinement. Our\napproach integrates an HS-layer and learnable affine transformations, which\naims to enhance the extraction and alignment of geometric information.\nAdditionally, we introduce a cross-cloud transformation mechanism that\nefficiently merges diverse data sources. Finally, we push the limits of our\nmodel by incorporating the shape prior information for translation and size\nerror prediction. We conducted extensive experiments to demonstrate the\neffectiveness of the proposed framework. Through extensive quantitative\nexperiments, we demonstrate significant improvement over the baseline method by\na large margin across all metrics.\n","authors":["Linfang Zheng","Tze Ho Elden Tse","Chen Wang","Yinghan Sun","Hua Chen","Ales Leonardis","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.11139v1.pdf","comment":"The IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  2024"},{"id":"http://arxiv.org/abs/2404.11129v1","updated":"2024-04-17T07:20:56Z","published":"2024-04-17T07:20:56Z","title":"Fact :Teaching MLLMs with Faithful, Concise and Transferable Rationales","summary":"  The remarkable performance of Multimodal Large Language Models (MLLMs) has\nunequivocally demonstrated their proficient understanding capabilities in\nhandling a wide array of visual tasks. Nevertheless, the opaque nature of their\nblack-box reasoning processes persists as an enigma, rendering them\nuninterpretable and struggling with hallucination. Their ability to execute\nintricate compositional reasoning tasks is also constrained, culminating in a\nstagnation of learning progression for these models. In this work, we introduce\nFact, a novel paradigm designed to generate multimodal rationales that are\nfaithful, concise, and transferable for teaching MLLMs. This paradigm utilizes\nverifiable visual programming to generate executable code guaranteeing\nfaithfulness and precision. Subsequently, through a series of operations\nincluding pruning, merging, and bridging, the rationale enhances its\nconciseness. Furthermore, we filter rationales that can be transferred to\nend-to-end paradigms from programming paradigms to guarantee transferability.\nEmpirical evidence from experiments demonstrates the superiority of our method\nacross models of varying parameter sizes, significantly enhancing their\ncompositional reasoning and generalization ability. Our approach also reduces\nhallucinations owing to its high correlation between images and text.\n","authors":["Minghe Gao","Shuang Chen","Liang Pang","Yuan Yao","Jisheng Dang","Wenqiao Zhang","Juncheng Li","Siliang Tang","Yueting Zhuang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2404.11129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11127v1","updated":"2024-04-17T07:17:47Z","published":"2024-04-17T07:17:47Z","title":"D-Aug: Enhancing Data Augmentation for Dynamic LiDAR Scenes","summary":"  Creating large LiDAR datasets with pixel-level labeling poses significant\nchallenges. While numerous data augmentation methods have been developed to\nreduce the reliance on manual labeling, these methods predominantly focus on\nstatic scenes and they overlook the importance of data augmentation for dynamic\nscenes, which is critical for autonomous driving. To address this issue, we\npropose D-Aug, a LiDAR data augmentation method tailored for augmenting dynamic\nscenes. D-Aug extracts objects and inserts them into dynamic scenes,\nconsidering the continuity of these objects across consecutive frames. For\nseamless insertion into dynamic scenes, we propose a reference-guided method\nthat involves dynamic collision detection and rotation alignment. Additionally,\nwe present a pixel-level road identification strategy to efficiently determine\nsuitable insertion positions. We validated our method using the nuScenes\ndataset with various 3D detection and tracking methods. Comparative experiments\ndemonstrate the superiority of D-Aug.\n","authors":["Jiaxing Zhao","Peng Zheng","Rui Ma"],"pdf_url":"https://arxiv.org/pdf/2404.11127v1.pdf","comment":"4pages, 4 figures"},{"id":"http://arxiv.org/abs/2404.02562v2","updated":"2024-04-17T07:13:27Z","published":"2024-04-03T08:33:08Z","title":"Representation Alignment Contrastive Regularization for Multi-Object\n  Tracking","summary":"  Achieving high-performance in multi-object tracking algorithms heavily relies\non modeling spatio-temporal relationships during the data association stage.\nMainstream approaches encompass rule-based and deep learning-based methods for\nspatio-temporal relationship modeling. While the former relies on physical\nmotion laws, offering wider applicability but yielding suboptimal results for\ncomplex object movements, the latter, though achieving high-performance, lacks\ninterpretability and involves complex module designs. This work aims to\nsimplify deep learning-based spatio-temporal relationship models and introduce\ninterpretability into features for data association. Specifically, a\nlightweight single-layer transformer encoder is utilized to model\nspatio-temporal relationships. To make features more interpretative, two\ncontrastive regularization losses based on representation alignment are\nproposed, derived from spatio-temporal consistency rules. By applying weighted\nsummation to affinity matrices, the aligned features can seamlessly integrate\ninto the data association stage of the original tracking workflow. Experimental\nresults showcase that our model enhances the majority of existing tracking\nnetworks' performance without excessive complexity, with minimal increase in\ntraining overhead and nearly negligible computational and storage costs.\n","authors":["Zhonglin Liu","Shujie Chen","Jianfeng Dong","Xun Wang","Di Zhou"],"pdf_url":"https://arxiv.org/pdf/2404.02562v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11120v1","updated":"2024-04-17T07:08:38Z","published":"2024-04-17T07:08:38Z","title":"TiNO-Edit: Timestep and Noise Optimization for Robust Diffusion-Based\n  Image Editing","summary":"  Despite many attempts to leverage pre-trained text-to-image models (T2I) like\nStable Diffusion (SD) for controllable image editing, producing good\npredictable results remains a challenge. Previous approaches have focused on\neither fine-tuning pre-trained T2I models on specific datasets to generate\ncertain kinds of images (e.g., with a specific object or person), or on\noptimizing the weights, text prompts, and/or learning features for each input\nimage in an attempt to coax the image generator to produce the desired result.\nHowever, these approaches all have shortcomings and fail to produce good\nresults in a predictable and controllable manner. To address this problem, we\npresent TiNO-Edit, an SD-based method that focuses on optimizing the noise\npatterns and diffusion timesteps during editing, something previously\nunexplored in the literature. With this simple change, we are able to generate\nresults that both better align with the original images and reflect the desired\nresult. Furthermore, we propose a set of new loss functions that operate in the\nlatent domain of SD, greatly speeding up the optimization when compared to\nprior approaches, which operate in the pixel domain. Our method can be easily\napplied to variations of SD including Textual Inversion and DreamBooth that\nencode new concepts and incorporate them into the edited results. We present a\nhost of image-editing capabilities enabled by our approach. Our code is\npublicly available at https://github.com/SherryXTChen/TiNO-Edit.\n","authors":["Sherry X. Chen","Yaron Vaxman","Elad Ben Baruch","David Asulin","Aviad Moreshet","Kuo-Chin Lien","Misha Sra","Pradeep Sen"],"pdf_url":"https://arxiv.org/pdf/2404.11120v1.pdf","comment":"Conference on Computer Vision and Pattern Recognition (CVPR) 2024"},{"id":"http://arxiv.org/abs/2404.11118v1","updated":"2024-04-17T07:06:22Z","published":"2024-04-17T07:06:22Z","title":"MHLR: Moving Haar Learning Rate Scheduler for Large-scale Face\n  Recognition Training with One GPU","summary":"  Face recognition (FR) has seen significant advancements due to the\nutilization of large-scale datasets. Training deep FR models on large-scale\ndatasets with multiple GPUs is now a common practice. In fact, computing power\nhas evolved into a foundational and indispensable resource in the area of deep\nlearning. It is nearly impossible to train a deep FR model without holding\nadequate hardware resources. Recognizing this challenge, some FR approaches\nhave started exploring ways to reduce the time complexity of the\nfully-connected layer in FR models. Unlike other approaches, this paper\nintroduces a simple yet highly effective approach, Moving Haar Learning Rate\n(MHLR) scheduler, for scheduling the learning rate promptly and accurately in\nthe training process. MHLR supports large-scale FR training with only one GPU,\nwhich is able to accelerate the model to 1/4 of its original training time\nwithout sacrificing more than 1% accuracy. More specifically, MHLR only needs\n$30$ hours to train the model ResNet100 on the dataset WebFace12M containing\nmore than 12M face images with 0.6M identities. Extensive experiments validate\nthe efficiency and effectiveness of MHLR.\n","authors":["Xueyuan Gong","Yain-whar Si","Zheng Zhang","Xiaochen Yuan","Ke Wang","Xinyuan Zhang","Cong Lin","Xiaoxiang Liu"],"pdf_url":"https://arxiv.org/pdf/2404.11118v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11111v1","updated":"2024-04-17T06:57:57Z","published":"2024-04-17T06:57:57Z","title":"CorrNet+: Sign Language Recognition and Translation via Spatial-Temporal\n  Correlation","summary":"  In sign language, the conveyance of human body trajectories predominantly\nrelies upon the coordinated movements of hands and facial expressions across\nsuccessive frames. Despite the recent advancements of sign language\nunderstanding methods, they often solely focus on individual frames, inevitably\noverlooking the inter-frame correlations that are essential for effectively\nmodeling human body trajectories. To address this limitation, this paper\nintroduces a spatial-temporal correlation network, denoted as CorrNet+, which\nexplicitly identifies body trajectories across multiple frames. In specific,\nCorrNet+ employs a correlation module and an identification module to build\nhuman body trajectories. Afterwards, a temporal attention module is followed to\nadaptively evaluate the contributions of different frames. The resultant\nfeatures offer a holistic perspective on human body movements, facilitating a\ndeeper understanding of sign language. As a unified model, CorrNet+ achieves\nnew state-of-the-art performance on two extensive sign language understanding\ntasks, including continuous sign language recognition (CSLR) and sign language\ntranslation (SLT). Especially, CorrNet+ surpasses previous methods equipped\nwith resource-intensive pose-estimation networks or pre-extracted heatmaps for\nhand and facial feature extraction. Compared with CorrNet, CorrNet+ achieves a\nsignificant performance boost across all benchmarks while halving the\ncomputational overhead. A comprehensive comparison with previous\nspatial-temporal reasoning methods verifies the superiority of CorrNet+. Code\nis available at https://github.com/hulianyuyy/CorrNet_Plus.\n","authors":["Lianyu Hu","Wei Feng","Liqing Gao","Zekang Liu","Liang Wan"],"pdf_url":"https://arxiv.org/pdf/2404.11111v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2303.03202"},{"id":"http://arxiv.org/abs/2404.11108v1","updated":"2024-04-17T06:47:17Z","published":"2024-04-17T06:47:17Z","title":"LADDER: An Efficient Framework for Video Frame Interpolation","summary":"  Video Frame Interpolation (VFI) is a crucial technique in various\napplications such as slow-motion generation, frame rate conversion, video frame\nrestoration etc. This paper introduces an efficient video frame interpolation\nframework that aims to strike a favorable balance between efficiency and\nquality. Our framework follows a general paradigm consisting of a flow\nestimator and a refinement module, while incorporating carefully designed\ncomponents. First of all, we adopt depth-wise convolution with large kernels in\nthe flow estimator that simultaneously reduces the parameters and enhances the\nreceptive field for encoding rich context and handling complex motion.\nSecondly, diverging from a common design for the refinement module with a\nUNet-structure (encoder-decoder structure), which we find redundant, our\ndecoder-only refinement module directly enhances the result from coarse to fine\nfeatures, offering a more efficient process. In addition, to address the\nchallenge of handling high-definition frames, we also introduce an innovative\nHD-aware augmentation strategy during training, leading to consistent\nenhancement on HD images. Extensive experiments are conducted on diverse\ndatasets, Vimeo90K, UCF101, Xiph and SNU-FILM. The results demonstrate that our\napproach achieves state-of-the-art performance with clear improvement while\nrequiring much less FLOPs and parameters, reaching to a better spot for\nbalancing efficiency and quality.\n","authors":["Tong Shen","Dong Li","Ziheng Gao","Lu Tian","Emad Barsoum"],"pdf_url":"https://arxiv.org/pdf/2404.11108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11104v1","updated":"2024-04-17T06:40:47Z","published":"2024-04-17T06:40:47Z","title":"Object Remover Performance Evaluation Methods using Class-wise Object\n  Removal Images","summary":"  Object removal refers to the process of erasing designated objects from an\nimage while preserving the overall appearance, and it is one area where image\ninpainting is widely used in real-world applications. The performance of an\nobject remover is quantitatively evaluated by measuring the quality of object\nremoval results, similar to how the performance of an image inpainter is\ngauged. Current works reporting quantitative performance evaluations utilize\noriginal images as references. In this letter, to validate the current\nevaluation methods cannot properly evaluate the performance of an object\nremover, we create a dataset with object removal ground truth and compare the\nevaluations made by the current methods using original images to those\nutilizing object removal ground truth images. The disparities between two\nevaluation sets validate that the current methods are not suitable for\nmeasuring the performance of an object remover. Additionally, we propose new\nevaluation methods tailored to gauge the performance of an object remover. The\nproposed methods evaluate the performance through class-wise object removal\nresults and utilize images without the target class objects as a comparison\nset. We confirm that the proposed methods can make judgments consistent with\nhuman evaluators in the COCO dataset, and that they can produce measurements\naligning with those using object removal ground truth in the self-acquired\ndataset.\n","authors":["Changsuk Oh","Dongseok Shim","Taekbeom Lee","H. Jin Kim"],"pdf_url":"https://arxiv.org/pdf/2404.11104v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11100v1","updated":"2024-04-17T06:36:17Z","published":"2024-04-17T06:36:17Z","title":"Synthesizing Realistic Data for Table Recognition","summary":"  To overcome the limitations and challenges of current automatic table data\nannotation methods and random table data synthesis approaches, we propose a\nnovel method for synthesizing annotation data specifically designed for table\nrecognition. This method utilizes the structure and content of existing complex\ntables, facilitating the efficient creation of tables that closely replicate\nthe authentic styles found in the target domain. By leveraging the actual\nstructure and content of tables from Chinese financial announcements, we have\ndeveloped the first extensive table annotation dataset in this domain. We used\nthis dataset to train several recent deep learning-based end-to-end table\nrecognition models. Additionally, we have established the inaugural benchmark\nfor real-world complex tables in the Chinese financial announcement domain,\nusing it to assess the performance of models trained on our synthetic data,\nthereby effectively validating our method's practicality and effectiveness.\nFurthermore, we applied our synthesis method to augment the FinTabNet dataset,\nextracted from English financial announcements, by increasing the proportion of\ntables with multiple spanning cells to introduce greater complexity. Our\nexperiments show that models trained on this augmented dataset achieve\ncomprehensive improvements in performance, especially in the recognition of\ntables with multiple spanning cells.\n","authors":["Qiyu Hou","Jun Wang","Meixuan Qiao","Lujun Tian"],"pdf_url":"https://arxiv.org/pdf/2404.11100v1.pdf","comment":"ICDAR 2024"},{"id":"http://arxiv.org/abs/2404.11098v1","updated":"2024-04-17T06:32:42Z","published":"2024-04-17T06:32:42Z","title":"LAPTOP-Diff: Layer Pruning and Normalized Distillation for Compressing\n  Diffusion Models","summary":"  In the era of AIGC, the demand for low-budget or even on-device applications\nof diffusion models emerged. In terms of compressing the Stable Diffusion\nmodels (SDMs), several approaches have been proposed, and most of them\nleveraged the handcrafted layer removal methods to obtain smaller U-Nets, along\nwith knowledge distillation to recover the network performance. However, such a\nhandcrafting manner of layer removal is inefficient and lacks scalability and\ngeneralization, and the feature distillation employed in the retraining phase\nfaces an imbalance issue that a few numerically significant feature loss terms\ndominate over others throughout the retraining process. To this end, we\nproposed the layer pruning and normalized distillation for compressing\ndiffusion models (LAPTOP-Diff). We, 1) introduced the layer pruning method to\ncompress SDM's U-Net automatically and proposed an effective one-shot pruning\ncriterion whose one-shot performance is guaranteed by its good additivity\nproperty, surpassing other layer pruning and handcrafted layer removal methods,\n2) proposed the normalized feature distillation for retraining, alleviated the\nimbalance issue. Using the proposed LAPTOP-Diff, we compressed the U-Nets of\nSDXL and SDM-v1.5 for the most advanced performance, achieving a minimal 4.0%\ndecline in PickScore at a pruning ratio of 50% while the comparative methods'\nminimal PickScore decline is 8.2%. We will release our code.\n","authors":["Dingkun Zhang","Sijia Li","Chen Chen","Qingsong Xie","Haonan Lu"],"pdf_url":"https://arxiv.org/pdf/2404.11098v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10312v2","updated":"2024-04-17T06:30:00Z","published":"2024-04-16T06:39:37Z","title":"OmniSSR: Zero-shot Omnidirectional Image Super-Resolution using Stable\n  Diffusion Model","summary":"  Omnidirectional images (ODIs) are commonly used in real-world visual tasks,\nand high-resolution ODIs help improve the performance of related visual tasks.\nMost existing super-resolution methods for ODIs use end-to-end learning\nstrategies, resulting in inferior realness of generated images and a lack of\neffective out-of-domain generalization capabilities in training methods. Image\ngeneration methods represented by diffusion model provide strong priors for\nvisual tasks and have been proven to be effectively applied to image\nrestoration tasks. Leveraging the image priors of the Stable Diffusion (SD)\nmodel, we achieve omnidirectional image super-resolution with both fidelity and\nrealness, dubbed as OmniSSR. Firstly, we transform the equirectangular\nprojection (ERP) images into tangent projection (TP) images, whose distribution\napproximates the planar image domain. Then, we use SD to iteratively sample\ninitial high-resolution results. At each denoising iteration, we further\ncorrect and update the initial results using the proposed Octadecaplex Tangent\nInformation Interaction (OTII) and Gradient Decomposition (GD) technique to\nensure better consistency. Finally, the TP images are transformed back to\nobtain the final high-resolution results. Our method is zero-shot, requiring no\ntraining or fine-tuning. Experiments of our method on two benchmark datasets\ndemonstrate the effectiveness of our proposed method.\n","authors":["Runyi Li","Xuhan Sheng","Weiqi Li","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.10312v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00425v2","updated":"2024-04-17T06:26:04Z","published":"2023-12-01T08:47:56Z","title":"Retina : Low-Power Eye Tracking with Event Camera and Spiking Hardware","summary":"  This paper introduces a neuromorphic methodology for eye tracking, harnessing\npure event data captured by a Dynamic Vision Sensor (DVS) camera. The framework\nintegrates a directly trained Spiking Neuron Network (SNN) regression model and\nleverages a state-of-the-art low power edge neuromorphic processor - Speck,\ncollectively aiming to advance the precision and efficiency of eye-tracking\nsystems. First, we introduce a representative event-based eye-tracking dataset,\n\"Ini-30\", which was collected with two glass-mounted DVS cameras from thirty\nvolunteers. Then,a SNN model, based on Integrate And Fire (IAF) neurons, named\n\"Retina\", is described , featuring only 64k parameters (6.63x fewer than the\nlatest) and achieving pupil tracking error of only 3.24 pixels in a 64x64 DVS\ninput. The continous regression output is obtained by means of convolution\nusing a non-spiking temporal 1D filter slided across the output spiking layer.\nFinally, we evaluate Retina on the neuromorphic processor, showing an\nend-to-end power between 2.89-4.8 mW and a latency of 5.57-8.01 mS dependent on\nthe time window. We also benchmark our model against the latest event-based\neye-tracking method, \"3ET\", which was built upon event frames. Results show\nthat Retina achieves superior precision with 1.24px less pupil centroid error\nand reduced computational complexity with 35 times fewer MAC operations. We\nhope this work will open avenues for further investigation of close-loop\nneuromorphic solutions and true event-based training pursuing edge performance.\n","authors":["Pietro Bonazzi","Sizhen Bian","Giovanni Lippolis","Yawei Li","Sadique Sheik","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2312.00425v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09831v2","updated":"2024-04-17T05:55:33Z","published":"2024-04-15T14:29:47Z","title":"Digging into contrastive learning for robust depth estimation with\n  diffusion models","summary":"  Recently, diffusion-based depth estimation methods have drawn widespread\nattention due to their elegant denoising patterns and promising performance.\nHowever, they are typically unreliable under adverse conditions prevalent in\nreal-world scenarios, such as rainy, snowy, etc. In this paper, we propose a\nnovel robust depth estimation method called D4RD, featuring a custom\ncontrastive learning mode tailored for diffusion models to mitigate performance\ndegradation in complex environments. Concretely, we integrate the strength of\nknowledge distillation into contrastive learning, building the `trinity'\ncontrastive scheme. This scheme utilizes the sampled noise of the forward\ndiffusion process as a natural reference, guiding the predicted noise in\ndiverse scenes toward a more stable and precise optimum. Moreover, we extend\nnoise-level trinity to encompass more generic feature and image levels,\nestablishing a multi-level contrast to distribute the burden of robust\nperception across the overall network. Before addressing complex scenarios, we\nenhance the stability of the baseline diffusion model with three\nstraightforward yet effective improvements, which facilitate convergence and\nremove depth outliers. Extensive experiments demonstrate that D4RD surpasses\nexisting state-of-the-art solutions on synthetic corruption datasets and\nreal-world weather conditions. The code for D4RD will be made available for\nfurther exploration and adoption.\n","authors":["Jiyuan Wang","Chunyu Lin","Lang Nie","Kang Liao","Shuwei Shao","Yao Zhao"],"pdf_url":"https://arxiv.org/pdf/2404.09831v2.pdf","comment":"8 pages,6 figures"},{"id":"http://arxiv.org/abs/2402.19474v3","updated":"2024-04-17T05:55:04Z","published":"2024-02-29T18:59:17Z","title":"The All-Seeing Project V2: Towards General Relation Comprehension of the\n  Open World","summary":"  We present the All-Seeing Project V2: a new model and dataset designed for\nunderstanding object relations in images. Specifically, we propose the\nAll-Seeing Model V2 (ASMv2) that integrates the formulation of text generation,\nobject localization, and relation comprehension into a relation conversation\n(ReC) task. Leveraging this unified task, our model excels not only in\nperceiving and recognizing all objects within the image but also in grasping\nthe intricate relation graph between them, diminishing the relation\nhallucination often encountered by Multi-modal Large Language Models (MLLMs).\nTo facilitate training and evaluation of MLLMs in relation understanding, we\ncreated the first high-quality ReC dataset ({AS-V2) which is aligned with the\nformat of standard instruction tuning data. In addition, we design a new\nbenchmark, termed Circular-based Relation Probing Evaluation (CRPE) for\ncomprehensively evaluating the relation comprehension capabilities of MLLMs.\nNotably, our ASMv2 achieves an overall accuracy of 52.04 on this relation-aware\nbenchmark, surpassing the 43.14 of LLaVA-1.5 by a large margin. We hope that\nour work can inspire more future research and contribute to the evolution\ntowards artificial general intelligence. Our project is released at\nhttps://github.com/OpenGVLab/all-seeing.\n","authors":["Weiyun Wang","Yiming Ren","Haowen Luo","Tiantong Li","Chenxiang Yan","Zhe Chen","Wenhai Wang","Qingyun Li","Lewei Lu","Xizhou Zhu","Yu Qiao","Jifeng Dai"],"pdf_url":"https://arxiv.org/pdf/2402.19474v3.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2404.08968v2","updated":"2024-04-17T05:42:52Z","published":"2024-04-13T11:13:56Z","title":"MCPNet: An Interpretable Classifier via Multi-Level Concept Prototypes","summary":"  Recent advancements in post-hoc and inherently interpretable methods have\nmarkedly enhanced the explanations of black box classifier models. These\nmethods operate either through post-analysis or by integrating concept learning\nduring model training. Although being effective in bridging the semantic gap\nbetween a model's latent space and human interpretation, these explanation\nmethods only partially reveal the model's decision-making process. The outcome\nis typically limited to high-level semantics derived from the last feature map.\nWe argue that the explanations lacking insights into the decision processes at\nlow and mid-level features are neither fully faithful nor useful. Addressing\nthis gap, we introduce the Multi-Level Concept Prototypes Classifier (MCPNet),\nan inherently interpretable model. MCPNet autonomously learns meaningful\nconcept prototypes across multiple feature map levels using Centered Kernel\nAlignment (CKA) loss and an energy-based weighted PCA mechanism, and it does so\nwithout reliance on predefined concept labels. Further, we propose a novel\nclassifier paradigm that learns and aligns multi-level concept prototype\ndistributions for classification purposes via Class-aware Concept Distribution\n(CCD) loss. Our experiments reveal that our proposed MCPNet while being\nadaptable to various model architectures, offers comprehensive multi-level\nexplanations while maintaining classification accuracy. Additionally, its\nconcept distribution-based classification approach shows improved\ngeneralization capabilities in few-shot classification scenarios.\n","authors":["Bor-Shiun Wang","Chien-Yi Wang","Wei-Chen Chiu"],"pdf_url":"https://arxiv.org/pdf/2404.08968v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2404.11070v1","updated":"2024-04-17T04:59:36Z","published":"2024-04-17T04:59:36Z","title":"Sky-GVIO: an enhanced GNSS/INS/Vision navigation with FCN-based\n  sky-segmentation in urban canyon","summary":"  Accurate, continuous, and reliable positioning is a critical component of\nachieving autonomous driving. However, in complex urban canyon environments,\nthe vulnerability of a stand-alone sensor and non-line-of-sight (NLOS) caused\nby high buildings, trees, and elevated structures seriously affect positioning\nresults. To address these challenges, a sky-view images segmentation algorithm\nbased on Fully Convolutional Network (FCN) is proposed for GNSS NLOS detection.\nBuilding upon this, a novel NLOS detection and mitigation algorithm (named\nS-NDM) is extended to the tightly coupled Global Navigation Satellite Systems\n(GNSS), Inertial Measurement Units (IMU), and visual feature system which is\ncalled Sky-GVIO, with the aim of achieving continuous and accurate positioning\nin urban canyon environments. Furthermore, the system harmonizes Single Point\nPositioning (SPP) with Real-Time Kinematic (RTK) methodologies to bolster its\noperational versatility and resilience. In urban canyon environments, the\npositioning performance of S-NDM algorithm proposed in this paper is evaluated\nunder different tightly coupled SPP-related and RTK-related models. The results\nexhibit that Sky-GVIO system achieves meter-level accuracy under SPP mode and\nsub-decimeter precision with RTK, surpassing the performance of GNSS/INS/Vision\nframeworks devoid of S-NDM. Additionally, the sky-view image dataset, inclusive\nof training and evaluation subsets, has been made publicly accessible for\nscholarly exploration at https://github.com/whuwangjr/sky-view-images .\n","authors":["Jingrong Wang","Bo Xu","Ronghe Jin","Shoujian Zhang","Kefu Gao","Jingnan Liu"],"pdf_url":"https://arxiv.org/pdf/2404.11070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11064v1","updated":"2024-04-17T04:46:27Z","published":"2024-04-17T04:46:27Z","title":"Rethinking 3D Dense Caption and Visual Grounding in A Unified Framework\n  through Prompt-based Localization","summary":"  3D Visual Grounding (3DVG) and 3D Dense Captioning (3DDC) are two crucial\ntasks in various 3D applications, which require both shared and complementary\ninformation in localization and visual-language relationships. Therefore,\nexisting approaches adopt the two-stage \"detect-then-describe/discriminate\"\npipeline, which relies heavily on the performance of the detector, resulting in\nsuboptimal performance. Inspired by DETR, we propose a unified framework,\n3DGCTR, to jointly solve these two distinct but closely related tasks in an\nend-to-end fashion. The key idea is to reconsider the prompt-based localization\nability of the 3DVG model. In this way, the 3DVG model with a well-designed\nprompt as input can assist the 3DDC task by extracting localization information\nfrom the prompt. In terms of implementation, we integrate a Lightweight Caption\nHead into the existing 3DVG network with a Caption Text Prompt as a connection,\neffectively harnessing the existing 3DVG model's inherent localization\ncapacity, thereby boosting 3DDC capability. This integration facilitates\nsimultaneous multi-task training on both tasks, mutually enhancing their\nperformance. Extensive experimental results demonstrate the effectiveness of\nthis approach. Specifically, on the ScanRefer dataset, 3DGCTR surpasses the\nstate-of-the-art 3DDC method by 4.3% in CIDEr@0.5IoU in MLE training and\nimproves upon the SOTA 3DVG method by 3.16% in Acc@0.25IoU.\n","authors":["Yongdong Luo","Haojia Lin","Xiawu Zheng","Yigeng Jiang","Fei Chao","Jie Hu","Guannan Jiang","Songan Zhang","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2404.11064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.03492v2","updated":"2024-04-17T04:07:47Z","published":"2023-06-06T08:19:30Z","title":"Efficient Anomaly Detection with Budget Annotation Using Semi-Supervised\n  Residual Transformer","summary":"  Anomaly Detection is challenging as usually only the normal samples are seen\nduring training and the detector needs to discover anomalies on-the-fly. The\nrecently proposed deep-learning-based approaches could somehow alleviate the\nproblem but there is still a long way to go in obtaining an industrial-class\nanomaly detector for real-world applications. On the other hand, in some\nparticular AD tasks, a few anomalous samples are labeled manually for achieving\nhigher accuracy. However, this performance gain is at the cost of considerable\nannotation efforts, which can be intractable in many practical scenarios.\n  In this work, the above two problems are addressed in a unified framework.\nFirstly, inspired by the success of the patch-matching-based AD algorithms, we\ntrain a sliding vision transformer over the residuals generated by a novel\nposition-constrained patch-matching. Secondly, the conventional pixel-wise\nsegmentation problem is cast into a block-wise classification problem. Thus the\nsliding transformer can attain even higher accuracy with much less annotation\nlabor. Thirdly, to further reduce the labeling cost, we propose to label the\nanomalous regions using only bounding boxes. The unlabeled regions caused by\nthe weak labels are effectively exploited using a highly-customized\nsemi-supervised learning scheme equipped with two novel data augmentation\nmethods. The proposed method outperforms all the state-of-the-art approaches\nusing all the evaluation metrics in both the unsupervised and supervised\nscenarios. On the popular MVTec-AD dataset, our SemiREST algorithm obtains the\nAverage Precision (AP) of 81.2% in the unsupervised condition and 84.4% AP for\nsupervised anomaly detection. Surprisingly, with the bounding-box-based\nsemi-supervisions, SemiREST still outperforms the SOTA methods with full\nsupervision (83.8% AP) on MVTec-AD.\n","authors":["Hanxi Li","Jingqi Wu","Hao Chen","Mingwen Wang","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2306.03492v2.pdf","comment":"20 pages,6 figures"},{"id":"http://arxiv.org/abs/2404.11054v1","updated":"2024-04-17T03:56:28Z","published":"2024-04-17T03:56:28Z","title":"Multilateral Temporal-view Pyramid Transformer for Video Inpainting\n  Detection","summary":"  The task of video inpainting detection is to expose the pixel-level inpainted\nregions within a video sequence. Existing methods usually focus on leveraging\nspatial and temporal inconsistencies. However, these methods typically employ\nfixed operations to combine spatial and temporal clues, limiting their\napplicability in different scenarios. In this paper, we introduce a novel\nMultilateral Temporal-view Pyramid Transformer ({\\em MumPy}) that collaborates\nspatial-temporal clues flexibly. Our method utilizes a newly designed\nmultilateral temporal-view encoder to extract various collaborations of\nspatial-temporal clues and introduces a deformable window-based temporal-view\ninteraction module to enhance the diversity of these collaborations.\nSubsequently, we develop a multi-pyramid decoder to aggregate the various types\nof features and generate detection maps. By adjusting the contribution strength\nof spatial and temporal clues, our method can effectively identify inpainted\nregions. We validate our method on existing datasets and also introduce a new\nchallenging and large-scale Video Inpainting dataset based on the YouTube-VOS\ndataset, which employs several more recent inpainting methods. The results\ndemonstrate the superiority of our method in both in-domain and cross-domain\nevaluation scenarios.\n","authors":["Ying Zhang","Bo Peng","Jiaran Zhou","Huiyu Zhou","Junyu Dong","Yuezun Li"],"pdf_url":"https://arxiv.org/pdf/2404.11054v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11052v1","updated":"2024-04-17T03:51:55Z","published":"2024-04-17T03:51:55Z","title":"Supervised Contrastive Vision Transformer for Breast Histopathological\n  Image Classification","summary":"  Invasive ductal carcinoma (IDC) is the most prevalent form of breast cancer.\nBreast tissue histopathological examination is critical in diagnosing and\nclassifying breast cancer. Although existing methods have shown promising\nresults, there is still room for improvement in the classification accuracy and\ngeneralization of IDC using histopathology images. We present a novel approach,\nSupervised Contrastive Vision Transformer (SupCon-ViT), for improving the\nclassification of invasive ductal carcinoma in terms of accuracy and\ngeneralization by leveraging the inherent strengths and advantages of both\ntransfer learning, i.e., pre-trained vision transformer, and supervised\ncontrastive learning. Our results on a benchmark breast cancer dataset\ndemonstrate that SupCon-Vit achieves state-of-the-art performance in IDC\nclassification, with an F1-score of 0.8188, precision of 0.7692, and\nspecificity of 0.8971, outperforming existing methods. In addition, the\nproposed model demonstrates resilience in scenarios with minimal labeled data,\nmaking it highly efficient in real-world clinical settings where labelled data\nis limited. Our findings suggest that supervised contrastive learning in\nconjunction with pre-trained vision transformers appears to be a viable\nstrategy for an accurate classification of IDC, thus paving the way for a more\nefficient and reliable diagnosis of breast cancer through histopathological\nimage analysis.\n","authors":["Mohammad Shiri","Jiangwen Sun"],"pdf_url":"https://arxiv.org/pdf/2404.11052v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2404.11051v1","updated":"2024-04-17T03:51:24Z","published":"2024-04-17T03:51:24Z","title":"WPS-Dataset: A benchmark for wood plate segmentation in bark removal\n  processing","summary":"  Using deep learning methods is a promising approach to improving bark removal\nefficiency and enhancing the quality of wood products. However, the lack of\npublicly available datasets for wood plate segmentation in bark removal\nprocessing poses challenges for researchers in this field. To address this\nissue, a benchmark for wood plate segmentation in bark removal processing named\nWPS-dataset is proposed in this study, which consists of 4863 images. We\ndesigned an image acquisition device and assembled it on a bark removal\nequipment to capture images in real industrial settings. We evaluated the\nWPS-dataset using six typical segmentation models. The models effectively learn\nand understand the WPS-dataset characteristics during training, resulting in\nhigh performance and accuracy in wood plate segmentation tasks. We believe that\nour dataset can lay a solid foundation for future research in bark removal\nprocessing and contribute to advancements in this field.\n","authors":["Rijun Wang","Guanghao Zhang","Fulong Liang","Bo Wang","Xiangwei Mou","Yesheng Chen"],"pdf_url":"https://arxiv.org/pdf/2404.11051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11046v1","updated":"2024-04-17T03:42:48Z","published":"2024-04-17T03:42:48Z","title":"Lightweight Unsupervised Federated Learning with Pretrained Vision\n  Language Model","summary":"  Federated learning aims to tackle the ``isolated data island\" problem, where\nit trains a collective model from physically isolated clients while\nsafeguarding the privacy of users' data. However, supervised federated learning\nnecessitates that each client labels their data for training, which can be both\ntime-consuming and resource-intensive, and may even be impractical for edge\ndevices. Moreover, the training and transmission of deep models present\nchallenges to the computation and communication capabilities of the clients. To\naddress these two inherent challenges in supervised federated learning, we\npropose a novel lightweight unsupervised federated learning approach that\nleverages unlabeled data on each client to perform lightweight model training\nand communication by harnessing pretrained vision-language models, such as\nCLIP. By capitalizing on the zero-shot prediction capability and the\nwell-trained image encoder of the pre-trained CLIP model, we have carefully\ncrafted an efficient and resilient self-training approach. This method refines\nthe initial zero-shot predicted pseudo-labels of unlabeled instances through\nthe sole training of a linear classifier on top of the fixed image encoder.\nAdditionally, to address data heterogeneity within each client, we propose a\nclass-balanced text feature sampling strategy for generating synthetic\ninstances in the feature space to support local training. Experiments are\nconducted on multiple benchmark datasets. The experimental results demonstrate\nthat our proposed method greatly enhances model performance in comparison to\nCLIP's zero-shot predictions and even outperforms supervised federated learning\nbenchmark methods given limited computational and communication overhead.\n","authors":["Hao Yan","Yuhong Guo"],"pdf_url":"https://arxiv.org/pdf/2404.11046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07922v4","updated":"2024-04-17T03:23:33Z","published":"2024-04-11T17:09:28Z","title":"LaVy: Vietnamese Multimodal Large Language Model","summary":"  Large Language Models (LLMs) and Multimodal Large language models (MLLMs)\nhave taken the world by storm with impressive abilities in complex reasoning\nand linguistic comprehension. Meanwhile there are plethora of works related to\nVietnamese Large Language Models, the lack of high-quality resources in\nmultimodality limits the progress of Vietnamese MLLMs. In this paper, we\npioneer in address this by introducing LaVy, a state-of-the-art Vietnamese\nMLLM, and we also introduce LaVy-Bench benchmark designated for evaluating\nMLLMs's understanding on Vietnamese visual language tasks. Our project is\npublic at https://github.com/baochi0212/LaVy\n","authors":["Chi Tran","Huong Le Thanh"],"pdf_url":"https://arxiv.org/pdf/2404.07922v4.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2306.08251v3","updated":"2024-04-17T03:14:21Z","published":"2023-06-14T05:34:02Z","title":"GBSD: Generative Bokeh with Stage Diffusion","summary":"  The bokeh effect is an artistic technique that blurs out-of-focus areas in a\nphotograph and has gained interest due to recent developments in text-to-image\nsynthesis and the ubiquity of smart-phone cameras and photo-sharing apps. Prior\nwork on rendering bokeh effects have focused on post hoc image manipulation to\nproduce similar blurring effects in existing photographs using classical\ncomputer graphics or neural rendering techniques, but have either depth\ndiscontinuity artifacts or are restricted to reproducing bokeh effects that are\npresent in the training data. More recent diffusion based models can synthesize\nimages with an artistic style, but either require the generation of\nhigh-dimensional masks, expensive fine-tuning, or affect global image\ncharacteristics. In this paper, we present GBSD, the first generative\ntext-to-image model that synthesizes photorealistic images with a bokeh style.\nMotivated by how image synthesis occurs progressively in diffusion models, our\napproach combines latent diffusion models with a 2-stage conditioning algorithm\nto render bokeh effects on semantically defined objects. Since we can focus the\neffect on objects, this semantic bokeh effect is more versatile than classical\nrendering techniques. We evaluate GBSD both quantitatively and qualitatively\nand demonstrate its ability to be applied in both text-to-image and\nimage-to-image settings.\n","authors":["Jieren Deng","Xin Zhou","Hao Tian","Zhihong Pan","Derek Aguiar"],"pdf_url":"https://arxiv.org/pdf/2306.08251v3.pdf","comment":"Short Version is accepted by International Conference on Acoustics,\n  Speech, and Signal Processing (ICASSP) 2024"},{"id":"http://arxiv.org/abs/2401.03907v2","updated":"2024-04-17T03:14:00Z","published":"2024-01-08T14:10:24Z","title":"RoboFusion: Towards Robust Multi-Modal 3D Object Detection via SAM","summary":"  Multi-modal 3D object detectors are dedicated to exploring secure and\nreliable perception systems for autonomous driving (AD). However, while\nachieving state-of-the-art (SOTA) performance on clean benchmark datasets, they\ntend to overlook the complexity and harsh conditions of real-world\nenvironments. Meanwhile, with the emergence of visual foundation models (VFMs),\nopportunities and challenges are presented for improving the robustness and\ngeneralization of multi-modal 3D object detection in autonomous driving.\nTherefore, we propose RoboFusion, a robust framework that leverages VFMs like\nSAM to tackle out-of-distribution (OOD) noise scenarios. We first adapt the\noriginal SAM for autonomous driving scenarios named SAM-AD. To align SAM or\nSAM-AD with multi-modal methods, we then introduce AD-FPN for upsampling the\nimage features extracted by SAM. We employ wavelet decomposition to denoise the\ndepth-guided images for further noise reduction and weather interference.\nLastly, we employ self-attention mechanisms to adaptively reweight the fused\nfeatures, enhancing informative features while suppressing excess noise. In\nsummary, our RoboFusion gradually reduces noise by leveraging the\ngeneralization and robustness of VFMs, thereby enhancing the resilience of\nmulti-modal 3D object detection. Consequently, our RoboFusion achieves\nstate-of-the-art performance in noisy scenarios, as demonstrated by the KITTI-C\nand nuScenes-C benchmarks.\n","authors":["Ziying Song","Guoxing Zhang","Lin Liu","Lei Yang","Shaoqing Xu","Caiyan Jia","Feiyang Jia","Li Wang"],"pdf_url":"https://arxiv.org/pdf/2401.03907v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11031v1","updated":"2024-04-17T03:13:58Z","published":"2024-04-17T03:13:58Z","title":"TaCOS: Task-Specific Camera Optimization with Simulation","summary":"  The performance of robots in their applications heavily depends on the\nquality of sensory input. However, designing sensor payloads and their\nparameters for specific robotic tasks is an expensive process that requires\nwell-established sensor knowledge and extensive experiments with physical\nhardware. With cameras playing a pivotal role in robotic perception, we\nintroduce a novel end-to-end optimization approach for co-designing a camera\nwith specific robotic tasks by combining derivative-free and gradient-based\noptimizers. The proposed method leverages recent computer graphics techniques\nand physical camera characteristics to prototype the camera in software,\nsimulate operational environments and tasks for robots, and optimize the camera\ndesign based on the desired tasks in a cost-effective way. We validate the\naccuracy of our camera simulation by comparing it with physical cameras, and\ndemonstrate the design of cameras with stronger performance than common\noff-the-shelf alternatives. Our approach supports the optimization of both\ncontinuous and discrete camera parameters, manufacturing constraints, and can\nbe generalized to a broad range of camera design scenarios including multiple\ncameras and unconventional cameras. This work advances the fully automated\ndesign of cameras for specific robotics tasks.\n","authors":["Chengyang Yan","Donald Dansereau"],"pdf_url":"https://arxiv.org/pdf/2404.11031v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.09276v3","updated":"2024-04-17T03:02:38Z","published":"2023-10-13T17:38:45Z","title":"Transformer-based Multimodal Change Detection with Multitask Consistency\n  Constraints","summary":"  Change detection plays a fundamental role in Earth observation for analyzing\ntemporal iterations over time. However, recent studies have largely neglected\nthe utilization of multimodal data that presents significant practical and\ntechnical advantages compared to single-modal approaches. This research focuses\non leveraging {pre-event} digital surface model (DSM) data and {post-event}\ndigital aerial images captured at different times for detecting change beyond\n2D. We observe that the current change detection methods struggle with the\nmultitask conflicts between semantic and height change detection tasks. To\naddress this challenge, we propose an efficient Transformer-based network that\nlearns shared representation between cross-dimensional inputs through\ncross-attention. {It adopts a consistency constraint to establish the\nmultimodal relationship. Initially, pseudo-changes are derived by employing\nheight change thresholding. Subsequently, the $L2$ distance between semantic\nand pseudo-changes within their overlapping regions is minimized. This\nexplicitly endows the height change detection (regression task) and semantic\nchange detection (classification task) with representation consistency.} A\nDSM-to-image multimodal dataset encompassing three cities in the Netherlands\nwas constructed. It lays a new foundation for beyond-2D change detection from\ncross-dimensional inputs. Compared to five state-of-the-art change detection\nmethods, our model demonstrates consistent multitask superiority in terms of\nsemantic and height change detection. Furthermore, the consistency strategy can\nbe seamlessly adapted to the other methods, yielding promising improvements.\n","authors":["Biyuan Liu","Huaixin Chen","Kun Li","Michael Ying Yang"],"pdf_url":"https://arxiv.org/pdf/2310.09276v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11025v1","updated":"2024-04-17T03:01:47Z","published":"2024-04-17T03:01:47Z","title":"Spatial-Aware Image Retrieval: A Hyperdimensional Computing Approach for\n  Efficient Similarity Hashing","summary":"  In the face of burgeoning image data, efficiently retrieving similar images\nposes a formidable challenge. Past research has focused on refining hash\nfunctions to distill images into compact indicators of resemblance. Initial\nattempts used shallow models, evolving to attention mechanism-based\narchitectures from Convolutional Neural Networks (CNNs) to advanced models.\nRecognizing limitations in gradient-based models for spatial information\nembedding, we propose an innovative image hashing method, NeuroHash leveraging\nHyperdimensional Computing (HDC). HDC symbolically encodes spatial information\ninto high-dimensional vectors, reshaping image representation. Our approach\ncombines pre-trained large vision models with HDC operations, enabling\nspatially encoded feature representations. Hashing with locality-sensitive\nhashing (LSH) ensures swift and efficient image retrieval. Notably, our\nframework allows dynamic hash manipulation for conditional image retrieval. Our\nwork introduces a transformative image hashing framework enabling spatial-aware\nconditional retrieval. By seamlessly combining DNN-based neural and HDC-based\nsymbolic models, our methodology breaks from traditional training, offering\nflexible and conditional image retrieval. Performance evaluations signify a\nparadigm shift in image-hashing methodologies, demonstrating enhanced retrieval\naccuracy.\n","authors":["Sanggeon Yun","Ryozo Masukawa","SungHeon Jeong","Mohsen Imani"],"pdf_url":"https://arxiv.org/pdf/2404.11025v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16627v2","updated":"2024-04-17T02:57:58Z","published":"2024-03-25T11:16:23Z","title":"SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions","summary":"  Recent advancements in diffusion models have positioned them at the forefront\nof image generation. Despite their superior performance, diffusion models are\nnot without drawbacks; they are characterized by complex architectures and\nsubstantial computational demands, resulting in significant latency due to\ntheir iterative sampling process. To mitigate these limitations, we introduce a\ndual approach involving model miniaturization and a reduction in sampling\nsteps, aimed at significantly decreasing model latency. Our methodology\nleverages knowledge distillation to streamline the U-Net and image decoder\narchitectures, and introduces an innovative one-step DM training technique that\nutilizes feature matching and score distillation. We present two models,\nSDXS-512 and SDXS-1024, achieving inference speeds of approximately 100 FPS\n(30x faster than SD v1.5) and 30 FPS (60x faster than SDXL) on a single GPU,\nrespectively. Moreover, our training approach offers promising applications in\nimage-conditioned control, facilitating efficient image-to-image translation.\n","authors":["Yuda Song","Zehao Sun","Xuanwu Yin"],"pdf_url":"https://arxiv.org/pdf/2403.16627v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10357v2","updated":"2024-04-17T02:48:49Z","published":"2024-04-16T07:44:52Z","title":"Optimization of Prompt Learning via Multi-Knowledge Representation for\n  Vision-Language Models","summary":"  Vision-Language Models (VLMs), such as CLIP, play a foundational role in\nvarious cross-modal applications. To fully leverage VLMs' potential in adapting\nto downstream tasks, context optimization methods like Prompt Tuning are\nessential. However, one key limitation is the lack of diversity in prompt\ntemplates, whether they are hand-crafted or learned through additional modules.\nThis limitation restricts the capabilities of pretrained VLMs and can result in\nincorrect predictions in downstream tasks. To address this challenge, we\npropose Context Optimization with Multi-Knowledge Representation (CoKnow), a\nframework that enhances Prompt Learning for VLMs with rich contextual\nknowledge. To facilitate CoKnow during inference, we trained lightweight\nsemantic knowledge mappers, which are capable of generating Multi-Knowledge\nRepresentation for an input image without requiring additional priors.\nExperimentally, We conducted extensive experiments on 11 publicly available\ndatasets, demonstrating that CoKnow outperforms a series of previous methods.\nWe will make all resources open-source: https://github.com/EMZucas/CoKnow.\n","authors":["Enming Zhang","Bingke Zhu","Yingying Chen","Qinghai Miao","Ming Tang","Jinqiao Wang"],"pdf_url":"https://arxiv.org/pdf/2404.10357v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11016v1","updated":"2024-04-17T02:47:39Z","published":"2024-04-17T02:47:39Z","title":"MaeFuse: Transferring Omni Features with Pretrained Masked Autoencoders\n  for Infrared and Visible Image Fusion via Guided Training","summary":"  In this research, we introduce MaeFuse, a novel autoencoder model designed\nfor infrared and visible image fusion (IVIF). The existing approaches for image\nfusion often rely on training combined with downstream tasks to obtain\nhigh-level visual information, which is effective in emphasizing target objects\nand delivering impressive results in visual quality and task-specific\napplications. MaeFuse, however, deviates from the norm. Instead of being driven\nby downstream tasks, our model utilizes a pretrained encoder from Masked\nAutoencoders (MAE), which facilities the omni features extraction for low-level\nreconstruction and high-level vision tasks, to obtain perception friendly\nfeatures with a low cost. In order to eliminate the domain gap of different\nmodal features and the block effect caused by the MAE encoder, we further\ndevelop a guided training strategy. This strategy is meticulously crafted to\nensure that the fusion layer seamlessly adjusts to the feature space of the\nencoder, gradually enhancing the fusion effect. It facilitates the\ncomprehensive integration of feature vectors from both infrared and visible\nmodalities, preserving the rich details inherent in each. MaeFuse not only\nintroduces a novel perspective in the realm of fusion techniques but also\nstands out with impressive performance across various public datasets.\n","authors":["Jiayang Li","Junjun Jiang","Pengwei Liang","Jiayi Ma"],"pdf_url":"https://arxiv.org/pdf/2404.11016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10971v2","updated":"2024-04-17T02:39:19Z","published":"2022-07-22T09:37:48Z","title":"Kinematics Modeling Network for Video-based Human Pose Estimation","summary":"  Estimating human poses from videos is critical in human-computer interaction.\nJoints cooperate rather than move independently during human movement. There\nare both spatial and temporal correlations between joints. Despite the positive\nresults of previous approaches, most focus on modeling the spatial correlation\nbetween joints while only straightforwardly integrating features along the\ntemporal dimension, ignoring the temporal correlation between joints. In this\nwork, we propose a plug-and-play kinematics modeling module (KMM) to explicitly\nmodel temporal correlations between joints across different frames by\ncalculating their temporal similarity. In this way, KMM can capture motion cues\nof the current joint relative to all joints in different time. Besides, we\nformulate video-based human pose estimation as a Markov Decision Process and\ndesign a novel kinematics modeling network (KIMNet) to simulate the Markov\nChain, allowing KIMNet to locate joints recursively. Our approach achieves\nstate-of-the-art results on two challenging benchmarks. In particular, KIMNet\nshows robustness to the occlusion. The code will be released at\nhttps://github.com/YHDang/KIMNet.\n","authors":["Yonghao Dang","Jianqin Yin","Shaojie Zhang","Jiping Liu","Yanzhu Hu"],"pdf_url":"https://arxiv.org/pdf/2207.10971v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11008v1","updated":"2024-04-17T02:36:02Z","published":"2024-04-17T02:36:02Z","title":"AKGNet: Attribute Knowledge-Guided Unsupervised Lung-Infected Area\n  Segmentation","summary":"  Lung-infected area segmentation is crucial for assessing the severity of lung\ndiseases. However, existing image-text multi-modal methods typically rely on\nlabour-intensive annotations for model training, posing challenges regarding\ntime and expertise. To address this issue, we propose a novel attribute\nknowledge-guided framework for unsupervised lung-infected area segmentation\n(AKGNet), which achieves segmentation solely based on image-text data without\nany mask annotation. AKGNet facilitates text attribute knowledge learning,\nattribute-image cross-attention fusion, and high-confidence-based pseudo-label\nexploration simultaneously. It can learn statistical information and capture\nspatial correlations between image and text attributes in the embedding space,\niteratively refining the mask to enhance segmentation. Specifically, we\nintroduce a text attribute knowledge learning module by extracting attribute\nknowledge and incorporating it into feature representations, enabling the model\nto learn statistical information and adapt to different attributes. Moreover,\nwe devise an attribute-image cross-attention module by calculating the\ncorrelation between attributes and images in the embedding space to capture\nspatial dependency information, thus selectively focusing on relevant regions\nwhile filtering irrelevant areas. Finally, a self-training mask improvement\nprocess is employed by generating pseudo-labels using high-confidence\npredictions to iteratively enhance the mask and segmentation. Experimental\nresults on a benchmark medical image dataset demonstrate the superior\nperformance of our method compared to state-of-the-art segmentation techniques\nin unsupervised scenarios.\n","authors":["Qing En","Yuhong Guo"],"pdf_url":"https://arxiv.org/pdf/2404.11008v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11003v1","updated":"2024-04-17T02:29:44Z","published":"2024-04-17T02:29:44Z","title":"InfoMatch: Entropy Neural Estimation for Semi-Supervised Image\n  Classification","summary":"  Semi-supervised image classification, leveraging pseudo supervision and\nconsistency regularization, has demonstrated remarkable success. However, the\nongoing challenge lies in fully exploiting the potential of unlabeled data. To\naddress this, we employ information entropy neural estimation to harness the\npotential of unlabeled samples. Inspired by contrastive learning, the entropy\nis estimated by maximizing a lower bound on mutual information across different\naugmented views. Moreover, we theoretically analyze that the information\nentropy of the posterior of an image classifier is approximated by maximizing\nthe likelihood function of the softmax predictions. Guided by these insights,\nwe optimize our model from both perspectives to ensure that the predicted\nprobability distribution closely aligns with the ground-truth distribution.\nGiven the theoretical connection to information entropy, we name our method\n\\textit{InfoMatch}. Through extensive experiments, we show its superior\nperformance.\n","authors":["Qi Han","Zhibo Tian","Chengwei Xia","Kun Zhan"],"pdf_url":"https://arxiv.org/pdf/2404.11003v1.pdf","comment":"IJCAI 2024"},{"id":"http://arxiv.org/abs/2308.13072v3","updated":"2024-04-17T02:09:54Z","published":"2023-08-24T20:29:09Z","title":"Full-dose Whole-body PET Synthesis from Low-dose PET Using\n  High-efficiency Denoising Diffusion Probabilistic Model: PET Consistency\n  Model","summary":"  Objective: Positron Emission Tomography (PET) has been a commonly used\nimaging modality in broad clinical applications. One of the most important\ntradeoffs in PET imaging is between image quality and radiation dose: high\nimage quality comes with high radiation exposure. Improving image quality is\ndesirable for all clinical applications while minimizing radiation exposure is\nneeded to reduce risk to patients. Approach: We introduce PET Consistency Model\n(PET-CM), an efficient diffusion-based method for generating high-quality\nfull-dose PET images from low-dose PET images. It employs a two-step process,\nadding Gaussian noise to full-dose PET images in the forward diffusion, and\nthen denoising them using a PET Shifted-window Vision Transformer (PET-VIT)\nnetwork in the reverse diffusion. The PET-VIT network learns a consistency\nfunction that enables direct denoising of Gaussian noise into clean full-dose\nPET images. PET-CM achieves state-of-the-art image quality while requiring\nsignificantly less computation time than other methods. Results: In experiments\ncomparing eighth-dose to full-dose images, PET-CM demonstrated impressive\nperformance with NMAE of 1.278+/-0.122%, PSNR of 33.783+/-0.824dB, SSIM of\n0.964+/-0.009, NCC of 0.968+/-0.011, HRS of 4.543, and SUV Error of\n0.255+/-0.318%, with an average generation time of 62 seconds per patient. This\nis a significant improvement compared to the state-of-the-art diffusion-based\nmodel with PET-CM reaching this result 12x faster. Similarly, in the\nquarter-dose to full-dose image experiments, PET-CM delivered competitive\noutcomes, achieving an NMAE of 0.973+/-0.066%, PSNR of 36.172+/-0.801dB, SSIM\nof 0.984+/-0.004, NCC of 0.990+/-0.005, HRS of 4.428, and SUV Error of\n0.151+/-0.192% using the same generation process, which underlining its high\nquantitative and clinical precision in both denoising scenario.\n","authors":["Shaoyan Pan","Elham Abouei","Junbo Peng","Joshua Qian","Jacob F Wynne","Tonghe Wang","Chih-Wei Chang","Justin Roper","Jonathon A Nye","Hui Mao","Xiaofeng Yang"],"pdf_url":"https://arxiv.org/pdf/2308.13072v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10992v1","updated":"2024-04-17T02:05:05Z","published":"2024-04-17T02:05:05Z","title":"How to deal with glare for improved perception of Autonomous Vehicles","summary":"  Vision sensors are versatile and can capture a wide range of visual cues,\nsuch as color, texture, shape, and depth. This versatility, along with the\nrelatively inexpensive availability of machine vision cameras, played an\nimportant role in adopting vision-based environment perception systems in\nautonomous vehicles (AVs). However, vision-based perception systems can be\neasily affected by glare in the presence of a bright source of light, such as\nthe sun or the headlights of the oncoming vehicle at night or simply by light\nreflecting off snow or ice-covered surfaces; scenarios encountered frequently\nduring driving. In this paper, we investigate various glare reduction\ntechniques, including the proposed saturated pixel-aware glare reduction\ntechnique for improved performance of the computer vision (CV) tasks employed\nby the perception layer of AVs. We evaluate these glare reduction methods based\non various performance metrics of the CV algorithms used by the perception\nlayer. Specifically, we considered object detection, object recognition, object\ntracking, depth estimation, and lane detection which are crucial for autonomous\ndriving. The experimental findings validate the efficacy of the proposed glare\nreduction approach, showcasing enhanced performance across diverse perception\ntasks and remarkable resilience against varying levels of glare.\n","authors":["Muhammad Z. Alam","Zeeshan Kaleem","Sousso Kelouwani"],"pdf_url":"https://arxiv.org/pdf/2404.10992v1.pdf","comment":"14 pages, 9 figures, Accepted IEEE TIV"},{"id":"http://arxiv.org/abs/2404.10096v2","updated":"2024-04-17T02:02:33Z","published":"2024-04-15T19:06:58Z","title":"Vision Augmentation Prediction Autoencoder with Attention Design\n  (VAPAAD)","summary":"  Recent advancements in sequence prediction have significantly improved the\naccuracy of video data interpretation; however, existing models often overlook\nthe potential of attention-based mechanisms for next-frame prediction. This\nstudy introduces the Vision Augmentation Prediction Autoencoder with Attention\nDesign (VAPAAD), an innovative approach that integrates attention mechanisms\ninto sequence prediction, enabling nuanced analysis and understanding of\ntemporal dynamics in video sequences. Utilizing the Moving MNIST dataset, we\ndemonstrate VAPAAD's robust performance and superior handling of complex\ntemporal data compared to traditional methods. VAPAAD combines data\naugmentation, ConvLSTM2D layers, and a custom-built self-attention mechanism to\neffectively focus on salient features within a sequence, enhancing predictive\naccuracy and context-aware analysis. This methodology not only adheres to human\ncognitive processes during video interpretation but also addresses limitations\nin conventional models, which often struggle with the variability inherent in\nvideo sequences. The experimental results confirm that VAPAAD outperforms\nexisting models, especially in integrating attention mechanisms, which\nsignificantly improve predictive performance.\n","authors":["Yiqiao Yin"],"pdf_url":"https://arxiv.org/pdf/2404.10096v2.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2404.10989v1","updated":"2024-04-17T01:53:03Z","published":"2024-04-17T01:53:03Z","title":"FairSSD: Understanding Bias in Synthetic Speech Detectors","summary":"  Methods that can generate synthetic speech which is perceptually\nindistinguishable from speech recorded by a human speaker, are easily\navailable. Several incidents report misuse of synthetic speech generated from\nthese methods to commit fraud. To counter such misuse, many methods have been\nproposed to detect synthetic speech. Some of these detectors are more\ninterpretable, can generalize to detect synthetic speech in the wild and are\nrobust to noise. However, limited work has been done on understanding bias in\nthese detectors. In this work, we examine bias in existing synthetic speech\ndetectors to determine if they will unfairly target a particular gender, age\nand accent group. We also inspect whether these detectors will have a higher\nmisclassification rate for bona fide speech from speech-impaired speakers w.r.t\nfluent speakers. Extensive experiments on 6 existing synthetic speech detectors\nusing more than 0.9 million speech signals demonstrate that most detectors are\ngender, age and accent biased, and future work is needed to ensure fairness. To\nsupport future research, we release our evaluation dataset, models used in our\nstudy and source code at https://gitlab.com/viper-purdue/fairssd.\n","authors":["Amit Kumar Singh Yadav","Kratika Bhagtani","Davide Salvi","Paolo Bestagini","Edward J. Delp"],"pdf_url":"https://arxiv.org/pdf/2404.10989v1.pdf","comment":"Accepted at CVPR 2024 (WMF)"},{"id":"http://arxiv.org/abs/2404.02155v2","updated":"2024-04-17T01:41:59Z","published":"2024-04-02T17:58:57Z","title":"Alpha Invariance: On Inverse Scaling Between Distance and Volume Density\n  in Neural Radiance Fields","summary":"  Scale-ambiguity in 3D scene dimensions leads to magnitude-ambiguity of\nvolumetric densities in neural radiance fields, i.e., the densities double when\nscene size is halved, and vice versa. We call this property alpha invariance.\nFor NeRFs to better maintain alpha invariance, we recommend 1) parameterizing\nboth distance and volume densities in log space, and 2) a\ndiscretization-agnostic initialization strategy to guarantee high ray\ntransmittance. We revisit a few popular radiance field models and find that\nthese systems use various heuristics to deal with issues arising from scene\nscaling. We test their behaviors and show our recipe to be more robust.\n","authors":["Joshua Ahn","Haochen Wang","Raymond A. Yeh","Greg Shakhnarovich"],"pdf_url":"https://arxiv.org/pdf/2404.02155v2.pdf","comment":"CVPR 2024. project page https://pals.ttic.edu/p/alpha-invariance"},{"id":"http://arxiv.org/abs/2404.10985v1","updated":"2024-04-17T01:35:52Z","published":"2024-04-17T01:35:52Z","title":"Pixel-Wise Symbol Spotting via Progressive Points Location for Parsing\n  CAD Images","summary":"  Parsing Computer-Aided Design (CAD) drawings is a fundamental step for CAD\nrevision, semantic-based management, and the generation of 3D prototypes in\nboth the architecture and engineering industries. Labeling symbols from a CAD\ndrawing is a challenging yet notorious task from a practical point of view. In\nthis work, we propose to label and spot symbols from CAD images that are\nconverted from CAD drawings. The advantage of spotting symbols from CAD images\nlies in the low requirement of labelers and the low-cost annotation. However,\npixel-wise spotting symbols from CAD images is challenging work. We propose a\npixel-wise point location via Progressive Gaussian Kernels (PGK) to balance\nbetween training efficiency and location accuracy. Besides, we introduce a\nlocal offset to the heatmap-based point location method. Based on the keypoints\ndetection, we propose a symbol grouping method to redraw the rectangle symbols\nin CAD images. We have released a dataset containing CAD images of equipment\nrooms from telecommunication industrial CAD drawings. Extensive experiments on\nthis real-world dataset show that the proposed method has good generalization\nability.\n","authors":["Junbiao Pang","Zailin Dong","Jiaxin Deng","Mengyuan Zhu","Yunwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.10985v1.pdf","comment":"10 pages, 10 figures,6 tables"},{"id":"http://arxiv.org/abs/2404.10980v1","updated":"2024-04-17T01:26:15Z","published":"2024-04-17T01:26:15Z","title":"Hyper Evidential Deep Learning to Quantify Composite Classification\n  Uncertainty","summary":"  Deep neural networks (DNNs) have been shown to perform well on exclusive,\nmulti-class classification tasks. However, when different classes have similar\nvisual features, it becomes challenging for human annotators to differentiate\nthem. This scenario necessitates the use of composite class labels. In this\npaper, we propose a novel framework called Hyper-Evidential Neural Network\n(HENN) that explicitly models predictive uncertainty due to composite class\nlabels in training data in the context of the belief theory called Subjective\nLogic (SL). By placing a grouped Dirichlet distribution on the class\nprobabilities, we treat predictions of a neural network as parameters of\nhyper-subjective opinions and learn the network that collects both single and\ncomposite evidence leading to these hyper-opinions by a deterministic DNN from\ndata. We introduce a new uncertainty type called vagueness originally designed\nfor hyper-opinions in SL to quantify composite classification uncertainty for\nDNNs. Our results demonstrate that HENN outperforms its state-of-the-art\ncounterparts based on four image datasets. The code and datasets are available\nat: https://github.com/Hugo101/HyperEvidentialNN.\n","authors":["Changbin Li","Kangshuo Li","Yuzhe Ou","Lance M. Kaplan","Audun J√∏sang","Jin-Hee Cho","Dong Hyun Jeong","Feng Chen"],"pdf_url":"https://arxiv.org/pdf/2404.10980v1.pdf","comment":"In Proceedings of The Twelfth International Conference on Learning\n  Representations, ICLR 2024"},{"id":"http://arxiv.org/abs/2404.10978v1","updated":"2024-04-17T01:23:49Z","published":"2024-04-17T01:23:49Z","title":"Leveraging 3D LiDAR Sensors to Enable Enhanced Urban Safety and Public\n  Health: Pedestrian Monitoring and Abnormal Activity Detection","summary":"  The integration of Light Detection and Ranging (LiDAR) and Internet of Things\n(IoT) technologies offers transformative opportunities for public health\ninformatics in urban safety and pedestrian well-being. This paper proposes a\nnovel framework utilizing these technologies for enhanced 3D object detection\nand activity classification in urban traffic scenarios. By employing elevated\nLiDAR, we obtain detailed 3D point cloud data, enabling precise pedestrian\nactivity monitoring. To overcome urban data scarcity, we create a specialized\ndataset through simulated traffic environments in Blender, facilitating\ntargeted model training. Our approach employs a modified Point\nVoxel-Region-based Convolutional Neural Network (PV-RCNN) for robust 3D\ndetection and PointNet for classifying pedestrian activities, significantly\nbenefiting urban traffic management and public health by offering insights into\npedestrian behavior and promoting safer urban environments. Our dual-model\napproach not only enhances urban traffic management but also contributes\nsignificantly to public health by providing insights into pedestrian behavior\nand promoting safer urban environment.\n","authors":["Nawfal Guefrachi","Jian Shi","Hakim Ghazzai","Ahmad Alsharoa"],"pdf_url":"https://arxiv.org/pdf/2404.10978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02145v2","updated":"2024-04-17T01:10:28Z","published":"2024-04-02T17:57:31Z","title":"Iterated Learning Improves Compositionality in Large Vision-Language\n  Models","summary":"  A fundamental characteristic common to both human vision and natural language\nis their compositional nature. Yet, despite the performance gains contributed\nby large vision and language pretraining, recent investigations find that\nmost-if not all-our state-of-the-art vision-language models struggle at\ncompositionality. They are unable to distinguish between images of \" a girl in\nwhite facing a man in black\" and \"a girl in black facing a man in white\".\nMoreover, prior work suggests that compositionality doesn't arise with scale:\nlarger model sizes or training data don't help. This paper develops a new\niterated training algorithm that incentivizes compositionality. We draw on\ndecades of cognitive science research that identifies cultural transmission-the\nneed to teach a new generation-as a necessary inductive prior that incentivizes\nhumans to develop compositional languages. Specifically, we reframe\nvision-language contrastive learning as the Lewis Signaling Game between a\nvision agent and a language agent, and operationalize cultural transmission by\niteratively resetting one of the agent's weights during training. After every\niteration, this training paradigm induces representations that become \"easier\nto learn\", a property of compositional languages: e.g. our model trained on\nCC3M and CC12M improves standard CLIP by 4.7%, 4.0% respectfully in the\nSugarCrepe benchmark.\n","authors":["Chenhao Zheng","Jieyu Zhang","Aniruddha Kembhavi","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2404.02145v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2404.10383v2","updated":"2024-04-17T01:05:07Z","published":"2024-04-16T08:25:36Z","title":"Learning to Score Sign Language with Two-stage Method","summary":"  Human action recognition and performance assessment have been hot research\ntopics in recent years. Recognition problems have mature solutions in the field\nof sign language, but past research in performance analysis has focused on\ncompetitive sports and medical training, overlooking the scoring assessment\n,which is an important part of sign language teaching digitalization. In this\npaper, we analyze the existing technologies for performance assessment and\nadopt methods that perform well in human pose reconstruction tasks combined\nwith motion rotation embedded expressions, proposing a two-stage sign language\nperformance evaluation pipeline. Our analysis shows that choosing\nreconstruction tasks in the first stage can provide more expressive features,\nand using smoothing methods can provide an effective reference for assessment.\nExperiments show that our method provides good score feedback mechanisms and\nhigh consistency with professional assessments compared to end-to-end\nevaluations.\n","authors":["Hongli Wen","Yang Xu"],"pdf_url":"https://arxiv.org/pdf/2404.10383v2.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2310.11700v2","updated":"2024-04-17T01:04:07Z","published":"2023-10-18T04:15:39Z","title":"Runner re-identification from single-view running video in the\n  open-world setting","summary":"  In many sports, player re-identification is crucial for automatic video\nprocessing and analysis. However, most of the current studies on player\nre-identification in multi- or single-view sports videos focus on\nre-identification in the closed-world setting using labeled image dataset, and\nplayer re-identification in the open-world setting for automatic video analysis\nis not well developed. In this paper, we propose a runner re-identification\nsystem that directly processes single-view video to address the open-world\nsetting. In the open-world setting, we cannot use labeled dataset and have to\nprocess video directly. The proposed system automatically processes raw video\nas input to identify runners, and it can identify runners even when they are\nframed out multiple times. For the automatic processing, we first detect the\nrunners in the video using the pre-trained YOLOv8 and the fine-tuned\nEfficientNet. We then track the runners using ByteTrack and detect their shoes\nwith the fine-tuned YOLOv8. Finally, we extract the image features of the\nrunners using an unsupervised method with the gated recurrent unit autoencoder\nand global and local features mixing. To improve the accuracy of runner\nre-identification, we use shoe images as local image features and dynamic\nfeatures of running sequence images. We evaluated the system on a running\npractice video dataset and showed that the proposed method identified runners\nwith higher accuracy than some state-of-the-art models in unsupervised\nre-identification. We also showed that our proposed local image feature and\nrunning dynamic feature were effective for runner re-identification. Our runner\nre-identification system can be useful for the automatic analysis of running\nvideos.\n","authors":["Tomohiro Suzuki","Kazushi Tsutsui","Kazuya Takeda","Keisuke Fujii"],"pdf_url":"https://arxiv.org/pdf/2310.11700v2.pdf","comment":"20 pages, 7 figures"},{"id":"http://arxiv.org/abs/2402.03557v2","updated":"2024-04-17T01:01:17Z","published":"2024-02-05T22:15:55Z","title":"Robust Analysis of Multi-Task Learning Efficiency: New Benchmarks on\n  Light-Weighed Backbones and Effective Measurement of Multi-Task Learning\n  Challenges by Feature Disentanglement","summary":"  One of the main motivations of MTL is to develop neural networks capable of\ninferring multiple tasks simultaneously. While countless methods have been\nproposed in the past decade investigating robust model architectures and\nefficient training algorithms, there is still lack of understanding of these\nmethods when applied on smaller feature extraction backbones, the\ngeneralizability of the commonly used fast approximation technique of replacing\nparameter-level gradients with feature level gradients, and lack of\ncomprehensive understanding of MTL challenges and how one can efficiently and\neffectively identify the challenges. In this paper, we focus on the\naforementioned efficiency aspects of existing MTL methods. We first carry out\nlarge-scale experiments of the methods with smaller backbones and on a the\nMetaGraspNet dataset as a new test ground. We also compare the existing methods\nwith and without using the fast gradient surrogate and empirically study the\ngeneralizability of this technique. Lastly, we propose Feature Disentanglement\nmeasure as a novel and efficient identifier of the challenges in MTL, and\npropose Ranking Similarity score as an evaluation metric for different\nidentifiers to prove the faithfulness of our method.\n","authors":["Dayou Mao","Yuhao Chen","Yifan Wu","Maximilian Gilles","Alexander Wong"],"pdf_url":"https://arxiv.org/pdf/2402.03557v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10966v1","updated":"2024-04-17T00:21:36Z","published":"2024-04-17T00:21:36Z","title":"Domain-Specific Block Selection and Paired-View Pseudo-Labeling for\n  Online Test-Time Adaptation","summary":"  Test-time adaptation (TTA) aims to adapt a pre-trained model to a new test\ndomain without access to source data after deployment. Existing approaches\ntypically rely on self-training with pseudo-labels since ground-truth cannot be\nobtained from test data. Although the quality of pseudo labels is important for\nstable and accurate long-term adaptation, it has not been previously addressed.\nIn this work, we propose DPLOT, a simple yet effective TTA framework that\nconsists of two components: (1) domain-specific block selection and (2)\npseudo-label generation using paired-view images. Specifically, we select\nblocks that involve domain-specific feature extraction and train these blocks\nby entropy minimization. After blocks are adjusted for current test domain, we\ngenerate pseudo-labels by averaging given test images and corresponding flipped\ncounterparts. By simply using flip augmentation, we prevent a decrease in the\nquality of the pseudo-labels, which can be caused by the domain gap resulting\nfrom strong augmentation. Our experimental results demonstrate that DPLOT\noutperforms previous TTA methods in CIFAR10-C, CIFAR100-C, and ImageNet-C\nbenchmarks, reducing error by up to 5.4%, 9.1%, and 2.9%, respectively. Also,\nwe provide an extensive analysis to demonstrate effectiveness of our framework.\nCode is available at\nhttps://github.com/gist-ailab/domain-specific-block-selection-and-paired-view-pseudo-labeling-for-online-TTA.\n","authors":["Yeonguk Yu","Sungho Shin","Seunghyeok Back","Minhwan Ko","Sangjun Noh","Kyoobin Lee"],"pdf_url":"https://arxiv.org/pdf/2404.10966v1.pdf","comment":"Accepted at CVPR 2024"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2401.12798v3","updated":"2024-04-17T17:00:34Z","published":"2024-01-23T14:31:12Z","title":"Gradient Flow of Energy: A General and Efficient Approach for Entity\n  Alignment Decoding","summary":"  Entity alignment (EA), a pivotal process in integrating multi-source\nKnowledge Graphs (KGs), seeks to identify equivalent entity pairs across these\ngraphs. Most existing approaches regard EA as a graph representation learning\ntask, concentrating on enhancing graph encoders. However, the decoding process\nin EA - essential for effective operation and alignment accuracy - has received\nlimited attention and remains tailored to specific datasets and model\narchitectures, necessitating both entity and additional explicit relation\nembeddings. This specificity limits its applicability, particularly in\nGNN-based models. To address this gap, we introduce a novel, generalized, and\nefficient decoding approach for EA, relying solely on entity embeddings. Our\nmethod optimizes the decoding process by minimizing Dirichlet energy, leading\nto the gradient flow within the graph, to maximize graph homophily. The\ndiscretization of the gradient flow produces a fast and scalable approach,\ntermed Triple Feature Propagation (TFP). TFP innovatively generalizes adjacency\nmatrices to multi-views matrices:entity-to-entity, entity-to-relation,\nrelation-to-entity, and relation-to-triple. The gradient flow through\ngeneralized matrices enables TFP to harness the multi-view structural\ninformation of KGs. Rigorous experimentation on diverse public datasets\ndemonstrates that our approach significantly enhances various EA methods.\nNotably, the approach achieves these advancements with less than 6 seconds of\nadditional computational time, establishing a new benchmark in efficiency and\nadaptability for future EA methods.\n","authors":["Yuanyi Wang","Haifeng Sun","Jingyu Wang","Qi Qi","Shaoling Sun","Jianxin Liao"],"pdf_url":"https://arxiv.org/pdf/2401.12798v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11519v1","updated":"2024-04-17T16:10:55Z","published":"2024-04-17T16:10:55Z","title":"Disentangled Cascaded Graph Convolution Networks for Multi-Behavior\n  Recommendation","summary":"  Multi-behavioral recommender systems have emerged as a solution to address\ndata sparsity and cold-start issues by incorporating auxiliary behaviors\nalongside target behaviors. However, existing models struggle to accurately\ncapture varying user preferences across different behaviors and fail to account\nfor diverse item preferences within behaviors. Various user preference factors\n(such as price or quality) entangled in the behavior may lead to\nsub-optimization problems. Furthermore, these models overlook the personalized\nnature of user behavioral preferences by employing uniform transformation\nnetworks for all users and items. To tackle these challenges, we propose the\nDisentangled Cascaded Graph Convolutional Network (Disen-CGCN), a novel\nmulti-behavior recommendation model. Disen-CGCN employs disentangled\nrepresentation techniques to effectively separate factors within user and item\nrepresentations, ensuring their independence. In addition, it incorporates a\nmulti-behavioral meta-network, enabling personalized feature transformation\nacross user and item behaviors. Furthermore, an attention mechanism captures\nuser preferences for different item factors within each behavior. By leveraging\nattention weights, we aggregate user and item embeddings separately for each\nbehavior, computing preference scores that predict overall user preferences for\nitems. Our evaluation on benchmark datasets demonstrates the superiority of\nDisen-CGCN over state-of-the-art models, showcasing an average performance\nimprovement of 7.07% and 9.00% on respective datasets. These results highlight\nDisen-CGCN's ability to effectively leverage multi-behavioral data, leading to\nmore accurate recommendations.\n","authors":["Zhiyong Cheng","Jianhua Dong","Fan Liu","Lei Zhu","Xun Yang","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2404.11519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06567v3","updated":"2024-04-17T15:58:36Z","published":"2024-03-11T10:06:45Z","title":"Leveraging Foundation Models for Content-Based Medical Image Retrieval\n  in Radiology","summary":"  Content-based image retrieval (CBIR) has the potential to significantly\nimprove diagnostic aid and medical research in radiology. Current CBIR systems\nface limitations due to their specialization to certain pathologies, limiting\ntheir utility. In response, we propose using vision foundation models as\npowerful and versatile off-the-shelf feature extractors for content-based\nmedical image retrieval. By benchmarking these models on a comprehensive\ndataset of 1.6 million 2D radiological images spanning four modalities and 161\npathologies, we identify weakly-supervised models as superior, achieving a P@1\nof up to 0.594. This performance not only competes with a specialized model but\ndoes so without the need for fine-tuning. Our analysis further explores the\nchallenges in retrieving pathological versus anatomical structures, indicating\nthat accurate retrieval of pathological features presents greater difficulty.\nDespite these challenges, our research underscores the vast potential of\nfoundation models for CBIR in radiology, proposing a shift towards versatile,\ngeneral-purpose medical image retrieval systems that do not require specific\ntuning.\n","authors":["Stefan Denner","David Zimmerer","Dimitrios Bounias","Markus Bujotzek","Shuhan Xiao","Lisa Kausch","Philipp Schader","Tobias Penzkofer","Paul F. J√§ger","Klaus Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2403.06567v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07219v2","updated":"2024-04-17T15:10:32Z","published":"2024-03-22T12:27:21Z","title":"Leave No One Behind: Online Self-Supervised Self-Distillation for\n  Sequential Recommendation","summary":"  Sequential recommendation methods play a pivotal role in modern\nrecommendation systems. A key challenge lies in accurately modeling user\npreferences in the face of data sparsity. To tackle this challenge, recent\nmethods leverage contrastive learning (CL) to derive self-supervision signals\nby maximizing the mutual information of two augmented views of the original\nuser behavior sequence. Despite their effectiveness, CL-based methods encounter\na limitation in fully exploiting self-supervision signals for users with\nlimited behavior data, as users with extensive behaviors naturally offer more\ninformation. To address this problem, we introduce a novel learning paradigm,\nnamed Online Self-Supervised Self-distillation for Sequential Recommendation\n($S^4$Rec), effectively bridging the gap between self-supervised learning and\nself-distillation methods. Specifically, we employ online clustering to\nproficiently group users by their distinct latent intents. Additionally, an\nadversarial learning strategy is utilized to ensure that the clustering\nprocedure is not affected by the behavior length factor. Subsequently, we\nemploy self-distillation to facilitate the transfer of knowledge from users\nwith extensive behaviors (teachers) to users with limited behaviors (students).\nExperiments conducted on four real-world datasets validate the effectiveness of\nthe proposed method.\n","authors":["Shaowei Wei","Zhengwei Wu","Xin Li","Qintong Wu","Zhiqiang Zhang","Jun Zhou","Lihong Gu","Jinjie Gu"],"pdf_url":"https://arxiv.org/pdf/2404.07219v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11457v1","updated":"2024-04-17T15:05:03Z","published":"2024-04-17T15:05:03Z","title":"Unifying Bias and Unfairness in Information Retrieval: A Survey of\n  Challenges and Opportunities with Large Language Models","summary":"  With the rapid advancement of large language models (LLMs), information\nretrieval (IR) systems, such as search engines and recommender systems, have\nundergone a significant paradigm shift. This evolution, while heralding new\nopportunities, introduces emerging challenges, particularly in terms of biases\nand unfairness, which may threaten the information ecosystem. In this paper, we\npresent a comprehensive survey of existing works on emerging and pressing bias\nand unfairness issues in IR systems when the integration of LLMs. We first\nunify bias and unfairness issues as distribution mismatch problems, providing a\ngroundwork for categorizing various mitigation strategies through distribution\nalignment. Subsequently, we systematically delve into the specific bias and\nunfairness issues arising from three critical stages of LLMs integration into\nIR systems: data collection, model development, and result evaluation. In doing\nso, we meticulously review and analyze recent literature, focusing on the\ndefinitions, characteristics, and corresponding mitigation strategies\nassociated with these issues. Finally, we identify and highlight some open\nproblems and challenges for future work, aiming to inspire researchers and\nstakeholders in the IR field and beyond to better understand and mitigate bias\nand unfairness issues of IR in this LLM era. We also consistently maintain a\nGitHub repository for the relevant papers and resources in this rising\ndirection at https://github.com/KID-22/LLM-IR-Bias-Fairness-Survey.\n","authors":["Sunhao Dai","Chen Xu","Shicheng Xu","Liang Pang","Zhenhua Dong","Jun Xu"],"pdf_url":"https://arxiv.org/pdf/2404.11457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11456v1","updated":"2024-04-17T15:05:00Z","published":"2024-04-17T15:05:00Z","title":"Deep Pattern Network for Click-Through Rate Prediction","summary":"  Click-through rate (CTR) prediction tasks play a pivotal role in real-world\napplications, particularly in recommendation systems and online advertising. A\nsignificant research branch in this domain focuses on user behavior modeling.\nCurrent research predominantly centers on modeling co-occurrence relationships\nbetween the target item and items previously interacted with by users in their\nhistorical data. However, this focus neglects the intricate modeling of user\nbehavior patterns. In reality, the abundance of user interaction records\nencompasses diverse behavior patterns, indicative of a spectrum of habitual\nparadigms. These patterns harbor substantial potential to significantly enhance\nCTR prediction performance. To harness the informational potential within user\nbehavior patterns, we extend Target Attention (TA) to Target Pattern Attention\n(TPA) to model pattern-level dependencies. Furthermore, three critical\nchallenges demand attention: the inclusion of unrelated items within behavior\npatterns, data sparsity in behavior patterns, and computational complexity\narising from numerous patterns. To address these challenges, we introduce the\nDeep Pattern Network (DPN), designed to comprehensively leverage information\nfrom user behavior patterns. DPN efficiently retrieves target-related user\nbehavior patterns using a target-aware attention mechanism. Additionally, it\ncontributes to refining user behavior patterns through a pre-training paradigm\nbased on self-supervised learning while promoting dependency learning within\nsparse patterns. Our comprehensive experiments, conducted across three public\ndatasets, substantiate the superior performance and broad compatibility of DPN.\n","authors":["Hengyu Zhang","Junwei Pan","Dapeng Liu","Jie Jiang","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2404.11456v1.pdf","comment":"12 pages, 10 figures, accepted by SIGIR2024"},{"id":"http://arxiv.org/abs/2403.17089v2","updated":"2024-04-17T15:00:58Z","published":"2024-03-25T18:25:10Z","title":"GOLF: Goal-Oriented Long-term liFe tasks supported by human-AI\n  collaboration","summary":"  The advent of ChatGPT and similar large language models (LLMs) has\nrevolutionized the human-AI interaction and information-seeking process.\nLeveraging LLMs as an alternative to search engines, users can now access\nsummarized information tailored to their queries, significantly reducing the\ncognitive load associated with navigating vast information resources. This\nshift underscores the potential of LLMs in redefining information access\nparadigms. Drawing on the foundation of task-focused information retrieval and\nLLMs' task planning ability, this research extends the scope of LLM\ncapabilities beyond routine task automation to support users in navigating\nlong-term and significant life tasks. It introduces the GOLF framework\n(Goal-Oriented Long-term liFe tasks), which focuses on enhancing LLMs' ability\nto assist in significant life decisions through goal orientation and long-term\nplanning. The methodology encompasses a comprehensive simulation study to test\nthe framework's efficacy, followed by model and human evaluations to develop a\ndataset benchmark for long-term life tasks, and experiments across different\nmodels and settings. By shifting the focus from short-term tasks to the broader\nspectrum of long-term life goals, this research underscores the transformative\npotential of LLMs in enhancing human decision-making processes and task\nmanagement, marking a significant step forward in the evolution of human-AI\ncollaboration.\n","authors":["Ben Wang"],"pdf_url":"https://arxiv.org/pdf/2403.17089v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08796v2","updated":"2024-04-17T13:47:00Z","published":"2024-04-12T20:03:06Z","title":"The Elephant in the Room: Rethinking the Usage of Pre-trained Language\n  Model in Sequential Recommendation","summary":"  Sequential recommendation (SR) has seen significant advancements with the\nhelp of Pre-trained Language Models (PLMs). Some PLM-based SR models directly\nuse PLM to encode user historical behavior's text sequences to learn user\nrepresentations, while there is seldom an in-depth exploration of the\ncapability and suitability of PLM in behavior sequence modeling. In this work,\nwe first conduct extensive model analyses between PLMs and PLM-based SR models,\ndiscovering great underutilization and parameter redundancy of PLMs in behavior\nsequence modeling. Inspired by this, we explore different lightweight usages of\nPLMs in SR, aiming to maximally stimulate the ability of PLMs for SR while\nsatisfying the efficiency and usability demands of practical systems. We\ndiscover that adopting behavior-tuned PLMs for item initializations of\nconventional ID-based SR models is the most economical framework of PLM-based\nSR, which would not bring in any additional inference cost but could achieve a\ndramatic performance boost compared with the original version. Extensive\nexperiments on five datasets show that our simple and universal framework leads\nto significant improvement compared to classical SR and SOTA PLM-based SR\nmodels without additional inference costs.\n","authors":["Zekai Qu","Ruobing Xie","Chaojun Xiao","Xingwu Sun","Zhanhui Kang"],"pdf_url":"https://arxiv.org/pdf/2404.08796v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2402.10381v2","updated":"2024-04-17T13:46:56Z","published":"2024-02-16T00:25:53Z","title":"UMAIR-FPS: User-aware Multi-modal Animation Illustration Recommendation\n  Fusion with Painting Style","summary":"  The rapid advancement of high-quality image generation models based on AI has\ngenerated a deluge of anime illustrations. Recommending illustrations to users\nwithin massive data has become a challenging and popular task. However,\nexisting anime recommendation systems have focused on text features but still\nneed to integrate image features. In addition, most multi-modal recommendation\nresearch is constrained by tightly coupled datasets, limiting its applicability\nto anime illustrations. We propose the User-aware Multi-modal Animation\nIllustration Recommendation Fusion with Painting Style (UMAIR-FPS) to tackle\nthese gaps. In the feature extract phase, for image features, we are the first\nto combine image painting style features with semantic features to construct a\ndual-output image encoder for enhancing representation. For text features, we\nobtain text embeddings based on fine-tuning Sentence-Transformers by\nincorporating domain knowledge that composes a variety of domain text pairs\nfrom multilingual mappings, entity relationships, and term explanation\nperspectives, respectively. In the multi-modal fusion phase, we novelly propose\na user-aware multi-modal contribution measurement mechanism to weight\nmulti-modal features dynamically according to user features at the interaction\nlevel and employ the DCN-V2 module to model bounded-degree multi-modal crosses\neffectively. UMAIR-FPS surpasses the stat-of-the-art baselines on large\nreal-world datasets, demonstrating substantial performance enhancements.\n","authors":["Yan Kang","Hao Lin","Mingjian Yang","Shin-Jye Lee"],"pdf_url":"https://arxiv.org/pdf/2402.10381v2.pdf","comment":"Accepted by DASFAA 2024 Research track"},{"id":"http://arxiv.org/abs/2209.04589v3","updated":"2024-04-17T13:16:02Z","published":"2022-09-10T04:21:25Z","title":"Causal Intervention for Fairness in Multi-behavior Recommendation","summary":"  Recommender systems usually learn user interests from various user behaviors,\nincluding clicks and post-click behaviors (e.g., like and favorite). However,\nthese behaviors inevitably exhibit popularity bias, leading to some unfairness\nissues: 1) for items with similar quality, more popular ones get more exposure;\nand 2) even worse the popular items with lower popularity might receive more\nexposure. Existing work on mitigating popularity bias blindly eliminates the\nbias and usually ignores the effect of item quality. We argue that the\nrelationships between different user behaviors (e.g., conversion rate) actually\nreflect the item quality. Therefore, to handle the unfairness issues, we\npropose to mitigate the popularity bias by considering multiple user behaviors.\n  In this work, we examine causal relationships behind the interaction\ngeneration procedure in multi-behavior recommendation. Specifically, we find\nthat: 1) item popularity is a confounder between the exposed items and users'\npost-click interactions, leading to the first unfairness; and 2) some hidden\nconfounders (e.g., the reputation of item producers) affect both item\npopularity and quality, resulting in the second unfairness. To alleviate these\nconfounding issues, we propose a causal framework to estimate the causal\neffect, which leverages backdoor adjustment to block the backdoor paths caused\nby the confounders. In the inference stage, we remove the negative effect of\npopularity and utilize the good effect of quality for recommendation.\nExperiments on two real-world datasets validate the effectiveness of our\nproposed framework, which enhances fairness without sacrificing recommendation\naccuracy.\n","authors":["Xi Wang","Wenjie Wang","Fuli Feng","Wenge Rong","Chuantao Yin","Zhang Xiong"],"pdf_url":"https://arxiv.org/pdf/2209.04589v3.pdf","comment":"This paper is accepted by IEEE Transactions on Computational Social\n  Systems"},{"id":"http://arxiv.org/abs/2404.11343v1","updated":"2024-04-17T13:03:07Z","published":"2024-04-17T13:03:07Z","title":"Large Language Models meet Collaborative Filtering: An Efficient\n  All-round LLM-based Recommender System","summary":"  Collaborative filtering recommender systems (CF-RecSys) have shown successive\nresults in enhancing the user experience on social media and e-commerce\nplatforms. However, as CF-RecSys struggles under cold scenarios with sparse\nuser-item interactions, recent strategies have focused on leveraging modality\ninformation of user/items (e.g., text or images) based on pre-trained modality\nencoders and Large Language Models (LLMs). Despite their effectiveness under\ncold scenarios, we observe that they underperform simple traditional\ncollaborative filtering models under warm scenarios due to the lack of\ncollaborative knowledge. In this work, we propose an efficient All-round\nLLM-based Recommender system, called A-LLMRec, that excels not only in the cold\nscenario but also in the warm scenario. Our main idea is to enable an LLM to\ndirectly leverage the collaborative knowledge contained in a pre-trained\nstate-of-the-art CF-RecSys so that the emergent ability of the LLM as well as\nthe high-quality user/item embeddings that are already trained by the\nstate-of-the-art CF-RecSys can be jointly exploited. This approach yields two\nadvantages: (1) model-agnostic, allowing for integration with various existing\nCF-RecSys, and (2) efficiency, eliminating the extensive fine-tuning typically\nrequired for LLM-based recommenders. Our extensive experiments on various\nreal-world datasets demonstrate the superiority of A-LLMRec in various\nscenarios, including cold/warm, few-shot, cold user, and cross-domain\nscenarios. Beyond the recommendation task, we also show the potential of\nA-LLMRec in generating natural language outputs based on the understanding of\nthe collaborative knowledge by performing a favorite genre prediction task. Our\ncode is available at https://github.com/ghdtjr/A-LLMRec .\n","authors":["Sein Kim","Hongseok Kang","Seungyoon Choi","Donghyun Kim","Minchul Yang","Chanyoung Park"],"pdf_url":"https://arxiv.org/pdf/2404.11343v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2404.11180v1","updated":"2024-04-17T08:50:29Z","published":"2024-04-17T08:50:29Z","title":"Causal Deconfounding via Confounder Disentanglement for Dual-Target\n  Cross-Domain Recommendation","summary":"  In recent years, dual-target Cross-Domain Recommendation (CDR) has been\nproposed to capture comprehensive user preferences in order to ultimately\nenhance the recommendation accuracy in both data-richer and data-sparser\ndomains simultaneously. However, in addition to users' true preferences, the\nuser-item interactions might also be affected by confounders (e.g., free\nshipping, sales promotion). As a result, dual-target CDR has to meet two\nchallenges: (1) how to effectively decouple observed confounders, including\nsingle-domain confounders and cross-domain confounders, and (2) how to preserve\nthe positive effects of observed confounders on predicted interactions, while\neliminating their negative effects on capturing comprehensive user preferences.\nTo address the above two challenges, we propose a Causal Deconfounding\nframework via Confounder Disentanglement for dual-target Cross-Domain\nRecommendation, called CD2CDR. In CD2CDR, we first propose a confounder\ndisentanglement module to effectively decouple observed single-domain and\ncross-domain confounders. We then propose a causal deconfounding module to\npreserve the positive effects of such observed confounders and eliminate their\nnegative effects via backdoor adjustment, thereby enhancing the recommendation\naccuracy in each domain. Extensive experiments conducted on five real-world\ndatasets demonstrate that CD2CDR significantly outperforms the state-of-the-art\nmethods.\n","authors":["Jiajie Zhu","Yan Wang","Feng Zhu","Zhu Sun"],"pdf_url":"https://arxiv.org/pdf/2404.11180v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.02046v4","updated":"2024-04-17T08:36:26Z","published":"2023-07-05T06:03:40Z","title":"Recommender Systems in the Era of Large Language Models (LLMs)","summary":"  With the prosperity of e-commerce and web applications, Recommender Systems\n(RecSys) have become an important component of our daily life, providing\npersonalized suggestions that cater to user preferences. While Deep Neural\nNetworks (DNNs) have made significant advancements in enhancing recommender\nsystems by modeling user-item interactions and incorporating textual side\ninformation, DNN-based methods still face limitations, such as difficulties in\nunderstanding users' interests and capturing textual side information,\ninabilities in generalizing to various recommendation scenarios and reasoning\non their predictions, etc. Meanwhile, the emergence of Large Language Models\n(LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural\nLanguage Processing (NLP) and Artificial Intelligence (AI), due to their\nremarkable abilities in fundamental responsibilities of language understanding\nand generation, as well as impressive generalization and reasoning\ncapabilities. As a result, recent studies have attempted to harness the power\nof LLMs to enhance recommender systems. Given the rapid evolution of this\nresearch direction in recommender systems, there is a pressing need for a\nsystematic overview that summarizes existing LLM-empowered recommender systems,\nto provide researchers in relevant fields with an in-depth understanding.\nTherefore, in this paper, we conduct a comprehensive review of LLM-empowered\nrecommender systems from various aspects including Pre-training, Fine-tuning,\nand Prompting. More specifically, we first introduce representative methods to\nharness the power of LLMs (as a feature encoder) for learning representations\nof users and items. Then, we review recent techniques of LLMs for enhancing\nrecommender systems from three paradigms, namely pre-training, fine-tuning, and\nprompting. Finally, we comprehensively discuss future directions in this\nemerging field.\n","authors":["Zihuai Zhao","Wenqi Fan","Jiatong Li","Yunqing Liu","Xiaowei Mei","Yiqi Wang","Zhen Wen","Fei Wang","Xiangyu Zhao","Jiliang Tang","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2307.02046v4.pdf","comment":"Accepted by IEEE TKDE"},{"id":"http://arxiv.org/abs/2404.11119v1","updated":"2024-04-17T07:07:41Z","published":"2024-04-17T07:07:41Z","title":"DRepMRec: A Dual Representation Learning Framework for Multimodal\n  Recommendation","summary":"  Multimodal Recommendation focuses mainly on how to effectively integrate\nbehavior and multimodal information in the recommendation task. Previous works\nsuffer from two major issues. Firstly, the training process tightly couples the\nbehavior module and multimodal module by jointly optimizing them using the\nsharing model parameters, which leads to suboptimal performance since behavior\nsignals and modality signals often provide opposite guidance for the parameters\nupdates. Secondly, previous approaches fail to take into account the\nsignificant distribution differences between behavior and modality when they\nattempt to fuse behavior and modality information. This resulted in a\nmisalignment between the representations of behavior and modality. To address\nthese challenges, in this paper, we propose a novel Dual Representation\nlearning framework for Multimodal Recommendation called DRepMRec, which\nintroduce separate dual lines for coupling problem and Behavior-Modal Alignment\n(BMA) for misalignment problem. Specifically, DRepMRec leverages two\nindependent lines of representation learning to calculate behavior and modal\nrepresentations. After obtaining separate behavior and modal representations,\nwe design a Behavior-Modal Alignment Module (BMA) to align and fuse the dual\nrepresentations to solve the misalignment problem. Furthermore, we integrate\nthe BMA into other recommendation models, resulting in consistent performance\nimprovements. To ensure dual representations maintain their semantic\nindependence during alignment, we introduce Similarity-Supervised Signal (SSS)\nfor representation learning. We conduct extensive experiments on three public\ndatasets and our method achieves state-of-the-art (SOTA) results. The source\ncode will be available upon acceptance.\n","authors":["Kangning Zhang","Yingjie Qin","Ruilong Su","Yifan Liu","Jiarui Jin","Weinan Zhang","Yong Yu"],"pdf_url":"https://arxiv.org/pdf/2404.11119v1.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2404.05893v2","updated":"2024-04-17T04:17:12Z","published":"2024-04-08T22:29:53Z","title":"Use of a Structured Knowledge Base Enhances Metadata Curation by Large\n  Language Models","summary":"  Metadata play a crucial role in ensuring the findability, accessibility,\ninteroperability, and reusability of datasets. This paper investigates the\npotential of large language models (LLMs), specifically GPT-4, to improve\nadherence to metadata standards. We conducted experiments on 200 random data\nrecords describing human samples relating to lung cancer from the NCBI\nBioSample repository, evaluating GPT-4's ability to suggest edits for adherence\nto metadata standards. We computed the adherence accuracy of field name-field\nvalue pairs through a peer review process, and we observed a marginal average\nimprovement in adherence to the standard data dictionary from 79% to 80%\n(p<0.01). We then prompted GPT-4 with domain information in the form of the\ntextual descriptions of CEDAR templates and recorded a significant improvement\nto 97% from 79% (p<0.01). These results indicate that, while LLMs may not be\nable to correct legacy metadata to ensure satisfactory adherence to standards\nwhen unaided, they do show promise for use in automated metadata curation when\nintegrated with a structured knowledge base.\n","authors":["Sowmya S. Sundaram","Benjamin Solomon","Avani Khatri","Anisha Laumas","Purvesh Khatri","Mark A. Musen"],"pdf_url":"https://arxiv.org/pdf/2404.05893v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10981v1","updated":"2024-04-17T01:27:42Z","published":"2024-04-17T01:27:42Z","title":"A Survey on Retrieval-Augmented Text Generation for Large Language\n  Models","summary":"  Retrieval-Augmented Generation (RAG) merges retrieval methods with deep\nlearning advancements to address the static limitations of large language\nmodels (LLMs) by enabling the dynamic integration of up-to-date external\ninformation. This methodology, focusing primarily on the text domain, provides\na cost-effective solution to the generation of plausible but incorrect\nresponses by LLMs, thereby enhancing the accuracy and reliability of their\noutputs through the use of real-world data. As RAG grows in complexity and\nincorporates multiple concepts that can influence its performance, this paper\norganizes the RAG paradigm into four categories: pre-retrieval, retrieval,\npost-retrieval, and generation, offering a detailed perspective from the\nretrieval viewpoint. It outlines RAG's evolution and discusses the field's\nprogression through the analysis of significant studies. Additionally, the\npaper introduces evaluation methods for RAG, addressing the challenges faced\nand proposing future research directions. By offering an organized framework\nand categorization, the study aims to consolidate existing research on RAG,\nclarify its technological underpinnings, and highlight its potential to broaden\nthe adaptability and applications of LLMs.\n","authors":["Yizheng Huang","Jimmy Huang"],"pdf_url":"https://arxiv.org/pdf/2404.10981v1.pdf","comment":"Ongoing work"},{"id":"http://arxiv.org/abs/2402.14301v2","updated":"2024-04-17T00:55:09Z","published":"2024-02-22T05:41:24Z","title":"GenSERP: Large Language Models for Whole Page Presentation","summary":"  The advent of large language models (LLMs) brings an opportunity to minimize\nthe effort in search engine result page (SERP) organization. In this paper, we\npropose GenSERP, a framework that leverages LLMs with vision in a few-shot\nsetting to dynamically organize intermediate search results, including\ngenerated chat answers, website snippets, multimedia data, knowledge panels\ninto a coherent SERP layout based on a user's query. Our approach has three\nmain stages: (1) An information gathering phase where the LLM continuously\norchestrates API tools to retrieve different types of items, and proposes\ncandidate layouts based on the retrieved items, until it's confident enough to\ngenerate the final result. (2) An answer generation phase where the LLM\npopulates the layouts with the retrieved content. In this phase, the LLM\nadaptively optimize the ranking of items and UX configurations of the SERP.\nConsequently, it assigns a location on the page to each item, along with the UX\ndisplay details. (3) A scoring phase where an LLM with vision scores all the\ngenerated SERPs based on how likely it can satisfy the user. It then send the\none with highest score to rendering. GenSERP features two generation paradigms.\nFirst, coarse-to-fine, which allow it to approach optimal layout in a more\nmanageable way, (2) beam search, which give it a better chance to hit the\noptimal solution compared to greedy decoding. Offline experimental results on\nreal-world data demonstrate how LLMs can contextually organize heterogeneous\nsearch results on-the-fly and provide a promising user experience.\n","authors":["Zhenning Zhang","Yunan Zhang","Suyu Ge","Guangwei Weng","Mridu Narang","Xia Song","Saurabh Tiwary"],"pdf_url":"https://arxiv.org/pdf/2402.14301v2.pdf","comment":"Microsoft corp policy"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2403.09863v5","updated":"2024-04-17T17:58:52Z","published":"2024-03-14T20:50:03Z","title":"Towards White Box Deep Learning","summary":"  Deep neural networks learn fragile \"shortcut\" features, rendering them\ndifficult to interpret (black box) and vulnerable to adversarial attacks. This\npaper proposes semantic features as a general architectural solution to this\nproblem. The main idea is to make features locality-sensitive in the adequate\nsemantic topology of the domain, thus introducing a strong regularization. The\nproof of concept network is lightweight, inherently interpretable and achieves\nalmost human-level adversarial test metrics - with no adversarial training!\nThese results and the general nature of the approach warrant further research\non semantic features. The code is available at\nhttps://github.com/314-Foundation/white-box-nn\n","authors":["Maciej Satkiewicz"],"pdf_url":"https://arxiv.org/pdf/2403.09863v5.pdf","comment":"16 pages, 12 figures, independent research, v5 changes: Expanded\n  Abstract and Related Work section; minor wording improvements"},{"id":"http://arxiv.org/abs/2404.11606v1","updated":"2024-04-17T17:55:17Z","published":"2024-04-17T17:55:17Z","title":"Learning to Solve the Constrained Most Probable Explanation Task in\n  Probabilistic Graphical Models","summary":"  We propose a self-supervised learning approach for solving the following\nconstrained optimization task in log-linear models or Markov networks. Let $f$\nand $g$ be two log-linear models defined over the sets $\\mathbf{X}$ and\n$\\mathbf{Y}$ of random variables respectively. Given an assignment $\\mathbf{x}$\nto all variables in $\\mathbf{X}$ (evidence) and a real number $q$, the\nconstrained most-probable explanation (CMPE) task seeks to find an assignment\n$\\mathbf{y}$ to all variables in $\\mathbf{Y}$ such that $f(\\mathbf{x},\n\\mathbf{y})$ is maximized and $g(\\mathbf{x}, \\mathbf{y})\\leq q$. In our\nproposed self-supervised approach, given assignments $\\mathbf{x}$ to\n$\\mathbf{X}$ (data), we train a deep neural network that learns to output\nnear-optimal solutions to the CMPE problem without requiring access to any\npre-computed solutions. The key idea in our approach is to use first principles\nand approximate inference methods for CMPE to derive novel loss functions that\nseek to push infeasible solutions towards feasible ones and feasible solutions\ntowards optimal ones. We analyze the properties of our proposed method and\nexperimentally demonstrate its efficacy on several benchmark problems.\n","authors":["Shivvrat Arya","Tahrima Rahman","Vibhav Gogate"],"pdf_url":"https://arxiv.org/pdf/2404.11606v1.pdf","comment":"Will appear in AISTATS 2024"},{"id":"http://arxiv.org/abs/2404.11599v1","updated":"2024-04-17T17:50:24Z","published":"2024-04-17T17:50:24Z","title":"Variational Bayesian Last Layers","summary":"  We introduce a deterministic variational formulation for training Bayesian\nlast layer neural networks. This yields a sampling-free, single-pass model and\nloss that effectively improves uncertainty estimation. Our variational Bayesian\nlast layer (VBLL) can be trained and evaluated with only quadratic complexity\nin last layer width, and is thus (nearly) computationally free to add to\nstandard architectures. We experimentally investigate VBLLs, and show that they\nimprove predictive accuracy, calibration, and out of distribution detection\nover baselines across both regression and classification. Finally, we\ninvestigate combining VBLL layers with variational Bayesian feature learning,\nyielding a lower variance collapsed variational inference method for Bayesian\nneural networks.\n","authors":["James Harrison","John Willes","Jasper Snoek"],"pdf_url":"https://arxiv.org/pdf/2404.11599v1.pdf","comment":"International Conference on Learning Representations (ICLR) 2024"},{"id":"http://arxiv.org/abs/2404.11597v1","updated":"2024-04-17T17:49:38Z","published":"2024-04-17T17:49:38Z","title":"Explainable Artificial Intelligence Techniques for Accurate Fault\n  Detection and Diagnosis: A Review","summary":"  As the manufacturing industry advances with sensor integration and\nautomation, the opaque nature of deep learning models in machine learning poses\na significant challenge for fault detection and diagnosis. And despite the\nrelated predictive insights Artificial Intelligence (AI) can deliver, advanced\nmachine learning engines often remain a black box. This paper reviews the\neXplainable AI (XAI) tools and techniques in this context. We explore various\nXAI methodologies, focusing on their role in making AI decision-making\ntransparent, particularly in critical scenarios where humans are involved. We\nalso discuss current limitations and potential future research that aims to\nbalance explainability with model performance while improving trustworthiness\nin the context of AI applications for critical industrial use cases.\n","authors":["Ahmed Maged","Salah Haridy","Herman Shen"],"pdf_url":"https://arxiv.org/pdf/2404.11597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02255v2","updated":"2024-04-17T17:44:44Z","published":"2023-12-04T18:56:08Z","title":"Re-Nerfing: Improving Novel Views Synthesis through Novel Views\n  Synthesis","summary":"  Neural Radiance Fields (NeRFs) have shown remarkable novel view synthesis\ncapabilities even in large-scale, unbounded scenes, albeit requiring hundreds\nof views or introducing artifacts in sparser settings. Their optimization\nsuffers from shape-radiance ambiguities wherever only a small visual overlap is\navailable. This leads to erroneous scene geometry and artifacts. In this paper,\nwe propose Re-Nerfing, a simple and general multi-stage data augmentation\napproach that leverages NeRF's own view synthesis ability to address these\nlimitations. With Re-Nerfing, we enhance the geometric consistency of novel\nviews as follows: First, we train a NeRF with the available views. Then, we use\nthe optimized NeRF to synthesize pseudo-views around the original ones with a\nview selection strategy to improve coverage and preserve view quality. Finally,\nwe train a second NeRF with both the original images and the pseudo views\nmasking out uncertain regions. Extensive experiments applying Re-Nerfing on\nvarious pipelines on the mip-NeRF 360 dataset, including Gaussian Splatting,\nprovide valuable insights into the improvements achievable without external\ndata or supervision, on denser and sparser input scenarios. Project page:\nhttps://renerfing.github.io\n","authors":["Felix Tristram","Stefano Gasperini","Nassir Navab","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2312.02255v2.pdf","comment":"Code will be released upon acceptance"},{"id":"http://arxiv.org/abs/2301.04104v2","updated":"2024-04-17T17:41:20Z","published":"2023-01-10T18:12:16Z","title":"Mastering Diverse Domains through World Models","summary":"  Developing a general algorithm that learns to solve tasks across a wide range\nof applications has been a fundamental challenge in artificial intelligence.\nAlthough current reinforcement learning algorithms can be readily applied to\ntasks similar to what they have been developed for, configuring them for new\napplication domains requires significant human expertise and experimentation.\nWe present DreamerV3, a general algorithm that outperforms specialized methods\nacross over 150 diverse tasks, with a single configuration. Dreamer learns a\nmodel of the environment and improves its behavior by imagining future\nscenarios. Robustness techniques based on normalization, balancing, and\ntransformations enable stable learning across domains. Applied out of the box,\nDreamer is the first algorithm to collect diamonds in Minecraft from scratch\nwithout human data or curricula. This achievement has been posed as a\nsignificant challenge in artificial intelligence that requires exploring\nfarsighted strategies from pixels and sparse rewards in an open world. Our work\nallows solving challenging control problems without extensive experimentation,\nmaking reinforcement learning broadly applicable.\n","authors":["Danijar Hafner","Jurgis Pasukonis","Jimmy Ba","Timothy Lillicrap"],"pdf_url":"https://arxiv.org/pdf/2301.04104v2.pdf","comment":"Website: https://danijar.com/dreamerv3"},{"id":"http://arxiv.org/abs/2404.11589v1","updated":"2024-04-17T17:38:56Z","published":"2024-04-17T17:38:56Z","title":"Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept\n  Understanding","summary":"  The rapid evolution of text-to-image diffusion models has opened the door of\ngenerative AI, enabling the translation of textual descriptions into visually\ncompelling images with remarkable quality. However, a persistent challenge\nwithin this domain is the optimization of prompts to effectively convey\nabstract concepts into concrete objects. For example, text encoders can hardly\nexpress \"peace\", while can easily illustrate olive branches and white doves.\nThis paper introduces a novel approach named Prompt Optimizer for Abstract\nConcepts (POAC) specifically designed to enhance the performance of\ntext-to-image diffusion models in interpreting and generating images from\nabstract concepts. We propose a Prompt Language Model (PLM), which is\ninitialized from a pre-trained language model, and then fine-tuned with a\ncurated dataset of abstract concept prompts. The dataset is created with GPT-4\nto extend the abstract concept to a scene and concrete objects. Our framework\nemploys a Reinforcement Learning (RL)-based optimization strategy, focusing on\nthe alignment between the generated images by a stable diffusion model and\noptimized prompts. Through extensive experiments, we demonstrate that our\nproposed POAC significantly improves the accuracy and aesthetic quality of\ngenerated images, particularly in the description of abstract concepts and\nalignment with optimized prompts. We also present a comprehensive analysis of\nour model's performance across diffusion models under different settings,\nshowcasing its versatility and effectiveness in enhancing abstract concept\nrepresentation.\n","authors":["Zezhong Fan","Xiaohan Li","Chenhao Fang","Topojoy Biswas","Kaushiki Nag","Jianpeng Xu","Kannan Achan"],"pdf_url":"https://arxiv.org/pdf/2404.11589v1.pdf","comment":"WWW 2024 Companion"},{"id":"http://arxiv.org/abs/2404.11578v1","updated":"2024-04-17T17:24:44Z","published":"2024-04-17T17:24:44Z","title":"Deep Policy Optimization with Temporal Logic Constraints","summary":"  Temporal logics, such as linear temporal logic (LTL), offer a precise means\nof specifying tasks for (deep) reinforcement learning (RL) agents. In our work,\nwe consider the setting where the task is specified by an LTL objective and\nthere is an additional scalar reward that we need to optimize. Previous works\nfocus either on learning a LTL task-satisfying policy alone or are restricted\nto finite state spaces. We make two contributions: First, we introduce an\nRL-friendly approach to this setting by formulating this problem as a single\noptimization objective. Our formulation guarantees that an optimal policy will\nbe reward-maximal from the set of policies that maximize the likelihood of\nsatisfying the LTL specification. Second, we address a sparsity issue that\noften arises for LTL-guided Deep RL policies by introducing Cycle Experience\nReplay (CyclER), a technique that automatically guides RL agents towards the\nsatisfaction of an LTL specification. Our experiments demonstrate the efficacy\nof CyclER in finding performant deep RL policies in both continuous and\ndiscrete experimental domains.\n","authors":["Ameesh Shah","Cameron Voloshin","Chenxi Yang","Abhinav Verma","Swarat Chaudhuri","Sanjit A. Seshia"],"pdf_url":"https://arxiv.org/pdf/2404.11578v1.pdf","comment":"preprint, 8 pages"},{"id":"http://arxiv.org/abs/2404.11577v1","updated":"2024-04-17T17:20:27Z","published":"2024-04-17T17:20:27Z","title":"Towards Reliable Empirical Machine Unlearning Evaluation: A\n  Game-Theoretic View","summary":"  Machine unlearning is the process of updating machine learning models to\nremove the information of specific training data samples, in order to comply\nwith data protection regulations that allow individuals to request the removal\nof their personal data. Despite the recent development of numerous unlearning\nalgorithms, reliable evaluation of these algorithms remains an open research\nquestion. In this work, we focus on membership inference attack (MIA) based\nevaluation, one of the most common approaches for evaluating unlearning\nalgorithms, and address various pitfalls of existing evaluation metrics that\nlack reliability. Specifically, we propose a game-theoretic framework that\nformalizes the evaluation process as a game between unlearning algorithms and\nMIA adversaries, measuring the data removal efficacy of unlearning algorithms\nby the capability of the MIA adversaries. Through careful design of the game,\nwe demonstrate that the natural evaluation metric induced from the game enjoys\nprovable guarantees that the existing evaluation metrics fail to satisfy.\nFurthermore, we propose a practical and efficient algorithm to estimate the\nevaluation metric induced from the game, and demonstrate its effectiveness\nthrough both theoretical analysis and empirical experiments. This work presents\na novel and reliable approach to empirically evaluating unlearning algorithms,\npaving the way for the development of more effective unlearning techniques.\n","authors":["Yiwen Tu","Pingbang Hu","Jiaqi Ma"],"pdf_url":"https://arxiv.org/pdf/2404.11577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08838v3","updated":"2024-04-17T17:20:04Z","published":"2024-04-12T22:53:41Z","title":"Predicting Traffic Congestion at Urban Intersections Using Data-Driven\n  Modeling","summary":"  Traffic congestion at intersections is a significant issue in urban areas,\nleading to increased commute times, safety hazards, and operational\ninefficiencies. This study aims to develop a predictive model for congestion at\nintersections in major U.S. cities, utilizing a dataset of trip-logging metrics\nfrom commercial vehicles across 4,800 intersections. The dataset encompasses 27\nfeatures, including intersection coordinates, street names, time of day, and\ntraffic metrics (Kashyap et al., 2019). Additional features, such as\nrainfall/snowfall percentage, distance from downtown and outskirts, and road\ntypes, were incorporated to enhance the model's predictive power. The\nmethodology involves data exploration, feature transformation, and handling\nmissing values through low-rank models and label encoding. The proposed model\nhas the potential to assist city planners and governments in anticipating\ntraffic hot spots, optimizing operations, and identifying infrastructure\nchallenges.\n","authors":["Tara Kelly","Jessica Gupta"],"pdf_url":"https://arxiv.org/pdf/2404.08838v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.07289v6","updated":"2024-04-17T17:16:55Z","published":"2021-06-14T10:36:25Z","title":"Decentralized Personalized Federated Learning for Min-Max Problems","summary":"  Personalized Federated Learning (PFL) has witnessed remarkable advancements,\nenabling the development of innovative machine learning applications that\npreserve the privacy of training data. However, existing theoretical research\nin this field has primarily focused on distributed optimization for\nminimization problems. This paper is the first to study PFL for saddle point\nproblems encompassing a broader range of optimization problems, that require\nmore than just solving minimization problems. In this work, we consider a\nrecently proposed PFL setting with the mixing objective function, an approach\ncombining the learning of a global model together with locally distributed\nlearners. Unlike most previous work, which considered only the centralized\nsetting, we work in a more general and decentralized setup that allows us to\ndesign and analyze more practical and federated ways to connect devices to the\nnetwork. We proposed new algorithms to address this problem and provide a\ntheoretical analysis of the smooth (strongly) convex-(strongly) concave saddle\npoint problems in stochastic and deterministic cases. Numerical experiments for\nbilinear problems and neural networks with adversarial noise demonstrate the\neffectiveness of the proposed methods.\n","authors":["Ekaterina Borodich","Aleksandr Beznosikov","Abdurakhmon Sadiev","Vadim Sushko","Nikolay Savelyev","Martin Tak√°ƒç","Alexander Gasnikov"],"pdf_url":"https://arxiv.org/pdf/2106.07289v6.pdf","comment":"33 pages, 3 algorithms, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2402.09906v2","updated":"2024-04-17T17:12:05Z","published":"2024-02-15T12:12:19Z","title":"Generative Representational Instruction Tuning","summary":"  All text-based language problems can be reduced to either generation or\nembedding. Current models only perform well at one or the other. We introduce\ngenerative representational instruction tuning (GRIT) whereby a large language\nmodel is trained to handle both generative and embedding tasks by\ndistinguishing between them through instructions. Compared to other open\nmodels, our resulting GritLM 7B sets a new state of the art on the Massive Text\nEmbedding Benchmark (MTEB) and outperforms all models up to its size on a range\nof generative tasks. By scaling up further, GritLM 8x7B outperforms all open\ngenerative language models that we tried while still being among the best\nembedding models. Notably, we find that GRIT matches training on only\ngenerative or embedding data, thus we can unify both at no performance loss.\nAmong other benefits, the unification via GRIT speeds up Retrieval-Augmented\nGeneration (RAG) by > 60% for long documents, by no longer requiring separate\nretrieval and generation models. Models, code, etc. are freely available at\nhttps://github.com/ContextualAI/gritlm.\n","authors":["Niklas Muennighoff","Hongjin Su","Liang Wang","Nan Yang","Furu Wei","Tao Yu","Amanpreet Singh","Douwe Kiela"],"pdf_url":"https://arxiv.org/pdf/2402.09906v2.pdf","comment":"66 pages (16 main), 25 figures, 34 tables"},{"id":"http://arxiv.org/abs/2404.11569v1","updated":"2024-04-17T17:11:47Z","published":"2024-04-17T17:11:47Z","title":"Simple Image Signal Processing using Global Context Guidance","summary":"  In modern smartphone cameras, the Image Signal Processor (ISP) is the core\nelement that converts the RAW readings from the sensor into perceptually\npleasant RGB images for the end users. The ISP is typically proprietary and\nhandcrafted and consists of several blocks such as white balance, color\ncorrection, and tone mapping. Deep learning-based ISPs aim to transform RAW\nimages into DSLR-like RGB images using deep neural networks. However, most\nlearned ISPs are trained using patches (small regions) due to computational\nlimitations. Such methods lack global context, which limits their efficacy on\nfull-resolution images and harms their ability to capture global properties\nsuch as color constancy or illumination. First, we propose a novel module that\ncan be integrated into any neural ISP to capture the global context information\nfrom the full RAW images. Second, we propose an efficient and simple neural ISP\nthat utilizes our proposed module. Our model achieves state-of-the-art results\non different benchmarks using diverse and real smartphone images.\n","authors":["Omar Elezabi","Marcos V. Conde","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2404.11569v1.pdf","comment":"Preprint under review"},{"id":"http://arxiv.org/abs/2404.11568v1","updated":"2024-04-17T17:11:31Z","published":"2024-04-17T17:11:31Z","title":"On the Scalability of GNNs for Molecular Graphs","summary":"  Scaling deep learning models has been at the heart of recent revolutions in\nlanguage modelling and image generation. Practitioners have observed a strong\nrelationship between model size, dataset size, and performance. However,\nstructure-based architectures such as Graph Neural Networks (GNNs) are yet to\nshow the benefits of scale mainly due to the lower efficiency of sparse\noperations, large data requirements, and lack of clarity about the\neffectiveness of various architectures. We address this drawback of GNNs by\nstudying their scaling behavior. Specifically, we analyze message-passing\nnetworks, graph Transformers, and hybrid architectures on the largest public\ncollection of 2D molecular graphs. For the first time, we observe that GNNs\nbenefit tremendously from the increasing scale of depth, width, number of\nmolecules, number of labels, and the diversity in the pretraining datasets,\nresulting in a 30.25% improvement when scaling to 1 billion parameters and\n28.98% improvement when increasing size of dataset to eightfold. We further\ndemonstrate strong finetuning scaling behavior on 38 tasks, outclassing\nprevious large models. We hope that our work paves the way for an era where\nfoundational GNNs drive pharmaceutical drug discovery.\n","authors":["Maciej Sypetkowski","Frederik Wenkel","Farimah Poursafaei","Nia Dickson","Karush Suri","Philip Fradkin","Dominique Beaini"],"pdf_url":"https://arxiv.org/pdf/2404.11568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06608v2","updated":"2024-04-17T16:58:36Z","published":"2023-12-11T18:39:05Z","title":"Information theory for data-driven model reduction in physics and\n  biology","summary":"  Model reduction is the construction of simple yet predictive descriptions of\nthe dynamics of many-body systems in terms of a few relevant variables. A\nprerequisite to model reduction is the identification of these relevant\nvariables, a task for which no general method exists. Here, we develop a\nsystematic approach based on the information bottleneck to identify the\nrelevant variables, defined as those most predictive of the future. We\nelucidate analytically the relation between these relevant variables and the\neigenfunctions of the transfer operator describing the dynamics. Further, we\nshow that in the limit of high compression, the relevant variables are directly\ndetermined by the slowest-decaying eigenfunctions. Our information-based\napproach indicates when to optimally stop increasing the complexity of the\nreduced model. Furthermore, it provides a firm foundation to construct\ninterpretable deep learning tools that perform model reduction. We illustrate\nhow these tools work in practice by considering uncurated videos of atmospheric\nflows from which our algorithms automatically extract the dominant slow\ncollective variables, as well as experimental videos of cyanobacteria colonies\nin which we discover an emergent synchronization order parameter.\n","authors":["Matthew S. Schmitt","Maciej Koch-Janusz","Michel Fruchart","Daniel S. Seara","Michael Rust","Vincenzo Vitelli"],"pdf_url":"https://arxiv.org/pdf/2312.06608v2.pdf","comment":"39 pages, 19 figures"},{"id":"http://arxiv.org/abs/2404.11553v1","updated":"2024-04-17T16:53:16Z","published":"2024-04-17T16:53:16Z","title":"Quantifying Multilingual Performance of Large Language Models Across\n  Languages","summary":"  The training process of Large Language Models (LLMs) requires extensive text\ncorpus. However, these data are often unevenly distributed in different\nlanguages. As a result, LLMs perform well on common languages, such as English,\nGerman, and French, but perform poorly on low-resource languages. However,\ncurrently there is no work to quantitatively measure the performance of LLMs in\nlow-resource languages. To fill this gap, we proposed the Language Ranker that\naims to benchmark and rank different languages according to the performance of\nLLMs on those languages. We employ the LLM's performance on the English corpus\nas a baseline to compare the performances of different languages and English.\nWe have the following three findings: 1. The performance rankings of different\nLLMs in all languages are roughly the same. 2. LLMs with different sizes have\nthe same partial order of performance. 3. There is a strong correlation between\nLlaMa2's performance in different languages and the proportion of the\npre-training corpus. These findings illustrate that the Language Ranker can be\nused as an indicator to measure the language performance of LLMs.\n","authors":["Zihao Li","Yucheng Shi","Zirui Liu","Fan Yang","Ninghao Liu","Mengnan Du"],"pdf_url":"https://arxiv.org/pdf/2404.11553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.11368v3","updated":"2024-04-17T16:36:36Z","published":"2022-11-21T11:35:25Z","title":"Precise Asymptotics for Spectral Methods in Mixed Generalized Linear\n  Models","summary":"  In a mixed generalized linear model, the objective is to learn multiple\nsignals from unlabeled observations: each sample comes from exactly one signal,\nbut it is not known which one. We consider the prototypical problem of\nestimating two statistically independent signals in a mixed generalized linear\nmodel with Gaussian covariates. Spectral methods are a popular class of\nestimators which output the top two eigenvectors of a suitable data-dependent\nmatrix. However, despite the wide applicability, their design is still obtained\nvia heuristic considerations, and the number of samples $n$ needed to guarantee\nrecovery is super-linear in the signal dimension $d$. In this paper, we develop\nexact asymptotics on spectral methods in the challenging proportional regime in\nwhich $n, d$ grow large and their ratio converges to a finite constant. By\ndoing so, we are able to optimize the design of the spectral method, and\ncombine it with a simple linear estimator, in order to minimize the estimation\nerror. Our characterization exploits a mix of tools from random matrices, free\nprobability and the theory of approximate message passing algorithms. Numerical\nsimulations for mixed linear regression and phase retrieval demonstrate the\nadvantage enabled by our analysis over existing designs of spectral methods.\n","authors":["Yihan Zhang","Marco Mondelli","Ramji Venkataramanan"],"pdf_url":"https://arxiv.org/pdf/2211.11368v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11538v1","updated":"2024-04-17T16:32:13Z","published":"2024-04-17T16:32:13Z","title":"GenFighter: A Generative and Evolutive Textual Attack Removal","summary":"  Adversarial attacks pose significant challenges to deep neural networks\n(DNNs) such as Transformer models in natural language processing (NLP). This\npaper introduces a novel defense strategy, called GenFighter, which enhances\nadversarial robustness by learning and reasoning on the training classification\ndistribution. GenFighter identifies potentially malicious instances deviating\nfrom the distribution, transforms them into semantically equivalent instances\naligned with the training data, and employs ensemble techniques for a unified\nand robust response. By conducting extensive experiments, we show that\nGenFighter outperforms state-of-the-art defenses in accuracy under attack and\nattack success rate metrics. Additionally, it requires a high number of queries\nper attack, making the attack more challenging in real scenarios. The ablation\nstudy shows that our approach integrates transfer learning, a\ngenerative/evolutive procedure, and an ensemble method, providing an effective\ndefense against NLP adversarial attacks.\n","authors":["Md Athikul Islam","Edoardo Serra","Sushil Jajodia"],"pdf_url":"https://arxiv.org/pdf/2404.11538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06079v2","updated":"2024-04-17T16:31:33Z","published":"2024-02-08T22:06:55Z","title":"DiscDiff: Latent Diffusion Model for DNA Sequence Generation","summary":"  This paper introduces a novel framework for DNA sequence generation,\ncomprising two key components: DiscDiff, a Latent Diffusion Model (LDM)\ntailored for generating discrete DNA sequences, and Absorb-Escape, a\npost-training algorithm designed to refine these sequences. Absorb-Escape\nenhances the realism of the generated sequences by correcting `round errors'\ninherent in the conversion process between latent and input spaces. Our\napproach not only sets new standards in DNA sequence generation but also\ndemonstrates superior performance over existing diffusion models, in generating\nboth short and long DNA sequences. Additionally, we introduce EPD-GenDNA, the\nfirst comprehensive, multi-species dataset for DNA generation, encompassing\n160,000 unique sequences from 15 species. We hope this study will advance the\ngenerative modelling of DNA, with potential implications for gene therapy and\nprotein production.\n","authors":["Zehui Li","Yuhao Ni","William A V Beardall","Guoxuan Xia","Akashaditya Das","Guy-Bart Stan","Yiren Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.06079v2.pdf","comment":"Different from the prior work \"Latent Diffusion Model for DNA\n  Sequence Generation\" (arXiv:2310.06150), we updated the evaluation framework\n  and compared the DiscDiff with other methods comprehensively. In addition, a\n  post-training framework is proposed to increase the quality of generated\n  sequences"},{"id":"http://arxiv.org/abs/2404.11536v1","updated":"2024-04-17T16:30:06Z","published":"2024-04-17T16:30:06Z","title":"FedPFT: Federated Proxy Fine-Tuning of Foundation Models","summary":"  Adapting Foundation Models (FMs) for downstream tasks through Federated\nLearning (FL) emerges a promising strategy for protecting data privacy and\nvaluable FMs. Existing methods fine-tune FM by allocating sub-FM to clients in\nFL, however, leading to suboptimal performance due to insufficient tuning and\ninevitable error accumulations of gradients. In this paper, we propose\nFederated Proxy Fine-Tuning (FedPFT), a novel method enhancing FMs adaptation\nin downstream tasks through FL by two key modules. First, the sub-FM\nconstruction module employs a layer-wise compression approach, facilitating\ncomprehensive FM fine-tuning across all layers by emphasizing those crucial\nneurons. Second, the sub-FM alignment module conducts a two-step\ndistillations-layer-level and neuron-level-before and during FL fine-tuning\nrespectively, to reduce error of gradient by accurately aligning sub-FM with FM\nunder theoretical guarantees. Experimental results on seven commonly used\ndatasets (i.e., four text and three vision) demonstrate the superiority of\nFedPFT.\n","authors":["Zhaopeng Peng","Xiaoliang Fan","Yufan Chen","Zheng Wang","Shirui Pan","Chenglu Wen","Ruisheng Zhang","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2404.11536v1.pdf","comment":"Accepted by IJCAI'24"},{"id":"http://arxiv.org/abs/2404.11534v1","updated":"2024-04-17T16:28:08Z","published":"2024-04-17T16:28:08Z","title":"Decomposing and Editing Predictions by Modeling Model Computation","summary":"  How does the internal computation of a machine learning model transform\ninputs into predictions? In this paper, we introduce a task called component\nmodeling that aims to address this question. The goal of component modeling is\nto decompose an ML model's prediction in terms of its components -- simple\nfunctions (e.g., convolution filters, attention heads) that are the \"building\nblocks\" of model computation. We focus on a special case of this task,\ncomponent attribution, where the goal is to estimate the counterfactual impact\nof individual components on a given prediction. We then present COAR, a\nscalable algorithm for estimating component attributions; we demonstrate its\neffectiveness across models, datasets, and modalities. Finally, we show that\ncomponent attributions estimated with COAR directly enable model editing across\nfive tasks, namely: fixing model errors, ``forgetting'' specific classes,\nboosting subpopulation robustness, localizing backdoor attacks, and improving\nrobustness to typographic attacks. We provide code for COAR at\nhttps://github.com/MadryLab/modelcomponents .\n","authors":["Harshay Shah","Andrew Ilyas","Aleksander Madry"],"pdf_url":"https://arxiv.org/pdf/2404.11534v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10636v2","updated":"2024-04-17T16:27:37Z","published":"2024-03-27T18:12:02Z","title":"What are human values, and how do we align AI to them?","summary":"  There is an emerging consensus that we need to align AI systems with human\nvalues (Gabriel, 2020; Ji et al., 2024), but it remains unclear how to apply\nthis to language models in practice. We split the problem of \"aligning to human\nvalues\" into three parts: first, eliciting values from people; second,\nreconciling those values into an alignment target for training ML models; and\nthird, actually training the model. In this paper, we focus on the first two\nparts, and ask the question: what are \"good\" ways to synthesize diverse human\ninputs about values into a target for aligning language models? To answer this\nquestion, we first define a set of 6 criteria that we believe must be satisfied\nfor an alignment target to shape model behavior in accordance with human\nvalues. We then propose a process for eliciting and reconciling values called\nMoral Graph Elicitation (MGE), which uses a large language model to interview\nparticipants about their values in particular contexts; our approach is\ninspired by the philosophy of values advanced by Taylor (1977), Chang (2004),\nand others. We trial MGE with a representative sample of 500 Americans, on 3\nintentionally divisive prompts (e.g. advice about abortion). Our results\ndemonstrate that MGE is promising for improving model alignment across all 6\ncriteria. For example, almost all participants (89.1%) felt well represented by\nthe process, and (89%) thought the final moral graph was fair, even if their\nvalue wasn't voted as the wisest. Our process often results in \"expert\" values\n(e.g. values from women who have solicited abortion advice) rising to the top\nof the moral graph, without defining who is considered an expert in advance.\n","authors":["Oliver Klingefjord","Ryan Lowe","Joe Edelman"],"pdf_url":"https://arxiv.org/pdf/2404.10636v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14608v3","updated":"2024-04-17T16:23:47Z","published":"2024-03-21T17:55:50Z","title":"Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey","summary":"  Large models represent a groundbreaking advancement in multiple application\nfields, enabling remarkable achievements across various tasks. However, their\nunprecedented scale comes with significant computational costs. These models,\noften consisting of billions of parameters, require vast amounts of\ncomputational resources for execution. Especially, the expansive scale and\ncomputational demands pose considerable challenges when customizing them for\nparticular downstream tasks, particularly over the hardware platforms\nconstrained by computational capabilities. Parameter Efficient Fine-Tuning\n(PEFT) provides a practical solution by efficiently adapt the large models over\nthe various downstream tasks. In particular, PEFT refers to the process of\nadjusting the parameters of a pre-trained large models to adapt it to a\nspecific task while minimizing the number of additional parameters introduced\nor computational resources required. This approach is particularly important\nwhen dealing with large language models with high parameter counts, as\nfine-tuning these models from scratch can be computationally expensive and\nresource-intensive, posing considerable challenges in the supporting system\nplatform design. In this survey, we present comprehensive studies of various\nPEFT algorithms, examining their performance and computational overhead.\nMoreover, we provide an overview of applications developed using different PEFT\nalgorithms and discuss common techniques employed to mitigate computation costs\nfor PEFT. In addition to the algorithmic perspective, we overview various\nreal-world system designs to investigate the implementation costs associated\nwith different PEFT algorithms. This survey serves as an indispensable resource\nfor researchers aiming to understand both the PEFT algorithm and its system\nimplementation, offering detailed insights into recent advancements and\npractical applications.\n","authors":["Zeyu Han","Chao Gao","Jinyang Liu","Jeff Zhang","Sai Qian Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.14608v3.pdf","comment":"24 pages, 12 figures"},{"id":"http://arxiv.org/abs/2306.17484v2","updated":"2024-04-17T16:19:48Z","published":"2023-06-30T08:54:47Z","title":"Landmark Guided Active Exploration with State-specific Balance\n  Coefficient","summary":"  Goal-conditioned hierarchical reinforcement learning (GCHRL) decomposes\nlong-horizon tasks into sub-tasks through a hierarchical framework and it has\ndemonstrated promising results across a variety of domains. However, the\nhigh-level policy's action space is often excessively large, presenting a\nsignificant challenge to effective exploration and resulting in potentially\ninefficient training. In this paper, we design a measure of prospect for\nsub-goals by planning in the goal space based on the goal-conditioned value\nfunction. Building upon the measure of prospect, we propose a landmark-guided\nexploration strategy by integrating the measures of prospect and novelty which\naims to guide the agent to explore efficiently and improve sample efficiency.\nIn order to dynamically consider the impact of prospect and novelty on\nexploration, we introduce a state-specific balance coefficient to balance the\nsignificance of prospect and novelty. The experimental results demonstrate that\nour proposed exploration strategy significantly outperforms the baseline\nmethods across multiple tasks.\n","authors":["Fei Cui","Jiaojiao Fang","Mengke Yang","Guizhong Liu"],"pdf_url":"https://arxiv.org/pdf/2306.17484v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04865v2","updated":"2024-04-17T16:19:29Z","published":"2023-12-08T06:46:18Z","title":"StructComp: Substituting propagation with Structural Compression in\n  Training Graph Contrastive Learning","summary":"  Graph contrastive learning (GCL) has become a powerful tool for learning\ngraph data, but its scalability remains a significant challenge. In this work,\nwe propose a simple yet effective training framework called Structural\nCompression (StructComp) to address this issue. Inspired by a sparse low-rank\napproximation on the diffusion matrix, StructComp trains the encoder with the\ncompressed nodes. This allows the encoder not to perform any message passing\nduring the training stage, and significantly reduces the number of sample pairs\nin the contrastive loss. We theoretically prove that the original GCL loss can\nbe approximated with the contrastive loss computed by StructComp. Moreover,\nStructComp can be regarded as an additional regularization term for GCL models,\nresulting in a more robust encoder. Empirical studies on various datasets show\nthat StructComp greatly reduces the time and memory consumption while improving\nmodel performance compared to the vanilla GCL models and scalable training\nmethods.\n","authors":["Shengzhong Zhang","Wenjie Yang","Xinyuan Cao","Hongwei Zhang","Zengfeng Huang"],"pdf_url":"https://arxiv.org/pdf/2312.04865v2.pdf","comment":"Accepted by ICLR 2024"},{"id":"http://arxiv.org/abs/2312.01201v3","updated":"2024-04-17T16:18:54Z","published":"2023-12-02T18:42:52Z","title":"PAC Privacy Preserving Diffusion Models","summary":"  Data privacy protection is garnering increased attention among researchers.\nDiffusion models (DMs), particularly with strict differential privacy, can\npotentially produce images with both high privacy and visual quality. However,\nchallenges arise such as in ensuring robust protection in privatizing specific\ndata attributes, areas where current models often fall short. To address these\nchallenges, we introduce the PAC Privacy Preserving Diffusion Model, a model\nleverages diffusion principles and ensure Probably Approximately Correct (PAC)\nprivacy. We enhance privacy protection by integrating a private classifier\nguidance into the Langevin Sampling Process. Additionally, recognizing the gap\nin measuring the privacy of models, we have developed a novel metric to gauge\nprivacy levels. Our model, assessed with this new metric and supported by\nGaussian matrix computations for the PAC bound, has shown superior performance\nin privacy protection over existing leading private generative models according\nto benchmark tests.\n","authors":["Qipan Xu","Youlong Ding","Xinxi Zhang","Jie Gao","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2312.01201v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11526v1","updated":"2024-04-17T16:16:50Z","published":"2024-04-17T16:16:50Z","title":"A Comparison of Traditional and Deep Learning Methods for Parameter\n  Estimation of the Ornstein-Uhlenbeck Process","summary":"  We consider the Ornstein-Uhlenbeck (OU) process, a stochastic process widely\nused in finance, physics, and biology. Parameter estimation of the OU process\nis a challenging problem. Thus, we review traditional tracking methods and\ncompare them with novel applications of deep learning to estimate the\nparameters of the OU process. We use a multi-layer perceptron to estimate the\nparameters of the OU process and compare its performance with traditional\nparameter estimation methods, such as the Kalman filter and maximum likelihood\nestimation. We find that the multi-layer perceptron can accurately estimate the\nparameters of the OU process given a large dataset of observed trajectories;\nhowever, traditional parameter estimation methods may be more suitable for\nsmaller datasets.\n","authors":["Jacob Fein-Ashley"],"pdf_url":"https://arxiv.org/pdf/2404.11526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18505v3","updated":"2024-04-17T16:13:54Z","published":"2023-05-29T15:00:09Z","title":"Provable Reward-Agnostic Preference-Based Reinforcement Learning","summary":"  Preference-based Reinforcement Learning (PbRL) is a paradigm in which an RL\nagent learns to optimize a task using pair-wise preference-based feedback over\ntrajectories, rather than explicit reward signals. While PbRL has demonstrated\npractical success in fine-tuning language models, existing theoretical work\nfocuses on regret minimization and fails to capture most of the practical\nframeworks. In this study, we fill in such a gap between theoretical PbRL and\npractical algorithms by proposing a theoretical reward-agnostic PbRL framework\nwhere exploratory trajectories that enable accurate learning of hidden reward\nfunctions are acquired before collecting any human feedback. Theoretical\nanalysis demonstrates that our algorithm requires less human feedback for\nlearning the optimal policy under preference-based models with linear\nparameterization and unknown transitions, compared to the existing theoretical\nliterature. Specifically, our framework can incorporate linear and low-rank\nMDPs with efficient sample complexity. Additionally, we investigate\nreward-agnostic RL with action-based comparison feedback and introduce an\nefficient querying algorithm tailored to this scenario.\n","authors":["Wenhao Zhan","Masatoshi Uehara","Wen Sun","Jason D. Lee"],"pdf_url":"https://arxiv.org/pdf/2305.18505v3.pdf","comment":"ICLR 2024 Spotlight"},{"id":"http://arxiv.org/abs/2404.11509v1","updated":"2024-04-17T16:05:03Z","published":"2024-04-17T16:05:03Z","title":"VC Theory for Inventory Policies","summary":"  Advances in computational power and AI have increased interest in\nreinforcement learning approaches to inventory management. This paper provides\na theoretical foundation for these approaches and investigates the benefits of\nrestricting to policy structures that are well-established by decades of\ninventory theory. In particular, we prove generalization guarantees for\nlearning several well-known classes of inventory policies, including base-stock\nand (s, S) policies, by leveraging the celebrated Vapnik-Chervonenkis (VC)\ntheory. We apply the concepts of the Pseudo-dimension and Fat-shattering\ndimension from VC theory to determine the generalizability of inventory\npolicies, that is, the difference between an inventory policy's performance on\ntraining data and its expected performance on unseen data. We focus on a\nclassical setting without contexts, but allow for an arbitrary distribution\nover demand sequences and do not make any assumptions such as independence over\ntime. We corroborate our supervised learning results using numerical\nsimulations.\n  Managerially, our theory and simulations translate to the following insights.\nFirst, there is a principle of \"learning less is more\" in inventory management:\ndepending on the amount of data available, it may be beneficial to restrict\noneself to a simpler, albeit suboptimal, class of inventory policies to\nminimize overfitting errors. Second, the number of parameters in a policy class\nmay not be the correct measure of overfitting error: in fact, the class of\npolicies defined by T time-varying base-stock levels exhibits a generalization\nerror comparable to that of the two-parameter (s, S) policy class. Finally, our\nresearch suggests situations in which it could be beneficial to incorporate the\nconcepts of base-stock and inventory position into black-box learning machines,\ninstead of having these machines directly learn the order quantity actions.\n","authors":["Yaqi Xie","Will Ma","Linwei Xin"],"pdf_url":"https://arxiv.org/pdf/2404.11509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11492v1","updated":"2024-04-17T15:47:26Z","published":"2024-04-17T15:47:26Z","title":"arcjetCV: an open-source software to analyze material ablation","summary":"  arcjetCV is an open-source Python software designed to automate time-resolved\nmeasurements of heatshield material recession and recession rates from arcjet\ntest video footage. This new automated and accessible capability greatly\nexceeds previous manual extraction methods, enabling rapid and detailed\ncharacterization of material recession for any sample with a profile video.\narcjetCV automates the video segmentation process using machine learning\nmodels, including a one-dimensional (1D) Convolutional Neural Network (CNN) to\ninfer the time-window of interest, a two-dimensional (2D) CNN for image and\nedge segmentation, and a Local Outlier Factor (LOF) for outlier filtering. A\ngraphical user interface (GUI) simplifies the user experience and an\napplication programming interface (API) allows users to call the core functions\nfrom scripts, enabling video batch processing. arcjetCV's capability to measure\ntime-resolved recession in turn enables characterization of non-linear\nprocesses (shrinkage, swelling, melt flows, etc.), contributing to higher\nfidelity validation and improved modeling of heatshield material performance.\nThe source code associated with this article can be found at\nhttps://github.com/magnus-haw/arcjetCV.\n","authors":["Alexandre Quintart","Magnus Haw","Federico Semeraro"],"pdf_url":"https://arxiv.org/pdf/2404.11492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04145v2","updated":"2024-04-17T15:45:11Z","published":"2023-10-06T10:36:28Z","title":"From Zero to Hero: Detecting Leaked Data through Synthetic Data\n  Injection and Model Querying","summary":"  Safeguarding the Intellectual Property (IP) of data has become critically\nimportant as machine learning applications continue to proliferate, and their\nsuccess heavily relies on the quality of training data. While various\nmechanisms exist to secure data during storage, transmission, and consumption,\nfewer studies have been developed to detect whether they are already leaked for\nmodel training without authorization. This issue is particularly challenging\ndue to the absence of information and control over the training process\nconducted by potential attackers.\n  In this paper, we concentrate on the domain of tabular data and introduce a\nnovel methodology, Local Distribution Shifting Synthesis (\\textsc{LDSS}), to\ndetect leaked data that are used to train classification models. The core\nconcept behind \\textsc{LDSS} involves injecting a small volume of synthetic\ndata--characterized by local shifts in class distribution--into the owner's\ndataset. This enables the effective identification of models trained on leaked\ndata through model querying alone, as the synthetic data injection results in a\npronounced disparity in the predictions of models trained on leaked and\nmodified datasets. \\textsc{LDSS} is \\emph{model-oblivious} and hence compatible\nwith a diverse range of classification models. We have conducted extensive\nexperiments on seven types of classification models across five real-world\ndatasets. The comprehensive results affirm the reliability, robustness,\nfidelity, security, and efficiency of \\textsc{LDSS}. Extending \\textsc{LDSS} to\nregression tasks further highlights its versatility and efficacy compared with\nbaseline methods.\n","authors":["Biao Wu","Qiang Huang","Anthony K. H. Tung"],"pdf_url":"https://arxiv.org/pdf/2310.04145v2.pdf","comment":"Accepted and To Appear in VLDB 2024"},{"id":"http://arxiv.org/abs/2404.11483v1","updated":"2024-04-17T15:40:45Z","published":"2024-04-17T15:40:45Z","title":"AgentKit: Flow Engineering with Graphs, not Coding","summary":"  We propose an intuitive LLM prompting framework (AgentKit) for\nmultifunctional agents. AgentKit offers a unified framework for explicitly\nconstructing a complex \"thought process\" from simple natural language prompts.\nThe basic building block in AgentKit is a node, containing a natural language\nprompt for a specific subtask. The user then puts together chains of nodes,\nlike stacking LEGO pieces. The chains of nodes can be designed to explicitly\nenforce a naturally structured \"thought process\". For example, for the task of\nwriting a paper, one may start with the thought process of 1) identify a core\nmessage, 2) identify prior research gaps, etc. The nodes in AgentKit can be\ndesigned and combined in different ways to implement multiple advanced\ncapabilities including on-the-fly hierarchical planning, reflection, and\nlearning from interactions. In addition, due to the modular nature and the\nintuitive design to simulate explicit human thought process, a basic agent\ncould be implemented as simple as a list of prompts for the subtasks and\ntherefore could be designed and tuned by someone without any programming\nexperience. Quantitatively, we show that agents designed through AgentKit\nachieve SOTA performance on WebShop and Crafter. These advances underscore\nAgentKit's potential in making LLM agents effective and accessible for a wider\nrange of applications. https://github.com/holmeswww/AgentKit\n","authors":["Yue Wu","Yewen Fan","So Yeon Min","Shrimai Prabhumoye","Stephen McAleer","Yonatan Bisk","Ruslan Salakhutdinov","Yuanzhi Li","Tom Mitchell"],"pdf_url":"https://arxiv.org/pdf/2404.11483v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11477v1","updated":"2024-04-17T15:32:58Z","published":"2024-04-17T15:32:58Z","title":"Discovering Nuclear Models from Symbolic Machine Learning","summary":"  Numerous phenomenological nuclear models have been proposed to describe\nspecific observables within different regions of the nuclear chart. However,\ndeveloping a unified model that describes the complex behavior of all nuclei\nremains an open challenge. Here, we explore whether novel symbolic Machine\nLearning (ML) can rediscover traditional nuclear physics models or identify\nalternatives with improved simplicity, fidelity, and predictive power. To\naddress this challenge, we developed a Multi-objective Iterated Symbolic\nRegression approach that handles symbolic regressions over multiple target\nobservables, accounts for experimental uncertainties and is robust against\nhigh-dimensional problems. As a proof of principle, we applied this method to\ndescribe the nuclear binding energies and charge radii of light and medium mass\nnuclei. Our approach identified simple analytical relationships based on the\nnumber of protons and neutrons, providing interpretable models with precision\ncomparable to state-of-the-art nuclear models. Additionally, we integrated this\nML-discovered model with an existing complementary model to estimate the limits\nof nuclear stability. These results highlight the potential of symbolic ML to\ndevelop accurate nuclear models and guide our description of complex many-body\nproblems.\n","authors":["Jose M. Munoz","Silviu M. Udrescu","Ronald F. Garcia Ruiz"],"pdf_url":"https://arxiv.org/pdf/2404.11477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11476v1","updated":"2024-04-17T15:32:56Z","published":"2024-04-17T15:32:56Z","title":"Taxonomy to Regulation: A (Geo)Political Taxonomy for AI Risks and\n  Regulatory Measures in the EU AI Act","summary":"  Technological innovations have shown remarkable capabilities to benefit and\nharm society alike. AI constitutes a democratized sophisticated technology\naccessible to large parts of society, including malicious actors. This work\nproposes a taxonomy focusing on on (geo)political risks associated with AI. It\nidentifies 12 risks in total divided into four categories: (1) Geopolitical\nPressures, (2) Malicious Usage, (3) Environmental, Social, and Ethical Risks,\nand (4) Privacy and Trust Violations. Incorporating a regulatory side, this\npaper conducts a policy assessment of the EU AI Act. Adopted in March 2023, the\nlandmark regulation has the potential to have a positive top-down impact\nconcerning AI risk reduction but needs regulatory adjustments to mitigate risks\nmore comprehensively. Regulatory exceptions for open-source models, excessively\nhigh parameters for the classification of GPAI models as a systemic risk, and\nthe exclusion of systems designed exclusively for military purposes from the\nregulation's obligations leave room for future action.\n","authors":["Sinan Arda"],"pdf_url":"https://arxiv.org/pdf/2404.11476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.18062v2","updated":"2024-04-17T15:23:12Z","published":"2023-03-30T12:36:46Z","title":"Solving morphological analogies: from retrieval to generation","summary":"  Analogical inference is a remarkable capability of human reasoning, and has\nbeen used to solve hard reasoning tasks. Analogy based reasoning (AR) has\ngained increasing interest from the artificial intelligence community and has\nshown its potential in multiple machine learning tasks such as classification,\ndecision making and recommendation with competitive results. We propose a deep\nlearning (DL) framework to address and tackle two key tasks in AR: analogy\ndetection and solving. The framework is thoroughly tested on the Siganalogies\ndataset of morphological analogical proportions (APs) between words, and shown\nto outperform symbolic approaches in many languages. Previous work have\nexplored the behavior of the Analogy Neural Network for classification (ANNc)\non analogy detection and of the Analogy Neural Network for retrieval (ANNr) on\nanalogy solving by retrieval, as well as the potential of an autoencoder (AE)\nfor analogy solving by generating the solution word. In this article we\nsummarize these findings and we extend them by combining ANNr and the AE\nembedding model, and checking the performance of ANNc as an retrieval method.\nThe combination of ANNr and AE outperforms the other approaches in almost all\ncases, and ANNc as a retrieval method achieves competitive or better\nperformance than 3CosMul. We conclude with general guidelines on using our\nframework to tackle APs with DL.\n","authors":["Esteban Marquer","Miguel Couceiro"],"pdf_url":"https://arxiv.org/pdf/2303.18062v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11470v1","updated":"2024-04-17T15:23:12Z","published":"2024-04-17T15:23:12Z","title":"A Federated Learning Approach to Privacy Preserving Offensive Language\n  Identification","summary":"  The spread of various forms of offensive speech online is an important\nconcern in social media. While platforms have been investing heavily in ways of\ncoping with this problem, the question of privacy remains largely unaddressed.\nModels trained to detect offensive language on social media are trained and/or\nfine-tuned using large amounts of data often stored in centralized servers.\nSince most social media data originates from end users, we propose a privacy\npreserving decentralized architecture for identifying offensive language online\nby introducing Federated Learning (FL) in the context of offensive language\nidentification. FL is a decentralized architecture that allows multiple models\nto be trained locally without the need for data sharing hence preserving users'\nprivacy. We propose a model fusion approach to perform FL. We trained multiple\ndeep learning models on four publicly available English benchmark datasets\n(AHSD, HASOC, HateXplain, OLID) and evaluated their performance in detail. We\nalso present initial cross-lingual experiments in English and Spanish. We show\nthat the proposed model fusion approach outperforms baselines in all the\ndatasets while preserving privacy.\n","authors":["Marcos Zampieri","Damith Premasiri","Tharindu Ranasinghe"],"pdf_url":"https://arxiv.org/pdf/2404.11470v1.pdf","comment":"Accepted to TRAC 2024 (Fourth Workshop on Threat, Aggression and\n  Cyberbullying) at LREC-COLING 2024 (The 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation)"},{"id":"http://arxiv.org/abs/2307.04569v2","updated":"2024-04-17T15:16:07Z","published":"2023-07-10T14:01:29Z","title":"Interpreting and generalizing deep learning in physics-based problems\n  with functional linear models","summary":"  Although deep learning has achieved remarkable success in various scientific\nmachine learning applications, its opaque nature poses concerns regarding\ninterpretability and generalization capabilities beyond the training data.\nInterpretability is crucial and often desired in modeling physical systems.\nMoreover, acquiring extensive datasets that encompass the entire range of input\nfeatures is challenging in many physics-based learning tasks, leading to\nincreased errors when encountering out-of-distribution (OOD) data. In this\nwork, motivated by the field of functional data analysis (FDA), we propose\ngeneralized functional linear models as an interpretable surrogate for a\ntrained deep learning model. We demonstrate that our model could be trained\neither based on a trained neural network (post-hoc interpretation) or directly\nfrom training data (interpretable operator learning). A library of generalized\nfunctional linear models with different kernel functions is considered and\nsparse regression is used to discover an interpretable surrogate model that\ncould be analytically presented. We present test cases in solid mechanics,\nfluid mechanics, and transport. Our results demonstrate that our model can\nachieve comparable accuracy to deep learning and can improve OOD generalization\nwhile providing more transparency and interpretability. Our study underscores\nthe significance of interpretable representation in scientific machine learning\nand showcases the potential of functional linear models as a tool for\ninterpreting and generalizing deep learning.\n","authors":["Amirhossein Arzani","Lingxiao Yuan","Pania Newell","Bei Wang"],"pdf_url":"https://arxiv.org/pdf/2307.04569v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06590v2","updated":"2024-04-17T15:14:30Z","published":"2024-02-09T18:10:38Z","title":"Predictive representations: building blocks of intelligence","summary":"  Adaptive behavior often requires predicting future events. The theory of\nreinforcement learning prescribes what kinds of predictive representations are\nuseful and how to compute them. This paper integrates these theoretical ideas\nwith work on cognition and neuroscience. We pay special attention to the\nsuccessor representation (SR) and its generalizations, which have been widely\napplied both as engineering tools and models of brain function. This\nconvergence suggests that particular kinds of predictive representations may\nfunction as versatile building blocks of intelligence.\n","authors":["Wilka Carvalho","Momchil S. Tomov","William de Cothi","Caswell Barry","Samuel J. Gershman"],"pdf_url":"https://arxiv.org/pdf/2402.06590v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07219v2","updated":"2024-04-17T15:10:32Z","published":"2024-03-22T12:27:21Z","title":"Leave No One Behind: Online Self-Supervised Self-Distillation for\n  Sequential Recommendation","summary":"  Sequential recommendation methods play a pivotal role in modern\nrecommendation systems. A key challenge lies in accurately modeling user\npreferences in the face of data sparsity. To tackle this challenge, recent\nmethods leverage contrastive learning (CL) to derive self-supervision signals\nby maximizing the mutual information of two augmented views of the original\nuser behavior sequence. Despite their effectiveness, CL-based methods encounter\na limitation in fully exploiting self-supervision signals for users with\nlimited behavior data, as users with extensive behaviors naturally offer more\ninformation. To address this problem, we introduce a novel learning paradigm,\nnamed Online Self-Supervised Self-distillation for Sequential Recommendation\n($S^4$Rec), effectively bridging the gap between self-supervised learning and\nself-distillation methods. Specifically, we employ online clustering to\nproficiently group users by their distinct latent intents. Additionally, an\nadversarial learning strategy is utilized to ensure that the clustering\nprocedure is not affected by the behavior length factor. Subsequently, we\nemploy self-distillation to facilitate the transfer of knowledge from users\nwith extensive behaviors (teachers) to users with limited behaviors (students).\nExperiments conducted on four real-world datasets validate the effectiveness of\nthe proposed method.\n","authors":["Shaowei Wei","Zhengwei Wu","Xin Li","Qintong Wu","Zhiqiang Zhang","Jun Zhou","Lihong Gu","Jinjie Gu"],"pdf_url":"https://arxiv.org/pdf/2404.07219v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11461v1","updated":"2024-04-17T15:09:31Z","published":"2024-04-17T15:09:31Z","title":"Using Game Engines and Machine Learning to Create Synthetic Satellite\n  Imagery for a Tabletop Verification Exercise","summary":"  Satellite imagery is regarded as a great opportunity for citizen-based\nmonitoring of activities of interest. Relevant imagery may however not be\navailable at sufficiently high resolution, quality, or cadence -- let alone be\nuniformly accessible to open-source analysts. This limits an assessment of the\ntrue long-term potential of citizen-based monitoring of nuclear activities\nusing publicly available satellite imagery. In this article, we demonstrate how\nmodern game engines combined with advanced machine-learning techniques can be\nused to generate synthetic imagery of sites of interest with the ability to\nchoose relevant parameters upon request; these include time of day, cloud\ncover, season, or level of activity onsite. At the same time, resolution and\noff-nadir angle can be adjusted to simulate different characteristics of the\nsatellite. While there are several possible use-cases for synthetic imagery,\nhere we focus on its usefulness to support tabletop exercises in which simple\nmonitoring scenarios can be examined to better understand verification\ncapabilities enabled by new satellite constellations and very short revisit\ntimes.\n","authors":["Johannes Hoster","Sara Al-Sayed","Felix Biessmann","Alexander Glaser","Kristian Hildebrand","Igor Moric","Tuong Vy Nguyen"],"pdf_url":"https://arxiv.org/pdf/2404.11461v1.pdf","comment":"Annual Meeting of the Institute of Nuclear Materials Management\n  (INMM), Vienna"},{"id":"http://arxiv.org/abs/2402.12219v2","updated":"2024-04-17T15:03:19Z","published":"2024-02-19T15:21:58Z","title":"Reformatted Alignment","summary":"  The quality of finetuning data is crucial for aligning large language models\n(LLMs) with human values. Current methods to improve data quality are either\nlabor-intensive or prone to factual errors caused by LLM hallucinations. This\npaper explores elevating the quality of existing instruction data to better\nalign with human values, introducing a simple and effective approach named\nReAlign, which reformats the responses of instruction data into a format that\nbetter aligns with pre-established criteria and the collated evidence. This\napproach minimizes human annotation, hallucination, and the difficulty in\nscaling, remaining orthogonal to existing alignment techniques. Experimentally,\nReAlign significantly boosts the general alignment ability, math reasoning,\nfactuality, and readability of the LLMs.\n  Encouragingly, without introducing any additional data or advanced training\ntechniques, and merely by reformatting the response, LLaMA-2-13B's mathematical\nreasoning ability on GSM8K can be improved from 46.77% to 56.63% in accuracy.\nAdditionally, a mere 5% of ReAlign data yields a 67% boost in general alignment\nability measured by the Alpaca dataset. This work highlights the need for\nfurther research into the science and mechanistic interpretability of LLMs. We\nhave made the associated code and data publicly accessible to support future\nstudies at https://github.com/GAIR-NLP/ReAlign.\n","authors":["Run-Ze Fan","Xuefeng Li","Haoyang Zou","Junlong Li","Shwai He","Ethan Chern","Jiewen Hu","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2402.12219v2.pdf","comment":"Homepage: https://gair-nlp.github.io/ReAlign/"},{"id":"http://arxiv.org/abs/2403.18807v4","updated":"2024-04-17T14:59:51Z","published":"2024-03-27T17:53:30Z","title":"ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth\n  Estimation","summary":"  In the absence of parallax cues, a learning-based single image depth\nestimation (SIDE) model relies heavily on shading and contextual cues in the\nimage. While this simplicity is attractive, it is necessary to train such\nmodels on large and varied datasets, which are difficult to capture. It has\nbeen shown that using embeddings from pre-trained foundational models, such as\nCLIP, improves zero shot transfer in several applications. Taking inspiration\nfrom this, in our paper we explore the use of global image priors generated\nfrom a pre-trained ViT model to provide more detailed contextual information.\nWe argue that the embedding vector from a ViT model, pre-trained on a large\ndataset, captures greater relevant information for SIDE than the usual route of\ngenerating pseudo image captions, followed by CLIP based text embeddings. Based\non this idea, we propose a new SIDE model using a diffusion backbone which is\nconditioned on ViT embeddings. Our proposed design establishes a new\nstate-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of\n0.059 (14% improvement) compared to 0.069 by the current SOTA (VPD). And on\nKITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to\n0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model\ntrained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%)\nover NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%,\n18%, 45%, 9%) by ZoeDepth. The project page is available at\nhttps://ecodepth-iitd.github.io\n","authors":["Suraj Patni","Aradhye Agarwal","Chetan Arora"],"pdf_url":"https://arxiv.org/pdf/2403.18807v4.pdf","comment":"IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n  2024"},{"id":"http://arxiv.org/abs/2404.11449v1","updated":"2024-04-17T14:55:27Z","published":"2024-04-17T14:55:27Z","title":"AI-Enhanced Cognitive Behavioral Therapy: Deep Learning and Large\n  Language Models for Extracting Cognitive Pathways from Social Media Texts","summary":"  Cognitive Behavioral Therapy (CBT) is an effective technique for addressing\nthe irrational thoughts stemming from mental illnesses, but it necessitates\nprecise identification of cognitive pathways to be successfully implemented in\npatient care. In current society, individuals frequently express negative\nemotions on social media on specific topics, often exhibiting cognitive\ndistortions, including suicidal behaviors in extreme cases. Yet, there is a\nnotable absence of methodologies for analyzing cognitive pathways that could\naid psychotherapists in conducting effective interventions online. In this\nstudy, we gathered data from social media and established the task of\nextracting cognitive pathways, annotating the data based on a cognitive\ntheoretical framework. We initially categorized the task of extracting\ncognitive pathways as a hierarchical text classification with four main\ncategories and nineteen subcategories. Following this, we structured a text\nsummarization task to help psychotherapists quickly grasp the essential\ninformation. Our experiments evaluate the performance of deep learning and\nlarge language models (LLMs) on these tasks. The results demonstrate that our\ndeep learning method achieved a micro-F1 score of 62.34% in the hierarchical\ntext classification task. Meanwhile, in the text summarization task, GPT-4\nattained a Rouge-1 score of 54.92 and a Rouge-2 score of 30.86, surpassing the\nexperimental deep learning model's performance. However, it may suffer from an\nissue of hallucination. We have made all models and codes publicly available to\nsupport further research in this field.\n","authors":["Meng Jiang","Yi Jing Yu","Qing Zhao","Jianqiang Li","Changwei Song","Hongzhi Qi","Wei Zhai","Dan Luo","Xiaoqin Wang","Guanghui Fu","Bing Xiang Yang"],"pdf_url":"https://arxiv.org/pdf/2404.11449v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10761v2","updated":"2024-04-17T14:38:04Z","published":"2024-04-16T17:41:17Z","title":"TorchSurv: A Lightweight Package for Deep Survival Analysis","summary":"  TorchSurv is a Python package that serves as a companion tool to perform deep\nsurvival modeling within the PyTorch environment. Unlike existing libraries\nthat impose specific parametric forms, TorchSurv enables the use of custom\nPyTorch-based deep survival models. With its lightweight design, minimal input\nrequirements, full PyTorch backend, and freedom from restrictive survival model\nparameterizations, TorchSurv facilitates efficient deep survival model\nimplementation and is particularly beneficial for high-dimensional and complex\ninput data scenarios.\n","authors":["M√©lodie Monod","Peter Krusche","Qian Cao","Berkman Sahiner","Nicholas Petrick","David Ohlssen","Thibaud Coroller"],"pdf_url":"https://arxiv.org/pdf/2404.10761v2.pdf","comment":"https://opensource.nibr.com/torchsurv/"},{"id":"http://arxiv.org/abs/2404.10179v2","updated":"2024-04-17T14:36:27Z","published":"2024-03-13T17:50:32Z","title":"Scaling Instructable Agents Across Many Simulated Worlds","summary":"  Building embodied AI systems that can follow arbitrary language instructions\nin any 3D environment is a key challenge for creating general AI. Accomplishing\nthis goal requires learning to ground language in perception and embodied\nactions, in order to accomplish complex tasks. The Scalable, Instructable,\nMultiworld Agent (SIMA) project tackles this by training agents to follow\nfree-form instructions across a diverse range of virtual 3D environments,\nincluding curated research environments as well as open-ended, commercial video\ngames. Our goal is to develop an instructable agent that can accomplish\nanything a human can do in any simulated 3D environment. Our approach focuses\non language-driven generality while imposing minimal assumptions. Our agents\ninteract with environments in real-time using a generic, human-like interface:\nthe inputs are image observations and language instructions and the outputs are\nkeyboard-and-mouse actions. This general approach is challenging, but it allows\nagents to ground language across many visually complex and semantically rich\nenvironments while also allowing us to readily run agents in new environments.\nIn this paper we describe our motivation and goal, the initial progress we have\nmade, and promising preliminary results on several diverse research\nenvironments and a variety of commercial video games.\n","authors":[" SIMA Team","Maria Abi Raad","Arun Ahuja","Catarina Barros","Frederic Besse","Andrew Bolt","Adrian Bolton","Bethanie Brownfield","Gavin Buttimore","Max Cant","Sarah Chakera","Stephanie C. Y. Chan","Jeff Clune","Adrian Collister","Vikki Copeman","Alex Cullum","Ishita Dasgupta","Dario de Cesare","Julia Di Trapani","Yani Donchev","Emma Dunleavy","Martin Engelcke","Ryan Faulkner","Frankie Garcia","Charles Gbadamosi","Zhitao Gong","Lucy Gonzales","Kshitij Gupta","Karol Gregor","Arne Olav Hallingstad","Tim Harley","Sam Haves","Felix Hill","Ed Hirst","Drew A. Hudson","Jony Hudson","Steph Hughes-Fitt","Danilo J. Rezende","Mimi Jasarevic","Laura Kampis","Rosemary Ke","Thomas Keck","Junkyung Kim","Oscar Knagg","Kavya Kopparapu","Andrew Lampinen","Shane Legg","Alexander Lerchner","Marjorie Limont","Yulan Liu","Maria Loks-Thompson","Joseph Marino","Kathryn Martin Cussons","Loic Matthey","Siobhan Mcloughlin","Piermaria Mendolicchio","Hamza Merzic","Anna Mitenkova","Alexandre Moufarek","Valeria Oliveira","Yanko Oliveira","Hannah Openshaw","Renke Pan","Aneesh Pappu","Alex Platonov","Ollie Purkiss","David Reichert","John Reid","Pierre Harvey Richemond","Tyson Roberts","Giles Ruscoe","Jaume Sanchez Elias","Tasha Sandars","Daniel P. Sawyer","Tim Scholtes","Guy Simmons","Daniel Slater","Hubert Soyer","Heiko Strathmann","Peter Stys","Allison C. Tam","Denis Teplyashin","Tayfun Terzi","Davide Vercelli","Bojan Vujatovic","Marcus Wainwright","Jane X. Wang","Zhengdong Wang","Daan Wierstra","Duncan Williams","Nathaniel Wong","Sarah York","Nick Young"],"pdf_url":"https://arxiv.org/pdf/2404.10179v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11428v1","updated":"2024-04-17T14:34:35Z","published":"2024-04-17T14:34:35Z","title":"Explainable Lung Disease Classification from Chest X-Ray Images\n  Utilizing Deep Learning and XAI","summary":"  Lung diseases remain a critical global health concern, and it's crucial to\nhave accurate and quick ways to diagnose them. This work focuses on classifying\ndifferent lung diseases into five groups: viral pneumonia, bacterial pneumonia,\nCOVID, tuberculosis, and normal lungs. Employing advanced deep learning\ntechniques, we explore a diverse range of models including CNN, hybrid models,\nensembles, transformers, and Big Transfer. The research encompasses\ncomprehensive methodologies such as hyperparameter tuning, stratified k-fold\ncross-validation, and transfer learning with fine-tuning.Remarkably, our\nfindings reveal that the Xception model, fine-tuned through 5-fold\ncross-validation, achieves the highest accuracy of 96.21\\%. This success shows\nthat our methods work well in accurately identifying different lung diseases.\nThe exploration of explainable artificial intelligence (XAI) methodologies\nfurther enhances our understanding of the decision-making processes employed by\nthese models, contributing to increased trust in their clinical applications.\n","authors":["Tanzina Taher Ifty","Saleh Ahmed Shafin","Shoeb Mohammad Shahriar","Tashfia Towhid"],"pdf_url":"https://arxiv.org/pdf/2404.11428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11422v1","updated":"2024-04-17T14:27:45Z","published":"2024-04-17T14:27:45Z","title":"Short-term wind speed forecasting model based on an attention-gated\n  recurrent neural network and error correction strategy","summary":"  The accurate wind speed series forecast is very pivotal to security of grid\ndispatching and the application of wind power. Nevertheless, on account of\ntheir nonlinear and non-stationary nature, their short-term forecast is\nextremely challenging. Therefore, this dissertation raises one short-term wind\nspeed forecast pattern on the foundation of attention with an improved gated\nrecurrent neural network (AtGRU) and a tactic of error correction. That model\nuses the AtGRU model as the preliminary predictor and the GRU model as the\nerror corrector. At the beginning, SSA (singular spectrum analysis) is employed\nin previous wind speed series for lessening the noise. Subsequently, historical\nwind speed series is going to be used for the predictor training. During this\nprocess, the prediction can have certain errors. The sequence of these errors\nprocessed by variational modal decomposition (VMD) is used to train the\ncorrector of error. The eventual forecast consequence is just the sum of\npredictor forecast and error corrector. The proposed SSA-AtGRU-VMD-GRU model\noutperforms the compared models in three case studies on Woodburn, St. Thomas,\nand Santa Cruz. It is indicated that the model evidently enhances the\ncorrection of the wind speed forecast.\n","authors":["Haojian Huang"],"pdf_url":"https://arxiv.org/pdf/2404.11422v1.pdf","comment":"23 pages, 11 figures, 6 tables, Technical Report"},{"id":"http://arxiv.org/abs/2210.07887v2","updated":"2024-04-17T14:20:02Z","published":"2022-10-14T15:13:10Z","title":"E2R: a Hierarchical-Learning inspired Novelty-Search method to generate\n  diverse repertoires of grasping trajectories","summary":"  Robotics grasping refers to the task of making a robotic system pick an\nobject by applying forces and torques on its surface. Despite the recent\nadvances in data-driven approaches, grasping remains an unsolved problem. Most\nof the works on this task are relying on priors and heavy constraints to avoid\nthe exploration problem. Novelty Search (NS) refers to evolutionary algorithms\nthat replace selection of best performing individuals with selection of the\nmost novel ones. Such methods have already shown promising results on hard\nexploration problems. In this work, we introduce a new NS-based method that can\ngenerate large datasets of grasping trajectories in a platform-agnostic manner.\nInspired by the hierarchical learning paradigm, our method decouples approach\nand prehension to make the behavioral space smoother. Experiments conducted on\n3 different robot-gripper setups and on several standard objects shows that our\nmethod outperforms state-of-the-art for generating diverse repertoire of\ngrasping trajectories, getting a higher successful run ratio, as well as a\nbetter diversity for both approach and prehension. Some of the generated\nsolutions have been successfully deployed on a real robot, showing the\nexploitability of the obtained repertoires.\n","authors":["Johann Huber","Oumar Sane","Alex Coninx","Faiz Ben Amar","Stephane Doncieux"],"pdf_url":"https://arxiv.org/pdf/2210.07887v2.pdf","comment":"7 pages, 6 figures. Preprint version"},{"id":"http://arxiv.org/abs/2305.13293v2","updated":"2024-04-17T14:13:52Z","published":"2023-05-22T17:51:35Z","title":"Time Fairness in Online Knapsack Problems","summary":"  The online knapsack problem is a classic problem in the field of online\nalgorithms. Its canonical version asks how to pack items of different values\nand weights arriving online into a capacity-limited knapsack so as to maximize\nthe total value of the admitted items. Although optimal competitive algorithms\nare known for this problem, they may be fundamentally unfair, i.e., individual\nitems may be treated inequitably in different ways. We formalize a\npractically-relevant notion of time fairness which effectively models a trade\noff between static and dynamic pricing in a motivating application such as\ncloud resource allocation, and show that existing algorithms perform poorly\nunder this metric. We propose a parameterized deterministic algorithm where the\nparameter precisely captures the Pareto-optimal trade-off between fairness\n(static pricing) and competitiveness (dynamic pricing). We show that\nrandomization is theoretically powerful enough to be simultaneously competitive\nand fair; however, it does not work well in experiments. To further improve the\ntrade-off between fairness and competitiveness, we develop a nearly-optimal\nlearning-augmented algorithm which is fair, consistent, and robust\n(competitive), showing substantial performance improvements in numerical\nexperiments.\n","authors":["Adam Lechowicz","Rik Sengupta","Bo Sun","Shahin Kamali","Mohammad Hajiesmaili"],"pdf_url":"https://arxiv.org/pdf/2305.13293v2.pdf","comment":"Accepted to ICLR 2024. 26 pages, 5 figures"},{"id":"http://arxiv.org/abs/2311.07620v3","updated":"2024-04-17T14:09:52Z","published":"2023-11-12T17:56:39Z","title":"EPIM: Efficient Processing-In-Memory Accelerators based on Epitome","summary":"  The utilization of large-scale neural networks on Processing-In-Memory (PIM)\naccelerators encounters challenges due to constrained on-chip memory capacity.\nTo tackle this issue, current works explore model compression algorithms to\nreduce the size of Convolutional Neural Networks (CNNs). Most of these\nalgorithms either aim to represent neural operators with reduced-size\nparameters (e.g., quantization) or search for the best combinations of neural\noperators (e.g., neural architecture search). Designing neural operators to\nalign with PIM accelerators' specifications is an area that warrants further\nstudy. In this paper, we introduce the Epitome, a lightweight neural operator\noffering convolution-like functionality, to craft memory-efficient CNN\noperators for PIM accelerators (EPIM). On the software side, we evaluate\nepitomes' latency and energy on PIM accelerators and introduce a PIM-aware\nlayer-wise design method to enhance their hardware efficiency. We apply\nepitome-aware quantization to further reduce the size of epitomes. On the\nhardware side, we modify the datapath of current PIM accelerators to\naccommodate epitomes and implement a feature map reuse technique to reduce\ncomputation cost. Experimental results reveal that our 3-bit quantized\nEPIM-ResNet50 attains 71.59% top-1 accuracy on ImageNet, reducing crossbar\nareas by 30.65 times. EPIM surpasses the state-of-the-art pruning methods on\nPIM.\n","authors":["Chenyu Wang","Zhen Dong","Daquan Zhou","Zhenhua Zhu","Yu Wang","Jiashi Feng","Kurt Keutzer"],"pdf_url":"https://arxiv.org/pdf/2311.07620v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11384v1","updated":"2024-04-17T13:44:29Z","published":"2024-04-17T13:44:29Z","title":"Exploring Key Point Analysis with Pairwise Generation and Graph\n  Partitioning","summary":"  Key Point Analysis (KPA), the summarization of multiple arguments into a\nconcise collection of key points, continues to be a significant and unresolved\nissue within the field of argument mining. Existing models adapt a two-stage\npipeline of clustering arguments or generating key points for argument\nclusters. This approach rely on semantic similarity instead of measuring the\nexistence of shared key points among arguments. Additionally, it only models\nthe intra-cluster relationship among arguments, disregarding the inter-cluster\nrelationship between arguments that do not share key points. To address these\nlimitations, we propose a novel approach for KPA with pairwise generation and\ngraph partitioning. Our objective is to train a generative model that can\nsimultaneously provide a score indicating the presence of shared key point\nbetween a pair of arguments and generate the shared key point. Subsequently, to\nmap generated redundant key points to a concise set of key points, we proceed\nto construct an arguments graph by considering the arguments as vertices, the\ngenerated key points as edges, and the scores as edge weights. We then propose\na graph partitioning algorithm to partition all arguments sharing the same key\npoints to the same subgraph. Notably, our experimental findings demonstrate\nthat our proposed model surpasses previous models when evaluated on both the\nArgKP and QAM datasets.\n","authors":["Xiao Li","Yong Jiang","Shen Huang","Pengjun Xie","Gong Cheng","Fei Huang"],"pdf_url":"https://arxiv.org/pdf/2404.11384v1.pdf","comment":"11 pages, 4 figures, 4 tables. Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2404.11374v1","updated":"2024-04-17T13:32:05Z","published":"2024-04-17T13:32:05Z","title":"Tensor Factorisation for Polypharmacy Side Effect Prediction","summary":"  Adverse reactions caused by drug combinations are an increasingly common\nphenomenon, making their accurate prediction an important challenge in modern\nmedicine. However, the polynomial nature of this problem renders lab-based\nidentification of adverse reactions insufficient. Dozens of computational\napproaches have therefore been proposed for the task in recent years, with\nvarying degrees of success. One group of methods that has seemingly been\nunder-utilised in this area is tensor factorisation, despite their clear\napplicability to this type of data. In this work, we apply three such models to\na benchmark dataset in order to compare them against established techniques. We\nfind, in contrast to previous reports, that for this task tensor factorisation\nmodels are competitive with state-of-the-art graph neural network models and we\nrecommend that future work in this field considers cheaper methods with linear\ncomplexity before running costly deep learning processes.\n","authors":["Oliver Lloyd","Yi Liu","Tom R. Gaunt"],"pdf_url":"https://arxiv.org/pdf/2404.11374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11354v1","updated":"2024-04-17T13:09:33Z","published":"2024-04-17T13:09:33Z","title":"Distributed Fractional Bayesian Learning for Adaptive Optimization","summary":"  This paper considers a distributed adaptive optimization problem, where all\nagents only have access to their local cost functions with a common unknown\nparameter, whereas they mean to collaboratively estimate the true parameter and\nfind the optimal solution over a connected network. A general mathematical\nframework for such a problem has not been studied yet. We aim to provide\nvaluable insights for addressing parameter uncertainty in distributed\noptimization problems and simultaneously find the optimal solution. Thus, we\npropose a novel Prediction while Optimization scheme, which utilizes\ndistributed fractional Bayesian learning through weighted averaging on the\nlog-beliefs to update the beliefs of unknown parameters, and distributed\ngradient descent for renewing the estimation of the optimal solution. Then\nunder suitable assumptions, we prove that all agents' beliefs and decision\nvariables converge almost surely to the true parameter and the optimal solution\nunder the true parameter, respectively. We further establish a sublinear\nconvergence rate for the belief sequence. Finally, numerical experiments are\nimplemented to corroborate the theoretical analysis.\n","authors":["Yaqun Yang","Jinlong Lei","Guanghui Wen","Yiguang Hong"],"pdf_url":"https://arxiv.org/pdf/2404.11354v1.pdf","comment":"16 pages, 6 figures"},{"id":"http://arxiv.org/abs/2404.11350v1","updated":"2024-04-17T13:08:26Z","published":"2024-04-17T13:08:26Z","title":"Calibrating Bayesian Learning via Regularization, Confidence\n  Minimization, and Selective Inference","summary":"  The application of artificial intelligence (AI) models in fields such as\nengineering is limited by the known difficulty of quantifying the reliability\nof an AI's decision. A well-calibrated AI model must correctly report its\naccuracy on in-distribution (ID) inputs, while also enabling the detection of\nout-of-distribution (OOD) inputs. A conventional approach to improve\ncalibration is the application of Bayesian ensembling. However, owing to\ncomputational limitations and model misspecification, practical ensembling\nstrategies do not necessarily enhance calibration. This paper proposes an\nextension of variational inference (VI)-based Bayesian learning that integrates\ncalibration regularization for improved ID performance, confidence minimization\nfor OOD detection, and selective calibration to ensure a synergistic use of\ncalibration regularization and confidence minimization. The scheme is\nconstructed successively by first introducing calibration-regularized Bayesian\nlearning (CBNN), then incorporating out-of-distribution confidence minimization\n(OCM) to yield CBNN-OCM, and finally integrating also selective calibration to\nproduce selective CBNN-OCM (SCBNN-OCM). Selective calibration rejects inputs\nfor which the calibration performance is expected to be insufficient. Numerical\nresults illustrate the trade-offs between ID accuracy, ID calibration, and OOD\ncalibration attained by both frequentist and Bayesian learning methods. Among\nthe main conclusions, SCBNN-OCM is seen to achieve best ID and OOD performance\nas compared to existing state-of-the-art approaches at the cost of rejecting a\nsufficiently large number of inputs.\n","authors":["Jiayi Huang","Sangwoo Park","Osvaldo Simeone"],"pdf_url":"https://arxiv.org/pdf/2404.11350v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2404.11341v1","updated":"2024-04-17T13:00:52Z","published":"2024-04-17T13:00:52Z","title":"The Causal Chambers: Real Physical Systems as a Testbed for AI\n  Methodology","summary":"  In some fields of AI, machine learning and statistics, the validation of new\nmethods and algorithms is often hindered by the scarcity of suitable real-world\ndatasets. Researchers must often turn to simulated data, which yields limited\ninformation about the applicability of the proposed methods to real problems.\nAs a step forward, we have constructed two devices that allow us to quickly and\ninexpensively produce large datasets from non-trivial but well-understood\nphysical systems. The devices, which we call causal chambers, are\ncomputer-controlled laboratories that allow us to manipulate and measure an\narray of variables from these physical systems, providing a rich testbed for\nalgorithms from a variety of fields. We illustrate potential applications\nthrough a series of case studies in fields such as causal discovery,\nout-of-distribution generalization, change point detection, independent\ncomponent analysis, and symbolic regression. For applications to causal\ninference, the chambers allow us to carefully perform interventions. We also\nprovide and empirically validate a causal model of each chamber, which can be\nused as ground truth for different tasks. All hardware and software is made\nopen source, and the datasets are publicly available at causalchamber.org or\nthrough the Python package causalchamber.\n","authors":["Juan L. Gamella","Jonas Peters","Peter B√ºhlmann"],"pdf_url":"https://arxiv.org/pdf/2404.11341v1.pdf","comment":"38 pages, 17 figures"},{"id":"http://arxiv.org/abs/2404.11335v1","updated":"2024-04-17T12:53:45Z","published":"2024-04-17T12:53:45Z","title":"SoccerNet Game State Reconstruction: End-to-End Athlete Tracking and\n  Identification on a Minimap","summary":"  Tracking and identifying athletes on the pitch holds a central role in\ncollecting essential insights from the game, such as estimating the total\ndistance covered by players or understanding team tactics. This tracking and\nidentification process is crucial for reconstructing the game state, defined by\nthe athletes' positions and identities on a 2D top-view of the pitch, (i.e. a\nminimap). However, reconstructing the game state from videos captured by a\nsingle camera is challenging. It requires understanding the position of the\nathletes and the viewpoint of the camera to localize and identify players\nwithin the field. In this work, we formalize the task of Game State\nReconstruction and introduce SoccerNet-GSR, a novel Game State Reconstruction\ndataset focusing on football videos. SoccerNet-GSR is composed of 200 video\nsequences of 30 seconds, annotated with 9.37 million line points for pitch\nlocalization and camera calibration, as well as over 2.36 million athlete\npositions on the pitch with their respective role, team, and jersey number.\nFurthermore, we introduce GS-HOTA, a novel metric to evaluate game state\nreconstruction methods. Finally, we propose and release an end-to-end baseline\nfor game state reconstruction, bootstrapping the research on this task. Our\nexperiments show that GSR is a challenging novel task, which opens the field\nfor future research. Our dataset and codebase are publicly available at\nhttps://github.com/SoccerNet/sn-gamestate.\n","authors":["Vladimir Somers","Victor Joos","Anthony Cioppa","Silvio Giancola","Seyed Abolfazl Ghasemzadeh","Floriane Magera","Baptiste Standaert","Amir Mohammad Mansourian","Xin Zhou","Shohreh Kasaei","Bernard Ghanem","Alexandre Alahi","Marc Van Droogenbroeck","Christophe De Vleeschouwer"],"pdf_url":"https://arxiv.org/pdf/2404.11335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.19226v2","updated":"2024-04-17T12:52:07Z","published":"2024-02-29T14:58:15Z","title":"Investigating Gender Fairness in Machine Learning-driven Personalized\n  Care for Chronic Pain","summary":"  Chronic pain significantly diminishes the quality of life for millions\nworldwide. While psychoeducation and therapy can improve pain outcomes, many\nindividuals experiencing pain lack access to evidence-based treatments or fail\nto complete the necessary number of sessions to achieve benefit. Reinforcement\nlearning (RL) shows potential in tailoring personalized pain management\ninterventions according to patients' individual needs while ensuring the\nefficient use of scarce clinical resources. However, clinicians, patients, and\nhealthcare decision-makers are concerned that RL solutions could exacerbate\ndisparities associated with patient characteristics like race or gender. In\nthis article, we study gender fairness in personalized pain care\nrecommendations using a real-world application of reinforcement learning\n(Piette et al., 2022a). Here, adhering to gender fairness translates to minimal\nor no disparity in the utility received by subpopulations as defined by gender.\nWe investigate whether the selection of relevant patient information (referred\nto as features) used to assist decision-making affects gender fairness. Our\nexperiments, conducted using real-world data Piette, 2022), indicate that\nincluded features can impact gender fairness. Moreover, we propose an RL\nsolution, NestedRecommendation, that demonstrates the ability: i) to adaptively\nlearn to select the features that optimize for utility and fairness, and ii) to\naccelerate feature selection and in turn, improve pain care recommendations\nfrom early on, by leveraging clinicians' domain expertise.\n","authors":["Pratik Gajane","Sean Newman","Mykola Pechenizkiy","John D. Piette"],"pdf_url":"https://arxiv.org/pdf/2402.19226v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11330v1","updated":"2024-04-17T12:45:59Z","published":"2024-04-17T12:45:59Z","title":"Toward Understanding the Disagreement Problem in Neural Network Feature\n  Attribution","summary":"  In recent years, neural networks have demonstrated their remarkable ability\nto discern intricate patterns and relationships from raw data. However,\nunderstanding the inner workings of these black box models remains challenging,\nyet crucial for high-stake decisions. Among the prominent approaches for\nexplaining these black boxes are feature attribution methods, which assign\nrelevance or contribution scores to each input variable for a model prediction.\nDespite the plethora of proposed techniques, ranging from gradient-based to\nbackpropagation-based methods, a significant debate persists about which method\nto use. Various evaluation metrics have been proposed to assess the\ntrustworthiness or robustness of their results. However, current research\nhighlights disagreement among state-of-the-art methods in their explanations.\nOur work addresses this confusion by investigating the explanations'\nfundamental and distributional behavior. Additionally, through a comprehensive\nsimulation study, we illustrate the impact of common scaling and encoding\ntechniques on the explanation quality, assess their efficacy across different\neffect sizes, and demonstrate the origin of inconsistency in rank-based\nevaluation metrics.\n","authors":["Niklas Koenen","Marvin N. Wright"],"pdf_url":"https://arxiv.org/pdf/2404.11330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11930v2","updated":"2024-04-17T12:27:25Z","published":"2023-09-21T09:44:39Z","title":"Bridging the Gap: Learning Pace Synchronization for Open-World\n  Semi-Supervised Learning","summary":"  In open-world semi-supervised learning, a machine learning model is tasked\nwith uncovering novel categories from unlabeled data while maintaining\nperformance on seen categories from labeled data. The central challenge is the\nsubstantial learning gap between seen and novel categories, as the model learns\nthe former faster due to accurate supervisory information. Moreover, capturing\nthe semantics of unlabeled novel category samples is also challenging due to\nthe missing label information. To address the above issues, we introduce 1) the\nadaptive synchronizing marginal loss which imposes class-specific negative\nmargins to alleviate the model bias towards seen classes, and 2) the\npseudo-label contrastive clustering which exploits pseudo-labels predicted by\nthe model to group unlabeled data from the same category together in the output\nspace. Extensive experiments on benchmark datasets demonstrate that previous\napproaches may significantly hinder novel class learning, whereas our method\nstrikingly balances the learning pace between seen and novel classes, achieving\na remarkable 3% average accuracy increase on the ImageNet dataset. Importantly,\nwe find that fine-tuning the self-supervised pre-trained model significantly\nboosts the performance, which is overlooked in prior literature. Our code is\navailable at https://github.com/yebo0216best/LPS-main.\n","authors":["Bo Ye","Kai Gan","Tong Wei","Min-Ling Zhang"],"pdf_url":"https://arxiv.org/pdf/2309.11930v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11311v1","updated":"2024-04-17T12:22:54Z","published":"2024-04-17T12:22:54Z","title":"Use of Parallel Explanatory Models to Enhance Transparency of Neural\n  Network Configurations for Cell Degradation Detection","summary":"  In a previous paper, we have shown that a recurrent neural network (RNN) can\nbe used to detect cellular network radio signal degradations accurately. We\nunexpectedly found, though, that accuracy gains diminished as we added layers\nto the RNN. To investigate this, in this paper, we build a parallel model to\nilluminate and understand the internal operation of neural networks, such as\nthe RNN, which store their internal state in order to process sequential\ninputs. This model is widely applicable in that it can be used with any input\ndomain where the inputs can be represented by a Gaussian mixture. By looking at\nthe RNN processing from a probability density function perspective, we are able\nto show how each layer of the RNN transforms the input distributions to\nincrease detection accuracy. At the same time we also discover a side effect\nacting to limit the improvement in accuracy. To demonstrate the fidelity of the\nmodel we validate it against each stage of RNN processing as well as the output\npredictions. As a result, we have been able to explain the reasons for the RNN\nperformance limits with useful insights for future designs for RNNs and similar\ntypes of neural network.\n","authors":["David Mulvey","Chuan Heng Foh","Muhammad Ali Imran","Rahim Tafazolli"],"pdf_url":"https://arxiv.org/pdf/2404.11311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11302v1","updated":"2024-04-17T12:13:18Z","published":"2024-04-17T12:13:18Z","title":"A Semantic Segmentation-guided Approach for Ground-to-Aerial Image\n  Matching","summary":"  Nowadays the accurate geo-localization of ground-view images has an important\nrole across domains as diverse as journalism, forensics analysis, transports,\nand Earth Observation. This work addresses the problem of matching a query\nground-view image with the corresponding satellite image without GPS data. This\nis done by comparing the features from a ground-view image and a satellite one,\ninnovatively leveraging the corresponding latter's segmentation mask through a\nthree-stream Siamese-like network. The proposed method, Semantic Align Net\n(SAN), focuses on limited Field-of-View (FoV) and ground panorama images\n(images with a FoV of 360{\\deg}). The novelty lies in the fusion of satellite\nimages in combination with their semantic segmentation masks, aimed at ensuring\nthat the model can extract useful features and focus on the significant parts\nof the images. This work shows how SAN through semantic analysis of images\nimproves the performance on the unlabelled CVUSA dataset for all the tested\nFoVs.\n","authors":["Francesco Pro","Nikolaos Dionelis","Luca Maiano","Bertrand Le Saux","Irene Amerini"],"pdf_url":"https://arxiv.org/pdf/2404.11302v1.pdf","comment":"6 pages, 2 figures, 2 tables, Submitted to IGARSS 2024"},{"id":"http://arxiv.org/abs/2404.11299v1","updated":"2024-04-17T12:12:48Z","published":"2024-04-17T12:12:48Z","title":"Learning from Unlabelled Data with Transformers: Domain Adaptation for\n  Semantic Segmentation of High Resolution Aerial Images","summary":"  Data from satellites or aerial vehicles are most of the times unlabelled.\nAnnotating such data accurately is difficult, requires expertise, and is costly\nin terms of time. Even if Earth Observation (EO) data were correctly labelled,\nlabels might change over time. Learning from unlabelled data within a\nsemi-supervised learning framework for segmentation of aerial images is\nchallenging. In this paper, we develop a new model for semantic segmentation of\nunlabelled images, the Non-annotated Earth Observation Semantic Segmentation\n(NEOS) model. NEOS performs domain adaptation as the target domain does not\nhave ground truth semantic segmentation masks. The distribution inconsistencies\nbetween the target and source domains are due to differences in acquisition\nscenes, environment conditions, sensors, and times. Our model aligns the\nlearned representations of the different domains to make them coincide. The\nevaluation results show that NEOS is successful and outperforms other models\nfor semantic segmentation of unlabelled data.\n","authors":["Nikolaos Dionelis","Francesco Pro","Luca Maiano","Irene Amerini","Bertrand Le Saux"],"pdf_url":"https://arxiv.org/pdf/2404.11299v1.pdf","comment":"6 pages, 7 figures, Submitted to IGARSS 2024"},{"id":"http://arxiv.org/abs/2404.10588v2","updated":"2024-04-17T12:09:17Z","published":"2024-04-16T14:13:44Z","title":"Do Counterfactual Examples Complicate Adversarial Training?","summary":"  We leverage diffusion models to study the robustness-performance tradeoff of\nrobust classifiers. Our approach introduces a simple, pretrained diffusion\nmethod to generate low-norm counterfactual examples (CEs): semantically altered\ndata which results in different true class membership. We report that the\nconfidence and accuracy of robust models on their clean training data are\nassociated with the proximity of the data to their CEs. Moreover, robust models\nperform very poorly when evaluated on the CEs directly, as they become\nincreasingly invariant to the low-norm, semantic changes brought by CEs. The\nresults indicate a significant overlap between non-robust and semantic\nfeatures, countering the common assumption that non-robust features are not\ninterpretable.\n","authors":["Eric Yeats","Cameron Darwin","Eduardo Ortega","Frank Liu","Hai Li"],"pdf_url":"https://arxiv.org/pdf/2404.10588v2.pdf","comment":"Accepted as a short paper to the GCV Workshop at CVPR'24"},{"id":"http://arxiv.org/abs/2404.11277v1","updated":"2024-04-17T11:34:14Z","published":"2024-04-17T11:34:14Z","title":"Quantum-inspired Techniques in Tensor Networks for Industrial Contexts","summary":"  In this paper we present a study of the applicability and feasibility of\nquantum-inspired algorithms and techniques in tensor networks for industrial\nenvironments and contexts, with a compilation of the available literature and\nan analysis of the use cases that may be affected by such methods. In addition,\nwe explore the limitations of such techniques in order to determine their\npotential scalability.\n","authors":["Alejandro Mata Ali","I√±igo Perez Delgado","Aitor Moreno Fdez. de Leceta"],"pdf_url":"https://arxiv.org/pdf/2404.11277v1.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2404.11269v1","updated":"2024-04-17T11:20:14Z","published":"2024-04-17T11:20:14Z","title":"DACAD: Domain Adaptation Contrastive Learning for Anomaly Detection in\n  Multivariate Time Series","summary":"  Time series anomaly detection (TAD) faces a significant challenge due to the\nscarcity of labelled data, which hinders the development of accurate detection\nmodels. Unsupervised domain adaptation (UDA) addresses this challenge by\nleveraging a labelled dataset from a related domain to detect anomalies in a\ntarget dataset. Existing domain adaptation techniques assume that the number of\nanomalous classes does not change between the source and target domains. In\nthis paper, we propose a novel Domain Adaptation Contrastive learning for\nAnomaly Detection in multivariate time series (DACAD) model to address this\nissue by combining UDA and contrastive representation learning. DACAD's\napproach includes an anomaly injection mechanism that introduces various types\nof synthetic anomalies, enhancing the model's ability to generalise across\nunseen anomalous classes in different domains. This method significantly\nbroadens the model's adaptability and robustness. Additionally, we propose a\nsupervised contrastive loss for the source domain and a self-supervised\ncontrastive triplet loss for the target domain, improving comprehensive feature\nrepresentation learning and extraction of domain-invariant features. Finally,\nan effective Centre-based Entropy Classifier (CEC) is proposed specifically for\nanomaly detection, facilitating accurate learning of normal boundaries in the\nsource domain. Our extensive evaluation across multiple real-world datasets\nagainst leading models in time series anomaly detection and UDA underscores\nDACAD's effectiveness. The results validate DACAD's superiority in transferring\nknowledge across domains and its potential to mitigate the challenge of limited\nlabelled data in time series anomaly detection.\n","authors":["Zahra Zamanzadeh Darban","Geoffrey I. Webb","Mahsa Salehi"],"pdf_url":"https://arxiv.org/pdf/2404.11269v1.pdf","comment":"11 pages, 2 figures, 5 tables"},{"id":"http://arxiv.org/abs/2404.08978v2","updated":"2024-04-17T10:59:59Z","published":"2024-04-13T12:02:19Z","title":"Incremental Residual Concept Bottleneck Models","summary":"  Concept Bottleneck Models (CBMs) map the black-box visual representations\nextracted by deep neural networks onto a set of interpretable concepts and use\nthe concepts to make predictions, enhancing the transparency of the\ndecision-making process. Multimodal pre-trained models can match visual\nrepresentations with textual concept embeddings, allowing for obtaining the\ninterpretable concept bottleneck without the expertise concept annotations.\nRecent research has focused on the concept bank establishment and the\nhigh-quality concept selection. However, it is challenging to construct a\ncomprehensive concept bank through humans or large language models, which\nseverely limits the performance of CBMs. In this work, we propose the\nIncremental Residual Concept Bottleneck Model (Res-CBM) to address the\nchallenge of concept completeness. Specifically, the residual concept\nbottleneck model employs a set of optimizable vectors to complete missing\nconcepts, then the incremental concept discovery module converts the\ncomplemented vectors with unclear meanings into potential concepts in the\ncandidate concept bank. Our approach can be applied to any user-defined concept\nbank, as a post-hoc processing method to enhance the performance of any CBMs.\nFurthermore, to measure the descriptive efficiency of CBMs, the Concept\nUtilization Efficiency (CUE) metric is proposed. Experiments show that the\nRes-CBM outperforms the current state-of-the-art methods in terms of both\naccuracy and efficiency and achieves comparable performance to black-box models\nacross multiple datasets.\n","authors":["Chenming Shang","Shiji Zhou","Hengyuan Zhang","Xinzhe Ni","Yujiu Yang","Yuwang Wang"],"pdf_url":"https://arxiv.org/pdf/2404.08978v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02761v3","updated":"2024-04-17T10:56:48Z","published":"2024-04-03T14:07:02Z","title":"AQuA -- Combining Experts' and Non-Experts' Views To Assess Deliberation\n  Quality in Online Discussions Using LLMs","summary":"  Measuring the quality of contributions in political online discussions is\ncrucial in deliberation research and computer science. Research has identified\nvarious indicators to assess online discussion quality, and with deep learning\nadvancements, automating these measures has become feasible. While some studies\nfocus on analyzing specific quality indicators, a comprehensive quality score\nincorporating various deliberative aspects is often preferred. In this work, we\nintroduce AQuA, an additive score that calculates a unified deliberative\nquality score from multiple indices for each discussion post. Unlike other\nsingular scores, AQuA preserves information on the deliberative aspects present\nin comments, enhancing model transparency. We develop adapter models for 20\ndeliberative indices, and calculate correlation coefficients between experts'\nannotations and the perceived deliberativeness by non-experts to weigh the\nindividual indices into a single deliberative score. We demonstrate that the\nAQuA score can be computed easily from pre-trained adapters and aligns well\nwith annotations on other datasets that have not be seen during training. The\nanalysis of experts' vs. non-experts' annotations confirms theoretical findings\nin the social science literature.\n","authors":["Maike Behrendt","Stefan Sylvius Wagner","Marc Ziegele","Lena Wilms","Anke Stoll","Dominique Heinbach","Stefan Harmeling"],"pdf_url":"https://arxiv.org/pdf/2404.02761v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2102.03311v3","updated":"2024-04-17T10:25:45Z","published":"2021-02-05T17:32:52Z","title":"Online Bin Packing with Predictions","summary":"  Bin packing is a classic optimization problem with a wide range of\napplications, from load balancing to supply chain management. In this work, we\nstudy the online variant of the problem, in which a sequence of items of\nvarious sizes must be placed into a minimum number of bins of uniform capacity.\nThe online algorithm is enhanced with a (potentially erroneous) prediction\nconcerning the frequency of item sizes in the sequence. We design and analyze\nonline algorithms with efficient tradeoffs between the consistency (i.e., the\ncompetitive ratio assuming no prediction error) and the robustness (i.e., the\ncompetitive ratio under adversarial error), and whose performance degrades\nnear-optimally as a function of the prediction error. This is the first\ntheoretical and experimental study of online bin packing under competitive\nanalysis, in the realistic setting of learnable predictions. Previous work\naddressed only extreme cases with respect to the prediction error, and relied\non overly powerful and error-free oracles.\n","authors":["Spyros Angelopoulos","Shahin Kamali","Kimia Shadkami"],"pdf_url":"https://arxiv.org/pdf/2102.03311v3.pdf","comment":"30 pages, 6 figures"},{"id":"http://arxiv.org/abs/2404.08483v2","updated":"2024-04-17T10:21:47Z","published":"2024-04-12T14:03:41Z","title":"Semantic Communication for Cooperative Multi-Task Processing over\n  Wireless Networks","summary":"  In this paper, we have expanded the current status of semantic communication\nlimited to processing one task to a more general system that can handle\nmultiple tasks concurrently. In pursuit of this, we first introduced our\ndefinition of the \"semantic source\", enabling the interpretation of multiple\nsemantics based on a single observation. A semantic encoder design is then\nintroduced, featuring the division of the encoder into a common unit and\nmultiple specific units enabling cooperative multi-task processing. Simulation\nresults demonstrate the effectiveness of the proposed semantic source and the\nsystem design. Our approach employs information maximization (infomax) and\nend-to-end design principles.\n","authors":["Ahmad Halimi Razlighi","Carsten Bockelmann","Armin Dekorsy"],"pdf_url":"https://arxiv.org/pdf/2404.08483v2.pdf","comment":"This work has been submitted to the IEEE Wireless Communications\n  Letters"},{"id":"http://arxiv.org/abs/2404.11224v1","updated":"2024-04-17T10:16:20Z","published":"2024-04-17T10:16:20Z","title":"Analytical results for uncertainty propagation through trained machine\n  learning regression models","summary":"  Machine learning (ML) models are increasingly being used in metrology\napplications. However, for ML models to be credible in a metrology context they\nshould be accompanied by principled uncertainty quantification. This paper\naddresses the challenge of uncertainty propagation through trained/fixed\nmachine learning (ML) regression models. Analytical expressions for the mean\nand variance of the model output are obtained/presented for certain input data\ndistributions and for a variety of ML models. Our results cover several popular\nML models including linear regression, penalised linear regression, kernel\nridge regression, Gaussian Processes (GPs), support vector machines (SVMs) and\nrelevance vector machines (RVMs). We present numerical experiments in which we\nvalidate our methods and compare them with a Monte Carlo approach from a\ncomputational efficiency point of view. We also illustrate our methods in the\ncontext of a metrology application, namely modelling the state-of-health of\nlithium-ion cells based upon Electrical Impedance Spectroscopy (EIS) data\n","authors":["Andrew Thompson"],"pdf_url":"https://arxiv.org/pdf/2404.11224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00622v3","updated":"2024-04-17T10:13:36Z","published":"2024-01-01T00:54:02Z","title":"Federated Class-Incremental Learning with New-Class Augmented\n  Self-Distillation","summary":"  Federated Learning (FL) enables collaborative model training among\nparticipants while guaranteeing the privacy of raw data. Mainstream FL\nmethodologies overlook the dynamic nature of real-world data, particularly its\ntendency to grow in volume and diversify in classes over time. This oversight\nresults in FL methods suffering from catastrophic forgetting, where the trained\nmodels inadvertently discard previously learned information upon assimilating\nnew data. In response to this challenge, we propose a novel Federated\nClass-Incremental Learning (FCIL) method, named \\underline{Fed}erated\n\\underline{C}lass-Incremental \\underline{L}earning with New-Class\n\\underline{A}ugmented \\underline{S}elf-Di\\underline{S}tillation (FedCLASS). The\ncore of FedCLASS is to enrich the class scores of historical models with new\nclass scores predicted by current models and utilize the combined knowledge for\nself-distillation, enabling a more sufficient and precise knowledge transfer\nfrom historical models to current models. Theoretical analyses demonstrate that\nFedCLASS stands on reliable foundations, considering scores of old classes\npredicted by historical models as conditional probabilities in the absence of\nnew classes, and the scores of new classes predicted by current models as the\nconditional probabilities of class scores derived from historical models.\nEmpirical experiments demonstrate the superiority of FedCLASS over four\nbaseline algorithms in reducing average forgetting rate and boosting global\naccuracy.\n","authors":["Zhiyuan Wu","Tianliu He","Sheng Sun","Yuwei Wang","Min Liu","Bo Gao","Xuefeng Jiang"],"pdf_url":"https://arxiv.org/pdf/2401.00622v3.pdf","comment":"9 pages, 2 figures, 4 tables"},{"id":"http://arxiv.org/abs/2311.04161v2","updated":"2024-04-17T10:12:59Z","published":"2023-11-07T17:39:17Z","title":"Breaking the Heavy-Tailed Noise Barrier in Stochastic Optimization\n  Problems","summary":"  We consider stochastic optimization problems with heavy-tailed noise with\nstructured density. For such problems, we show that it is possible to get\nfaster rates of convergence than $\\mathcal{O}(K^{-2(\\alpha - 1)/\\alpha})$, when\nthe stochastic gradients have finite moments of order $\\alpha \\in (1, 2]$. In\nparticular, our analysis allows the noise norm to have an unbounded\nexpectation. To achieve these results, we stabilize stochastic gradients, using\nsmoothed medians of means. We prove that the resulting estimates have\nnegligible bias and controllable variance. This allows us to carefully\nincorporate them into clipped-SGD and clipped-SSTM and derive new\nhigh-probability complexity bounds in the considered setup.\n","authors":["Nikita Puchkin","Eduard Gorbunov","Nikolay Kutuzov","Alexander Gasnikov"],"pdf_url":"https://arxiv.org/pdf/2311.04161v2.pdf","comment":"AISTATS 2024. 60 pages, 3 figures. Changes in V2: small typos were\n  fixed, extra experiments and discussion were added. Code:\n  https://github.com/Kutuz4/AISTATS2024_SMoM"},{"id":"http://arxiv.org/abs/2403.09347v2","updated":"2024-04-17T10:07:14Z","published":"2024-03-14T12:51:58Z","title":"BurstAttention: An Efficient Distributed Attention Framework for\n  Extremely Long Sequences","summary":"  Effective attention modules have played a crucial role in the success of\nTransformer-based large language models (LLMs), but the quadratic time and\nmemory complexities of these attention modules also pose a challenge when\nprocessing long sequences. One potential solution for the long sequence problem\nis to utilize distributed clusters to parallelize the computation of attention\nmodules across multiple devices (e.g., GPUs). However, adopting a distributed\napproach inevitably introduces extra memory overheads to store local attention\nresults and incurs additional communication costs to aggregate local results\ninto global ones. In this paper, we propose a distributed attention framework\nnamed ``BurstAttention'' to optimize memory access and communication operations\nat both the global cluster and local device levels. In our experiments, we\ncompare BurstAttention with other competitive distributed attention solutions\nfor long sequence processing. The experimental results under different length\nsettings demonstrate that BurstAttention offers significant advantages for\nprocessing long sequences compared with these competitive baselines, reducing\n40% communication overheads and achieving 1.37 X speedup during training 128K\nsequence length on 32 X A100.\n","authors":["Sun Ao","Weilin Zhao","Xu Han","Cheng Yang","Zhiyuan Liu","Chuan Shi","Maosong Sun","Shengnan Wang","Teng Su"],"pdf_url":"https://arxiv.org/pdf/2403.09347v2.pdf","comment":"13 pages, 7 figures"},{"id":"http://arxiv.org/abs/2404.11216v1","updated":"2024-04-17T10:00:56Z","published":"2024-04-17T10:00:56Z","title":"Position Engineering: Boosting Large Language Models through Positional\n  Information Manipulation","summary":"  The performance of large language models (LLMs) is significantly influenced\nby the quality of the prompts provided. In response, researchers have developed\nenormous prompt engineering strategies aimed at modifying the prompt text to\nenhance task performance. In this paper, we introduce a novel technique termed\nposition engineering, which offers a more efficient way to guide large language\nmodels. Unlike prompt engineering, which requires substantial effort to modify\nthe text provided to LLMs, position engineering merely involves altering the\npositional information in the prompt without modifying the text itself. We have\nevaluated position engineering in two widely-used LLM scenarios:\nretrieval-augmented generation (RAG) and in-context learning (ICL). Our\nfindings show that position engineering substantially improves upon the\nbaseline in both cases. Position engineering thus represents a promising new\nstrategy for exploiting the capabilities of large language models.\n","authors":["Zhiyuan He","Huiqiang Jiang","Zilong Wang","Yuqing Yang","Luna Qiu","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2404.11216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11207v1","updated":"2024-04-17T09:39:07Z","published":"2024-04-17T09:39:07Z","title":"Exploring the Transferability of Visual Prompting for Multimodal Large\n  Language Models","summary":"  Although Multimodal Large Language Models (MLLMs) have demonstrated promising\nversatile capabilities, their performance is still inferior to specialized\nmodels on downstream tasks, which makes adaptation necessary to enhance their\nutility. However, fine-tuning methods require independent training for every\nmodel, leading to huge computation and memory overheads. In this paper, we\npropose a novel setting where we aim to improve the performance of diverse\nMLLMs with a group of shared parameters optimized for a downstream task. To\nachieve this, we propose Transferable Visual Prompting (TVP), a simple and\neffective approach to generate visual prompts that can transfer to different\nmodels and improve their performance on downstream tasks after trained on only\none model. We introduce two strategies to address the issue of cross-model\nfeature corruption of existing visual prompting methods and enhance the\ntransferability of the learned prompts, including 1) Feature Consistency\nAlignment: which imposes constraints to the prompted feature changes to\nmaintain task-agnostic knowledge; 2) Task Semantics Enrichment: which\nencourages the prompted images to contain richer task-specific semantics with\nlanguage guidance. We validate the effectiveness of TVP through extensive\nexperiments with 6 modern MLLMs on a wide variety of tasks ranging from object\nrecognition and counting to multimodal reasoning and hallucination correction.\n","authors":["Yichi Zhang","Yinpeng Dong","Siyuan Zhang","Tianzan Min","Hang Su","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2404.11207v1.pdf","comment":"Accepted in CVPR 2024 as Poster (Highlight)"},{"id":"http://arxiv.org/abs/2309.15178v2","updated":"2024-04-17T09:36:47Z","published":"2023-09-26T18:20:20Z","title":"Zero-Shot Reinforcement Learning from Low Quality Data","summary":"  Zero-shot reinforcement learning (RL) promises to provide agents that can\nperform any task in an environment after an offline, reward-free pre-training\nphase. Methods leveraging successor measures and successor features have shown\nstrong performance in this setting, but require access to large heterogenous\ndatasets for pre-training which cannot be expected for most real problems.\nHere, we explore how the performance of zero-shot RL methods degrades when\ntrained on small homogeneous datasets, and propose fixes inspired by\nconservatism, a well-established feature of performant single-task offline RL\nalgorithms. We evaluate our proposals across various datasets, domains and\ntasks, and show that conservative zero-shot RL algorithms outperform their\nnon-conservative counterparts on low quality datasets, and perform no worse on\nhigh quality datasets. Somewhat surprisingly, our proposals also outperform\nbaselines that get to see the task during training. Our code is available via\nhttps://enjeeneer.io/projects/zero-shot-rl/.\n","authors":["Scott Jeen","Tom Bewley","Jonathan M. Cullen"],"pdf_url":"https://arxiv.org/pdf/2309.15178v2.pdf","comment":"Project page: https://enjeeneer.io/projects/zero-shot-rl/"},{"id":"http://arxiv.org/abs/2310.02977v2","updated":"2024-04-17T09:09:17Z","published":"2023-10-04T17:12:18Z","title":"T$^3$Bench: Benchmarking Current Progress in Text-to-3D Generation","summary":"  Recent methods in text-to-3D leverage powerful pretrained diffusion models to\noptimize NeRF. Notably, these methods are able to produce high-quality 3D\nscenes without training on 3D data. Due to the open-ended nature of the task,\nmost studies evaluate their results with subjective case studies and user\nexperiments, thereby presenting a challenge in quantitatively addressing the\nquestion: How has current progress in Text-to-3D gone so far? In this paper, we\nintroduce T$^3$Bench, the first comprehensive text-to-3D benchmark containing\ndiverse text prompts of three increasing complexity levels that are specially\ndesigned for 3D generation. To assess both the subjective quality and the text\nalignment, we propose two automatic metrics based on multi-view images produced\nby the 3D contents. The quality metric combines multi-view text-image scores\nand regional convolution to detect quality and view inconsistency. The\nalignment metric uses multi-view captioning and GPT-4 evaluation to measure\ntext-3D consistency. Both metrics closely correlate with different dimensions\nof human judgments, providing a paradigm for efficiently evaluating text-to-3D\nmodels. The benchmarking results, shown in Fig. 1, reveal performance\ndifferences among an extensive 10 prevalent text-to-3D methods. Our analysis\nfurther highlights the common struggles for current methods on generating\nsurroundings and multi-object scenes, as well as the bottleneck of leveraging\n2D guidance for 3D generation. Our project page is available at:\nhttps://t3bench.com.\n","authors":["Yuze He","Yushi Bai","Matthieu Lin","Wang Zhao","Yubin Hu","Jenny Sheng","Ran Yi","Juanzi Li","Yong-Jin Liu"],"pdf_url":"https://arxiv.org/pdf/2310.02977v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2310.03031v2","updated":"2024-04-17T09:04:28Z","published":"2023-09-21T07:54:25Z","title":"How Prevalent is Gender Bias in ChatGPT? -- Exploring German and English\n  ChatGPT Responses","summary":"  With the introduction of ChatGPT, OpenAI made large language models (LLM)\naccessible to users with limited IT expertise. However, users with no\nbackground in natural language processing (NLP) might lack a proper\nunderstanding of LLMs. Thus the awareness of their inherent limitations, and\ntherefore will take the systems' output at face value. In this paper, we\nsystematically analyse prompts and the generated responses to identify possible\nproblematic issues with a special focus on gender biases, which users need to\nbe aware of when processing the system's output. We explore how ChatGPT reacts\nin English and German if prompted to answer from a female, male, or neutral\nperspective. In an in-depth investigation, we examine selected prompts and\nanalyse to what extent responses differ if the system is prompted several times\nin an identical way. On this basis, we show that ChatGPT is indeed useful for\nhelping non-IT users draft texts for their daily work. However, it is\nabsolutely crucial to thoroughly check the system's responses for biases as\nwell as for syntactic and grammatical mistakes.\n","authors":["Stefanie Urchs","Veronika Thurner","Matthias A√üenmacher","Christian Heumann","Stephanie Thiemichen"],"pdf_url":"https://arxiv.org/pdf/2310.03031v2.pdf","comment":"Accepted @ \"1st Workshop on Biased Data in Conversational Agents\"\n  (co-located with ECML PKDD 2023). This is the author's version of the work.\n  The definite version of record will be published in the proceedings"},{"id":"http://arxiv.org/abs/2404.11181v1","updated":"2024-04-17T08:53:59Z","published":"2024-04-17T08:53:59Z","title":"KI-GAN: Knowledge-Informed Generative Adversarial Networks for Enhanced\n  Multi-Vehicle Trajectory Forecasting at Signalized Intersections","summary":"  Reliable prediction of vehicle trajectories at signalized intersections is\ncrucial to urban traffic management and autonomous driving systems. However, it\npresents unique challenges, due to the complex roadway layout at intersections,\ninvolvement of traffic signal controls, and interactions among different types\nof road users. To address these issues, we present in this paper a novel model\ncalled Knowledge-Informed Generative Adversarial Network (KI-GAN), which\nintegrates both traffic signal information and multi-vehicle interactions to\npredict vehicle trajectories accurately. Additionally, we propose a specialized\nattention pooling method that accounts for vehicle orientation and proximity at\nintersections. Based on the SinD dataset, our KI-GAN model is able to achieve\nan Average Displacement Error (ADE) of 0.05 and a Final Displacement Error\n(FDE) of 0.12 for a 6-second observation and 6-second prediction cycle. When\nthe prediction window is extended to 9 seconds, the ADE and FDE values are\nfurther reduced to 0.11 and 0.26, respectively. These results demonstrate the\neffectiveness of the proposed KI-GAN model in vehicle trajectory prediction\nunder complex scenarios at signalized intersections, which represents a\nsignificant advancement in the target field.\n","authors":["Chuheng Wei","Guoyuan Wu","Matthew J. Barth","Amr Abdelraouf","Rohit Gupta","Kyungtae Han"],"pdf_url":"https://arxiv.org/pdf/2404.11181v1.pdf","comment":"10 pages, 2 figures, accepted by CVPRW"},{"id":"http://arxiv.org/abs/2404.11172v1","updated":"2024-04-17T08:42:42Z","published":"2024-04-17T08:42:42Z","title":"Deep Neural Networks via Complex Network Theory: a Perspective","summary":"  Deep Neural Networks (DNNs) can be represented as graphs whose links and\nvertices iteratively process data and solve tasks sub-optimally. Complex\nNetwork Theory (CNT), merging statistical physics with graph theory, provides a\nmethod for interpreting neural networks by analysing their weights and neuron\nstructures. However, classic works adapt CNT metrics that only permit a\ntopological analysis as they do not account for the effect of the input data.\nIn addition, CNT metrics have been applied to a limited range of architectures,\nmainly including Fully Connected neural networks. In this work, we extend the\nexisting CNT metrics with measures that sample from the DNNs' training\ndistribution, shifting from a purely topological analysis to one that connects\nwith the interpretability of deep learning. For the novel metrics, in addition\nto the existing ones, we provide a mathematical formalisation for Fully\nConnected, AutoEncoder, Convolutional and Recurrent neural networks, of which\nwe vary the activation functions and the number of hidden layers. We show that\nthese metrics differentiate DNNs based on the architecture, the number of\nhidden layers, and the activation function. Our contribution provides a method\nrooted in physics for interpreting DNNs that offers insights beyond the\ntraditional input-output relationship and the CNT topological analysis.\n","authors":["Emanuele La Malfa","Gabriele La Malfa","Giuseppe Nicosia","Vito Latora"],"pdf_url":"https://arxiv.org/pdf/2404.11172v1.pdf","comment":"IJCAI'24 (full paper, main track)"},{"id":"http://arxiv.org/abs/2404.11171v1","updated":"2024-04-17T08:40:54Z","published":"2024-04-17T08:40:54Z","title":"Personalized Heart Disease Detection via ECG Digital Twin Generation","summary":"  Heart diseases rank among the leading causes of global mortality,\ndemonstrating a crucial need for early diagnosis and intervention. Most\ntraditional electrocardiogram (ECG) based automated diagnosis methods are\ntrained at population level, neglecting the customization of personalized ECGs\nto enhance individual healthcare management. A potential solution to address\nthis limitation is to employ digital twins to simulate symptoms of diseases in\nreal patients. In this paper, we present an innovative prospective learning\napproach for personalized heart disease detection, which generates digital\ntwins of healthy individuals' anomalous ECGs and enhances the model sensitivity\nto the personalized symptoms. In our approach, a vector quantized feature\nseparator is proposed to locate and isolate the disease symptom and normal\nsegments in ECG signals with ECG report guidance. Thus, the ECG digital twins\ncan simulate specific heart diseases used to train a personalized heart disease\ndetection model. Experiments demonstrate that our approach not only excels in\ngenerating high-fidelity ECG signals but also improves personalized heart\ndisease detection. Moreover, our approach ensures robust privacy protection,\nsafeguarding patient data in model development.\n","authors":["Yaojun Hu","Jintai Chen","Lianting Hu","Dantong Li","Jiahuan Yan","Haochao Ying","Huiying Liang","Jian Wu"],"pdf_url":"https://arxiv.org/pdf/2404.11171v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08555v2","updated":"2024-04-17T08:38:54Z","published":"2023-12-13T23:00:48Z","title":"KDAS: Knowledge Distillation via Attention Supervision Framework for\n  Polyp Segmentation","summary":"  Polyp segmentation, a contentious issue in medical imaging, has seen numerous\nproposed methods aimed at improving the quality of segmented masks. While\ncurrent state-of-the-art techniques yield impressive results, the size and\ncomputational cost of these models create challenges for practical industry\napplications. To address this challenge, we present KDAS, a Knowledge\nDistillation framework that incorporates attention supervision, and our\nproposed Symmetrical Guiding Module. This framework is designed to facilitate a\ncompact student model with fewer parameters, allowing it to learn the strengths\nof the teacher model and mitigate the inconsistency between teacher features\nand student features, a common challenge in Knowledge Distillation, via the\nSymmetrical Guiding Module. Through extensive experiments, our compact models\ndemonstrate their strength by achieving competitive results with\nstate-of-the-art methods, offering a promising approach to creating compact\nmodels with high accuracy for polyp segmentation and in the medical imaging\nfield. The implementation is available on https://github.com/huyquoctrinh/KDAS.\n","authors":["Quoc-Huy Trinh","Minh-Van Nguyen","Phuoc-Thao Vo Thi"],"pdf_url":"https://arxiv.org/pdf/2312.08555v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05771v2","updated":"2024-04-17T08:36:14Z","published":"2023-12-10T05:33:40Z","title":"Hacking Task Confounder in Meta-Learning","summary":"  Meta-learning enables rapid generalization to new tasks by learning knowledge\nfrom various tasks. It is intuitively assumed that as the training progresses,\na model will acquire richer knowledge, leading to better generalization\nperformance. However, our experiments reveal an unexpected result: there is\nnegative knowledge transfer between tasks, affecting generalization\nperformance. To explain this phenomenon, we conduct Structural Causal Models\n(SCMs) for causal analysis. Our investigation uncovers the presence of spurious\ncorrelations between task-specific causal factors and labels in meta-learning.\nFurthermore, the confounding factors differ across different batches. We refer\nto these confounding factors as ``Task Confounders\". Based on these findings,\nwe propose a plug-and-play Meta-learning Causal Representation Learner\n(MetaCRL) to eliminate task confounders. It encodes decoupled generating\nfactors from multiple tasks and utilizes an invariant-based bi-level\noptimization mechanism to ensure their causality for meta-learning. Extensive\nexperiments on various benchmark datasets demonstrate that our work achieves\nstate-of-the-art (SOTA) performance.\n","authors":["Jingyao Wang","Yi Ren","Zeen Song","Jianqi Zhang","Changwen Zheng","Wenwen Qiang"],"pdf_url":"https://arxiv.org/pdf/2312.05771v2.pdf","comment":"Accepted by IJCAI 2024, 9 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2404.11163v1","updated":"2024-04-17T08:26:34Z","published":"2024-04-17T08:26:34Z","title":"LongVQ: Long Sequence Modeling with Vector Quantization on Structured\n  Memory","summary":"  Transformer models have been successful in various sequence processing tasks,\nbut the self-attention mechanism's computational cost limits its practicality\nfor long sequences. Although there are existing attention variants that improve\ncomputational efficiency, they have a limited ability to abstract global\ninformation effectively based on their hand-crafted mixing strategies. On the\nother hand, state-space models (SSMs) are tailored for long sequences but\ncannot capture complicated local information. Therefore, the combination of\nthem as a unified token mixer is a trend in recent long-sequence models.\nHowever, the linearized attention degrades performance significantly even when\nequipped with SSMs. To address the issue, we propose a new method called\nLongVQ. LongVQ uses the vector quantization (VQ) technique to compress the\nglobal abstraction as a length-fixed codebook, enabling the linear-time\ncomputation of the attention matrix. This technique effectively maintains\ndynamic global and local patterns, which helps to complement the lack of\nlong-range dependency issues. Our experiments on the Long Range Arena\nbenchmark, autoregressive language modeling, and image and speech\nclassification demonstrate the effectiveness of LongVQ. Our model achieves\nsignificant improvements over other sequence models, including variants of\nTransformers, Convolutions, and recent State Space Models.\n","authors":["Zicheng Liu","Li Wang","Siyuan Li","Zedong Wang","Haitao Lin","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2404.11163v1.pdf","comment":"Published at IJCAI 2024. arXiv admin note: text overlap with\n  arXiv:2212.08136 by other authors"},{"id":"http://arxiv.org/abs/2404.11161v1","updated":"2024-04-17T08:21:02Z","published":"2024-04-17T08:21:02Z","title":"Pre-processing matters: A segment search method for WSI classification","summary":"  Pre-processing for whole slide images can affect classification performance\nboth in the training and inference stages. Our study analyzes the impact of\npre-processing parameters on inference and training across single- and\nmultiple-domain datasets. However, searching for an optimal parameter set is\ntime-consuming. To overcome this, we propose a novel Similarity-based Simulated\nAnnealing approach for fast parameter tuning to enhance inference performance\non single-domain data. Our method demonstrates significant performance\nimprovements in accuracy, which raise accuracy from 0.512 to 0.847 in a single\ndomain. We further extend our insight into training performance in multi-domain\ndata by employing a novel Bayesian optimization to search optimal\npre-processing parameters, resulting in a high AUC of 0.967. We highlight that\nbetter pre-processing for WSI can contribute to further accuracy improvement in\nthe histology area.\n","authors":["Jun Wang","Yufei Cui","Yu Mao","Nan Guan","Chun Jason Xue"],"pdf_url":"https://arxiv.org/pdf/2404.11161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11148v1","updated":"2024-04-17T07:59:33Z","published":"2024-04-17T07:59:33Z","title":"Explainable Machine Learning System for Predicting Chronic Kidney\n  Disease in High-Risk Cardiovascular Patients","summary":"  As the global population ages, the incidence of Chronic Kidney Disease (CKD)\nis rising. CKD often remains asymptomatic until advanced stages, which\nsignificantly burdens both the healthcare system and patient quality of life.\nThis research developed an explainable machine learning system for predicting\nCKD in patients with cardiovascular risks, utilizing medical history and\nlaboratory data. The Random Forest model achieved the highest sensitivity of\n88.2%. The study introduces a comprehensive explainability framework that\nextends beyond traditional feature importance methods, incorporating global and\nlocal interpretations, bias inspection, biomedical relevance, and safety\nassessments. Key predictive features identified in global interpretation were\nthe use of diabetic and ACEI/ARB medications, and initial eGFR values. Local\ninterpretation provided model insights through counterfactual explanations,\nwhich aligned with other system parts. After conducting a bias inspection, it\nwas found that the initial eGFR values and CKD predictions exhibited some bias,\nbut no significant gender bias was identified. The model's logic, extracted by\nscoped rules, was confirmed to align with existing medical literature. The\nsafety assessment tested potentially dangerous cases and confirmed that the\nmodel behaved safely. This system enhances the explainability, reliability, and\naccountability of the model, promoting its potential integration into\nhealthcare settings and compliance with upcoming regulatory standards, and\nshowing promise for broader applications in healthcare machine learning.\n","authors":["Nantika Nguycharoen"],"pdf_url":"https://arxiv.org/pdf/2404.11148v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.19867v2","updated":"2024-04-17T07:57:44Z","published":"2024-03-28T22:26:38Z","title":"Finding Decision Tree Splits in Streaming and Massively Parallel Models","summary":"  In this work, we provide data stream algorithms that compute optimal splits\nin decision tree learning. In particular, given a data stream of observations\n$x_i$ and their labels $y_i$, the goal is to find the optimal split point $j$\nthat divides the data into two sets such that the mean squared error (for\nregression) or misclassification rate (for classification) is minimized. We\nprovide various fast streaming algorithms that use sublinear space and a small\nnumber of passes for these problems. These algorithms can also be extended to\nthe massively parallel computation model. Our work, while not directly\ncomparable, complements the seminal work of Domingos and Hulten (KDD 2000).\n","authors":["Huy Pham","Hoang Ta","Hoa T. Vu"],"pdf_url":"https://arxiv.org/pdf/2403.19867v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13034v4","updated":"2024-04-17T07:54:45Z","published":"2024-01-23T19:00:02Z","title":"Locality Sensitive Sparse Encoding for Learning World Models Online","summary":"  Acquiring an accurate world model online for model-based reinforcement\nlearning (MBRL) is challenging due to data nonstationarity, which typically\ncauses catastrophic forgetting for neural networks (NNs). From the online\nlearning perspective, a Follow-The-Leader (FTL) world model is desirable, which\noptimally fits all previous experiences at each round. Unfortunately, NN-based\nmodels need re-training on all accumulated data at every interaction step to\nachieve FTL, which is computationally expensive for lifelong agents. In this\npaper, we revisit models that can achieve FTL with incremental updates.\nSpecifically, our world model is a linear regression model supported by\nnonlinear random features. The linear part ensures efficient FTL update while\nthe nonlinear random feature empowers the fitting of complex environments. To\nbest trade off model capacity and computation efficiency, we introduce a\nlocality sensitive sparse encoding, which allows us to conduct efficient sparse\nupdates even with very high dimensional nonlinear features. We validate the\nrepresentation power of our encoding and verify that it allows efficient online\nlearning under data covariate shift. We also show, in the Dyna MBRL setting,\nthat our world models learned online using a single pass of trajectory data\neither surpass or match the performance of deep world models trained with\nreplay and other continual learning methods.\n","authors":["Zichen Liu","Chao Du","Wee Sun Lee","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2401.13034v4.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2404.09326v2","updated":"2024-04-17T07:46:28Z","published":"2024-04-14T18:57:38Z","title":"Weight Copy and Low-Rank Adaptation for Few-Shot Distillation of Vision\n  Transformers","summary":"  Few-shot knowledge distillation recently emerged as a viable approach to\nharness the knowledge of large-scale pre-trained models, using limited data and\ncomputational resources. In this paper, we propose a novel few-shot feature\ndistillation approach for vision transformers. Our approach is based on two key\nsteps. Leveraging the fact that vision transformers have a consistent\ndepth-wise structure, we first copy the weights from intermittent layers of\nexisting pre-trained vision transformers (teachers) into shallower\narchitectures (students), where the intermittence factor controls the\ncomplexity of the student transformer with respect to its teacher. Next, we\nemploy an enhanced version of Low-Rank Adaptation (LoRA) to distill knowledge\ninto the student in a few-shot scenario, aiming to recover the information\nprocessing carried out by the skipped teacher layers. We present comprehensive\nexperiments with supervised and self-supervised transformers as teachers, on\nfive data sets from various domains, including natural, medical and satellite\nimages. The empirical results confirm the superiority of our approach over\ncompetitive baselines. Moreover, the ablation results demonstrate the\nusefulness of each component of the proposed pipeline.\n","authors":["Diana-Nicoleta Grigore","Mariana-Iuliana Georgescu","Jon Alvarez Justo","Tor Johansen","Andreea Iuliana Ionescu","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2404.09326v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.14598v5","updated":"2024-04-17T07:38:55Z","published":"2022-10-26T10:12:31Z","title":"Manifold Gaussian Variational Bayes on the Precision Matrix","summary":"  We propose an optimization algorithm for Variational Inference (VI) in\ncomplex models. Our approach relies on natural gradient updates where the\nvariational space is a Riemann manifold. We develop an efficient algorithm for\nGaussian Variational Inference whose updates satisfy the positive definite\nconstraint on the variational covariance matrix. Our Manifold Gaussian\nVariational Bayes on the Precision matrix (MGVBP) solution provides simple\nupdate rules, is straightforward to implement, and the use of the precision\nmatrix parametrization has a significant computational advantage. Due to its\nblack-box nature, MGVBP stands as a ready-to-use solution for VI in complex\nmodels. Over five datasets, we empirically validate our feasible approach on\ndifferent statistical and econometric models, discussing its performance with\nrespect to baseline methods.\n","authors":["Martin Magris","Mostafa Shabani","Alexandros Iosifidis"],"pdf_url":"https://arxiv.org/pdf/2210.14598v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15838v2","updated":"2024-04-17T07:31:57Z","published":"2023-08-30T08:21:46Z","title":"Adaptive Lasso, Transfer Lasso, and Beyond: An Asymptotic Perspective","summary":"  This paper presents a comprehensive exploration of the theoretical properties\ninherent in the Adaptive Lasso and the Transfer Lasso. The Adaptive Lasso, a\nwell-established method, employs regularization divided by initial estimators\nand is characterized by asymptotic normality and variable selection\nconsistency. In contrast, the recently proposed Transfer Lasso employs\nregularization subtracted by initial estimators with the demonstrated capacity\nto curtail non-asymptotic estimation errors. A pivotal question thus emerges:\nGiven the distinct ways the Adaptive Lasso and the Transfer Lasso employ\ninitial estimators, what benefits or drawbacks does this disparity confer upon\neach method? This paper conducts a theoretical examination of the asymptotic\nproperties of the Transfer Lasso, thereby elucidating its differentiation from\nthe Adaptive Lasso. Informed by the findings of this analysis, we introduce a\nnovel method, one that amalgamates the strengths and compensates for the\nweaknesses of both methods. The paper concludes with validations of our theory\nand comparisons of the methods via simulation experiments.\n","authors":["Masaaki Takada","Hironori Fujisawa"],"pdf_url":"https://arxiv.org/pdf/2308.15838v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11130v1","updated":"2024-04-17T07:21:17Z","published":"2024-04-17T07:21:17Z","title":"Learning epidemic trajectories through Kernel Operator Learning: from\n  modelling to optimal control","summary":"  Since infectious pathogens start spreading into a susceptible population,\nmathematical models can provide policy makers with reliable forecasts and\nscenario analyses, which can be concretely implemented or solely consulted. In\nthese complex epidemiological scenarios, machine learning architectures can\nplay an important role, since they directly reconstruct data-driven models\ncircumventing the specific modelling choices and the parameter calibration,\ntypical of classical compartmental models. In this work, we discuss the\nefficacy of Kernel Operator Learning (KOL) to reconstruct population dynamics\nduring epidemic outbreaks, where the transmission rate is ruled by an input\nstrategy. In particular, we introduce two surrogate models, named KOL-m and\nKOL-$\\partial$, which reconstruct in two different ways the evolution of the\nepidemics. Moreover, we evaluate the generalization performances of the two\napproaches with different kernels, including the Neural Tangent Kernels, and\ncompare them with a classical neural network model learning method. Employing\nsynthetic but semi-realistic data, we show how the two introduced approaches\nare suitable for realizing fast and robust forecasts and scenario analyses, and\nhow these approaches are competitive for determining optimal intervention\nstrategies with respect to specific performance measures.\n","authors":["Giovanni Ziarelli","Nicola Parolini","Marco Verani"],"pdf_url":"https://arxiv.org/pdf/2404.11130v1.pdf","comment":"39 pages, 13 figures"},{"id":"http://arxiv.org/abs/2404.11117v1","updated":"2024-04-17T07:01:41Z","published":"2024-04-17T07:01:41Z","title":"Variational quantization for state space models","summary":"  Forecasting tasks using large datasets gathering thousands of heterogeneous\ntime series is a crucial statistical problem in numerous sectors. The main\nchallenge is to model a rich variety of time series, leverage any available\nexternal signals and provide sharp predictions with statistical guarantees. In\nthis work, we propose a new forecasting model that combines discrete state\nspace hidden Markov models with recent neural network architectures and\ntraining procedures inspired by vector quantized variational autoencoders. We\nintroduce a variational discrete posterior distribution of the latent states\ngiven the observations and a two-stage training procedure to alternatively\ntrain the parameters of the latent states and of the emission distributions. By\nlearning a collection of emission laws and temporarily activating them\ndepending on the hidden process dynamics, the proposed method allows to explore\nlarge datasets and leverage available external signals. We assess the\nperformance of the proposed method using several datasets and show that it\noutperforms other state-of-the-art solutions.\n","authors":["Etienne David","Jean Bellot","Sylvain Le Corff"],"pdf_url":"https://arxiv.org/pdf/2404.11117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11116v1","updated":"2024-04-17T07:01:29Z","published":"2024-04-17T07:01:29Z","title":"Music Enhancement with Deep Filters: A Technical Report for The ICASSP\n  2024 Cadenza Challenge","summary":"  In this challenge, we disentangle the deep filters from the original\nDeepfilterNet and incorporate them into our Spec-UNet-based network to further\nimprove a hybrid Demucs (hdemucs) based remixing pipeline. The motivation\nbehind the use of the deep filter component lies at its potential in better\nhandling temporal fine structures. We demonstrate an incremental improvement in\nboth the Signal-to-Distortion Ratio (SDR) and the Hearing Aid Audio Quality\nIndex (HAAQI) metrics when comparing the performance of hdemucs against\ndifferent versions of our model.\n","authors":["Keren Shao","Ke Chen","Shlomo Dubnov"],"pdf_url":"https://arxiv.org/pdf/2404.11116v1.pdf","comment":"2 pages, 2 figures, 1 tables, Proceedings of the International\n  Conference on Acoustics, Speech, and Signal Processing, ICASSP 2024"},{"id":"http://arxiv.org/abs/2404.11114v1","updated":"2024-04-17T07:00:20Z","published":"2024-04-17T07:00:20Z","title":"Reuse out-of-year data to enhance land cover mappingvia feature\n  disentanglement and contrastive learning","summary":"  Timely up-to-date land use/land cover (LULC) maps play a pivotal role in\nsupporting agricultural territory management, environmental monitoring and\nfacilitating well-informed and sustainable decision-making. Typically, when\ncreating a land cover (LC) map, precise ground truth data is collected through\ntime-consuming and expensive field campaigns. This data is then utilized in\nconjunction with satellite image time series (SITS) through advanced machine\nlearning algorithms to get the final map. Unfortunately, each time this process\nis repeated (e.g., annually over a region to estimate agricultural production\nor potential biodiversity loss), new ground truth data must be collected,\nleading to the complete disregard of previously gathered reference data despite\nthe substantial financial and time investment they have required. How to make\nvalue of historical data, from the same or similar study sites, to enhance the\ncurrent LULC mapping process constitutes a significant challenge that could\nenable the financial and human-resource efforts invested in previous data\ncampaigns to be valued again. Aiming to tackle this important challenge, we\nhere propose a deep learning framework based on recent advances in domain\nadaptation and generalization to combine remote sensing and reference data\ncoming from two different domains (e.g. historical data and fresh ones) to\nameliorate the current LC mapping process. Our approach, namely REFeD (data\nReuse with Effective Feature Disentanglement for land cover mapping), leverages\na disentanglement strategy, based on contrastive learning, where invariant and\nspecific per-domain features are derived to recover the intrinsic information\nrelated to the downstream LC mapping task and alleviate possible distribution\nshifts between domains. Additionally, REFeD is equipped with an effective\nsupervision scheme where feature disentanglement is further enforced via\nmultiple levels of supervision at different granularities. The experimental\nassessment over two study areas covering extremely diverse and contrasted\nlandscapes, namely Koumbia (located in the West-Africa region, in Burkina Faso)\nand Centre Val de Loire (located in centre Europe, France), underlines the\nquality of our framework and the obtained findings demonstrate that out-of-year\ninformation coming from the same (or similar) study site, at different periods\nof time, can constitute a valuable additional source of information to enhance\nthe LC mapping process.\n","authors":["Cassio F. Dantas","Raffaele Gaetano","Claudia Paris","Dino Ienco"],"pdf_url":"https://arxiv.org/pdf/2404.11114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.00493v3","updated":"2024-04-17T06:37:30Z","published":"2023-07-02T06:48:19Z","title":"Fourier-Mixed Window Attention: Accelerating Informer for Long Sequence\n  Time-Series Forecasting","summary":"  We study a fast local-global window-based attention method to accelerate\nInformer for long sequence time-series forecasting. While window attention\nbeing local is a considerable computational saving, it lacks the ability to\ncapture global token information which is compensated by a subsequent Fourier\ntransform block. Our method, named FWin, does not rely on query sparsity\nhypothesis and an empirical approximation underlying the ProbSparse attention\nof Informer. Through experiments on univariate and multivariate datasets, we\nshow that FWin transformers improve the overall prediction accuracies of\nInformer while accelerating its inference speeds by 1.6 to 2 times. We also\nprovide a mathematical definition of FWin attention, and prove that it is\nequivalent to the canonical full attention under the block diagonal\ninvertibility (BDI) condition of the attention matrix. The BDI is shown\nexperimentally to hold with high probability for typical benchmark datasets.\n","authors":["Nhat Thanh Tran","Jack Xin"],"pdf_url":"https://arxiv.org/pdf/2307.00493v3.pdf","comment":"19 pages (main), 11 pages (appendix), 8 figures"},{"id":"http://arxiv.org/abs/2404.11100v1","updated":"2024-04-17T06:36:17Z","published":"2024-04-17T06:36:17Z","title":"Synthesizing Realistic Data for Table Recognition","summary":"  To overcome the limitations and challenges of current automatic table data\nannotation methods and random table data synthesis approaches, we propose a\nnovel method for synthesizing annotation data specifically designed for table\nrecognition. This method utilizes the structure and content of existing complex\ntables, facilitating the efficient creation of tables that closely replicate\nthe authentic styles found in the target domain. By leveraging the actual\nstructure and content of tables from Chinese financial announcements, we have\ndeveloped the first extensive table annotation dataset in this domain. We used\nthis dataset to train several recent deep learning-based end-to-end table\nrecognition models. Additionally, we have established the inaugural benchmark\nfor real-world complex tables in the Chinese financial announcement domain,\nusing it to assess the performance of models trained on our synthetic data,\nthereby effectively validating our method's practicality and effectiveness.\nFurthermore, we applied our synthesis method to augment the FinTabNet dataset,\nextracted from English financial announcements, by increasing the proportion of\ntables with multiple spanning cells to introduce greater complexity. Our\nexperiments show that models trained on this augmented dataset achieve\ncomprehensive improvements in performance, especially in the recognition of\ntables with multiple spanning cells.\n","authors":["Qiyu Hou","Jun Wang","Meixuan Qiao","Lujun Tian"],"pdf_url":"https://arxiv.org/pdf/2404.11100v1.pdf","comment":"ICDAR 2024"},{"id":"http://arxiv.org/abs/2402.16565v2","updated":"2024-04-17T06:31:27Z","published":"2024-02-26T13:43:25Z","title":"Partial Rankings of Optimizers","summary":"  We introduce a framework for benchmarking optimizers according to multiple\ncriteria over various test functions. Based on a recently introduced union-free\ngeneric depth function for partial orders/rankings, it fully exploits the\nordinal information and allows for incomparability. Our method describes the\ndistribution of all partial orders/rankings, avoiding the notorious\nshortcomings of aggregation. This permits to identify test functions that\nproduce central or outlying rankings of optimizers and to assess the quality of\nbenchmarking suites.\n","authors":["Julian Rodemann","Hannah Blocher"],"pdf_url":"https://arxiv.org/pdf/2402.16565v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.17310v4","updated":"2024-04-17T06:17:59Z","published":"2023-09-29T15:08:28Z","title":"Leave-one-out Distinguishability in Machine Learning","summary":"  We introduce an analytical framework to quantify the changes in a machine\nlearning algorithm's output distribution following the inclusion of a few data\npoints in its training set, a notion we define as leave-one-out\ndistinguishability (LOOD). This is key to measuring data **memorization** and\ninformation **leakage** as well as the **influence** of training data points in\nmachine learning. We illustrate how our method broadens and refines existing\nempirical measures of memorization and privacy risks associated with training\ndata. We use Gaussian processes to model the randomness of machine learning\nalgorithms, and validate LOOD with extensive empirical analysis of leakage\nusing membership inference attacks. Our analytical framework enables us to\ninvestigate the causes of leakage and where the leakage is high. For example,\nwe analyze the influence of activation functions, on data memorization.\nAdditionally, our method allows us to identify queries that disclose the most\ninformation about the training data in the leave-one-out setting. We illustrate\nhow optimal queries can be used for accurate **reconstruction** of training\ndata.\n","authors":["Jiayuan Ye","Anastasia Borovykh","Soufiane Hayou","Reza Shokri"],"pdf_url":"https://arxiv.org/pdf/2309.17310v4.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2404.11093v1","updated":"2024-04-17T06:17:08Z","published":"2024-04-17T06:17:08Z","title":"Neural Network Approach for Non-Markovian Dissipative Dynamics of\n  Many-Body Open Quantum Systems","summary":"  Simulating the dynamics of open quantum systems coupled to non-Markovian\nenvironments remains an outstanding challenge due to exponentially scaling\ncomputational costs. We present an artificial intelligence strategy to overcome\nthis obstacle by integrating the neural quantum states approach into the\ndissipaton-embedded quantum master equation in second quantization (DQME-SQ).\nOur approach utilizes restricted Boltzmann machines (RBMs) to compactly\nrepresent the reduced density tensor, explicitly encoding the combined effects\nof system-environment correlations and nonMarkovian memory. Applied to model\nsystems exhibiting prominent effects of system-environment correlation and\nnon-Markovian memory, our approach achieves comparable accuracy to conventional\nhierarchical equations of motion, while requiring significantly fewer dynamical\nvariables. The novel RBM-based DQME-SQ approach paves the way for investigating\nnon-Markovian open quantum dynamics in previously intractable regimes, with\nimplications spanning various frontiers of modern science.\n","authors":["Long Cao","Liwei Ge","Daochi Zhang","Xiang Li","Yao Wang","Rui-Xue Xu","YiJing Yan","Xiao Zheng"],"pdf_url":"https://arxiv.org/pdf/2404.11093v1.pdf","comment":"7 pages, 5 figures"},{"id":"http://arxiv.org/abs/2011.06716v2","updated":"2024-04-17T05:44:10Z","published":"2020-11-13T01:39:44Z","title":"Dependency-based Anomaly Detection: a General Framework and\n  Comprehensive Evaluation","summary":"  Anomaly detection is crucial for understanding unusual behaviors in data, as\nanomalies offer valuable insights. This paper introduces Dependency-based\nAnomaly Detection (DepAD), a general framework that utilizes variable\ndependencies to uncover meaningful anomalies with better interpretability.\nDepAD reframes unsupervised anomaly detection as supervised feature selection\nand prediction tasks, which allows users to tailor anomaly detection algorithms\nto their specific problems and data. We extensively evaluate representative\noff-the-shelf techniques for the DepAD framework. Two DepAD algorithms emerge\nas all-rounders and superior performers in handling a wide range of datasets\ncompared to nine state-of-the-art anomaly detection methods. Additionally, we\ndemonstrate that DepAD algorithms provide new and insightful interpretations\nfor detected anomalies.\n","authors":["Sha Lu","Lin Liu","Kui Yu","Thuc Duy Le","Jixue Liu","Jiuyong Li"],"pdf_url":"https://arxiv.org/pdf/2011.06716v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08968v2","updated":"2024-04-17T05:42:52Z","published":"2024-04-13T11:13:56Z","title":"MCPNet: An Interpretable Classifier via Multi-Level Concept Prototypes","summary":"  Recent advancements in post-hoc and inherently interpretable methods have\nmarkedly enhanced the explanations of black box classifier models. These\nmethods operate either through post-analysis or by integrating concept learning\nduring model training. Although being effective in bridging the semantic gap\nbetween a model's latent space and human interpretation, these explanation\nmethods only partially reveal the model's decision-making process. The outcome\nis typically limited to high-level semantics derived from the last feature map.\nWe argue that the explanations lacking insights into the decision processes at\nlow and mid-level features are neither fully faithful nor useful. Addressing\nthis gap, we introduce the Multi-Level Concept Prototypes Classifier (MCPNet),\nan inherently interpretable model. MCPNet autonomously learns meaningful\nconcept prototypes across multiple feature map levels using Centered Kernel\nAlignment (CKA) loss and an energy-based weighted PCA mechanism, and it does so\nwithout reliance on predefined concept labels. Further, we propose a novel\nclassifier paradigm that learns and aligns multi-level concept prototype\ndistributions for classification purposes via Class-aware Concept Distribution\n(CCD) loss. Our experiments reveal that our proposed MCPNet while being\nadaptable to various model architectures, offers comprehensive multi-level\nexplanations while maintaining classification accuracy. Additionally, its\nconcept distribution-based classification approach shows improved\ngeneralization capabilities in few-shot classification scenarios.\n","authors":["Bor-Shiun Wang","Chien-Yi Wang","Wei-Chen Chiu"],"pdf_url":"https://arxiv.org/pdf/2404.08968v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2404.11075v1","updated":"2024-04-17T05:16:12Z","published":"2024-04-17T05:16:12Z","title":"EEG_GLT-Net: Optimising EEG Graphs for Real-time Motor Imagery Signals\n  Classification","summary":"  Brain-Computer Interfaces connect the brain to external control devices,\nnecessitating the accurate translation of brain signals such as from\nelectroencephalography (EEG) into executable commands. Graph Neural Networks\n(GCN) have been increasingly applied for classifying EEG Motor Imagery signals,\nprimarily because they incorporates the spatial relationships among EEG\nchannels, resulting in improved accuracy over traditional convolutional\nmethods. Recent advances by GCNs-Net in real-time EEG MI signal classification\nutilised Pearson Coefficient Correlation (PCC) for constructing adjacency\nmatrices, yielding significant results on the PhysioNet dataset. Our paper\nintroduces the EEG Graph Lottery Ticket (EEG_GLT) algorithm, an innovative\ntechnique for constructing adjacency matrices for EEG channels. It does not\nrequire pre-existing knowledge of inter-channel relationships, and it can be\ntailored to suit both individual subjects and GCN model architectures. Our\nfindings demonstrated that the PCC method outperformed the Geodesic approach by\n9.65% in mean accuracy, while our EEG_GLT matrix consistently exceeded the\nperformance of the PCC method by a mean accuracy of 13.39%. Also, we found that\nthe construction of the adjacency matrix significantly influenced accuracy, to\na greater extent than GCN model configurations. A basic GCN configuration\nutilising our EEG_GLT matrix exceeded the performance of even the most complex\nGCN setup with a PCC matrix in average accuracy. Our EEG_GLT method also\nreduced MACs by up to 97% compared to the PCC method, while maintaining or\nenhancing accuracy. In conclusion, the EEG_GLT algorithm marks a breakthrough\nin the development of optimal adjacency matrices, effectively boosting both\ncomputational accuracy and efficiency, making it well-suited for real-time\nclassification of EEG MI signals that demand intensive computational resources.\n","authors":["Htoo Wai Aung","Jiao Jiao Li","Yang An","Steven W. Su"],"pdf_url":"https://arxiv.org/pdf/2404.11075v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11068v1","updated":"2024-04-17T04:55:33Z","published":"2024-04-17T04:55:33Z","title":"ScaleFold: Reducing AlphaFold Initial Training Time to 10 Hours","summary":"  AlphaFold2 has been hailed as a breakthrough in protein folding. It can\nrapidly predict protein structures with lab-grade accuracy. However, its\nimplementation does not include the necessary training code. OpenFold is the\nfirst trainable public reimplementation of AlphaFold. AlphaFold training\nprocedure is prohibitively time-consuming, and gets diminishing benefits from\nscaling to more compute resources. In this work, we conducted a comprehensive\nanalysis on the AlphaFold training procedure based on Openfold, identified that\ninefficient communications and overhead-dominated computations were the key\nfactors that prevented the AlphaFold training from effective scaling. We\nintroduced ScaleFold, a systematic training method that incorporated\noptimizations specifically for these factors. ScaleFold successfully scaled the\nAlphaFold training to 2080 NVIDIA H100 GPUs with high resource utilization. In\nthe MLPerf HPC v3.0 benchmark, ScaleFold finished the OpenFold benchmark in\n7.51 minutes, shown over $6\\times$ speedup than the baseline. For training the\nAlphaFold model from scratch, ScaleFold completed the pretraining in 10 hours,\na significant improvement over the seven days required by the original\nAlphaFold pretraining baseline.\n","authors":["Feiwen Zhu","Arkadiusz Nowaczynski","Rundong Li","Jie Xin","Yifei Song","Michal Marcinkiewicz","Sukru Burc Eryilmaz","Jun Yang","Michael Andersch"],"pdf_url":"https://arxiv.org/pdf/2404.11068v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.09633v8","updated":"2024-04-17T04:55:19Z","published":"2023-06-16T05:32:24Z","title":"The False Dawn: Reevaluating Google's Reinforcement Learning for Chip\n  Macro Placement","summary":"  Reinforcement learning (RL) for physical design of silicon chips in a Google\n2021 Nature paper stirred controversy due to poorly documented claims that\nraised eyebrows and drew critical media coverage. The paper withheld critical\nmethodology steps and most inputs needed to reproduce results. Our\nmeta-analysis shows how two separate evaluations filled in the gaps and\ndemonstrated that Google RL lags behind (i) human designers, (ii) a well-known\nalgorithm (Simulated Annealing), and (iii) generally-available commercial\nsoftware, while being slower; and in a 2023 open research contest, RL methods\nweren't in top 5. Crosschecked data indicate that the integrity of the Nature\npaper is substantially undermined owing to errors in conduct, analysis and\nreporting. Before publishing, Google rebuffed internal allegations of fraud,\nwhich still stand. We note policy implications and conclusions for chip design.\n","authors":["Igor L. Markov"],"pdf_url":"https://arxiv.org/pdf/2306.09633v8.pdf","comment":"15 pages, 1 figure, 4 tables, 80 references"},{"id":"http://arxiv.org/abs/2312.03991v2","updated":"2024-04-17T04:54:39Z","published":"2023-12-07T02:17:45Z","title":"MICRO: Model-Based Offline Reinforcement Learning with a Conservative\n  Bellman Operator","summary":"  Offline reinforcement learning (RL) faces a significant challenge of\ndistribution shift. Model-free offline RL penalizes the Q value for\nout-of-distribution (OOD) data or constrains the policy closed to the behavior\npolicy to tackle this problem, but this inhibits the exploration of the OOD\nregion. Model-based offline RL, which uses the trained environment model to\ngenerate more OOD data and performs conservative policy optimization within\nthat model, has become an effective method for this problem. However, the\ncurrent model-based algorithms rarely consider agent robustness when\nincorporating conservatism into policy. Therefore, the new model-based offline\nalgorithm with a conservative Bellman operator (MICRO) is proposed. This method\ntrades off performance and robustness via introducing the robust Bellman\noperator into the algorithm. Compared with previous model-based algorithms with\nrobust adversarial models, MICRO can significantly reduce the computation cost\nby only choosing the minimal Q value in the state uncertainty set. Extensive\nexperiments demonstrate that MICRO outperforms prior RL algorithms in offline\nRL benchmark and is considerably robust to adversarial perturbations.\n","authors":["Xiao-Yin Liu","Xiao-Hu Zhou","Guotao Li","Hao Li","Mei-Jiang Gui","Tian-Yu Xiang","De-Xing Huang","Zeng-Guang Hou"],"pdf_url":"https://arxiv.org/pdf/2312.03991v2.pdf","comment":"Accepted by IJCAI 2024 (the 33rd International Joint Conference on\n  Artificial Intelligence)"},{"id":"http://arxiv.org/abs/2306.05836v3","updated":"2024-04-17T04:27:10Z","published":"2023-06-09T12:09:15Z","title":"Can Large Language Models Infer Causation from Correlation?","summary":"  Causal inference is one of the hallmarks of human intelligence. While the\nfield of CausalNLP has attracted much interest in the recent years, existing\ncausal inference datasets in NLP primarily rely on discovering causality from\nempirical knowledge (e.g., commonsense knowledge). In this work, we propose the\nfirst benchmark dataset to test the pure causal inference skills of large\nlanguage models (LLMs). Specifically, we formulate a novel task Corr2Cause,\nwhich takes a set of correlational statements and determines the causal\nrelationship between the variables. We curate a large-scale dataset of more\nthan 200K samples, on which we evaluate seventeen existing LLMs. Through our\nexperiments, we identify a key shortcoming of LLMs in terms of their causal\ninference skills, and show that these models achieve almost close to random\nperformance on the task. This shortcoming is somewhat mitigated when we try to\nre-purpose LLMs for this skill via finetuning, but we find that these models\nstill fail to generalize -- they can only perform causal inference in\nin-distribution settings when variable names and textual expressions used in\nthe queries are similar to those in the training set, but fail in\nout-of-distribution settings generated by perturbing these queries. Corr2Cause\nis a challenging task for LLMs, and would be helpful in guiding future research\non improving LLMs' pure reasoning skills and generalizability. Our data is at\nhttps://huggingface.co/datasets/causalnlp/corr2cause. Our code is at\nhttps://github.com/causalNLP/corr2cause.\n","authors":["Zhijing Jin","Jiarui Liu","Zhiheng Lyu","Spencer Poff","Mrinmaya Sachan","Rada Mihalcea","Mona Diab","Bernhard Sch√∂lkopf"],"pdf_url":"https://arxiv.org/pdf/2306.05836v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2309.15098v2","updated":"2024-04-17T04:25:21Z","published":"2023-09-26T17:48:55Z","title":"Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of\n  Language Models","summary":"  We investigate the internal behavior of Transformer-based Large Language\nModels (LLMs) when they generate factually incorrect text. We propose modeling\nfactual queries as constraint satisfaction problems and use this framework to\ninvestigate how the LLM interacts internally with factual constraints. We find\na strong positive relationship between the LLM's attention to constraint tokens\nand the factual accuracy of generations. We curate a suite of 10 datasets\ncontaining over 40,000 prompts to study the task of predicting factual errors\nwith the Llama-2 family across all scales (7B, 13B, 70B). We propose SAT Probe,\na method probing attention patterns, that can predict factual errors and\nfine-grained constraint satisfaction, and allow early error identification. The\napproach and findings take another step towards using the mechanistic\nunderstanding of LLMs to enhance their reliability.\n","authors":["Mert Yuksekgonul","Varun Chandrasekaran","Erik Jones","Suriya Gunasekar","Ranjita Naik","Hamid Palangi","Ece Kamar","Besmira Nushi"],"pdf_url":"https://arxiv.org/pdf/2309.15098v2.pdf","comment":"Published at ICLR 2024"},{"id":"http://arxiv.org/abs/2311.14632v2","updated":"2024-04-17T04:16:15Z","published":"2023-11-24T17:56:44Z","title":"Differentially Private SGD Without Clipping Bias: An Error-Feedback\n  Approach","summary":"  Differentially Private Stochastic Gradient Descent with Gradient Clipping\n(DPSGD-GC) is a powerful tool for training deep learning models using sensitive\ndata, providing both a solid theoretical privacy guarantee and high efficiency.\nHowever, using DPSGD-GC to ensure Differential Privacy (DP) comes at the cost\nof model performance degradation due to DP noise injection and gradient\nclipping. Existing research has extensively analyzed the theoretical\nconvergence of DPSGD-GC, and has shown that it only converges when using large\nclipping thresholds that are dependent on problem-specific parameters.\nUnfortunately, these parameters are often unknown in practice, making it hard\nto choose the optimal clipping threshold. Therefore, in practice, DPSGD-GC\nsuffers from degraded performance due to the {\\it constant} bias introduced by\nthe clipping.\n  In our work, we propose a new error-feedback (EF) DP algorithm as an\nalternative to DPSGD-GC, which not only offers a diminishing utility bound\nwithout inducing a constant clipping bias, but more importantly, it allows for\nan arbitrary choice of clipping threshold that is independent of the problem.\nWe establish an algorithm-specific DP analysis for our proposed algorithm,\nproviding privacy guarantees based on R{\\'e}nyi DP. Additionally, we\ndemonstrate that under mild conditions, our algorithm can achieve nearly the\nsame utility bound as DPSGD without gradient clipping. Our empirical results on\nCifar-10/100 and E2E datasets, show that the proposed algorithm achieves higher\naccuracies than DPSGD while maintaining the same level of DP guarantee.\n","authors":["Xinwei Zhang","Zhiqi Bu","Zhiwei Steven Wu","Mingyi Hong"],"pdf_url":"https://arxiv.org/pdf/2311.14632v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11056v1","updated":"2024-04-17T04:08:38Z","published":"2024-04-17T04:08:38Z","title":"LMEraser: Large Model Unlearning through Adaptive Prompt Tuning","summary":"  To address the growing demand for privacy protection in machine learning, we\npropose a novel and efficient machine unlearning approach for \\textbf{L}arge\n\\textbf{M}odels, called \\textbf{LM}Eraser. Existing unlearning research suffers\nfrom entangled training data and complex model architectures, incurring\nextremely high computational costs for large models. LMEraser takes a\ndivide-and-conquer strategy with a prompt tuning architecture to isolate data\ninfluence. The training dataset is partitioned into public and private\ndatasets. Public data are used to train the backbone of the model. Private data\nare adaptively clustered based on their diversity, and each cluster is used to\noptimize a prompt separately. This adaptive prompt tuning mechanism reduces\nunlearning costs and maintains model performance. Experiments demonstrate that\nLMEraser achieves a $100$-fold reduction in unlearning costs without\ncompromising accuracy compared to prior work. Our code is available at:\n\\url{https://github.com/lmeraser/lmeraser}.\n","authors":["Jie Xu","Zihan Wu","Cong Wang","Xiaohua Jia"],"pdf_url":"https://arxiv.org/pdf/2404.11056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.01884v4","updated":"2024-04-17T04:06:03Z","published":"2022-04-04T23:15:00Z","title":"Policy Learning with Competing Agents","summary":"  Decision makers often aim to learn a treatment assignment policy under a\ncapacity constraint on the number of agents that they can treat. When agents\ncan respond strategically to such policies, competition arises, complicating\nestimation of the optimal policy. In this paper, we study capacity-constrained\ntreatment assignment in the presence of such interference. We consider a\ndynamic model where the decision maker allocates treatments at each time step\nand heterogeneous agents myopically best respond to the previous treatment\nassignment policy. When the number of agents is large but finite, we show that\nthe threshold for receiving treatment under a given policy converges to the\npolicy's mean-field equilibrium threshold. Based on this result, we develop a\nconsistent estimator for the policy gradient. In a semi-synthetic experiment\nwith data from the National Education Longitudinal Study of 1988, we\ndemonstrate that this estimator can be used for learning capacity-constrained\npolicies in the presence of strategic behavior.\n","authors":["Roshni Sahoo","Stefan Wager"],"pdf_url":"https://arxiv.org/pdf/2204.01884v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11052v1","updated":"2024-04-17T03:51:55Z","published":"2024-04-17T03:51:55Z","title":"Supervised Contrastive Vision Transformer for Breast Histopathological\n  Image Classification","summary":"  Invasive ductal carcinoma (IDC) is the most prevalent form of breast cancer.\nBreast tissue histopathological examination is critical in diagnosing and\nclassifying breast cancer. Although existing methods have shown promising\nresults, there is still room for improvement in the classification accuracy and\ngeneralization of IDC using histopathology images. We present a novel approach,\nSupervised Contrastive Vision Transformer (SupCon-ViT), for improving the\nclassification of invasive ductal carcinoma in terms of accuracy and\ngeneralization by leveraging the inherent strengths and advantages of both\ntransfer learning, i.e., pre-trained vision transformer, and supervised\ncontrastive learning. Our results on a benchmark breast cancer dataset\ndemonstrate that SupCon-Vit achieves state-of-the-art performance in IDC\nclassification, with an F1-score of 0.8188, precision of 0.7692, and\nspecificity of 0.8971, outperforming existing methods. In addition, the\nproposed model demonstrates resilience in scenarios with minimal labeled data,\nmaking it highly efficient in real-world clinical settings where labelled data\nis limited. Our findings suggest that supervised contrastive learning in\nconjunction with pre-trained vision transformers appears to be a viable\nstrategy for an accurate classification of IDC, thus paving the way for a more\nefficient and reliable diagnosis of breast cancer through histopathological\nimage analysis.\n","authors":["Mohammad Shiri","Jiangwen Sun"],"pdf_url":"https://arxiv.org/pdf/2404.11052v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2404.11049v1","updated":"2024-04-17T03:44:58Z","published":"2024-04-17T03:44:58Z","title":"Stepwise Alignment for Constrained Language Model Policy Optimization","summary":"  Safety and trustworthiness are indispensable requirements for applying AI\nsystems based on large language models (LLMs) in real-world applications. This\npaper formulates a human value alignment as a language model policy\noptimization problem to maximize reward under a safety constraint and then\nproposes an algorithm called Stepwise Alignment for Constrained Policy\nOptimization (SACPO). A key idea behind SACPO, supported by theory, is that the\noptimal policy incorporating both reward and safety can be directly obtained\nfrom a reward-aligned policy. Based on this key idea, SACPO aligns the LLMs\nwith each metric step-wise while leveraging simple yet powerful alignment\nalgorithms such as direct preference optimization (DPO). SACPO provides many\nbenefits such as simplicity, stability, computational efficiency, and\nflexibility regarding algorithms and dataset selection. Under mild assumption,\nour theoretical analysis provides the upper bounds regarding near-optimality\nand safety constraint violation. Our experimental results show that SACPO can\nfine-tune Alpaca-7B better than the state-of-the-art method in terms of both\nhelpfulness and harmlessness\n","authors":["Akifumi Wachi","Thien Q Tran","Rei Sato","Takumi Tanabe","Yohei Akimoto"],"pdf_url":"https://arxiv.org/pdf/2404.11049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11046v1","updated":"2024-04-17T03:42:48Z","published":"2024-04-17T03:42:48Z","title":"Lightweight Unsupervised Federated Learning with Pretrained Vision\n  Language Model","summary":"  Federated learning aims to tackle the ``isolated data island\" problem, where\nit trains a collective model from physically isolated clients while\nsafeguarding the privacy of users' data. However, supervised federated learning\nnecessitates that each client labels their data for training, which can be both\ntime-consuming and resource-intensive, and may even be impractical for edge\ndevices. Moreover, the training and transmission of deep models present\nchallenges to the computation and communication capabilities of the clients. To\naddress these two inherent challenges in supervised federated learning, we\npropose a novel lightweight unsupervised federated learning approach that\nleverages unlabeled data on each client to perform lightweight model training\nand communication by harnessing pretrained vision-language models, such as\nCLIP. By capitalizing on the zero-shot prediction capability and the\nwell-trained image encoder of the pre-trained CLIP model, we have carefully\ncrafted an efficient and resilient self-training approach. This method refines\nthe initial zero-shot predicted pseudo-labels of unlabeled instances through\nthe sole training of a linear classifier on top of the fixed image encoder.\nAdditionally, to address data heterogeneity within each client, we propose a\nclass-balanced text feature sampling strategy for generating synthetic\ninstances in the feature space to support local training. Experiments are\nconducted on multiple benchmark datasets. The experimental results demonstrate\nthat our proposed method greatly enhances model performance in comparison to\nCLIP's zero-shot predictions and even outperforms supervised federated learning\nbenchmark methods given limited computational and communication overhead.\n","authors":["Hao Yan","Yuhong Guo"],"pdf_url":"https://arxiv.org/pdf/2404.11046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11041v1","updated":"2024-04-17T03:34:27Z","published":"2024-04-17T03:34:27Z","title":"On the Empirical Complexity of Reasoning and Planning in LLMs","summary":"  Large Language Models (LLMs) work surprisingly well for some complex\nreasoning problems via chain-of-thought (CoT) or tree-of-thought (ToT), but the\nunderlying reasons remain unclear. We seek to understand the performance of\nthese methods by conducting experimental case studies and linking the outcomes\nto sample and computational complexity in machine learning. We found that if\nproblems can be decomposed into a sequence of reasoning steps and learning to\npredict the next step has a low sample and computational complexity, explicitly\noutlining the reasoning chain with all necessary information for predicting the\nnext step may improve performance. Conversely, for problems where predicting\nthe next step is computationally hard, adopting ToT may yield better reasoning\noutcomes than attempting to formulate a short reasoning chain.\n","authors":["Liwei Kang","Zirui Zhao","David Hsu","Wee Sun Lee"],"pdf_url":"https://arxiv.org/pdf/2404.11041v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11036v1","updated":"2024-04-17T03:25:54Z","published":"2024-04-17T03:25:54Z","title":"Cross-Platform Hate Speech Detection with Weakly Supervised Causal\n  Disentanglement","summary":"  Content moderation faces a challenging task as social media's ability to\nspread hate speech contrasts with its role in promoting global connectivity.\nWith rapidly evolving slang and hate speech, the adaptability of conventional\ndeep learning to the fluid landscape of online dialogue remains limited. In\nresponse, causality inspired disentanglement has shown promise by segregating\nplatform specific peculiarities from universal hate indicators. However, its\ndependency on available ground truth target labels for discerning these nuances\nfaces practical hurdles with the incessant evolution of platforms and the\nmutable nature of hate speech. Using confidence based reweighting and\ncontrastive regularization, this study presents HATE WATCH, a novel framework\nof weakly supervised causal disentanglement that circumvents the need for\nexplicit target labeling and effectively disentangles input features into\ninvariant representations of hate. Empirical validation across platforms two\nwith target labels and two without positions HATE WATCH as a novel method in\ncross platform hate speech detection with superior performance. HATE WATCH\nadvances scalable content moderation techniques towards developing safer online\ncommunities.\n","authors":["Paras Sheth","Tharindu Kumarage","Raha Moraffah","Aman Chadha","Huan Liu"],"pdf_url":"https://arxiv.org/pdf/2404.11036v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07922v4","updated":"2024-04-17T03:23:33Z","published":"2024-04-11T17:09:28Z","title":"LaVy: Vietnamese Multimodal Large Language Model","summary":"  Large Language Models (LLMs) and Multimodal Large language models (MLLMs)\nhave taken the world by storm with impressive abilities in complex reasoning\nand linguistic comprehension. Meanwhile there are plethora of works related to\nVietnamese Large Language Models, the lack of high-quality resources in\nmultimodality limits the progress of Vietnamese MLLMs. In this paper, we\npioneer in address this by introducing LaVy, a state-of-the-art Vietnamese\nMLLM, and we also introduce LaVy-Bench benchmark designated for evaluating\nMLLMs's understanding on Vietnamese visual language tasks. Our project is\npublic at https://github.com/baochi0212/LaVy\n","authors":["Chi Tran","Huong Le Thanh"],"pdf_url":"https://arxiv.org/pdf/2404.07922v4.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2404.11032v1","updated":"2024-04-17T03:20:42Z","published":"2024-04-17T03:20:42Z","title":"CORE: Data Augmentation for Link Prediction via Information Bottleneck","summary":"  Link prediction (LP) is a fundamental task in graph representation learning,\nwith numerous applications in diverse domains. However, the generalizability of\nLP models is often compromised due to the presence of noisy or spurious\ninformation in graphs and the inherent incompleteness of graph data. To address\nthese challenges, we draw inspiration from the Information Bottleneck principle\nand propose a novel data augmentation method, COmplete and REduce (CORE) to\nlearn compact and predictive augmentations for LP models. In particular, CORE\naims to recover missing edges in graphs while simultaneously removing noise\nfrom the graph structures, thereby enhancing the model's robustness and\nperformance. Extensive experiments on multiple benchmark datasets demonstrate\nthe applicability and superiority of CORE over state-of-the-art methods,\nshowcasing its potential as a leading approach for robust LP in graph\nrepresentation learning.\n","authors":["Kaiwen Dong","Zhichun Guo","Nitesh V. Chawla"],"pdf_url":"https://arxiv.org/pdf/2404.11032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15903v2","updated":"2024-04-17T02:59:30Z","published":"2024-02-24T20:50:29Z","title":"ESFL: Efficient Split Federated Learning over Resource-Constrained\n  Heterogeneous Wireless Devices","summary":"  Federated learning (FL) allows multiple parties (distributed devices) to\ntrain a machine learning model without sharing raw data. How to effectively and\nefficiently utilize the resources on devices and the central server is a highly\ninteresting yet challenging problem. In this paper, we propose an efficient\nsplit federated learning algorithm (ESFL) to take full advantage of the\npowerful computing capabilities at a central server under a split federated\nlearning framework with heterogeneous end devices (EDs). By splitting the model\ninto different submodels between the server and EDs, our approach jointly\noptimizes user-side workload and server-side computing resource allocation by\nconsidering users' heterogeneity. We formulate the whole optimization problem\nas a mixed-integer non-linear program, which is an NP-hard problem, and develop\nan iterative approach to obtain an approximate solution efficiently. Extensive\nsimulations have been conducted to validate the significantly increased\nefficiency of our ESFL approach compared with standard federated learning,\nsplit learning, and splitfed learning.\n","authors":["Guangyu Zhu","Yiqin Deng","Xianhao Chen","Haixia Zhang","Yuguang Fang","Tan F. Wong"],"pdf_url":"https://arxiv.org/pdf/2402.15903v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11023v1","updated":"2024-04-17T02:57:42Z","published":"2024-04-17T02:57:42Z","title":"Advancing Social Intelligence in AI Agents: Technical Challenges and\n  Open Questions","summary":"  Building socially-intelligent AI agents (Social-AI) is a multidisciplinary,\nmultimodal research goal that involves creating agents that can sense,\nperceive, reason about, learn from, and respond to affect, behavior, and\ncognition of other agents (human or artificial). Progress towards Social-AI has\naccelerated in the past decade across several computing communities, including\nnatural language processing, machine learning, robotics, human-machine\ninteraction, computer vision, and speech. Natural language processing, in\nparticular, has been prominent in Social-AI research, as language plays a key\nrole in constructing the social world. In this position paper, we identify a\nset of underlying technical challenges and open questions for researchers\nacross computing communities to advance Social-AI. We anchor our discussion in\nthe context of social intelligence concepts and prior progress in Social-AI\nresearch.\n","authors":["Leena Mathur","Paul Pu Liang","Louis-Philippe Morency"],"pdf_url":"https://arxiv.org/pdf/2404.11023v1.pdf","comment":"Position Paper, Under Review, 19 pages, 2 figures"},{"id":"http://arxiv.org/abs/2404.11019v1","updated":"2024-04-17T02:52:11Z","published":"2024-04-17T02:52:11Z","title":"You do not have to train Graph Neural Networks at all on text-attributed\n  graphs","summary":"  Graph structured data, specifically text-attributed graphs (TAG), effectively\nrepresent relationships among varied entities. Such graphs are essential for\nsemi-supervised node classification tasks. Graph Neural Networks (GNNs) have\nemerged as a powerful tool for handling this graph-structured data. Although\ngradient descent is commonly utilized for training GNNs for node\nclassification, this study ventures into alternative methods, eliminating the\niterative optimization processes. We introduce TrainlessGNN, a linear GNN model\ncapitalizing on the observation that text encodings from the same class often\ncluster together in a linear subspace. This model constructs a weight matrix to\nrepresent each class's node attribute subspace, offering an efficient approach\nto semi-supervised node classification on TAG. Extensive experiments reveal\nthat our trainless models can either match or even surpass their conventionally\ntrained counterparts, demonstrating the possibility of refraining from gradient\ndescent in certain configurations.\n","authors":["Kaiwen Dong","Zhichun Guo","Nitesh V. Chawla"],"pdf_url":"https://arxiv.org/pdf/2404.11019v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2306.12037v2","updated":"2024-04-17T02:51:49Z","published":"2023-06-21T06:05:34Z","title":"Distributed Random Reshuffling Methods with Improved Convergence","summary":"  This paper proposes two distributed random reshuffling methods, namely\nGradient Tracking with Random Reshuffling (GT-RR) and Exact Diffusion with\nRandom Reshuffling (ED-RR), to solve the distributed optimization problem over\na connected network, where a set of agents aim to minimize the average of their\nlocal cost functions. Both algorithms invoke random reshuffling (RR) update for\neach agent, inherit favorable characteristics of RR for minimizing smooth\nnonconvex objective functions, and improve the performance of previous\ndistributed random reshuffling methods both theoretically and empirically.\nSpecifically, both GT-RR and ED-RR achieve the convergence rate of\n$O(1/[(1-\\lambda)^{1/3}m^{1/3}T^{2/3}])$ in driving the (minimum) expected\nsquared norm of the gradient to zero, where $T$ denotes the number of epochs,\n$m$ is the sample size for each agent, and $1-\\lambda$ represents the spectral\ngap of the mixing matrix. When the objective functions further satisfy the\nPolyak-{\\L}ojasiewicz (PL) condition, we show GT-RR and ED-RR both achieve\n$O(1/[(1-\\lambda)mT^2])$ convergence rate in terms of the averaged expected\ndifferences between the agents' function values and the global minimum value.\nNotably, both results are comparable to the convergence rates of centralized RR\nmethods (up to constant factors depending on the network topology) and\noutperform those of previous distributed random reshuffling algorithms.\nMoreover, we support the theoretical findings with a set of numerical\nexperiments.\n","authors":["Kun Huang","Linli Zhou","Shi Pu"],"pdf_url":"https://arxiv.org/pdf/2306.12037v2.pdf","comment":"16 pages, 8 figures"},{"id":"http://arxiv.org/abs/2404.11018v1","updated":"2024-04-17T02:49:26Z","published":"2024-04-17T02:49:26Z","title":"Many-Shot In-Context Learning","summary":"  Large language models (LLMs) excel at few-shot in-context learning (ICL) --\nlearning from a few examples provided in context at inference, without any\nweight updates. Newly expanded context windows allow us to investigate ICL with\nhundreds or thousands of examples -- the many-shot regime. Going from few-shot\nto many-shot, we observe significant performance gains across a wide variety of\ngenerative and discriminative tasks. While promising, many-shot ICL can be\nbottlenecked by the available amount of human-generated examples. To mitigate\nthis limitation, we explore two new settings: Reinforced and Unsupervised ICL.\nReinforced ICL uses model-generated chain-of-thought rationales in place of\nhuman examples. Unsupervised ICL removes rationales from the prompt altogether,\nand prompts the model only with domain-specific questions. We find that both\nReinforced and Unsupervised ICL can be quite effective in the many-shot regime,\nparticularly on complex reasoning tasks. Finally, we demonstrate that, unlike\nfew-shot learning, many-shot learning is effective at overriding pretraining\nbiases and can learn high-dimensional functions with numerical inputs. Our\nanalysis also reveals the limitations of next-token prediction loss as an\nindicator of downstream ICL performance.\n","authors":["Rishabh Agarwal","Avi Singh","Lei M. Zhang","Bernd Bohnet","Stephanie Chan","Ankesh Anand","Zaheer Abbas","Azade Nova","John D. Co-Reyes","Eric Chu","Feryal Behbahani","Aleksandra Faust","Hugo Larochelle"],"pdf_url":"https://arxiv.org/pdf/2404.11018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11015v1","updated":"2024-04-17T02:46:59Z","published":"2024-04-17T02:46:59Z","title":"FedFa: A Fully Asynchronous Training Paradigm for Federated Learning","summary":"  Federated learning has been identified as an efficient decentralized training\nparadigm for scaling the machine learning model training on a large number of\ndevices while guaranteeing the data privacy of the trainers. FedAvg has become\na foundational parameter update strategy for federated learning, which has been\npromising to eliminate the effect of the heterogeneous data across clients and\nguarantee convergence. However, the synchronization parameter update barriers\nfor each communication round during the training significant time on waiting,\nslowing down the training procedure. Therefore, recent state-of-the-art\nsolutions propose using semi-asynchronous approaches to mitigate the waiting\ntime cost with guaranteed convergence. Nevertheless, emerging semi-asynchronous\napproaches are unable to eliminate the waiting time completely.\n  We propose a full asynchronous training paradigm, called FedFa, which can\nguarantee model convergence and eliminate the waiting time completely for\nfederated learning by using a few buffered results on the server for parameter\nupdating. Further, we provide theoretical proof of the convergence rate for our\nproposed FedFa. Extensive experimental results indicate our approach\neffectively improves the training performance of federated learning by up to 6x\nand 4x speedup compared to the state-of-the-art synchronous and\nsemi-asynchronous strategies while retaining high accuracy in both IID and\nNon-IID scenarios.\n","authors":["Haotian Xu","Zhaorui Zhang","Sheng Di","Benben Liu","Alharthi Khalid","Jiannong Cao"],"pdf_url":"https://arxiv.org/pdf/2404.11015v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11013v1","updated":"2024-04-17T02:44:25Z","published":"2024-04-17T02:44:25Z","title":"Control Theoretic Approach to Fine-Tuning and Transfer Learning","summary":"  Given a training set in the form of a paired $(\\mathcal{X},\\mathcal{Y})$, we\nsay that the control system $\\dot{x} = f(x,u)$ has learned the paired set via\nthe control $u^*$ if the system steers each point of $\\mathcal{X}$ to its\ncorresponding target in $\\mathcal{Y}$. Most existing methods for finding a\ncontrol function $u^*$ require learning of a new control function if the\ntraining set is updated. To overcome this limitation, we introduce the concept\nof $\\textit{tuning without forgetting}$. We develop $\\textit{an iterative\nalgorithm}$ to tune the control function $u^*$ when the training set expands,\nwhereby points already in the paired set are still matched, and new training\nsamples are learned. More specifically, at each update of our method, the\ncontrol $u^*$ is projected onto the kernel of the end-point mapping generated\nby the controlled dynamics at the learned samples. It ensures keeping the end\npoints for the previously learned samples constant while iteratively learning\nadditional samples. Our work contributes to the scalability of control methods,\noffering a novel approach to adaptively handle training set expansions.\n","authors":["Erkan Bayram","Shenyu Liu","Mohamed-Ali Belabbas","Tamer Ba≈üar"],"pdf_url":"https://arxiv.org/pdf/2404.11013v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10997v1","updated":"2024-04-17T02:17:23Z","published":"2024-04-17T02:17:23Z","title":"Online Algorithms with Limited Data Retention","summary":"  We introduce a model of online algorithms subject to strict constraints on\ndata retention. An online learning algorithm encounters a stream of data\npoints, one per round, generated by some stationary process. Crucially, each\ndata point can request that it be removed from memory $m$ rounds after it\narrives. To model the impact of removal, we do not allow the algorithm to store\nany information or calculations between rounds other than a subset of the data\npoints (subject to the retention constraints). At the conclusion of the stream,\nthe algorithm answers a statistical query about the full dataset. We ask: what\nlevel of performance can be guaranteed as a function of $m$?\n  We illustrate this framework for multidimensional mean estimation and linear\nregression problems. We show it is possible to obtain an exponential\nimprovement over a baseline algorithm that retains all data as long as\npossible. Specifically, we show that $m = \\textsc{Poly}(d, \\log(1/\\epsilon))$\nretention suffices to achieve mean squared error $\\epsilon$ after observing\n$O(1/\\epsilon)$ $d$-dimensional data points. This matches the error bound of\nthe optimal, yet infeasible, algorithm that retains all data forever. We also\nshow a nearly matching lower bound on the retention required to guarantee error\n$\\epsilon$. One implication of our results is that data retention laws are\ninsufficient to guarantee the right to be forgotten even in a non-adversarial\nworld in which firms merely strive to (approximately) optimize the performance\nof their algorithms.\n  Our approach makes use of recent developments in the multidimensional random\nsubset sum problem to simulate the progression of stochastic gradient descent\nunder a model of adversarial noise, which may be of independent interest.\n","authors":["Nicole Immorlica","Brendan Lucier","Markus Mobius","James Siderius"],"pdf_url":"https://arxiv.org/pdf/2404.10997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10995v1","updated":"2024-04-17T02:17:05Z","published":"2024-04-17T02:17:05Z","title":"Clipped SGD Algorithms for Privacy Preserving Performative Prediction:\n  Bias Amplification and Remedies","summary":"  Clipped stochastic gradient descent (SGD) algorithms are among the most\npopular algorithms for privacy preserving optimization that reduces the leakage\nof users' identity in model training. This paper studies the convergence\nproperties of these algorithms in a performative prediction setting, where the\ndata distribution may shift due to the deployed prediction model. For example,\nthe latter is caused by strategical users during the training of loan policy\nfor banks. Our contributions are two-fold. First, we show that the\nstraightforward implementation of a projected clipped SGD (PCSGD) algorithm may\nconverge to a biased solution compared to the performative stable solution. We\nquantify the lower and upper bound for the magnitude of the bias and\ndemonstrate a bias amplification phenomenon where the bias grows with the\nsensitivity of the data distribution. Second, we suggest two remedies to the\nbias amplification effect. The first one utilizes an optimal step size design\nfor PCSGD that takes the privacy guarantee into account. The second one uses\nthe recently proposed DiceSGD algorithm [Zhang et al., 2024]. We show that the\nlatter can successfully remove the bias and converge to the performative stable\nsolution. Numerical experiments verify our analysis.\n","authors":["Qiang Li","Michal Yemini","Hoi-To Wai"],"pdf_url":"https://arxiv.org/pdf/2404.10995v1.pdf","comment":"22 pages, 11 figures"},{"id":"http://arxiv.org/abs/2403.20208v5","updated":"2024-04-17T02:11:56Z","published":"2024-03-29T14:41:21Z","title":"Unleashing the Potential of Large Language Models for Predictive Tabular\n  Tasks in Data Science","summary":"  In the domain of data science, the predictive tasks of classification,\nregression, and imputation of missing values are commonly encountered\nchallenges associated with tabular data. This research endeavors to apply Large\nLanguage Models (LLMs) towards addressing these predictive tasks. Despite their\nproficiency in comprehending natural language, LLMs fall short in dealing with\nstructured tabular data. This limitation stems from their lacking exposure to\nthe intricacies of tabular data during their foundational training. Our\nresearch aims to mitigate this gap by compiling a comprehensive corpus of\ntables annotated with instructions and executing large-scale training of\nLlama-2 on this enriched dataset. Furthermore, we investigate the practical\napplication of applying the trained model to zero-shot prediction, few-shot\nprediction, and in-context learning scenarios. Through extensive experiments,\nour methodology has shown significant improvements over existing benchmarks.\nThese advancements highlight the efficacy of tailoring LLM training to solve\ntable-related problems in data science, thereby establishing a new benchmark in\nthe utilization of LLMs for enhancing tabular intelligence.\n","authors":["Yazheng Yang","Yuqi Wang","Sankalok Sen","Lei Li","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2403.20208v5.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2404.10991v1","updated":"2024-04-17T02:04:10Z","published":"2024-04-17T02:04:10Z","title":"Function Approximation for Reinforcement Learning Controller for Energy\n  from Spread Waves","summary":"  The industrial multi-generator Wave Energy Converters (WEC) must handle\nmultiple simultaneous waves coming from different directions called spread\nwaves. These complex devices in challenging circumstances need controllers with\nmultiple objectives of energy capture efficiency, reduction of structural\nstress to limit maintenance, and proactive protection against high waves. The\nMulti-Agent Reinforcement Learning (MARL) controller trained with the Proximal\nPolicy Optimization (PPO) algorithm can handle these complexities. In this\npaper, we explore different function approximations for the policy and critic\nnetworks in modeling the sequential nature of the system dynamics and find that\nthey are key to better performance. We investigated the performance of a fully\nconnected neural network (FCN), LSTM, and Transformer model variants with\nvarying depths and gated residual connections. Our results show that the\ntransformer model of moderate depth with gated residual connections around the\nmulti-head attention, multi-layer perceptron, and the transformer block (STrXL)\nproposed in this paper is optimal and boosts energy efficiency by an average of\n22.1% for these complex spread waves over the existing spring damper (SD)\ncontroller. Furthermore, unlike the default SD controller, the transformer\ncontroller almost eliminated the mechanical stress from the rotational yaw\nmotion for angled waves. Demo: https://tinyurl.com/yueda3jh\n","authors":["Soumyendu Sarkar","Vineet Gundecha","Sahand Ghorbanpour","Alexander Shmakov","Ashwin Ramesh Babu","Avisek Naug","Alexandre Pichard","Mathieu Cocho"],"pdf_url":"https://arxiv.org/pdf/2404.10991v1.pdf","comment":"IJCAI 2023, Proceedings of the Thirty-Second International Joint\n  Conference on Artificial IntelligenceAugust 2023"},{"id":"http://arxiv.org/abs/2312.08358v2","updated":"2024-04-17T01:58:09Z","published":"2023-12-13T18:51:34Z","title":"Distributional Preference Learning: Understanding and Accounting for\n  Hidden Context in RLHF","summary":"  In practice, preference learning from human feedback depends on incomplete\ndata with hidden context. Hidden context refers to data that affects the\nfeedback received, but which is not represented in the data used to train a\npreference model. This captures common issues of data collection, such as\nhaving human annotators with varied preferences, cognitive processes that\nresult in seemingly irrational behavior, and combining data labeled according\nto different criteria. We prove that standard applications of preference\nlearning, including reinforcement learning from human feedback (RLHF),\nimplicitly aggregate over hidden contexts according to a well-known voting rule\ncalled Borda count. We show this can produce counter-intuitive results that are\nvery different from other methods which implicitly aggregate via expected\nutility. Furthermore, our analysis formalizes the way that preference learning\nfrom users with diverse values tacitly implements a social choice function. A\nkey implication of this result is that annotators have an incentive to\nmisreport their preferences in order to influence the learned model, leading to\nvulnerabilities in the deployment of RLHF. As a step towards mitigating these\nproblems, we introduce a class of methods called distributional preference\nlearning (DPL). DPL methods estimate a distribution of possible score values\nfor each alternative in order to better account for hidden context.\nExperimental results indicate that applying DPL to RLHF for LLM chatbots\nidentifies hidden context in the data and significantly reduces subsequent\njailbreak vulnerability. Our code and data are available at\nhttps://github.com/cassidylaidlaw/hidden-context\n","authors":["Anand Siththaranjan","Cassidy Laidlaw","Dylan Hadfield-Menell"],"pdf_url":"https://arxiv.org/pdf/2312.08358v2.pdf","comment":"Presented at ICLR 2024"},{"id":"http://arxiv.org/abs/2404.10989v1","updated":"2024-04-17T01:53:03Z","published":"2024-04-17T01:53:03Z","title":"FairSSD: Understanding Bias in Synthetic Speech Detectors","summary":"  Methods that can generate synthetic speech which is perceptually\nindistinguishable from speech recorded by a human speaker, are easily\navailable. Several incidents report misuse of synthetic speech generated from\nthese methods to commit fraud. To counter such misuse, many methods have been\nproposed to detect synthetic speech. Some of these detectors are more\ninterpretable, can generalize to detect synthetic speech in the wild and are\nrobust to noise. However, limited work has been done on understanding bias in\nthese detectors. In this work, we examine bias in existing synthetic speech\ndetectors to determine if they will unfairly target a particular gender, age\nand accent group. We also inspect whether these detectors will have a higher\nmisclassification rate for bona fide speech from speech-impaired speakers w.r.t\nfluent speakers. Extensive experiments on 6 existing synthetic speech detectors\nusing more than 0.9 million speech signals demonstrate that most detectors are\ngender, age and accent biased, and future work is needed to ensure fairness. To\nsupport future research, we release our evaluation dataset, models used in our\nstudy and source code at https://gitlab.com/viper-purdue/fairssd.\n","authors":["Amit Kumar Singh Yadav","Kratika Bhagtani","Davide Salvi","Paolo Bestagini","Edward J. Delp"],"pdf_url":"https://arxiv.org/pdf/2404.10989v1.pdf","comment":"Accepted at CVPR 2024 (WMF)"},{"id":"http://arxiv.org/abs/2404.10984v1","updated":"2024-04-17T01:31:00Z","published":"2024-04-17T01:31:00Z","title":"Graph Continual Learning with Debiased Lossless Memory Replay","summary":"  Real-life graph data often expands continually, rendering the learning of\ngraph neural networks (GNNs) on static graph data impractical. Graph continual\nlearning (GCL) tackles this problem by continually adapting GNNs to the\nexpanded graph of the current task while maintaining the performance over the\ngraph of previous tasks. Memory replay-based methods, which aim to replay data\nof previous tasks when learning new tasks, have been explored as one principled\napproach to mitigate the forgetting of the knowledge learned from the previous\ntasks. In this paper we extend this methodology with a novel framework, called\nDebiased Lossless Memory replay (DeLoMe). Unlike existing methods that sample\nnodes/edges of previous graphs to construct the memory, DeLoMe learns small\nlossless synthetic node representations as the memory. The learned memory can\nnot only preserve the graph data privacy but also capture the holistic graph\ninformation, for which the sampling-based methods are not viable. Further,\nprior methods suffer from bias toward the current task due to the data\nimbalance between the classes in the memory data and the current data. A\ndebiased GCL loss function is devised in DeLoMe to effectively alleviate this\nbias. Extensive experiments on four graph datasets show the effectiveness of\nDeLoMe under both class- and task-incremental learning settings.\n","authors":["Chaoxi Niu","Guansong Pang","Ling Chen"],"pdf_url":"https://arxiv.org/pdf/2404.10984v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2404.10980v1","updated":"2024-04-17T01:26:15Z","published":"2024-04-17T01:26:15Z","title":"Hyper Evidential Deep Learning to Quantify Composite Classification\n  Uncertainty","summary":"  Deep neural networks (DNNs) have been shown to perform well on exclusive,\nmulti-class classification tasks. However, when different classes have similar\nvisual features, it becomes challenging for human annotators to differentiate\nthem. This scenario necessitates the use of composite class labels. In this\npaper, we propose a novel framework called Hyper-Evidential Neural Network\n(HENN) that explicitly models predictive uncertainty due to composite class\nlabels in training data in the context of the belief theory called Subjective\nLogic (SL). By placing a grouped Dirichlet distribution on the class\nprobabilities, we treat predictions of a neural network as parameters of\nhyper-subjective opinions and learn the network that collects both single and\ncomposite evidence leading to these hyper-opinions by a deterministic DNN from\ndata. We introduce a new uncertainty type called vagueness originally designed\nfor hyper-opinions in SL to quantify composite classification uncertainty for\nDNNs. Our results demonstrate that HENN outperforms its state-of-the-art\ncounterparts based on four image datasets. The code and datasets are available\nat: https://github.com/Hugo101/HyperEvidentialNN.\n","authors":["Changbin Li","Kangshuo Li","Yuzhe Ou","Lance M. Kaplan","Audun J√∏sang","Jin-Hee Cho","Dong Hyun Jeong","Feng Chen"],"pdf_url":"https://arxiv.org/pdf/2404.10980v1.pdf","comment":"In Proceedings of The Twelfth International Conference on Learning\n  Representations, ICLR 2024"},{"id":"http://arxiv.org/abs/2404.10978v1","updated":"2024-04-17T01:23:49Z","published":"2024-04-17T01:23:49Z","title":"Leveraging 3D LiDAR Sensors to Enable Enhanced Urban Safety and Public\n  Health: Pedestrian Monitoring and Abnormal Activity Detection","summary":"  The integration of Light Detection and Ranging (LiDAR) and Internet of Things\n(IoT) technologies offers transformative opportunities for public health\ninformatics in urban safety and pedestrian well-being. This paper proposes a\nnovel framework utilizing these technologies for enhanced 3D object detection\nand activity classification in urban traffic scenarios. By employing elevated\nLiDAR, we obtain detailed 3D point cloud data, enabling precise pedestrian\nactivity monitoring. To overcome urban data scarcity, we create a specialized\ndataset through simulated traffic environments in Blender, facilitating\ntargeted model training. Our approach employs a modified Point\nVoxel-Region-based Convolutional Neural Network (PV-RCNN) for robust 3D\ndetection and PointNet for classifying pedestrian activities, significantly\nbenefiting urban traffic management and public health by offering insights into\npedestrian behavior and promoting safer urban environments. Our dual-model\napproach not only enhances urban traffic management but also contributes\nsignificantly to public health by providing insights into pedestrian behavior\nand promoting safer urban environment.\n","authors":["Nawfal Guefrachi","Jian Shi","Hakim Ghazzai","Ahmad Alsharoa"],"pdf_url":"https://arxiv.org/pdf/2404.10978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10976v1","updated":"2024-04-17T01:17:10Z","published":"2024-04-17T01:17:10Z","title":"Group-Aware Coordination Graph for Multi-Agent Reinforcement Learning","summary":"  Cooperative Multi-Agent Reinforcement Learning (MARL) necessitates seamless\ncollaboration among agents, often represented by an underlying relation graph.\nExisting methods for learning this graph primarily focus on agent-pair\nrelations, neglecting higher-order relationships. While several approaches\nattempt to extend cooperation modelling to encompass behaviour similarities\nwithin groups, they commonly fall short in concurrently learning the latent\ngraph, thereby constraining the information exchange among partially observed\nagents. To overcome these limitations, we present a novel approach to infer the\nGroup-Aware Coordination Graph (GACG), which is designed to capture both the\ncooperation between agent pairs based on current observations and group-level\ndependencies from behaviour patterns observed across trajectories. This graph\nis further used in graph convolution for information exchange between agents\nduring decision-making. To further ensure behavioural consistency among agents\nwithin the same group, we introduce a group distance loss, which promotes group\ncohesion and encourages specialization between groups. Our evaluations,\nconducted on StarCraft II micromanagement tasks, demonstrate GACG's superior\nperformance. An ablation study further provides experimental evidence of the\neffectiveness of each component of our method.\n","authors":["Wei Duan","Jie Lu","Junyu Xuan"],"pdf_url":"https://arxiv.org/pdf/2404.10976v1.pdf","comment":"Accepted by IJCAI 2024"},{"id":"http://arxiv.org/abs/2310.11700v2","updated":"2024-04-17T01:04:07Z","published":"2023-10-18T04:15:39Z","title":"Runner re-identification from single-view running video in the\n  open-world setting","summary":"  In many sports, player re-identification is crucial for automatic video\nprocessing and analysis. However, most of the current studies on player\nre-identification in multi- or single-view sports videos focus on\nre-identification in the closed-world setting using labeled image dataset, and\nplayer re-identification in the open-world setting for automatic video analysis\nis not well developed. In this paper, we propose a runner re-identification\nsystem that directly processes single-view video to address the open-world\nsetting. In the open-world setting, we cannot use labeled dataset and have to\nprocess video directly. The proposed system automatically processes raw video\nas input to identify runners, and it can identify runners even when they are\nframed out multiple times. For the automatic processing, we first detect the\nrunners in the video using the pre-trained YOLOv8 and the fine-tuned\nEfficientNet. We then track the runners using ByteTrack and detect their shoes\nwith the fine-tuned YOLOv8. Finally, we extract the image features of the\nrunners using an unsupervised method with the gated recurrent unit autoencoder\nand global and local features mixing. To improve the accuracy of runner\nre-identification, we use shoe images as local image features and dynamic\nfeatures of running sequence images. We evaluated the system on a running\npractice video dataset and showed that the proposed method identified runners\nwith higher accuracy than some state-of-the-art models in unsupervised\nre-identification. We also showed that our proposed local image feature and\nrunning dynamic feature were effective for runner re-identification. Our runner\nre-identification system can be useful for the automatic analysis of running\nvideos.\n","authors":["Tomohiro Suzuki","Kazushi Tsutsui","Kazuya Takeda","Keisuke Fujii"],"pdf_url":"https://arxiv.org/pdf/2310.11700v2.pdf","comment":"20 pages, 7 figures"},{"id":"http://arxiv.org/abs/2404.09466v2","updated":"2024-04-17T01:03:00Z","published":"2024-04-15T05:35:09Z","title":"Scoring Intervals using Non-Hierarchical Transformer For Automatic Piano\n  Transcription","summary":"  The neural semi-Markov Conditional Random Field (semi-CRF) framework has\ndemonstrated promise for event-based piano transcription. In this framework,\nall events (notes or pedals) are represented as closed intervals tied to\nspecific event types. The neural semi-CRF approach requires an interval scoring\nmatrix that assigns a score for every candidate interval. However, designing an\nefficient and expressive architecture for scoring intervals is not trivial. In\nthis paper, we introduce a simple method for scoring intervals using scaled\ninner product operations that resemble how attention scoring is done in\ntransformers. We show theoretically that, due to the special structure from\nencoding the non-overlapping intervals, under a mild condition, the inner\nproduct operations are expressive enough to represent an ideal scoring matrix\nthat can yield the correct transcription result. We then demonstrate that an\nencoder-only non-hierarchical transformer backbone, operating only on a\nlow-time-resolution feature map, is capable of transcribing piano notes and\npedals with high accuracy and time precision. The experiment shows that our\napproach achieves the new state-of-the-art performance across all subtasks in\nterms of the F1 measure on the Maestro dataset.\n","authors":["Yujia Yan","Zhiyao Duan"],"pdf_url":"https://arxiv.org/pdf/2404.09466v2.pdf","comment":"Fixed Typos"},{"id":"http://arxiv.org/abs/2402.14301v2","updated":"2024-04-17T00:55:09Z","published":"2024-02-22T05:41:24Z","title":"GenSERP: Large Language Models for Whole Page Presentation","summary":"  The advent of large language models (LLMs) brings an opportunity to minimize\nthe effort in search engine result page (SERP) organization. In this paper, we\npropose GenSERP, a framework that leverages LLMs with vision in a few-shot\nsetting to dynamically organize intermediate search results, including\ngenerated chat answers, website snippets, multimedia data, knowledge panels\ninto a coherent SERP layout based on a user's query. Our approach has three\nmain stages: (1) An information gathering phase where the LLM continuously\norchestrates API tools to retrieve different types of items, and proposes\ncandidate layouts based on the retrieved items, until it's confident enough to\ngenerate the final result. (2) An answer generation phase where the LLM\npopulates the layouts with the retrieved content. In this phase, the LLM\nadaptively optimize the ranking of items and UX configurations of the SERP.\nConsequently, it assigns a location on the page to each item, along with the UX\ndisplay details. (3) A scoring phase where an LLM with vision scores all the\ngenerated SERPs based on how likely it can satisfy the user. It then send the\none with highest score to rendering. GenSERP features two generation paradigms.\nFirst, coarse-to-fine, which allow it to approach optimal layout in a more\nmanageable way, (2) beam search, which give it a better chance to hit the\noptimal solution compared to greedy decoding. Offline experimental results on\nreal-world data demonstrate how LLMs can contextually organize heterogeneous\nsearch results on-the-fly and provide a promising user experience.\n","authors":["Zhenning Zhang","Yunan Zhang","Suyu Ge","Guangwei Weng","Mridu Narang","Xia Song","Saurabh Tiwary"],"pdf_url":"https://arxiv.org/pdf/2402.14301v2.pdf","comment":"Microsoft corp policy"},{"id":"http://arxiv.org/abs/2402.04437v3","updated":"2024-04-17T00:24:57Z","published":"2024-02-06T22:15:09Z","title":"Structured Entity Extraction Using Large Language Models","summary":"  Recent advances in machine learning have significantly impacted the field of\ninformation extraction, with Large Language Models (LLMs) playing a pivotal\nrole in extracting structured information from unstructured text. Prior works\ntypically represent information extraction as triplet-centric and use classical\nmetrics such as precision and recall for evaluation. We reformulate the task to\nbe entity-centric, enabling the use of diverse metrics that can provide more\ninsights from various perspectives. We contribute to the field by introducing\nStructured Entity Extraction (SEE) and proposing the Approximate Entity Set\nOverlaP (AESOP) metric, designed to appropriately assess model performance.\nLater, we introduce a new model that harnesses the power of LLMs for enhanced\neffectiveness and efficiency by decomposing the extraction task into multiple\nstages. Quantitative and human side-by-side evaluations confirm that our model\noutperforms baselines, offering promising directions for future advancements in\nstructured entity extraction.\n","authors":["Haolun Wu","Ye Yuan","Liana Mikaelyan","Alexander Meulemans","Xue Liu","James Hensman","Bhaskar Mitra"],"pdf_url":"https://arxiv.org/pdf/2402.04437v3.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2404.11375v1","updated":"2024-04-17T13:33:09Z","published":"2024-04-17T13:33:09Z","title":"Text-controlled Motion Mamba: Text-Instructed Temporal Grounding of\n  Human Motion","summary":"  Human motion understanding is a fundamental task with diverse practical\napplications, facilitated by the availability of large-scale motion capture\ndatasets. Recent studies focus on text-motion tasks, such as text-based motion\ngeneration, editing and question answering. In this study, we introduce the\nnovel task of text-based human motion grounding (THMG), aimed at precisely\nlocalizing temporal segments corresponding to given textual descriptions within\nuntrimmed motion sequences. Capturing global temporal information is crucial\nfor the THMG task. However, transformer-based models that rely on global\ntemporal self-attention face challenges when handling long untrimmed sequences\ndue to the quadratic computational cost. We address these challenges by\nproposing Text-controlled Motion Mamba (TM-Mamba), a unified model that\nintegrates temporal global context, language query control, and spatial graph\ntopology with only linear memory cost. The core of the model is a\ntext-controlled selection mechanism which dynamically incorporates global\ntemporal information based on text query. The model is further enhanced to be\ntopology-aware through the integration of relational embeddings. For\nevaluation, we introduce BABEL-Grounding, the first text-motion dataset that\nprovides detailed textual descriptions of human actions along with their\ncorresponding temporal segments. Extensive evaluations demonstrate the\neffectiveness of TM-Mamba on BABEL-Grounding.\n","authors":["Xinghan Wang","Zixi Kang","Yadong Mu"],"pdf_url":"https://arxiv.org/pdf/2404.11375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.15401v4","updated":"2024-04-17T12:40:41Z","published":"2023-06-27T11:54:57Z","title":"Explainable Multimodal Emotion Reasoning: a Promising Way to Open-set\n  Emotion Recognition","summary":"  Multimodal emotion recognition is an active research topic in artificial\nintelligence. Its main goal is to integrate multi-modalities to identify human\nemotional states. Current works generally assume accurate emotion labels for\nbenchmark datasets and focus on developing more effective architectures.\nHowever, emotions have inherent ambiguity and subjectivity. To obtain more\nreliable labels, existing datasets usually restrict the label space to some\nbasic categories, then hire multiple annotators and use majority voting to\nselect the most likely label. However, this process may cause some correct but\nnon-candidate or non-majority labels to be ignored. To improve reliability\nwithout ignoring subtle emotions, we propose a new task called\n``\\textbf{Explainable Multimodal Emotion Reasoning (EMER)}''. In contrast to\ntraditional tasks that focus on predicting emotions, EMER takes a step further\nby providing explanations for these predictions. Through this task, we can\nextract more reliable labels since each label has a certain basis. Meanwhile,\nwe use LLMs to disambiguate unimodal descriptions and generate more complete\nmultimodal EMER descriptions. From them, we can extract subtle labels,\nproviding a promising approach for open-set emotion recognition. This paper\npresents our initial efforts, where we introduce a new dataset, establish\nbaselines, and define evaluation metrics. In addition, EMER can also be used as\na benchmark dataset to evaluate the audio-video-text understanding capabilities\nof multimodal LLMs. To facilitate further research, we will make the code and\ndata available at: https://github.com/zeroQiaoba/AffectGPT.\n","authors":["Zheng Lian","Licai Sun","Haiyang Sun","Hao Gu","Zhuofan Wen","Siyuan Zhang","Shun Chen","Mingyu Xu","Ke Xu","Lan Chen","Jiangyan Yi","Bin Liu","Jianhua Tao"],"pdf_url":"https://arxiv.org/pdf/2306.15401v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11209v1","updated":"2024-04-17T09:45:43Z","published":"2024-04-17T09:45:43Z","title":"Prompt-Guided Generation of Structured Chest X-Ray Report Using a\n  Pre-trained LLM","summary":"  Medical report generation automates radiology descriptions from images,\neasing the burden on physicians and minimizing errors. However, current methods\nlack structured outputs and physician interactivity for clear, clinically\nrelevant reports. Our method introduces a prompt-guided approach to generate\nstructured chest X-ray reports using a pre-trained large language model (LLM).\nFirst, we identify anatomical regions in chest X-rays to generate focused\nsentences that center on key visual elements, thereby establishing a structured\nreport foundation with anatomy-based sentences. We also convert the detected\nanatomy into textual prompts conveying anatomical comprehension to the LLM.\nAdditionally, the clinical context prompts guide the LLM to emphasize\ninteractivity and clinical requirements. By integrating anatomy-focused\nsentences and anatomy/clinical prompts, the pre-trained LLM can generate\nstructured chest X-ray reports tailored to prompted anatomical regions and\nclinical contexts. We evaluate using language generation and clinical\neffectiveness metrics, demonstrating strong performance.\n","authors":["Hongzhao Li","Hongyu Wang","Xia Sun","Hua He","Jun Feng"],"pdf_url":"https://arxiv.org/pdf/2404.11209v1.pdf","comment":"Accepted by IEEE Conference on Multimedia Expo 2024"},{"id":"http://arxiv.org/abs/2404.11119v1","updated":"2024-04-17T07:07:41Z","published":"2024-04-17T07:07:41Z","title":"DRepMRec: A Dual Representation Learning Framework for Multimodal\n  Recommendation","summary":"  Multimodal Recommendation focuses mainly on how to effectively integrate\nbehavior and multimodal information in the recommendation task. Previous works\nsuffer from two major issues. Firstly, the training process tightly couples the\nbehavior module and multimodal module by jointly optimizing them using the\nsharing model parameters, which leads to suboptimal performance since behavior\nsignals and modality signals often provide opposite guidance for the parameters\nupdates. Secondly, previous approaches fail to take into account the\nsignificant distribution differences between behavior and modality when they\nattempt to fuse behavior and modality information. This resulted in a\nmisalignment between the representations of behavior and modality. To address\nthese challenges, in this paper, we propose a novel Dual Representation\nlearning framework for Multimodal Recommendation called DRepMRec, which\nintroduce separate dual lines for coupling problem and Behavior-Modal Alignment\n(BMA) for misalignment problem. Specifically, DRepMRec leverages two\nindependent lines of representation learning to calculate behavior and modal\nrepresentations. After obtaining separate behavior and modal representations,\nwe design a Behavior-Modal Alignment Module (BMA) to align and fuse the dual\nrepresentations to solve the misalignment problem. Furthermore, we integrate\nthe BMA into other recommendation models, resulting in consistent performance\nimprovements. To ensure dual representations maintain their semantic\nindependence during alignment, we introduce Similarity-Supervised Signal (SSS)\nfor representation learning. We conduct extensive experiments on three public\ndatasets and our method achieves state-of-the-art (SOTA) results. The source\ncode will be available upon acceptance.\n","authors":["Kangning Zhang","Yingjie Qin","Ruilong Su","Yifan Liu","Jiarui Jin","Weinan Zhang","Yong Yu"],"pdf_url":"https://arxiv.org/pdf/2404.11119v1.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2404.11116v1","updated":"2024-04-17T07:01:29Z","published":"2024-04-17T07:01:29Z","title":"Music Enhancement with Deep Filters: A Technical Report for The ICASSP\n  2024 Cadenza Challenge","summary":"  In this challenge, we disentangle the deep filters from the original\nDeepfilterNet and incorporate them into our Spec-UNet-based network to further\nimprove a hybrid Demucs (hdemucs) based remixing pipeline. The motivation\nbehind the use of the deep filter component lies at its potential in better\nhandling temporal fine structures. We demonstrate an incremental improvement in\nboth the Signal-to-Distortion Ratio (SDR) and the Hearing Aid Audio Quality\nIndex (HAAQI) metrics when comparing the performance of hdemucs against\ndifferent versions of our model.\n","authors":["Keren Shao","Ke Chen","Shlomo Dubnov"],"pdf_url":"https://arxiv.org/pdf/2404.11116v1.pdf","comment":"2 pages, 2 figures, 1 tables, Proceedings of the International\n  Conference on Acoustics, Speech, and Signal Processing, ICASSP 2024"},{"id":"http://arxiv.org/abs/2404.10989v1","updated":"2024-04-17T01:53:03Z","published":"2024-04-17T01:53:03Z","title":"FairSSD: Understanding Bias in Synthetic Speech Detectors","summary":"  Methods that can generate synthetic speech which is perceptually\nindistinguishable from speech recorded by a human speaker, are easily\navailable. Several incidents report misuse of synthetic speech generated from\nthese methods to commit fraud. To counter such misuse, many methods have been\nproposed to detect synthetic speech. Some of these detectors are more\ninterpretable, can generalize to detect synthetic speech in the wild and are\nrobust to noise. However, limited work has been done on understanding bias in\nthese detectors. In this work, we examine bias in existing synthetic speech\ndetectors to determine if they will unfairly target a particular gender, age\nand accent group. We also inspect whether these detectors will have a higher\nmisclassification rate for bona fide speech from speech-impaired speakers w.r.t\nfluent speakers. Extensive experiments on 6 existing synthetic speech detectors\nusing more than 0.9 million speech signals demonstrate that most detectors are\ngender, age and accent biased, and future work is needed to ensure fairness. To\nsupport future research, we release our evaluation dataset, models used in our\nstudy and source code at https://gitlab.com/viper-purdue/fairssd.\n","authors":["Amit Kumar Singh Yadav","Kratika Bhagtani","Davide Salvi","Paolo Bestagini","Edward J. Delp"],"pdf_url":"https://arxiv.org/pdf/2404.10989v1.pdf","comment":"Accepted at CVPR 2024 (WMF)"}]},"2024-04-18T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2404.12390v1","updated":"2024-04-18T17:59:54Z","published":"2024-04-18T17:59:54Z","title":"BLINK: Multimodal Large Language Models Can See but Not Perceive","summary":"  We introduce Blink, a new benchmark for multimodal language models (LLMs)\nthat focuses on core visual perception abilities not found in other\nevaluations. Most of the Blink tasks can be solved by humans \"within a blink\"\n(e.g., relative depth estimation, visual correspondence, forensics detection,\nand multi-view reasoning). However, we find these perception-demanding tasks\ncast significant challenges for current multimodal LLMs because they resist\nmediation through natural language. Blink reformats 14 classic computer vision\ntasks into 3,807 multiple-choice questions, paired with single or multiple\nimages and visual prompting. While humans get 95.70% accuracy on average, Blink\nis surprisingly challenging for existing multimodal LLMs: even the\nbest-performing GPT-4V and Gemini achieve accuracies of 51.26% and 45.72%, only\n13.17% and 7.63% higher than random guessing, indicating that such perception\nabilities have not \"emerged\" yet in recent multimodal LLMs. Our analysis also\nhighlights that specialist CV models could solve these problems much better,\nsuggesting potential pathways for future improvements. We believe Blink will\nstimulate the community to help multimodal LLMs catch up with human-level\nvisual perception.\n","authors":["Xingyu Fu","Yushi Hu","Bangzheng Li","Yu Feng","Haoyu Wang","Xudong Lin","Dan Roth","Noah A. Smith","Wei-Chiu Ma","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2404.12390v1.pdf","comment":"Multimodal Benchmark, Project Url: https://zeyofu.github.io/blink/"},{"id":"http://arxiv.org/abs/2404.12387v1","updated":"2024-04-18T17:59:48Z","published":"2024-04-18T17:59:48Z","title":"Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language\n  Models","summary":"  We introduce Reka Core, Flash, and Edge, a series of powerful multimodal\nlanguage models trained from scratch by Reka. Reka models are able to process\nand reason with text, images, video, and audio inputs. This technical report\ndiscusses details of training some of these models and provides comprehensive\nevaluation results. We show that Reka Edge and Reka Flash are not only\nstate-of-the-art but also outperform many much larger models, delivering\noutsized values for their respective compute class. Meanwhile, our most capable\nand largest model, Reka Core, approaches the best frontier models on both\nautomatic evaluations and blind human evaluations. On image question answering\nbenchmarks (e.g. MMMU, VQAv2), Core performs competitively to GPT4-V.\nMeanwhile, on multimodal chat, Core ranks as the second most preferred model\nunder a blind third-party human evaluation setup, outperforming other models\nsuch as Claude 3 Opus. On text benchmarks, Core not only performs competitively\nto other frontier models on a set of well-established benchmarks (e.g. MMLU,\nGSM8K) but also outperforms GPT4-0613 on human evaluation. On video question\nanswering (Perception-Test), Core outperforms Gemini Ultra. Models are shipped\nin production at http://chat.reka.ai . A showcase of non cherry picked\nqualitative examples can also be found at http://showcase.reka.ai .\n","authors":["Aitor Ormazabal","Che Zheng","Cyprien de Masson d'Autume","Dani Yogatama","Deyu Fu","Donovan Ong","Eric Chen","Eugenie Lamprecht","Hai Pham","Isaac Ong","Kaloyan Aleksiev","Lei Li","Matthew Henderson","Max Bain","Mikel Artetxe","Nishant Relan","Piotr Padlewski","Qi Liu","Ren Chen","Samuel Phua","Yazheng Yang","Yi Tay","Yuqi Wang","Zhongkai Zhu","Zhihui Xie"],"pdf_url":"https://arxiv.org/pdf/2404.12387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12365v1","updated":"2024-04-18T17:48:05Z","published":"2024-04-18T17:48:05Z","title":"When LLMs are Unfit Use FastFit: Fast and Effective Text Classification\n  with Many Classes","summary":"  We present FastFit, a method, and a Python package design to provide fast and\naccurate few-shot classification, especially for scenarios with many\nsemantically similar classes. FastFit utilizes a novel approach integrating\nbatch contrastive learning and token-level similarity score. Compared to\nexisting few-shot learning packages, such as SetFit, Transformers, or few-shot\nprompting of large language models via API calls, FastFit significantly\nimproves multiclass classification performance in speed and accuracy across\nFewMany, our newly curated English benchmark, and Multilingual datasets.\nFastFit demonstrates a 3-20x improvement in training speed, completing training\nin just a few seconds. The FastFit package is now available on GitHub and PyPi,\npresenting a user-friendly solution for NLP practitioners.\n","authors":["Asaf Yehudai","Elron Bendel"],"pdf_url":"https://arxiv.org/pdf/2404.12365v1.pdf","comment":"Accepted to NAACL"},{"id":"http://arxiv.org/abs/2306.11371v3","updated":"2024-04-18T17:36:53Z","published":"2023-06-20T08:27:42Z","title":"Visually grounded few-shot word learning in low-resource settings","summary":"  We propose a visually grounded speech model that learns new words and their\nvisual depictions from just a few word-image example pairs. Given a set of test\nimages and a spoken query, we ask the model which image depicts the query word.\nPrevious work has simplified this few-shot learning problem by either using an\nartificial setting with digit word-image pairs or by using a large number of\nexamples per class. Moreover, all previous studies were performed using English\nspeech-image data. We propose an approach that can work on natural word-image\npairs but with less examples, i.e. fewer shots, and then illustrate how this\napproach can be applied for multimodal few-shot learning in a real low-resource\nlanguage, Yor\\`ub\\'a. Our approach involves using the given word-image example\npairs to mine new unsupervised word-image training pairs from large collections\nof unlabelled speech and images. Additionally, we use a word-to-image attention\nmechanism to determine word-image similarity. With this new model, we achieve\nbetter performance with fewer shots than previous approaches on an existing\nEnglish benchmark. Many of the model's mistakes are due to confusion between\nvisual concepts co-occurring in similar contexts. The experiments on Yor\\`ub\\'a\nshow the benefit of transferring knowledge from a multimodal model trained on a\nlarger set of English speech-image data.\n","authors":["Leanne Nortje","Dan Oneata","Herman Kamper"],"pdf_url":"https://arxiv.org/pdf/2306.11371v3.pdf","comment":"Accepted to TASLP. arXiv admin note: substantial text overlap with\n  arXiv:2305.15937"},{"id":"http://arxiv.org/abs/2404.12342v1","updated":"2024-04-18T17:16:16Z","published":"2024-04-18T17:16:16Z","title":"Large Language Models in Targeted Sentiment Analysis","summary":"  In this paper we investigate the use of decoder-based generative transformers\nfor extracting sentiment towards the named entities in Russian news articles.\nWe study sentiment analysis capabilities of instruction-tuned large language\nmodels (LLMs). We consider the dataset of RuSentNE-2023 in our study. The first\ngroup of experiments was aimed at the evaluation of zero-shot capabilities of\nLLMs with closed and open transparencies. The second covers the fine-tuning of\nFlan-T5 using the \"chain-of-thought\" (CoT) three-hop reasoning framework\n(THoR). We found that the results of the zero-shot approaches are similar to\nthe results achieved by baseline fine-tuned encoder-based transformers\n(BERT-base). Reasoning capabilities of the fine-tuned Flan-T5 models with THoR\nachieve at least 5% increment with the base-size model compared to the results\nof the zero-shot experiment. The best results of sentiment analysis on\nRuSentNE-2023 were achieved by fine-tuned Flan-T5-xl, which surpassed the\nresults of previous state-of-the-art transformer-based classifiers. Our CoT\napplication framework is publicly available:\nhttps://github.com/nicolay-r/Reasoning-for-Sentiment-Analysis-Framework\n","authors":["Nicolay Rusnachenko","Anton Golubev","Natalia Loukachevitch"],"pdf_url":"https://arxiv.org/pdf/2404.12342v1.pdf","comment":"Fine-tuned Flan-T5-xl outperforms the top #1 results of\n  transformer-based classifier in RuSentNE-2023 competition, to appear in\n  Lobachevskii Journal of Mathematics No.8/2024 proceedings"},{"id":"http://arxiv.org/abs/2404.02124v3","updated":"2024-04-18T17:12:19Z","published":"2024-04-02T17:31:58Z","title":"Exploring Automated Distractor Generation for Math Multiple-choice\n  Questions via Large Language Models","summary":"  Multiple-choice questions (MCQs) are ubiquitous in almost all levels of\neducation since they are easy to administer, grade, and are a reliable format\nin assessments and practices. One of the most important aspects of MCQs is the\ndistractors, i.e., incorrect options that are designed to target common errors\nor misconceptions among real students. To date, the task of crafting\nhigh-quality distractors largely remains a labor and time-intensive process for\nteachers and learning content designers, which has limited scalability. In this\nwork, we study the task of automated distractor generation in the domain of\nmath MCQs and explore a wide variety of large language model (LLM)-based\napproaches, from in-context learning to fine-tuning. We conduct extensive\nexperiments using a real-world math MCQ dataset and find that although LLMs can\ngenerate some mathematically valid distractors, they are less adept at\nanticipating common errors or misconceptions among real students.\n","authors":["Wanyong Feng","Jaewook Lee","Hunter McNichols","Alexander Scarlatos","Digory Smith","Simon Woodhead","Nancy Otero Ornelas","Andrew Lan"],"pdf_url":"https://arxiv.org/pdf/2404.02124v3.pdf","comment":"NAACL 2024 findings"},{"id":"http://arxiv.org/abs/2404.03027v2","updated":"2024-04-18T17:11:53Z","published":"2024-04-03T19:23:18Z","title":"JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal\n  Large Language Models against Jailbreak Attacks","summary":"  With the rapid advancements in Multimodal Large Language Models (MLLMs),\nsecuring these models against malicious inputs while aligning them with human\nvalues has emerged as a critical challenge. In this paper, we investigate an\nimportant and unexplored question of whether techniques that successfully\njailbreak Large Language Models (LLMs) can be equally effective in jailbreaking\nMLLMs. To explore this issue, we introduce JailBreakV-28K, a pioneering\nbenchmark designed to assess the transferability of LLM jailbreak techniques to\nMLLMs, thereby evaluating the robustness of MLLMs against diverse jailbreak\nattacks. Utilizing a dataset of 2, 000 malicious queries that is also proposed\nin this paper, we generate 20, 000 text-based jailbreak prompts using advanced\njailbreak attacks on LLMs, alongside 8, 000 image-based jailbreak inputs from\nrecent MLLMs jailbreak attacks, our comprehensive dataset includes 28, 000 test\ncases across a spectrum of adversarial scenarios. Our evaluation of 10\nopen-source MLLMs reveals a notably high Attack Success Rate (ASR) for attacks\ntransferred from LLMs, highlighting a critical vulnerability in MLLMs that\nstems from their text-processing capabilities. Our findings underscore the\nurgent need for future research to address alignment vulnerabilities in MLLMs\nfrom both textual and visual inputs.\n","authors":["Weidi Luo","Siyuan Ma","Xiaogeng Liu","Xiaoyu Guo","Chaowei Xiao"],"pdf_url":"https://arxiv.org/pdf/2404.03027v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.02022v2","updated":"2024-04-18T17:06:17Z","published":"2023-08-03T20:29:27Z","title":"Efficient Sentiment Analysis: A Resource-Aware Evaluation of Feature\n  Extraction Techniques, Ensembling, and Deep Learning Models","summary":"  While reaching for NLP systems that maximize accuracy, other important\nmetrics of system performance are often overlooked. Prior models are easily\nforgotten despite their possible suitability in settings where large computing\nresources are unavailable or relatively more costly. In this paper, we perform\na broad comparative evaluation of document-level sentiment analysis models with\na focus on resource costs that are important for the feasibility of model\ndeployment and general climate consciousness. Our experiments consider\ndifferent feature extraction techniques, the effect of ensembling,\ntask-specific deep learning modeling, and domain-independent large language\nmodels (LLMs). We find that while a fine-tuned LLM achieves the best accuracy,\nsome alternate configurations provide huge (up to 24, 283 *) resource savings\nfor a marginal (<1%) loss in accuracy. Furthermore, we find that for smaller\ndatasets, the differences in accuracy shrink while the difference in resource\nconsumption grows further.\n","authors":["Mahammed Kamruzzaman","Gene Louis Kim"],"pdf_url":"https://arxiv.org/pdf/2308.02022v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12318v1","updated":"2024-04-18T16:52:36Z","published":"2024-04-18T16:52:36Z","title":"Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual\n  Alignment","summary":"  Aligning language models (LMs) based on human-annotated preference data is a\ncrucial step in obtaining practical and performant LM-based systems. However,\nmultilingual human preference data are difficult to obtain at scale, making it\nchallenging to extend this framework to diverse languages. In this work, we\nevaluate a simple approach for zero-shot cross-lingual alignment, where a\nreward model is trained on preference data in one source language and directly\napplied to other target languages. On summarization and open-ended dialog\ngeneration, we show that this method is consistently successful under\ncomprehensive evaluation settings, including human evaluation: cross-lingually\naligned models are preferred by humans over unaligned models on up to >70% of\nevaluation instances. We moreover find that a different-language reward model\nsometimes yields better aligned models than a same-language reward model. We\nalso identify best practices when there is no language-specific data for even\nsupervised finetuning, another component in alignment.\n","authors":["Zhaofeng Wu","Ananth Balashankar","Yoon Kim","Jacob Eisenstein","Ahmad Beirami"],"pdf_url":"https://arxiv.org/pdf/2404.12318v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13919v3","updated":"2024-04-18T16:50:43Z","published":"2024-02-21T16:33:22Z","title":"SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in\n  Clinical Summarization","summary":"  Large Language Models (LLMs) such as GPT & Llama have demonstrated\nsignificant achievements in summarization tasks but struggle with factual\ninaccuracies, a critical issue in clinical NLP applications where errors could\nlead to serious consequences. To counter the high costs and limited\navailability of expert-annotated data for factual alignment, this study\nintroduces an innovative pipeline that utilizes >100B parameter GPT variants\nlike GPT-3.5 & GPT-4 to act as synthetic experts to generate high-quality\nsynthetics feedback aimed at enhancing factual consistency in clinical note\nsummarization. Our research primarily focuses on edit feedback generated by\nthese synthetic feedback experts without additional human annotations,\nmirroring and optimizing the practical scenario in which medical professionals\nrefine AI system outputs. Although such 100B+ parameter GPT variants have\nproven to demonstrate expertise in various clinical NLP tasks, such as the\nMedical Licensing Examination, there is scant research on their capacity to act\nas synthetic feedback experts and deliver expert-level edit feedback for\nimproving the generation quality of weaker (<10B parameter) LLMs like GPT-2\n(1.5B) & Llama 2 (7B) in clinical domain. So in this work, we leverage 100B+\nGPT variants to act as synthetic feedback experts offering expert-level edit\nfeedback, that is used to reduce hallucinations and align weaker (<10B\nparameter) LLMs with medical facts using two distinct alignment algorithms (DPO\n& SALT), endeavoring to narrow the divide between AI-generated content and\nfactual accuracy. This highlights the substantial potential of LLM-based\nsynthetic edits in enhancing the alignment of clinical factuality.\n","authors":["Prakamya Mishra","Zonghai Yao","Parth Vashisht","Feiyun Ouyang","Beining Wang","Vidhi Dhaval Mody","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2402.13919v3.pdf","comment":"Equal contribution for the first two authors"},{"id":"http://arxiv.org/abs/2404.12299v1","updated":"2024-04-18T16:24:12Z","published":"2024-04-18T16:24:12Z","title":"Simultaneous Interpretation Corpus Construction by Large Language Models\n  in Distant Language Pair","summary":"  In Simultaneous Machine Translation (SiMT) systems, training with a\nsimultaneous interpretation (SI) corpus is an effective method for achieving\nhigh-quality yet low-latency systems. However, it is very challenging to curate\nsuch a corpus due to limitations in the abilities of annotators, and hence,\nexisting SI corpora are limited. Therefore, we propose a method to convert\nexisting speech translation corpora into interpretation-style data, maintaining\nthe original word order and preserving the entire source content using Large\nLanguage Models (LLM-SI-Corpus). We demonstrate that fine-tuning SiMT models in\ntext-to-text and speech-to-text settings with the LLM-SI-Corpus reduces\nlatencies while maintaining the same level of quality as the models trained\nwith offline datasets. The LLM-SI-Corpus is available at\n\\url{https://github.com/yusuke1997/LLM-SI-Corpus}.\n","authors":["Yusuke Sakai","Mana Makinae","Hidetaka Kamigaito","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2404.12299v1.pdf","comment":"23 pages, 9 figures"},{"id":"http://arxiv.org/abs/2402.15758v2","updated":"2024-04-18T16:23:16Z","published":"2024-02-24T08:10:39Z","title":"Chimera: A Lossless Decoding Method for Accelerating Large Language\n  Models Inference by Fusing all Tokens","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks. However, their widespread application is hindered by the\nresource-intensive decoding process. To address this challenge, current\napproaches have incorporated additional decoding heads to enable parallel\nprediction of multiple subsequent tokens, thereby achieving inference\nacceleration. Nevertheless, the accuracy of these decoding heads falls short of\nthe auto-regressive decoding approach.\n  In light of these limitations, we propose Chimera, a novel framework\nspecifically designed for speculative sampling. Within this framework, we\nintroduce a lightweight draft model that effectively utilizes previously\ngenerated tokens to predict subsequent words. To ensure both accuracy and\nefficiency, we present two strategies within the lightweight draft model.\nFirstly, we focus on capturing short-range dependencies at the bottom layer.\nSecondly, we leverage the readily available representations from the original\nLLM.Through empirical evaluation on the Vicuna and LlaMA-2 series, Chimera\ndemonstrates impressive results, achieving an average latency speedup ratio of\n2.7x compared to the vanilla auto-regressive decoding approach. This highlights\nthe potential of our proposed framework in significantly improving the\nefficiency of large language models during the decoding process.\n","authors":["Ziqian Zeng","Jiahong Yu","Qianshi Pang","Zihao Wang","Huiping Zhuang","Hongen Shao","Xiaofeng Zou"],"pdf_url":"https://arxiv.org/pdf/2402.15758v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.11527v3","updated":"2024-04-18T16:20:19Z","published":"2023-05-19T08:51:11Z","title":"InstructIE: A Bilingual Instruction-based Information Extraction Dataset","summary":"  Large language models can perform well on general natural language tasks, but\ntheir effectiveness is still not optimal for information extraction. Recent\nworks indicate that the main reason lies in the lack of extensive data on\ninformation extraction instructions. Note that the existing datasets on\ninformation extraction instructions not only have limited coverage but also\ninvolve high construction costs. To address this issue, we introduce\nInstructIE, a bilingual instruction-based information extraction dataset, which\ncovers 12 diverse domains. Specifically, we propose KG2Instruction, a framework\nspecifically for the automatic generation of such datasets. Experimental\nresults demonstrate that large language models trained with InstructIE can not\nonly obtain better information extraction capabilities but also enhance\nzero-shot performance compared with baselines.\n","authors":["Honghao Gui","Shuofei Qiao","Jintian Zhang","Hongbin Ye","Mengshu Sun","Lei Liang","Jeff Z. Pan","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.11527v3.pdf","comment":"Work in progress; project homepage:\n  https://www.zjukg.org/project/InstructIE/ dataset:\n  https://huggingface.co/datasets/zjunlp/InstructIE"},{"id":"http://arxiv.org/abs/2404.12291v1","updated":"2024-04-18T16:11:17Z","published":"2024-04-18T16:11:17Z","title":"Augmenting emotion features in irony detection with Large language\n  modeling","summary":"  This study introduces a novel method for irony detection, applying Large\nLanguage Models (LLMs) with prompt-based learning to facilitate emotion-centric\ntext augmentation. Traditional irony detection techniques typically fall short\ndue to their reliance on static linguistic features and predefined knowledge\nbases, often overlooking the nuanced emotional dimensions integral to irony. In\ncontrast, our methodology augments the detection process by integrating subtle\nemotional cues, augmented through LLMs, into three benchmark pre-trained NLP\nmodels - BERT, T5, and GPT-2 - which are widely recognized as foundational in\nirony detection. We assessed our method using the SemEval-2018 Task 3 dataset\nand observed substantial enhancements in irony detection capabilities.\n","authors":["Yucheng Lin","Yuhan Xia","Yunfei Long"],"pdf_url":"https://arxiv.org/pdf/2404.12291v1.pdf","comment":"11 pages, 3 tables, 2 figures. Submitted to the 25th Chinese Lexical\n  Semantics Workshop"},{"id":"http://arxiv.org/abs/2404.12289v1","updated":"2024-04-18T16:10:38Z","published":"2024-04-18T16:10:38Z","title":"Resilience through Scene Context in Visual Referring Expression\n  Generation","summary":"  Scene context is well known to facilitate humans' perception of visible\nobjects. In this paper, we investigate the role of context in Referring\nExpression Generation (REG) for objects in images, where existing research has\noften focused on distractor contexts that exert pressure on the generator. We\ntake a new perspective on scene context in REG and hypothesize that contextual\ninformation can be conceived of as a resource that makes REG models more\nresilient and facilitates the generation of object descriptions, and object\ntypes in particular. We train and test Transformer-based REG models with target\nrepresentations that have been artificially obscured with noise to varying\ndegrees. We evaluate how properties of the models' visual context affect their\nprocessing and performance. Our results show that even simple scene contexts\nmake models surprisingly resilient to perturbations, to the extent that they\ncan identify referent types even when visual information about the target is\ncompletely missing.\n","authors":["Simeon Junker","Sina Zarrie√ü"],"pdf_url":"https://arxiv.org/pdf/2404.12289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12283v1","updated":"2024-04-18T15:58:56Z","published":"2024-04-18T15:58:56Z","title":"Enhancing Embedding Performance through Large Language Model-based Text\n  Enrichment and Rewriting","summary":"  Embedding models are crucial for various natural language processing tasks\nbut can be limited by factors such as limited vocabulary, lack of context, and\ngrammatical errors. This paper proposes a novel approach to improve embedding\nperformance by leveraging large language models (LLMs) to enrich and rewrite\ninput text before the embedding process. By utilizing ChatGPT 3.5 to provide\nadditional context, correct inaccuracies, and incorporate metadata, the\nproposed method aims to enhance the utility and accuracy of embedding models.\nThe effectiveness of this approach is evaluated on three datasets:\nBanking77Classification, TwitterSemEval 2015, and Amazon Counter-factual\nClassification. Results demonstrate significant improvements over the baseline\nmodel on the TwitterSemEval 2015 dataset, with the best-performing prompt\nachieving a score of 85.34 compared to the previous best of 81.52 on the\nMassive Text Embedding Benchmark (MTEB) Leaderboard. However, performance on\nthe other two datasets was less impressive, highlighting the importance of\nconsidering domain-specific characteristics. The findings suggest that\nLLM-based text enrichment has shown promising results to improve embedding\nperformance, particularly in certain domains. Hence, numerous limitations in\nthe process of embedding can be avoided.\n","authors":["Nicholas Harris","Anand Butani","Syed Hashmy"],"pdf_url":"https://arxiv.org/pdf/2404.12283v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12274v1","updated":"2024-04-18T15:47:00Z","published":"2024-04-18T15:47:00Z","title":"Advancing the Robustness of Large Language Models through Self-Denoised\n  Smoothing","summary":"  Although large language models (LLMs) have achieved significant success,\ntheir vulnerability to adversarial perturbations, including recent jailbreak\nattacks, has raised considerable concerns. However, the increasing size of\nthese models and their limited access make improving their robustness a\nchallenging task. Among various defense strategies, randomized smoothing has\nshown great potential for LLMs, as it does not require full access to the\nmodel's parameters or fine-tuning via adversarial training. However, randomized\nsmoothing involves adding noise to the input before model prediction, and the\nfinal model's robustness largely depends on the model's performance on these\nnoise corrupted data. Its effectiveness is often limited by the model's\nsub-optimal performance on noisy data. To address this issue, we propose to\nleverage the multitasking nature of LLMs to first denoise the noisy inputs and\nthen to make predictions based on these denoised versions. We call this\nprocedure self-denoised smoothing. Unlike previous denoised smoothing\ntechniques in computer vision, which require training a separate model to\nenhance the robustness of LLMs, our method offers significantly better\nefficiency and flexibility. Our experimental results indicate that our method\nsurpasses existing methods in both empirical and certified robustness in\ndefending against adversarial attacks for both downstream tasks and human\nalignments (i.e., jailbreak attacks). Our code is publicly available at\nhttps://github.com/UCSB-NLP-Chang/SelfDenoise\n","authors":["Jiabao Ji","Bairu Hou","Zhen Zhang","Guanhua Zhang","Wenqi Fan","Qing Li","Yang Zhang","Gaowen Liu","Sijia Liu","Shiyu Chang"],"pdf_url":"https://arxiv.org/pdf/2404.12274v1.pdf","comment":"Accepted by NAACL 2024. Jiabao, Bairu, Zhen, Guanhua contributed\n  equally. This is an updated version of the paper: arXiv:2307.07171"},{"id":"http://arxiv.org/abs/2404.12273v1","updated":"2024-04-18T15:46:26Z","published":"2024-04-18T15:46:26Z","title":"FedEval-LLM: Federated Evaluation of Large Language Models on Downstream\n  Tasks with Collective Wisdom","summary":"  Federated Learning (FL) has emerged as a promising solution for collaborative\ntraining of large language models (LLMs). However, the integration of LLMs into\nFL introduces new challenges, particularly concerning the evaluation of LLMs.\nTraditional evaluation methods that rely on labeled test sets and\nsimilarity-based metrics cover only a subset of the acceptable answers, thereby\nfailing to accurately reflect the performance of LLMs on generative tasks.\nMeanwhile, although automatic evaluation methods that leverage advanced LLMs\npresent potential, they face critical risks of data leakage due to the need to\ntransmit data to external servers and suboptimal performance on downstream\ntasks due to the lack of domain knowledge. To address these issues, we propose\na Federated Evaluation framework of Large Language Models, named FedEval-LLM,\nthat provides reliable performance measurements of LLMs on downstream tasks\nwithout the reliance on labeled test sets and external tools, thus ensuring\nstrong privacy-preserving capability. FedEval-LLM leverages a consortium of\npersonalized LLMs from participants as referees to provide domain knowledge and\ncollective evaluation capability, thus aligning to the respective downstream\ntasks and mitigating uncertainties and biases associated with a single referee.\nExperimental results demonstrate a significant improvement in the evaluation\ncapability of personalized evaluation models on downstream tasks. When applied\nto FL, these evaluation models exhibit strong agreement with human preference\nand RougeL-score on meticulously curated test sets. FedEval-LLM effectively\novercomes the limitations of traditional metrics and the reliance on external\nservices, making it a promising framework for the evaluation of LLMs within\ncollaborative training scenarios.\n","authors":["Yuanqin He","Yan Kang","Lixin Fan","Qiang Yang"],"pdf_url":"https://arxiv.org/pdf/2404.12273v1.pdf","comment":"In Progress"},{"id":"http://arxiv.org/abs/2310.08475v5","updated":"2024-04-18T15:46:22Z","published":"2023-10-12T16:32:44Z","title":"Can We Edit Multimodal Large Language Models?","summary":"  In this paper, we focus on editing Multimodal Large Language Models (MLLMs).\nCompared to editing single-modal LLMs, multimodal model editing is more\nchallenging, which demands a higher level of scrutiny and careful consideration\nin the editing process. To facilitate research in this area, we construct a new\nbenchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite\nof innovative metrics for evaluation. We conduct comprehensive experiments\ninvolving various model editing baselines and analyze the impact of editing\ndifferent components for multimodal LLMs. Empirically, we notice that previous\nbaselines can implement editing multimodal LLMs to some extent, but the effect\nis still barely satisfactory, indicating the potential difficulty of this task.\nWe hope that our work can provide the NLP community with insights. Code and\ndataset are available in https://github.com/zjunlp/EasyEdit.\n","authors":["Siyuan Cheng","Bozhong Tian","Qingbin Liu","Xi Chen","Yongheng Wang","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.08475v5.pdf","comment":"EMNLP 2023. Add the Exact Match/Accuracy results of Reliability and\n  T-Generality"},{"id":"http://arxiv.org/abs/2404.12253v1","updated":"2024-04-18T15:21:34Z","published":"2024-04-18T15:21:34Z","title":"Toward Self-Improvement of LLMs via Imagination, Searching, and\n  Criticizing","summary":"  Despite the impressive capabilities of Large Language Models (LLMs) on\nvarious tasks, they still struggle with scenarios that involves complex\nreasoning and planning. Recent work proposed advanced prompting techniques and\nthe necessity of fine-tuning with high-quality data to augment LLMs' reasoning\nabilities. However, these approaches are inherently constrained by data\navailability and quality. In light of this, self-correction and self-learning\nemerge as viable solutions, employing strategies that allow LLMs to refine\ntheir outputs and learn from self-assessed rewards. Yet, the efficacy of LLMs\nin self-refining its response, particularly in complex reasoning and planning\ntask, remains dubious. In this paper, we introduce AlphaLLM for the\nself-improvements of LLMs, which integrates Monte Carlo Tree Search (MCTS) with\nLLMs to establish a self-improving loop, thereby enhancing the capabilities of\nLLMs without additional annotations. Drawing inspiration from the success of\nAlphaGo, AlphaLLM addresses the unique challenges of combining MCTS with LLM\nfor self-improvement, including data scarcity, the vastness search spaces of\nlanguage tasks, and the subjective nature of feedback in language tasks.\nAlphaLLM is comprised of prompt synthesis component, an efficient MCTS approach\ntailored for language tasks, and a trio of critic models for precise feedback.\nOur experimental results in mathematical reasoning tasks demonstrate that\nAlphaLLM significantly enhances the performance of LLMs without additional\nannotations, showing the potential for self-improvement in LLMs.\n","authors":["Ye Tian","Baolin Peng","Linfeng Song","Lifeng Jin","Dian Yu","Haitao Mi","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2404.12253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01949v2","updated":"2024-04-18T15:08:44Z","published":"2023-11-03T14:39:20Z","title":"Hint-enhanced In-Context Learning wakes Large Language Models up for\n  knowledge-intensive tasks","summary":"  In-context learning (ICL) ability has emerged with the increasing scale of\nlarge language models (LLMs), enabling them to learn input-label mappings from\ndemonstrations and perform well on downstream tasks. However, under the\nstandard ICL setting, LLMs may sometimes neglect query-related information in\ndemonstrations, leading to incorrect predictions. To address this limitation,\nwe propose a new paradigm called Hint-enhanced In-Context Learning (HICL) to\nexplore the power of ICL in open-domain question answering, an important form\nin knowledge-intensive tasks. HICL leverages LLMs' reasoning ability to extract\nquery-related knowledge from demonstrations, then concatenates the knowledge to\nprompt LLMs in a more explicit way. Furthermore, we track the source of this\nknowledge to identify specific examples, and introduce a Hint-related Example\nRetriever (HER) to select informative examples for enhanced demonstrations. We\nevaluate HICL with HER on 3 open-domain QA benchmarks, and observe average\nperformance gains of 2.89 EM score and 2.52 F1 score on gpt-3.5-turbo, 7.62 EM\nscore and 7.27 F1 score on LLaMA-2-Chat-7B compared with standard setting.\n","authors":["Yifan Wang","Qingyan Guo","Xinzhe Ni","Chufan Shi","Lemao Liu","Haiyun Jiang","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2311.01949v2.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2404.12242v1","updated":"2024-04-18T15:02:35Z","published":"2024-04-18T15:02:35Z","title":"CMNEE: A Large-Scale Document-Level Event Extraction Dataset based on\n  Open-Source Chinese Military News","summary":"  Extracting structured event knowledge, including event triggers and\ncorresponding arguments, from military texts is fundamental to many\napplications, such as intelligence analysis and decision assistance. However,\nevent extraction in the military field faces the data scarcity problem, which\nimpedes the research of event extraction models in this domain. To alleviate\nthis problem, we propose CMNEE, a large-scale, document-level open-source\nChinese Military News Event Extraction dataset. It contains 17,000 documents\nand 29,223 events, which are all manually annotated based on a pre-defined\nschema for the military domain including 8 event types and 11 argument role\ntypes. We designed a two-stage, multi-turns annotation strategy to ensure the\nquality of CMNEE and reproduced several state-of-the-art event extraction\nmodels with a systematic evaluation. The experimental results on CMNEE fall\nshorter than those on other domain datasets obviously, which demonstrates that\nevent extraction for military domain poses unique challenges and requires\nfurther research efforts. Our code and data can be obtained from\nhttps://github.com/Mzzzhu/CMNEE.\n","authors":["Mengna Zhu","Zijie Xu","Kaisheng Zeng","Kaiming Xiao","Mao Wang","Wenjun Ke","Hongbin Huang"],"pdf_url":"https://arxiv.org/pdf/2404.12242v1.pdf","comment":"13 pages, 7 figures, accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2404.12241v1","updated":"2024-04-18T15:01:00Z","published":"2024-04-18T15:01:00Z","title":"Introducing v0.5 of the AI Safety Benchmark from MLCommons","summary":"  This paper introduces v0.5 of the AI Safety Benchmark, which has been created\nby the MLCommons AI Safety Working Group. The AI Safety Benchmark has been\ndesigned to assess the safety risks of AI systems that use chat-tuned language\nmodels. We introduce a principled approach to specifying and constructing the\nbenchmark, which for v0.5 covers only a single use case (an adult chatting to a\ngeneral-purpose assistant in English), and a limited set of personas (i.e.,\ntypical users, malicious users, and vulnerable users). We created a new\ntaxonomy of 13 hazard categories, of which 7 have tests in the v0.5 benchmark.\nWe plan to release version 1.0 of the AI Safety Benchmark by the end of 2024.\nThe v1.0 benchmark will provide meaningful insights into the safety of AI\nsystems. However, the v0.5 benchmark should not be used to assess the safety of\nAI systems. We have sought to fully document the limitations, flaws, and\nchallenges of v0.5. This release of v0.5 of the AI Safety Benchmark includes\n(1) a principled approach to specifying and constructing the benchmark, which\ncomprises use cases, types of systems under test (SUTs), language and context,\npersonas, tests, and test items; (2) a taxonomy of 13 hazard categories with\ndefinitions and subcategories; (3) tests for seven of the hazard categories,\neach comprising a unique set of test items, i.e., prompts. There are 43,090\ntest items in total, which we created with templates; (4) a grading system for\nAI systems against the benchmark; (5) an openly available platform, and\ndownloadable tool, called ModelBench that can be used to evaluate the safety of\nAI systems on the benchmark; (6) an example evaluation report which benchmarks\nthe performance of over a dozen openly available chat-tuned language models;\n(7) a test specification for the benchmark.\n","authors":["Bertie Vidgen","Adarsh Agrawal","Ahmed M. Ahmed","Victor Akinwande","Namir Al-Nuaimi","Najla Alfaraj","Elie Alhajjar","Lora Aroyo","Trupti Bavalatti","Borhane Blili-Hamelin","Kurt Bollacker","Rishi Bomassani","Marisa Ferrara Boston","Sim√©on Campos","Kal Chakra","Canyu Chen","Cody Coleman","Zacharie Delpierre Coudert","Leon Derczynski","Debojyoti Dutta","Ian Eisenberg","James Ezick","Heather Frase","Brian Fuller","Ram Gandikota","Agasthya Gangavarapu","Ananya Gangavarapu","James Gealy","Rajat Ghosh","James Goel","Usman Gohar","Sujata Goswami","Scott A. Hale","Wiebke Hutiri","Joseph Marvin Imperial","Surgan Jandial","Nick Judd","Felix Juefei-Xu","Foutse Khomh","Bhavya Kailkhura","Hannah Rose Kirk","Kevin Klyman","Chris Knotz","Michael Kuchnik","Shachi H. Kumar","Chris Lengerich","Bo Li","Zeyi Liao","Eileen Peters Long","Victor Lu","Yifan Mai","Priyanka Mary Mammen","Kelvin Manyeki","Sean McGregor","Virendra Mehta","Shafee Mohammed","Emanuel Moss","Lama Nachman","Dinesh Jinenhally Naganna","Amin Nikanjam","Besmira Nushi","Luis Oala","Iftach Orr","Alicia Parrish","Cigdem Patlak","William Pietri","Forough Poursabzi-Sangdeh","Eleonora Presani","Fabrizio Puletti","Paul R√∂ttger","Saurav Sahay","Tim Santos","Nino Scherrer","Alice Schoenauer Sebag","Patrick Schramowski","Abolfazl Shahbazi","Vin Sharma","Xudong Shen","Vamsi Sistla","Leonard Tang","Davide Testuggine","Vithursan Thangarasa","Elizabeth Anne Watkins","Rebecca Weiss","Chris Welty","Tyler Wilbers","Adina Williams","Carole-Jean Wu","Poonam Yadav","Xianjun Yang","Yi Zeng","Wenhui Zhang","Fedor Zhdanov","Jiacheng Zhu","Percy Liang","Peter Mattson","Joaquin Vanschoren"],"pdf_url":"https://arxiv.org/pdf/2404.12241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12224v1","updated":"2024-04-18T14:38:32Z","published":"2024-04-18T14:38:32Z","title":"Length Generalization of Causal Transformers without Position Encoding","summary":"  Generalizing to longer sentences is important for recent Transformer-based\nlanguage models. Besides algorithms manipulating explicit position features,\nthe success of Transformers without position encodings (NoPE) provides a new\nway to overcome the challenge. In this paper, we study the length\ngeneralization property of NoPE. We find that although NoPE can extend to\nlonger sequences than the commonly used explicit position encodings, it still\nhas a limited context length. We identify a connection between the failure of\nNoPE's generalization and the distraction of attention distributions. We\npropose a parameter-efficient tuning for searching attention heads' best\ntemperature hyper-parameters, which substantially expands NoPE's context size.\nExperiments on long sequence language modeling, the synthetic passkey retrieval\ntask and real-world long context tasks show that NoPE can achieve competitive\nperformances with state-of-the-art length generalization algorithms. The source\ncode is publicly accessible\n","authors":["Jie Wang","Tao Ji","Yuanbin Wu","Hang Yan","Tao Gui","Qi Zhang","Xuanjing Huang","Xiaoling Wang"],"pdf_url":"https://arxiv.org/pdf/2404.12224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12195v1","updated":"2024-04-18T13:57:18Z","published":"2024-04-18T13:57:18Z","title":"OpenBezoar: Small, Cost-Effective and Open Models Trained on Mixes of\n  Instruction Data","summary":"  Instruction fine-tuning pretrained LLMs for diverse downstream tasks has\ndemonstrated remarkable success and has captured the interest of both academics\nand practitioners. To ensure such fine-tuned LLMs align with human preferences,\ntechniques such as RLHF and DPO have emerged. At the same time, there is\nincreasing interest in smaller parameter counts for models. In this work, using\nOpenLLaMA 3Bv2 as a base model, we describe the recipe used to fine-tune the\nOpenBezoar family of models. In this recipe: We first generate synthetic\ninstruction fine-tuning data using an open and commercially non-restrictive\ninstruction fine-tuned variant of the Falcon-40B model under three schemes\nbased on: LaMini-LM, WizardLM/Evol-Instruct (with databricks-dolly-15k as a\nseed dataset) and Orca (with the Flan Collection as a seed dataset), then\nfilter these generations using GPT-4 as a human proxy. We then perform\ncost-effective QLoRA-based supervised fine-tuning sequentially with each\nscheme. The resulting checkpoint is further fine-tuned with a subset of the\nHH-RLHF dataset to minimize distribution shift prior to using the DPO loss to\nobtain the final checkpoint. Evaluation is done with the LM Eval Harness\ntasks/metrics as well as on MT-Bench using the \"LLM-as-a-judge\" framework with\nClaude 2.1, with the finding that the final checkpoint,\n\"OpenBezoar-HH-RLHF-DPO\", demonstrates superior performance over many models at\nthe 3B parameter scale, even outperforming the top model in one of the\ncategories on the Huggingface Open LLM Leaderboard. We release\n\"OpenBezoar-SFT\", \"OpenBezoar-HH-RLHF-SFT\", \"OpenBezoar-HH-RLHF-DPO\"\ncheckpoints, alongside our generated datasets on HuggingFace at\nhttps://huggingface.co/collections/SurgeGlobal/open-bezoar-6620a24923e12127e9e2b9cc\nand our codebase at\nhttps://bitbucket.org/paladinanalytics/workspace/projects/OP.\n","authors":["Chandeepa Dissanayake","Lahiru Lowe","Sachith Gunasekara","Yasiru Ratnayake"],"pdf_url":"https://arxiv.org/pdf/2404.12195v1.pdf","comment":"25 pages, 27 Figures, 8 Tables"},{"id":"http://arxiv.org/abs/2404.12177v1","updated":"2024-04-18T13:31:57Z","published":"2024-04-18T13:31:57Z","title":"EuSQuAD: Automatically Translated and Aligned SQuAD2.0 for Basque","summary":"  The widespread availability of Question Answering (QA) datasets in English\nhas greatly facilitated the advancement of the Natural Language Processing\n(NLP) field. However, the scarcity of such resources for minority languages,\nsuch as Basque, poses a substantial challenge for these communities. In this\ncontext, the translation and alignment of existing QA datasets plays a crucial\nrole in narrowing this technological gap. This work presents EuSQuAD, the first\ninitiative dedicated to automatically translating and aligning SQuAD2.0 into\nBasque, resulting in more than 142k QA examples. We demonstrate EuSQuAD's value\nthrough extensive qualitative analysis and QA experiments supported with\nEuSQuAD as training data. These experiments are evaluated with a new\nhuman-annotated dataset.\n","authors":["Aitor Garc√≠a-Pablos","Naiara Perez","Montse Cuadros"],"pdf_url":"https://arxiv.org/pdf/2404.12177v1.pdf","comment":"Under review in the journal of Procesamiento de Lenguaje Natural"},{"id":"http://arxiv.org/abs/2404.12174v1","updated":"2024-04-18T13:31:05Z","published":"2024-04-18T13:31:05Z","title":"Claim Check-Worthiness Detection: How Well do LLMs Grasp Annotation\n  Guidelines?","summary":"  The increasing threat of disinformation calls for automating parts of the\nfact-checking pipeline. Identifying text segments requiring fact-checking is\nknown as claim detection (CD) and claim check-worthiness detection (CW), the\nlatter incorporating complex domain-specific criteria of worthiness and often\nframed as a ranking task. Zero- and few-shot LLM prompting is an attractive\noption for both tasks, as it bypasses the need for labeled datasets and allows\nverbalized claim and worthiness criteria to be directly used for prompting. We\nevaluate the LLMs' predictive and calibration accuracy on five CD/CW datasets\nfrom diverse domains, each utilizing a different worthiness criterion. We\ninvestigate two key aspects: (1) how best to distill factuality and worthiness\ncriteria into a prompt and (2) what amount of context to provide for each\nclaim. To this end, we experiment with varying the level of prompt verbosity\nand the amount of contextual information provided to the model. Our results\nshow that optimal prompt verbosity is domain-dependent, adding context does not\nimprove performance, and confidence scores can be directly used to produce\nreliable check-worthiness rankings.\n","authors":["Laura Majer","Jan ≈†najder"],"pdf_url":"https://arxiv.org/pdf/2404.12174v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12171v1","updated":"2024-04-18T13:25:29Z","published":"2024-04-18T13:25:29Z","title":"Stance Detection on Social Media with Fine-Tuned Large Language Models","summary":"  Stance detection, a key task in natural language processing, determines an\nauthor's viewpoint based on textual analysis. This study evaluates the\nevolution of stance detection methods, transitioning from early machine\nlearning approaches to the groundbreaking BERT model, and eventually to modern\nLarge Language Models (LLMs) such as ChatGPT, LLaMa-2, and Mistral-7B. While\nChatGPT's closed-source nature and associated costs present challenges, the\nopen-source models like LLaMa-2 and Mistral-7B offers an encouraging\nalternative. Initially, our research focused on fine-tuning ChatGPT, LLaMa-2,\nand Mistral-7B using several publicly available datasets. Subsequently, to\nprovide a comprehensive comparison, we assess the performance of these models\nin zero-shot and few-shot learning scenarios. The results underscore the\nexceptional ability of LLMs in accurately detecting stance, with all tested\nmodels surpassing existing benchmarks. Notably, LLaMa-2 and Mistral-7B\ndemonstrate remarkable efficiency and potential for stance detection, despite\ntheir smaller sizes compared to ChatGPT. This study emphasizes the potential of\nLLMs in stance detection and calls for more extensive research in this field.\n","authors":["ƒ∞lker G√ºl","R√©mi Lebret","Karl Aberer"],"pdf_url":"https://arxiv.org/pdf/2404.12171v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07982v2","updated":"2024-04-18T13:21:34Z","published":"2024-04-11T17:58:05Z","title":"Language Imbalance Can Boost Cross-lingual Generalisation","summary":"  Multilinguality is crucial for extending recent advancements in language\nmodelling to diverse linguistic communities. To maintain high performance while\nrepresenting multiple languages, multilingual models ideally align\nrepresentations, allowing what is learned in one language to generalise to\nothers. Prior research has emphasised the importance of parallel data and\nshared vocabulary elements as key factors for such alignment. In this study, we\ninvestigate an unintuitive novel driver of cross-lingual generalisation:\nlanguage imbalance. In controlled experiments on perfectly equivalent cloned\nlanguages, we observe that the existence of a predominant language during\ntraining boosts the performance of less frequent languages and leads to\nstronger alignment of model representations across languages. Furthermore, we\nfind that this trend is amplified with scale: with large enough models or long\nenough training, we observe that bilingual training data with a 90/10 language\nsplit yields better performance on both languages than a balanced 50/50 split.\nBuilding on these insights, we design training schemes that can improve\nperformance in all cloned languages, even without altering the training data.\nAs we extend our analysis to real languages, we find that infrequent languages\nstill benefit from frequent ones, yet whether language imbalance causes\ncross-lingual generalisation there is not conclusive.\n","authors":["Anton Sch√§fer","Shauli Ravfogel","Thomas Hofmann","Tiago Pimentel","Imanol Schlag"],"pdf_url":"https://arxiv.org/pdf/2404.07982v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.10757v2","updated":"2024-04-18T13:08:07Z","published":"2023-07-20T10:42:16Z","title":"Vesper: A Compact and Effective Pretrained Model for Speech Emotion\n  Recognition","summary":"  This paper presents a paradigm that adapts general large-scale pretrained\nmodels (PTMs) to speech emotion recognition task. Although PTMs shed new light\non artificial general intelligence, they are constructed with general tasks in\nmind, and thus, their efficacy for specific tasks can be further improved.\nAdditionally, employing PTMs in practical applications can be challenging due\nto their considerable size. Above limitations spawn another research direction,\nnamely, optimizing large-scale PTMs for specific tasks to generate\ntask-specific PTMs that are both compact and effective. In this paper, we focus\non the speech emotion recognition task and propose an improved emotion-specific\npretrained encoder called Vesper. Vesper is pretrained on a speech dataset\nbased on WavLM and takes into account emotional characteristics. To enhance\nsensitivity to emotional information, Vesper employs an emotion-guided masking\nstrategy to identify the regions that need masking. Subsequently, Vesper\nemploys hierarchical and cross-layer self-supervision to improve its ability to\ncapture acoustic and semantic representations, both of which are crucial for\nemotion recognition. Experimental results on the IEMOCAP, MELD, and CREMA-D\ndatasets demonstrate that Vesper with 4 layers outperforms WavLM Base with 12\nlayers, and the performance of Vesper with 12 layers surpasses that of WavLM\nLarge with 24 layers.\n","authors":["Weidong Chen","Xiaofen Xing","Peihao Chen","Xiangmin Xu"],"pdf_url":"https://arxiv.org/pdf/2307.10757v2.pdf","comment":"This paper was accepted by IEEE Transactions on Affective Computing\n  2024"},{"id":"http://arxiv.org/abs/2404.12152v1","updated":"2024-04-18T12:58:36Z","published":"2024-04-18T12:58:36Z","title":"FecTek: Enhancing Term Weight in Lexicon-Based Retrieval with Feature\n  Context and Term-level Knowledge","summary":"  Lexicon-based retrieval has gained siginificant popularity in text retrieval\ndue to its efficient and robust performance. To further enhance performance of\nlexicon-based retrieval, researchers have been diligently incorporating\nstate-of-the-art methodologies like Neural retrieval and text-level contrastive\nlearning approaches. Nonetheless, despite the promising outcomes, current\nlexicon-based retrieval methods have received limited attention in exploring\nthe potential benefits of feature context representations and term-level\nknowledge guidance. In this paper, we introduce an innovative method by\nintroducing FEature Context and TErm-level Knowledge modules(FecTek). To\neffectively enrich the feature context representations of term weight, the\nFeature Context Module (FCM) is introduced, which leverages the power of BERT's\nrepresentation to determine dynamic weights for each element in the embedding.\nAdditionally, we develop a term-level knowledge guidance module (TKGM) for\neffectively utilizing term-level knowledge to intelligently guide the modeling\nprocess of term weight. Evaluation of the proposed method on MS Marco benchmark\ndemonstrates its superiority over the previous state-of-the-art approaches.\n","authors":["Zunran Wang","Zhonghua Li","Wei Shen","Qi Ye","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2404.12152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12150v1","updated":"2024-04-18T12:55:18Z","published":"2024-04-18T12:55:18Z","title":"Aligning language models with human preferences","summary":"  Language models (LMs) trained on vast quantities of text data can acquire\nsophisticated skills such as generating summaries, answering questions or\ngenerating code. However, they also manifest behaviors that violate human\npreferences, e.g., they can generate offensive content, falsehoods or\nperpetuate social biases. In this thesis, I explore several approaches to\naligning LMs with human preferences. First, I argue that aligning LMs can be\nseen as Bayesian inference: conditioning a prior (base, pretrained LM) on\nevidence about human preferences (Chapter 2). Conditioning on human preferences\ncan be implemented in numerous ways. In Chapter 3, I investigate the relation\nbetween two approaches to finetuning pretrained LMs using feedback given by a\nscoring function: reinforcement learning from human feedback (RLHF) and\ndistribution matching. I show that RLHF can be seen as a special case of\ndistribution matching but distributional matching is strictly more general. In\nchapter 4, I show how to extend the distribution matching to conditional\nlanguage models. Finally, in chapter 5 I explore a different root: conditioning\nan LM on human preferences already during pretraining. I show that involving\nhuman feedback from the very start tends to be more effective than using it\nonly during supervised finetuning. Overall, these results highlight the room\nfor alignment techniques different from and complementary to RLHF.\n","authors":["Tomasz Korbak"],"pdf_url":"https://arxiv.org/pdf/2404.12150v1.pdf","comment":"PhD thesis"},{"id":"http://arxiv.org/abs/2404.12145v1","updated":"2024-04-18T12:48:17Z","published":"2024-04-18T12:48:17Z","title":"From Form(s) to Meaning: Probing the Semantic Depths of Language Models\n  Using Multisense Consistency","summary":"  The staggering pace with which the capabilities of large language models\n(LLMs) are increasing, as measured by a range of commonly used natural language\nunderstanding (NLU) benchmarks, raises many questions regarding what\n\"understanding\" means for a language model and how it compares to human\nunderstanding. This is especially true since many LLMs are exclusively trained\non text, casting doubt on whether their stellar benchmark performances are\nreflective of a true understanding of the problems represented by these\nbenchmarks, or whether LLMs simply excel at uttering textual forms that\ncorrelate with what someone who understands the problem would say. In this\nphilosophically inspired work, we aim to create some separation between form\nand meaning, with a series of tests that leverage the idea that world\nunderstanding should be consistent across presentational modes - inspired by\nFregean senses - of the same meaning. Specifically, we focus on consistency\nacross languages as well as paraphrases. Taking GPT-3.5 as our object of study,\nwe evaluate multisense consistency across five different languages and various\ntasks. We start the evaluation in a controlled setting, asking the model for\nsimple facts, and then proceed with an evaluation on four popular NLU\nbenchmarks. We find that the model's multisense consistency is lacking and run\nseveral follow-up analyses to verify that this lack of consistency is due to a\nsense-dependent task understanding. We conclude that, in this aspect, the\nunderstanding of LLMs is still quite far from being consistent and human-like,\nand deliberate on how this impacts their utility in the context of learning\nabout human language and understanding.\n","authors":["Xenia Ohmer","Elia Bruni","Dieuwke Hupkes"],"pdf_url":"https://arxiv.org/pdf/2404.12145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12132v1","updated":"2024-04-18T12:33:57Z","published":"2024-04-18T12:33:57Z","title":"Enhancing Suicide Risk Assessment: A Speech-Based Automated Approach in\n  Emergency Medicine","summary":"  The delayed access to specialized psychiatric assessments and care for\npatients at risk of suicidal tendencies in emergency departments creates a\nnotable gap in timely intervention, hindering the provision of adequate mental\nhealth support during critical situations. To address this, we present a\nnon-invasive, speech-based approach for automatic suicide risk assessment. For\nour study, we have collected a novel dataset of speech recordings from $20$\npatients from which we extract three sets of features, including wav2vec,\ninterpretable speech and acoustic features, and deep learning-based spectral\nrepresentations. We proceed by conducting a binary classification to assess\nsuicide risk in a leave-one-subject-out fashion. Our most effective speech\nmodel achieves a balanced accuracy of $66.2\\,\\%$. Moreover, we show that\nintegrating our speech model with a series of patients' metadata, such as the\nhistory of suicide attempts or access to firearms, improves the overall result.\nThe metadata integration yields a balanced accuracy of $94.4\\,\\%$, marking an\nabsolute improvement of $28.2\\,\\%$, demonstrating the efficacy of our proposed\napproaches for automatic suicide risk assessment in emergency medicine.\n","authors":["Shahin Amiriparian","Maurice Gerczuk","Justina Lutz","Wolfgang Strube","Irina Papazova","Alkomiet Hasan","Alexander Kathan","Bj√∂rn W. Schuller"],"pdf_url":"https://arxiv.org/pdf/2404.12132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01805v3","updated":"2024-04-18T12:04:12Z","published":"2024-02-02T09:45:33Z","title":"Can LLMs perform structured graph reasoning?","summary":"  Pretrained Large Language Models (LLMs) have demonstrated various reasoning\ncapabilities through language-based prompts alone, particularly in unstructured\ntask settings (tasks purely based on language semantics). However, LLMs often\nstruggle with structured tasks, because of the inherent incompatibility of\ninput representation. Reducing structured tasks to uni-dimensional language\nsemantics often renders the problem trivial. Keeping the trade-off between LLM\ncompatibility and structure complexity in mind, we design various graph\nreasoning tasks as a proxy to semi-structured tasks in this paper, in order to\ntest the ability to navigate through representations beyond plain text in\nvarious LLMs. Particularly, we design 10 distinct problems of graph traversal,\neach representing increasing levels of complexity, and benchmark 5 different\ninstruct-finetuned LLMs (GPT-4, GPT-3.5, Claude-2, Llama-2 and Palm-2) on the\naforementioned tasks. Further, we analyse the performance of models across\nvarious settings such as varying sizes of graphs as well as different forms of\nk-shot prompting. We highlight various limitations, biases and properties of\nLLMs through this benchmarking process, such as an inverse relation to the\naverage degrees of freedom of traversal per node in graphs, the overall\nnegative impact of k-shot prompting on graph reasoning tasks, and a positive\nresponse bias which prevents LLMs from identifying the absence of a valid\nsolution. Finally, we introduce a new prompting technique specially designed\nfor graph traversal tasks (PathCompare), which demonstrates a notable increase\nin the performance of LLMs in comparison to standard prompting techniques such\nas Chain-of-Thought (CoT).\n","authors":["Palaash Agrawal","Shavak Vasania","Cheston Tan"],"pdf_url":"https://arxiv.org/pdf/2402.01805v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12104v1","updated":"2024-04-18T11:38:25Z","published":"2024-04-18T11:38:25Z","title":"Ethical-Lens: Curbing Malicious Usages of Open-Source Text-to-Image\n  Models","summary":"  The burgeoning landscape of text-to-image models, exemplified by innovations\nsuch as Midjourney and DALLE 3, has revolutionized content creation across\ndiverse sectors. However, these advancements bring forth critical ethical\nconcerns, particularly with the misuse of open-source models to generate\ncontent that violates societal norms. Addressing this, we introduce\nEthical-Lens, a framework designed to facilitate the value-aligned usage of\ntext-to-image tools without necessitating internal model revision. Ethical-Lens\nensures value alignment in text-to-image models across toxicity and bias\ndimensions by refining user commands and rectifying model outputs. Systematic\nevaluation metrics, combining GPT4-V, HEIM, and FairFace scores, assess\nalignment capability. Our experiments reveal that Ethical-Lens enhances\nalignment capabilities to levels comparable with or superior to commercial\nmodels like DALLE 3, ensuring user-generated content adheres to ethical\nstandards while maintaining image quality. This study indicates the potential\nof Ethical-Lens to ensure the sustainable development of open-source\ntext-to-image tools and their beneficial integration into society. Our code is\navailable at https://github.com/yuzhu-cai/Ethical-Lens.\n","authors":["Yuzhu Cai","Sheng Yin","Yuxi Wei","Chenxin Xu","Weibo Mao","Felix Juefei-Xu","Siheng Chen","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2404.12104v1.pdf","comment":"42 pages, 17 figures, 29 tables"},{"id":"http://arxiv.org/abs/2404.12096v1","updated":"2024-04-18T11:29:23Z","published":"2024-04-18T11:29:23Z","title":"LongEmbed: Extending Embedding Models for Long Context Retrieval","summary":"  Embedding models play a pivot role in modern NLP applications such as IR and\nRAG. While the context limit of LLMs has been pushed beyond 1 million tokens,\nembedding models are still confined to a narrow context window not exceeding 8k\ntokens, refrained from application scenarios requiring long inputs such as\nlegal contracts. This paper explores context window extension of existing\nembedding models, pushing the limit to 32k without requiring additional\ntraining. First, we examine the performance of current embedding models for\nlong context retrieval on our newly constructed LongEmbed benchmark. LongEmbed\ncomprises two synthetic tasks and four carefully chosen real-world tasks,\nfeaturing documents of varying length and dispersed target information.\nBenchmarking results underscore huge room for improvement in these models.\nBased on this, comprehensive experiments show that training-free context window\nextension strategies like position interpolation can effectively extend the\ncontext window of existing embedding models by several folds, regardless of\ntheir original context being 512 or beyond 4k. Furthermore, for models\nemploying absolute position encoding (APE), we show the possibility of further\nfine-tuning to harvest notable performance gains while strictly preserving\noriginal behavior for short inputs. For models using rotary position embedding\n(RoPE), significant enhancements are observed when employing RoPE-specific\nmethods, such as NTK and SelfExtend, indicating RoPE's superiority over APE for\ncontext window extension. To facilitate future research, we release E5-Base-4k\nand E5-RoPE-Base, along with the LongEmbed benchmark.\n","authors":["Dawei Zhu","Liang Wang","Nan Yang","Yifan Song","Wenhao Wu","Furu Wei","Sujian Li"],"pdf_url":"https://arxiv.org/pdf/2404.12096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12077v1","updated":"2024-04-18T10:59:54Z","published":"2024-04-18T10:59:54Z","title":"TIMIT Speaker Profiling: A Comparison of Multi-task learning and\n  Single-task learning Approaches","summary":"  This study employs deep learning techniques to explore four speaker profiling\ntasks on the TIMIT dataset, namely gender classification, accent\nclassification, age estimation, and speaker identification, highlighting the\npotential and challenges of multi-task learning versus single-task models. The\nmotivation for this research is twofold: firstly, to empirically assess the\nadvantages and drawbacks of multi-task learning over single-task models in the\ncontext of speaker profiling; secondly, to emphasize the undiminished\nsignificance of skillful feature engineering for speaker recognition tasks. The\nfindings reveal challenges in accent classification, and multi-task learning is\nfound advantageous for tasks of similar complexity. Non-sequential features are\nfavored for speaker recognition, but sequential ones can serve as starting\npoints for complex models. The study underscores the necessity of meticulous\nexperimentation and parameter tuning for deep learning models.\n","authors":["Rong Wang","Kun Sun"],"pdf_url":"https://arxiv.org/pdf/2404.12077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12065v1","updated":"2024-04-18T10:25:42Z","published":"2024-04-18T10:25:42Z","title":"RAGAR, Your Falsehood RADAR: RAG-Augmented Reasoning for Political\n  Fact-Checking using Multimodal Large Language Models","summary":"  The escalating challenge of misinformation, particularly in the context of\npolitical discourse, necessitates advanced solutions for fact-checking. We\nintroduce innovative approaches to enhance the reliability and efficiency of\nmultimodal fact-checking through the integration of Large Language Models\n(LLMs) with Retrieval-augmented Generation (RAG)- based advanced reasoning\ntechniques. This work proposes two novel methodologies, Chain of RAG (CoRAG)\nand Tree of RAG (ToRAG). The approaches are designed to handle multimodal\nclaims by reasoning the next questions that need to be answered based on\nprevious evidence. Our approaches improve the accuracy of veracity predictions\nand the generation of explanations over the traditional fact-checking approach\nof sub-question generation with chain of thought veracity prediction. By\nemploying multimodal LLMs adept at analyzing both text and images, this\nresearch advances the capability of automated systems in identifying and\ncountering misinformation.\n","authors":["M. Abdul Khaliq","P. Chang","M. Ma","B. Pflugfelder","F. Miletiƒá"],"pdf_url":"https://arxiv.org/pdf/2404.12065v1.pdf","comment":"8 pages, submitted to ACL Rolling Review"},{"id":"http://arxiv.org/abs/2404.12059v1","updated":"2024-04-18T10:17:04Z","published":"2024-04-18T10:17:04Z","title":"Constituents Correspond to Word Sequence Patterns among Sentences with\n  Equivalent Predicate-Argument Structures: Unsupervised Constituency Parsing\n  by Span Matching","summary":"  Unsupervised constituency parsing is about identifying word sequences that\nform a syntactic unit (i.e., constituents) in a target sentence. Linguists\nidentify the constituent by evaluating a set of Predicate-Argument Structure\n(PAS) equivalent sentences where we find the constituent corresponds to\nfrequent word sequences. However, such information is unavailable to previous\nparsing methods which identify the constituent by observing sentences with\ndiverse PAS. In this study, we empirically verify that \\textbf{constituents\ncorrespond to word sequence patterns in the PAS-equivalent sentence set}. We\npropose a frequency-based method \\emph{span-overlap}, applying the word\nsequence pattern to computational unsupervised parsing for the first time.\nParsing experiments show that the span-overlap parser outperforms\nstate-of-the-art parsers in eight out of ten languages. Further discrimination\nanalysis confirms that the span-overlap method can non-trivially separate\nconstituents from non-constituents. This result highlights the utility of the\nword sequence pattern. Additionally, we discover a multilingual phenomenon:\n\\textbf{participant-denoting constituents are more frequent than event-denoting\nconstituents}. The phenomenon indicates a behavioral difference between the two\nconstituent types, laying the foundation for future labeled unsupervised\nparsing.\n","authors":["Junjie Chen","Xiangheng He","Danushka Bollegala","Yusuke Miyao"],"pdf_url":"https://arxiv.org/pdf/2404.12059v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12050v1","updated":"2024-04-18T10:06:00Z","published":"2024-04-18T10:06:00Z","title":"emrQA-msquad: A Medical Dataset Structured with the SQuAD V2.0\n  Framework, Enriched with emrQA Medical Information","summary":"  Machine Reading Comprehension (MRC) holds a pivotal role in shaping Medical\nQuestion Answering Systems (QAS) and transforming the landscape of accessing\nand applying medical information. However, the inherent challenges in the\nmedical field, such as complex terminology and question ambiguity, necessitate\ninnovative solutions. One key solution involves integrating specialized medical\ndatasets and creating dedicated datasets. This strategic approach enhances the\naccuracy of QAS, contributing to advancements in clinical decision-making and\nmedical research. To address the intricacies of medical terminology, a\nspecialized dataset was integrated, exemplified by a novel Span extraction\ndataset derived from emrQA but restructured into 163,695 questions and 4,136\nmanually obtained answers, this new dataset was called emrQA-msquad dataset.\nAdditionally, for ambiguous questions, a dedicated medical dataset for the Span\nextraction task was introduced, reinforcing the system's robustness. The\nfine-tuning of models such as BERT, RoBERTa, and Tiny RoBERTa for medical\ncontexts significantly improved response accuracy within the F1-score range of\n0.75 to 1.00 from 10.1% to 37.4%, 18.7% to 44.7% and 16.0% to 46.8%,\nrespectively. Finally, emrQA-msquad dataset is publicy available at\nhttps://huggingface.co/datasets/Eladio/emrqa-msquad.\n","authors":["Jimenez Eladio","Hao Wu"],"pdf_url":"https://arxiv.org/pdf/2404.12050v1.pdf","comment":"The dataset is available in\n  https://huggingface.co/datasets/Eladio/emrqa-msquad"},{"id":"http://arxiv.org/abs/2311.06503v2","updated":"2024-04-18T10:02:47Z","published":"2023-11-11T07:56:40Z","title":"Knowledgeable Preference Alignment for LLMs in Domain-specific Question\n  Answering","summary":"  Deploying large language models (LLMs) to real scenarios for domain-specific\nquestion answering (QA) is a key thrust for LLM applications, which poses\nnumerous challenges, especially in ensuring that responses are both\naccommodating to user requirements and appropriately leveraging domain-specific\nknowledge bases. They are the two major difficulties for LLM application as\nvanilla fine-tuning falls short of addressing. Combining these requirements, we\nconceive of them as the requirement for the model's preference to be\nharmoniously aligned with humans'. Thus, we introduce Knowledgeable Preference\nAlignmenT (KnowPAT), which constructs two kinds of preference sets to tackle\nthe two issues. Besides, we design a new alignment objective to align the LLM\npreference with different human preferences uniformly, aiming to optimize LLM\nperformance in real-world, domain-specific QA settings. Adequate experiments\nand comprehensive comparisons with 15 baseline methods illustrate that our\nKnowPAT is a superior pipeline for real-scenario domain-specific QA with LLMs.\n","authors":["Yichi Zhang","Zhuo Chen","Yin Fang","Yanxi Lu","Fangming Li","Wen Zhang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2311.06503v2.pdf","comment":"Work in progress. Code is available at\n  https://github.com/zjukg/KnowPAT"},{"id":"http://arxiv.org/abs/2404.12042v1","updated":"2024-04-18T09:52:50Z","published":"2024-04-18T09:52:50Z","title":"Exploring Boundaries and Intensities in Offensive and Hate Speech:\n  Unveiling the Complex Spectrum of Social Media Discourse","summary":"  The prevalence of digital media and evolving sociopolitical dynamics have\nsignificantly amplified the dissemination of hateful content. Existing studies\nmainly focus on classifying texts into binary categories, often overlooking the\ncontinuous spectrum of offensiveness and hatefulness inherent in the text. In\nthis research, we present an extensive benchmark dataset for Amharic,\ncomprising 8,258 tweets annotated for three distinct tasks: category\nclassification, identification of hate targets, and rating offensiveness and\nhatefulness intensities. Our study highlights that a considerable majority of\ntweets belong to the less offensive and less hate intensity levels,\nunderscoring the need for early interventions by stakeholders. The prevalence\nof ethnic and political hatred targets, with significant overlaps in our\ndataset, emphasizes the complex relationships within Ethiopia's sociopolitical\nlandscape. We build classification and regression models and investigate the\nefficacy of models in handling these tasks. Our results reveal that hate and\noffensive speech can not be addressed by a simplistic binary classification,\ninstead manifesting as variables across a continuous range of values. The\nAfro-XLMR-large model exhibits the best performances achieving F1-scores of\n75.30%, 70.59%, and 29.42% for the category, target, and regression tasks,\nrespectively. The 80.22% correlation coefficient of the Afro-XLMR-large model\nindicates strong alignments.\n","authors":["Abinew Ali Ayele","Esubalew Alemneh Jalew","Adem Chanie Ali","Seid Muhie Yimam","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2404.12042v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12041v1","updated":"2024-04-18T09:52:18Z","published":"2024-04-18T09:52:18Z","title":"Can We Catch the Elephant? The Evolvement of Hallucination Evaluation on\n  Natural Language Generation: A Survey","summary":"  Hallucination in Natural Language Generation (NLG) is like the elephant in\nthe room, obvious but often overlooked until recent achievements significantly\nimproved the fluency and grammatical accuracy of generated text. For Large\nLanguage Models (LLMs), hallucinations can happen in various downstream tasks\nand casual conversations, which need accurate assessment to enhance reliability\nand safety. However, current studies on hallucination evaluation vary greatly,\nand people still find it difficult to sort out and select the most appropriate\nevaluation methods. Moreover, as NLP research gradually shifts to the domain of\nLLMs, it brings new challenges to this direction. This paper provides a\ncomprehensive survey on the evolvement of hallucination evaluation methods,\naiming to address three key aspects: 1) Diverse definitions and granularity of\nfacts; 2) The categories of automatic evaluators and their applicability; 3)\nUnresolved issues and future directions.\n","authors":["Siya Qi","Yulan He","Zheng Yuan"],"pdf_url":"https://arxiv.org/pdf/2404.12041v1.pdf","comment":"19 pages in total, with 9 pages as main body. Under review as a\n  conference paper at CoLM 2024"},{"id":"http://arxiv.org/abs/2404.12038v1","updated":"2024-04-18T09:46:25Z","published":"2024-04-18T09:46:25Z","title":"Uncovering Safety Risks in Open-source LLMs through Concept Activation\n  Vector","summary":"  Current open-source large language models (LLMs) are often undergone careful\nsafety alignment before public release. Some attack methods have also been\nproposed that help check for safety vulnerabilities in LLMs to ensure alignment\nrobustness. However, many of these methods have moderate attack success rates.\nEven when successful, the harmfulness of their outputs cannot be guaranteed,\nleading to suspicions that these methods have not accurately identified the\nsafety vulnerabilities of LLMs. In this paper, we introduce a LLM attack method\nutilizing concept-based model explanation, where we extract safety concept\nactivation vectors (SCAVs) from LLMs' activation space, enabling efficient\nattacks on well-aligned LLMs like LLaMA-2, achieving near 100% attack success\nrate as if LLMs are completely unaligned. This suggests that LLMs, even after\nthorough safety alignment, could still pose potential risks to society upon\npublic release. To evaluate the harmfulness of outputs resulting with various\nattack methods, we propose a comprehensive evaluation method that reduces the\npotential inaccuracies of existing evaluations, and further validate that our\nmethod causes more harmful content. Additionally, we discover that the SCAVs\nshow some transferability across different open-source LLMs.\n","authors":["Zhihao Xu","Ruixuan Huang","Xiting Wang","Fangzhao Wu","Jing Yao","Xing Xie"],"pdf_url":"https://arxiv.org/pdf/2404.12038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12022v1","updated":"2024-04-18T09:17:06Z","published":"2024-04-18T09:17:06Z","title":"Parallel Decoding via Hidden Transfer for Lossless Large Language Model\n  Acceleration","summary":"  Large language models (LLMs) have recently shown remarkable performance\nacross a wide range of tasks. However, the substantial number of parameters in\nLLMs contributes to significant latency during model inference. This is\nparticularly evident when utilizing autoregressive decoding methods, which\ngenerate one token in a single forward process, thereby not fully capitalizing\non the parallel computing capabilities of GPUs. In this paper, we propose a\nnovel parallel decoding approach, namely \\textit{hidden transfer}, which\ndecodes multiple successive tokens simultaneously in a single forward pass. The\nidea is to transfer the intermediate hidden states of the previous context to\nthe \\textit{pseudo} hidden states of the future tokens to be generated, and\nthen the pseudo hidden states will pass the following transformer layers\nthereby assimilating more semantic information and achieving superior\npredictive accuracy of the future tokens.\n  Besides, we use the novel tree attention mechanism to simultaneously generate\nand verify multiple candidates of output sequences, which ensure the lossless\ngeneration and further improves the generation efficiency of our method.\nExperiments demonstrate the effectiveness of our method. We conduct a lot of\nanalytic experiments to prove our motivation. In terms of acceleration metrics,\nwe outperform all the single-model acceleration techniques, including Medusa\nand Self-Speculative decoding.\n","authors":["Pengfei Wu","Jiahao Liu","Zhuocheng Gong","Qifan Wang","Jinpeng Li","Jingang Wang","Xunliang Cai","Dongyan Zhao"],"pdf_url":"https://arxiv.org/pdf/2404.12022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12014v1","updated":"2024-04-18T09:04:39Z","published":"2024-04-18T09:04:39Z","title":"Enhance Robustness of Language Models Against Variation Attack through\n  Graph Integration","summary":"  The widespread use of pre-trained language models (PLMs) in natural language\nprocessing (NLP) has greatly improved performance outcomes. However, these\nmodels' vulnerability to adversarial attacks (e.g., camouflaged hints from drug\ndealers), particularly in the Chinese language with its rich character\ndiversity/variation and complex structures, hatches vital apprehension. In this\nstudy, we propose a novel method, CHinese vAriatioN Graph Enhancement (CHANGE),\nto increase the robustness of PLMs against character variation attacks in\nChinese content. CHANGE presents a novel approach for incorporating a Chinese\ncharacter variation graph into the PLMs. Through designing different\nsupplementary tasks utilizing the graph structure, CHANGE essentially enhances\nPLMs' interpretation of adversarially manipulated text. Experiments conducted\nin a multitude of NLP tasks show that CHANGE outperforms current language\nmodels in combating against adversarial attacks and serves as a valuable\ncontribution to robust language model research. These findings contribute to\nthe groundwork on robust language models and highlight the substantial\npotential of graph-guided pre-training strategies for real-world applications.\n","authors":["Zi Xiong","Lizhi Qing","Yangyang Kang","Jiawei Liu","Hongsong Li","Changlong Sun","Xiaozhong Liu","Wei Lu"],"pdf_url":"https://arxiv.org/pdf/2404.12014v1.pdf","comment":"12 pages, 4 figures, accepted by COLING 2024"},{"id":"http://arxiv.org/abs/2404.12013v1","updated":"2024-04-18T09:04:15Z","published":"2024-04-18T09:04:15Z","title":"Sequential Compositional Generalization in Multimodal Models","summary":"  The rise of large-scale multimodal models has paved the pathway for\ngroundbreaking advances in generative modeling and reasoning, unlocking\ntransformative applications in a variety of complex tasks. However, a pressing\nquestion that remains is their genuine capability for stronger forms of\ngeneralization, which has been largely underexplored in the multimodal setting.\nOur study aims to address this by examining sequential compositional\ngeneralization using \\textsc{CompAct} (\\underline{Comp}ositional\n\\underline{Act}ivities)\\footnote{Project Page:\n\\url{http://cyberiada.github.io/CompAct}}, a carefully constructed,\nperceptually grounded dataset set within a rich backdrop of egocentric kitchen\nactivity videos. Each instance in our dataset is represented with a combination\nof raw video footage, naturally occurring sound, and crowd-sourced step-by-step\ndescriptions. More importantly, our setup ensures that the individual concepts\nare consistently distributed across training and evaluation sets, while their\ncompositions are novel in the evaluation set. We conduct a comprehensive\nassessment of several unimodal and multimodal models. Our findings reveal that\nbi-modal and tri-modal models exhibit a clear edge over their text-only\ncounterparts. This highlights the importance of multimodality while charting a\ntrajectory for future research in this domain.\n","authors":["Semih Yagcioglu","Osman Batur ƒ∞nce","Aykut Erdem","Erkut Erdem","Desmond Elliott","Deniz Yuret"],"pdf_url":"https://arxiv.org/pdf/2404.12013v1.pdf","comment":"Accepted to the main conference of NAACL (2024) as a long paper"},{"id":"http://arxiv.org/abs/2404.12010v1","updated":"2024-04-18T09:02:45Z","published":"2024-04-18T09:02:45Z","title":"ParaFusion: A Large-Scale LLM-Driven English Paraphrase Dataset Infused\n  with High-Quality Lexical and Syntactic Diversity","summary":"  Paraphrase generation is a pivotal task in natural language processing (NLP).\nExisting datasets in the domain lack syntactic and lexical diversity, resulting\nin paraphrases that closely resemble the source sentences. Moreover, these\ndatasets often contain hate speech and noise, and may unintentionally include\nnon-English language sentences. This research introduces ParaFusion, a\nlarge-scale, high-quality English paraphrase dataset developed using Large\nLanguage Models (LLM) to address these challenges. ParaFusion augments existing\ndatasets with high-quality data, significantly enhancing both lexical and\nsyntactic diversity while maintaining close semantic similarity. It also\nmitigates the presence of hate speech and reduces noise, ensuring a cleaner and\nmore focused English dataset. Results show that ParaFusion offers at least a\n25% improvement in both syntactic and lexical diversity, measured across\nseveral metrics for each data source. The paper also aims to set a gold\nstandard for paraphrase evaluation as it contains one of the most comprehensive\nevaluation strategies to date. The results underscore the potential of\nParaFusion as a valuable resource for improving NLP applications.\n","authors":["Lasal Jayawardena","Prasan Yapa"],"pdf_url":"https://arxiv.org/pdf/2404.12010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12006v1","updated":"2024-04-18T08:56:47Z","published":"2024-04-18T08:56:47Z","title":"Variational Multi-Modal Hypergraph Attention Network for Multi-Modal\n  Relation Extraction","summary":"  Multi-modal relation extraction (MMRE) is a challenging task that aims to\nidentify relations between entities in text leveraging image information.\nExisting methods are limited by their neglect of the multiple entity pairs in\none sentence sharing very similar contextual information (ie, the same text and\nimage), resulting in increased difficulty in the MMRE task. To address this\nlimitation, we propose the Variational Multi-Modal Hypergraph Attention Network\n(VM-HAN) for multi-modal relation extraction. Specifically, we first construct\na multi-modal hypergraph for each sentence with the corresponding image, to\nestablish different high-order intra-/inter-modal correlations for different\nentity pairs in each sentence. We further design the Variational Hypergraph\nAttention Networks (V-HAN) to obtain representational diversity among different\nentity pairs using Gaussian distribution and learn a better hypergraph\nstructure via variational attention. VM-HAN achieves state-of-the-art\nperformance on the multi-modal relation extraction task, outperforming existing\nmethods in terms of accuracy and efficiency.\n","authors":["Qian Li","Cheng Ji","Shu Guo","Yong Zhao","Qianren Mao","Shangguang Wang","Yuntao Wei","Jianxin Li"],"pdf_url":"https://arxiv.org/pdf/2404.12006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11999v1","updated":"2024-04-18T08:49:38Z","published":"2024-04-18T08:49:38Z","title":"Token-level Direct Preference Optimization","summary":"  Fine-tuning pre-trained Large Language Models (LLMs) is essential to align\nthem with human values and intentions. This process often utilizes methods like\npairwise comparisons and KL divergence against a reference LLM, focusing on the\nevaluation of full answers generated by the models. However, the generation of\nthese responses occurs in a token level, following a sequential,\nauto-regressive fashion. In this paper, we introduce Token-level Direct\nPreference Optimization (TDPO), a novel approach to align LLMs with human\npreferences by optimizing policy at the token level. Unlike previous methods,\nwhich face challenges in divergence efficiency, TDPO incorporates forward KL\ndivergence constraints for each token, improving alignment and diversity.\nUtilizing the Bradley-Terry model for a token-based reward system, TDPO\nenhances the regulation of KL divergence, while preserving simplicity without\nthe need for explicit reward modeling. Experimental results across various text\ntasks demonstrate TDPO's superior performance in balancing alignment with\ngeneration diversity. Notably, fine-tuning with TDPO strikes a better balance\nthan DPO in the controlled sentiment generation and single-turn dialogue\ndatasets, and significantly improves the quality of generated responses\ncompared to both DPO and PPO-based RLHF methods. Our code is open-sourced at\nhttps://github.com/Vance0124/Token-level-Direct-Preference-Optimization.\n","authors":["Yongcheng Zeng","Guoqing Liu","Weiyu Ma","Ning Yang","Haifeng Zhang","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2404.11999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11978v1","updated":"2024-04-18T08:14:53Z","published":"2024-04-18T08:14:53Z","title":"EVIT: Event-Oriented Instruction Tuning for Event Reasoning","summary":"  Events refer to specific occurrences, incidents, or happenings that take\nplace under a particular background. Event reasoning aims to infer events\naccording to certain relations and predict future events. The cutting-edge\ntechniques for event reasoning play a crucial role in various natural language\nprocessing applications. Large language models (LLMs) have made significant\nadvancements in event reasoning owing to their wealth of knowledge and\nreasoning capabilities. However, smaller instruction-tuned models currently in\nuse do not consistently demonstrate exceptional proficiency in managing these\ntasks. This discrepancy arises from the absence of explicit modeling of events\nand the interconnections of them within their instruction data. Consequently,\nthese models face challenges in comprehending event structures and semantics\nwhile struggling to bridge the gap between their interpretations and human\nunderstanding of events. Additionally, their limitations in grasping event\nrelations lead to constrained event reasoning abilities to effectively deduce\nand incorporate pertinent event knowledge. In this paper, we propose\nEvent-Oriented Instruction Tuning (EvIT) to train our LLM. Specifically, we\nfirst propose a novel structure named event quadruple which contains the\nstructure and semantics of events and is complete in the event representation.\nWe then design event-relation learning based on the structures. We encapsulate\nthe learning into the instruction-tuning formulation to better stimulate the\nevent reasoning capacity of our model. We design a heuristic unsupervised\nmethod to mine event quadruple from a large-scale corpus. At last, we finetune\na Llama model on our Event-Oriented Instruction Tuning. We conduct extensive\nexperiments on event reasoning tasks on several datasets. Automatic and human\nevaluations demonstrate EvIT achieves competitive performances on event\nreasoning.\n","authors":["Zhengwei Tao","Xiancai Chen","Zhi Jin","Xiaoying Bai","Haiyan Zhao","Yiwei Lou"],"pdf_url":"https://arxiv.org/pdf/2404.11978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.10390v4","updated":"2024-04-18T08:13:58Z","published":"2023-08-20T23:47:23Z","title":"LibriSQA: A Novel Dataset and Framework for Spoken Question Answering\n  with Large Language Models","summary":"  While Large Language Models (LLMs) have demonstrated commendable performance\nacross a myriad of domains and tasks, existing LLMs still exhibit a palpable\ndeficit in handling multimodal functionalities, especially for the Spoken\nQuestion Answering (SQA) task which necessitates precise alignment and deep\ninteraction between speech and text features. To address the SQA challenge on\nLLMs, we initially curated the free-form and open-ended LibriSQA dataset from\nLibrispeech, comprising Part I with natural conversational formats and Part II\nencompassing multiple-choice questions followed by answers and analytical\nsegments. Both parts collectively include 107k SQA pairs that cover various\ntopics. Given the evident paucity of existing speech-text LLMs, we propose a\nlightweight, end-to-end framework to execute the SQA task on the LibriSQA,\nwitnessing significant results. By reforming ASR into the SQA format, we\nfurther substantiate our framework's capability in handling ASR tasks. Our\nempirical findings bolster the LLMs' aptitude for aligning and comprehending\nmultimodal information, paving the way for the development of universal\nmultimodal LLMs. The dataset and demo can be found at\nhttps://github.com/ZihanZhaoSJTU/LibriSQA.\n","authors":["Zihan Zhao","Yiyang Jiang","Heyang Liu","Yanfeng Wang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2308.10390v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11972v1","updated":"2024-04-18T07:59:53Z","published":"2024-04-18T07:59:53Z","title":"Aligning Language Models to Explicitly Handle Ambiguity","summary":"  In spoken languages, utterances are often shaped to be incomplete or vague\nfor efficiency. This can lead to varying interpretations of the same input,\nbased on different assumptions about the context. To ensure reliable user-model\ninteractions in such scenarios, it is crucial for models to adeptly handle the\ninherent ambiguity in user queries. However, conversational agents built upon\neven the most recent large language models (LLMs) face challenges in processing\nambiguous inputs, primarily due to the following two hurdles: (1) LLMs are not\ndirectly trained to handle inputs that are too ambiguous to be properly\nmanaged; (2) the degree of ambiguity in an input can vary according to the\nintrinsic knowledge of the LLMs, which is difficult to investigate. To address\nthese issues, this paper proposes a method to align LLMs to explicitly handle\nambiguous inputs. Specifically, we introduce a proxy task that guides LLMs to\nutilize their intrinsic knowledge to self-disambiguate a given input. We\nquantify the information gain from the disambiguation procedure as a measure of\nthe extent to which the models perceive their inputs as ambiguous. This measure\nserves as a cue for selecting samples deemed ambiguous from the models'\nperspectives, which are then utilized for alignment. Experimental results from\nseveral question-answering datasets demonstrate that the LLMs fine-tuned with\nour approach are capable of handling ambiguous inputs while still performing\ncompetitively on clear questions within the task.\n","authors":["Hyuhng Joon Kim","Youna Kim","Cheonbok Park","Junyeob Kim","Choonghyun Park","Kang Min Yoo","Sang-goo Lee","Taeuk Kim"],"pdf_url":"https://arxiv.org/pdf/2404.11972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08354v2","updated":"2024-04-18T07:59:12Z","published":"2024-04-12T09:48:58Z","title":"Gaining More Insight into Neural Semantic Parsing with Challenging\n  Benchmarks","summary":"  The Parallel Meaning Bank (PMB) serves as a corpus for semantic processing\nwith a focus on semantic parsing and text generation. Currently, we witness an\nexcellent performance of neural parsers and generators on the PMB. This might\nsuggest that such semantic processing tasks have by and large been solved. We\nargue that this is not the case and that performance scores from the past on\nthe PMB are inflated by non-optimal data splits and test sets that are too\neasy. In response, we introduce several changes. First, instead of the prior\nrandom split, we propose a more systematic splitting approach to improve the\nreliability of the standard test data. Second, except for the standard test\nset, we also propose two challenge sets: one with longer texts including\ndiscourse structure, and one that addresses compositional generalization. We\nevaluate five neural models for semantic parsing and meaning-to-text\ngeneration. Our results show that model performance declines (in some cases\ndramatically) on the challenge sets, revealing the limitations of neural models\nwhen confronting such challenges.\n","authors":["Xiao Zhang","Chunliu Wang","Rik van Noord","Johan Bos"],"pdf_url":"https://arxiv.org/pdf/2404.08354v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11968v1","updated":"2024-04-18T07:55:02Z","published":"2024-04-18T07:55:02Z","title":"P-NAL: an Effective and Interpretable Entity Alignment Method","summary":"  Entity alignment (EA) aims to find equivalent entities between two Knowledge\nGraphs. Existing embedding-based EA methods usually encode entities as\nembeddings, triples as embeddings' constraint and learn to align the\nembeddings. The structural and side information are usually utilized via\nembedding propagation, aggregation or interaction. However, the details of the\nunderlying logical inference steps among the alignment process are usually\nomitted, resulting in inadequate inference process. In this paper, we introduce\nP-NAL, an entity alignment method that captures two types of logical inference\npaths with Non-Axiomatic Logic (NAL). Type 1 is the bridge-like inference path\nbetween to-be-aligned entity pairs, consisting of two relation/attribute\ntriples and a similarity sentence between the other two entities. Type 2 links\nthe entity pair by their embeddings. P-NAL iteratively aligns entities and\nrelations by integrating the conclusions of the inference paths. Moreover, our\nmethod is logically interpretable and extensible due to the expressiveness of\nNAL. Our proposed method is suitable for various EA settings. Experimental\nresults show that our method outperforms state-of-the-art methods in terms of\nHits@1, achieving 0.98+ on all three datasets of DBP15K with both supervised\nand unsupervised settings. To our knowledge, we present the first in-depth\nanalysis of entity alignment's basic principles from a unified logical\nperspective.\n","authors":["Chuanhao Xu","Jingwei Cheng","Fu Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.11968v1.pdf","comment":"13 pages, 2 figures"},{"id":"http://arxiv.org/abs/2309.10931v3","updated":"2024-04-18T07:54:25Z","published":"2023-09-19T21:07:52Z","title":"A Family of Pretrained Transformer Language Models for Russian","summary":"  Transformer language models (LMs) are fundamental to NLP research\nmethodologies and applications in various languages. However, developing such\nmodels specifically for the Russian language has received little attention.\nThis paper introduces a collection of 13 Russian Transformer LMs, which spans\nencoder (ruBERT, ruRoBERTa, ruELECTRA), decoder (ruGPT-3), and encoder-decoder\n(ruT5, FRED-T5) architectures. We provide a report on the model architecture\ndesign and pretraining, and the results of evaluating their generalization\nabilities on Russian language understanding and generation datasets and\nbenchmarks. By pretraining and releasing these specialized Transformer LMs, we\naim to broaden the scope of the NLP research directions and enable the\ndevelopment of industrial solutions for the Russian language.\n","authors":["Dmitry Zmitrovich","Alexander Abramov","Andrey Kalmykov","Maria Tikhonova","Ekaterina Taktasheva","Danil Astafurov","Mark Baushenko","Artem Snegirev","Vitalii Kadulin","Sergey Markov","Tatiana Shavrina","Vladislav Mikhailov","Alena Fenogenova"],"pdf_url":"https://arxiv.org/pdf/2309.10931v3.pdf","comment":"to appear in LREC-COLING-2024"},{"id":"http://arxiv.org/abs/2404.11086v2","updated":"2024-04-18T07:41:23Z","published":"2024-04-17T05:57:17Z","title":"ViLLM-Eval: A Comprehensive Evaluation Suite for Vietnamese Large\n  Language Models","summary":"  The rapid advancement of large language models (LLMs) necessitates the\ndevelopment of new benchmarks to accurately assess their capabilities. To\naddress this need for Vietnamese, this work aims to introduce ViLLM-Eval, the\ncomprehensive evaluation suite designed to measure the advanced knowledge and\nreasoning abilities of foundation models within a Vietnamese context.\nViLLM-Eval consists of multiple-choice questions and predict next word tasks\nspanning various difficulty levels and diverse disciplines, ranging from\nhumanities to science and engineering. A thorough evaluation of the most\nadvanced LLMs on ViLLM-Eval revealed that even the best performing models have\nsignificant room for improvement in understanding and responding to Vietnamese\nlanguage tasks. ViLLM-Eval is believed to be instrumental in identifying key\nstrengths and weaknesses of foundation models, ultimately promoting their\ndevelopment and enhancing their performance for Vietnamese users. This paper\nprovides a thorough overview of ViLLM-Eval as part of the Vietnamese Large\nLanguage Model shared task, held within the 10th International Workshop on\nVietnamese Language and Speech Processing (VLSP 2023).\n","authors":["Trong-Hieu Nguyen","Anh-Cuong Le","Viet-Cuong Nguyen"],"pdf_url":"https://arxiv.org/pdf/2404.11086v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2305.08322 by other authors"},{"id":"http://arxiv.org/abs/2404.11459v2","updated":"2024-04-18T07:32:52Z","published":"2024-04-17T15:07:06Z","title":"Octopus v3: Technical Report for On-device Sub-billion Multimodal AI\n  Agent","summary":"  A multimodal AI agent is characterized by its ability to process and learn\nfrom various types of data, including natural language, visual, and audio\ninputs, to inform its actions. Despite advancements in large language models\nthat incorporate visual data, such as GPT-4V, effectively translating\nimage-based data into actionable outcomes for AI agents continues to be\nchallenging. In this paper, we introduce a multimodal model that incorporates\nthe concept of functional token specifically designed for AI agent\napplications. To ensure compatibility with edge devices, our model is optimized\nto a compact size of less than 1B parameters. Like GPT-4, our model can process\nboth English and Chinese. We demonstrate that this model is capable of\noperating efficiently on a wide range of edge devices, including as constrained\nas a Raspberry Pi.\n","authors":["Wei Chen","Zhiyuan Li"],"pdf_url":"https://arxiv.org/pdf/2404.11459v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14497v2","updated":"2024-04-18T07:27:00Z","published":"2023-05-23T19:58:30Z","title":"Self-Polish: Enhance Reasoning in Large Language Models via Problem\n  Refinement","summary":"  To enhance the multi-step reasoning capabilities of large language models,\nresearchers have extensively explored prompting methods, notably the\nChain-of-Thought (CoT) method which explicitly elicits human-like rationales.\nHowever, they have inadvertently overlooked the potential of enhancing model\nreasoning performance by formulating higher-quality problems. In this work, we\nstart from the problem side and propose Self-Polish (SP), a novel method that\nfacilitates the model's reasoning by guiding it to progressively refine the\ngiven problems to be more comprehensible and solvable. We also explore several\nautomatic prompting varients and propose the Self-Polish prompt bank for the\ncommunity. SP is orthogonal to all other prompting methods of answer/reasoning\nside like CoT, allowing for seamless integration with state-of-the-art\ntechniques for further improvement. Thorough experiments show that the proposed\nmethod attains notable and consistent effectiveness on five reasoning\nbenchmarks across different models. Furthermore, our method also showcases\nimpressive performance on robustness evaluation. Codes and prompts are\navailable at https://github.com/WooooDyy/Self-Polish.\n","authors":["Zhiheng Xi","Senjie Jin","Yuhao Zhou","Rui Zheng","Songyang Gao","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2305.14497v2.pdf","comment":"Accepted to EMNLP 2023 Findings. Codes and prompts are available at\n  https://github.com/WooooDyy/Self-Polish"},{"id":"http://arxiv.org/abs/2311.08329v4","updated":"2024-04-18T07:08:21Z","published":"2023-11-14T17:18:08Z","title":"KTRL+F: Knowledge-Augmented In-Document Search","summary":"  We introduce a new problem KTRL+F, a knowledge-augmented in-document search\ntask that necessitates real-time identification of all semantic targets within\na document with the awareness of external sources through a single natural\nquery. KTRL+F addresses following unique challenges for in-document search:\n1)utilizing knowledge outside the document for extended use of additional\ninformation about targets, and 2) balancing between real-time applicability\nwith the performance. We analyze various baselines in KTRL+F and find\nlimitations of existing models, such as hallucinations, high latency, or\ndifficulties in leveraging external knowledge. Therefore, we propose a\nKnowledge-Augmented Phrase Retrieval model that shows a promising balance\nbetween speed and performance by simply augmenting external knowledge in phrase\nembedding. We also conduct a user study to verify whether solving KTRL+F can\nenhance search experience for users. It demonstrates that even with our simple\nmodel, users can reduce the time for searching with less queries and reduced\nextra visits to other sources for collecting evidence. We encourage the\nresearch community to work on KTRL+F to enhance more efficient in-document\ninformation access.\n","authors":["Hanseok Oh","Haebin Shin","Miyoung Ko","Hyunji Lee","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2311.08329v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02491v3","updated":"2024-04-18T07:03:58Z","published":"2024-04-03T05:58:57Z","title":"Measuring Social Norms of Large Language Models","summary":"  We present a new challenge to examine whether large language models\nunderstand social norms. In contrast to existing datasets, our dataset requires\na fundamental understanding of social norms to solve. Our dataset features the\nlargest set of social norm skills, consisting of 402 skills and 12,383\nquestions covering a wide set of social norms ranging from opinions and\narguments to culture and laws. We design our dataset according to the K-12\ncurriculum. This enables the direct comparison of the social understanding of\nlarge language models to humans, more specifically, elementary students. While\nprior work generates nearly random accuracy on our benchmark, recent large\nlanguage models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the\nperformance significantly, only slightly below human performance. We then\npropose a multi-agent framework based on large language models to improve the\nmodels' ability to understand social norms. This method further improves large\nlanguage models to be on par with humans. Given the increasing adoption of\nlarge language models in real-world applications, our finding is particularly\nimportant and presents a unique direction for future improvements. The proposed\nmethod and dataset are available in\nhttps://huggingface.co/datasets/socialdataset2024/social.\n","authors":["Ye Yuan","Kexin Tang","Jianhao Shen","Ming Zhang","Chenguang Wang"],"pdf_url":"https://arxiv.org/pdf/2404.02491v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08977v2","updated":"2024-04-18T06:54:55Z","published":"2024-04-13T11:58:28Z","title":"RoNID: New Intent Discovery with Generated-Reliable Labels and\n  Cluster-friendly Representations","summary":"  New Intent Discovery (NID) strives to identify known and reasonably deduce\nnovel intent groups in the open-world scenario. But current methods face issues\nwith inaccurate pseudo-labels and poor representation learning, creating a\nnegative feedback loop that degrades overall model performance, including\naccuracy and the adjusted rand index. To address the aforementioned challenges,\nwe propose a Robust New Intent Discovery (RoNID) framework optimized by an\nEM-style method, which focuses on constructing reliable pseudo-labels and\nobtaining cluster-friendly discriminative representations. RoNID comprises two\nmain modules: reliable pseudo-label generation module and cluster-friendly\nrepresentation learning module. Specifically, the pseudo-label generation\nmodule assigns reliable synthetic labels by solving an optimal transport\nproblem in the E-step, which effectively provides high-quality supervised\nsignals for the input of the cluster-friendly representation learning module.\nTo learn cluster-friendly representation with strong intra-cluster compactness\nand large inter-cluster separation, the representation learning module combines\nintra-cluster and inter-cluster contrastive learning in the M-step to feed more\ndiscriminative features into the generation module. RoNID can be performed\niteratively to ultimately yield a robust model with reliable pseudo-labels and\ncluster-friendly representations. Experimental results on multiple benchmarks\ndemonstrate our method brings substantial improvements over previous\nstate-of-the-art methods by a large margin of +1~+4 points.\n","authors":["Shun Zhang","Chaoran Yan","Jian Yang","Changyu Ren","Jiaqi Bai","Tongliang Li","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2404.08977v2.pdf","comment":"DASFAA 2024"},{"id":"http://arxiv.org/abs/2401.16158v2","updated":"2024-04-18T06:53:38Z","published":"2024-01-29T13:46:37Z","title":"Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual\n  Perception","summary":"  Mobile device agent based on Multimodal Large Language Models (MLLM) is\nbecoming a popular application. In this paper, we introduce Mobile-Agent, an\nautonomous multi-modal mobile device agent. Mobile-Agent first leverages visual\nperception tools to accurately identify and locate both the visual and textual\nelements within the app's front-end interface. Based on the perceived vision\ncontext, it then autonomously plans and decomposes the complex operation task,\nand navigates the mobile Apps through operations step by step. Different from\nprevious solutions that rely on XML files of Apps or mobile system metadata,\nMobile-Agent allows for greater adaptability across diverse mobile operating\nenvironments in a vision-centric way, thereby eliminating the necessity for\nsystem-specific customizations. To assess the performance of Mobile-Agent, we\nintroduced Mobile-Eval, a benchmark for evaluating mobile device operations.\nBased on Mobile-Eval, we conducted a comprehensive evaluation of Mobile-Agent.\nThe experimental results indicate that Mobile-Agent achieved remarkable\naccuracy and completion rates. Even with challenging instructions, such as\nmulti-app operations, Mobile-Agent can still complete the requirements. Code\nand model will be open-sourced at https://github.com/X-PLUG/MobileAgent.\n","authors":["Junyang Wang","Haiyang Xu","Jiabo Ye","Ming Yan","Weizhou Shen","Ji Zhang","Fei Huang","Jitao Sang"],"pdf_url":"https://arxiv.org/pdf/2401.16158v2.pdf","comment":"Accepted by ICLR 2024 Workshop in Large Language Model (LLM) Agents"},{"id":"http://arxiv.org/abs/2402.15473v2","updated":"2024-04-18T06:38:22Z","published":"2024-02-23T18:05:06Z","title":"Leveraging Domain Knowledge for Efficient Reward Modelling in RLHF: A\n  Case-Study in E-Commerce Opinion Summarization","summary":"  Reinforcement Learning from Human Feedback (RLHF) has become a dominating\nstrategy in aligning Language Models (LMs) with human values/goals. The key to\nthe strategy is learning a reward model ($\\varphi$), which can reflect the\nlatent reward model of humans. While this strategy has proven effective, the\ntraining methodology requires a lot of human preference annotation (usually in\nthe order of tens of thousands) to train $\\varphi$. Such a large-scale\nannotation is justifiable when it's a one-time effort, and the reward model is\nuniversally applicable. However, human goals are subjective and depend on the\ntask, requiring task-specific preference annotations, which can be impractical\nto fulfill. To address this challenge, we propose a novel approach to infuse\ndomain knowledge into $\\varphi$, which reduces the amount of preference\nannotation required ($21\\times$), omits Alignment Tax, and provides some\ninterpretability. We validate our approach in E-Commerce Opinion Summarization,\nwith a significant reduction in dataset size (to just $940$ samples) while\nadvancing the SOTA ($\\sim4$ point ROUGE-L improvement, $68\\%$ of times\npreferred by humans over SOTA). Our contributions include a novel Reward\nModeling technique and two new datasets: PromptOpinSumm (supervised data for\nOpinion Summarization) and OpinPref (a gold-standard human preference dataset).\nThe proposed methodology opens up avenues for efficient RLHF, making it more\nadaptable to applications with varying human values. We release the artifacts\n(Code: github.com/efficient-rlhf. PromptOpinSumm: hf.co/prompt-opin-summ.\nOpinPref: hf.co/opin-pref) for usage under MIT License.\n","authors":["Swaroop Nath","Tejpalsingh Siledar","Sankara Sri Raghava Ravindra Muddu","Rupasai Rangaraju","Harshad Khadilkar","Pushpak Bhattacharyya","Suman Banerjee","Amey Patil","Sudhanshu Shekhar Singh","Muthusamy Chelliah","Nikesh Garera"],"pdf_url":"https://arxiv.org/pdf/2402.15473v2.pdf","comment":"19 pages, 6 figures, 21 tables"},{"id":"http://arxiv.org/abs/2404.11932v1","updated":"2024-04-18T06:20:50Z","published":"2024-04-18T06:20:50Z","title":"CrossIn: An Efficient Instruction Tuning Approach for Cross-Lingual\n  Knowledge Alignment","summary":"  Multilingual proficiency presents a significant challenge for large language\nmodels (LLMs). English-centric models are usually suboptimal in other\nlanguages, particularly those that are linguistically distant from English.\nThis performance discrepancy mainly stems from the imbalanced distribution of\ntraining data across languages during pre-training and instruction tuning\nstages. To address this problem, we propose a novel approach called CrossIn,\nwhich utilizes a mixed composition of cross-lingual instruction tuning data.\nOur method leverages the compressed representation shared by various languages\nto efficiently enhance the model's task-solving capabilities and multilingual\nproficiency within a single process. In addition, we introduce a multi-task and\nmulti-faceted benchmark to evaluate the effectiveness of CrossIn. Experimental\nresults demonstrate that our method substantially improves performance across\ntasks and languages, and we provide extensive insights into the impact of\ncross-lingual data volume and the integration of translation data on enhancing\nmultilingual consistency and accuracy.\n","authors":["Geyu Lin","Bin Wang","Zhengyuan Liu","Nancy F. Chen"],"pdf_url":"https://arxiv.org/pdf/2404.11932v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2404.11916v1","updated":"2024-04-18T05:43:50Z","published":"2024-04-18T05:43:50Z","title":"SKIP: Skill-Localized Prompt Tuning for Inference Speed Boost-Up","summary":"  Prompt-tuning methods have shown comparable performance as\nparameter-efficient fine-tuning (PEFT) methods in various natural language\nunderstanding tasks. However, existing prompt tuning methods still utilize the\nentire model architecture; thus, they fail to accelerate inference speed in the\napplication. In this paper, we propose a novel approach called SKIll-localized\nPrompt tuning (SKIP), which is extremely efficient in inference time. Our\nmethod significantly enhances inference efficiency by investigating and\nutilizing a skill-localized subnetwork in a language model. Surprisingly, our\nmethod improves the inference speed up to 160% while pruning 52% of the\nparameters. Furthermore, we demonstrate that our method is applicable across\nvarious transformer-based architectures, thereby confirming its practicality\nand scalability.\n","authors":["Nakyeong Yang","Junseok Kim","Jiwon Moon","Yunah Jang","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2404.11916v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2404.11912v1","updated":"2024-04-18T05:25:54Z","published":"2024-04-18T05:25:54Z","title":"TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding","summary":"  With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable to long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce.\n","authors":["Hanshi Sun","Zhuoming Chen","Xinyu Yang","Yuandong Tian","Beidi Chen"],"pdf_url":"https://arxiv.org/pdf/2404.11912v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16038v2","updated":"2024-04-18T05:22:01Z","published":"2024-03-24T06:49:07Z","title":"Monotonic Paraphrasing Improves Generalization of Language Model\n  Prompting","summary":"  Performance of large language models (LLMs) may vary with different prompts\nor instructions of even the same task. One commonly recognized factor for this\nphenomenon is the model's familiarity with the given prompt or instruction,\nwhich is typically estimated by its perplexity. However, finding the prompt\nwith the lowest perplexity is challenging, given the enormous space of possible\nprompting phrases. In this paper, we propose monotonic paraphrasing (MonoPara),\nan end-to-end decoding strategy that paraphrases given prompts or instructions\ninto their lower perplexity counterparts based on an ensemble of a paraphrase\nLM for prompt (or instruction) rewriting, and a target LM (i.e. the prompt or\ninstruction executor) that constrains the generation for lower perplexity. The\nensemble decoding process can efficiently paraphrase the original prompt\nwithout altering its semantic meaning, while monotonically decreasing the\nperplexity of each generation as calculated by the target LM. We explore in\ndetail both greedy and search-based decoding as two alternative decoding\nschemes of MonoPara. Notably, MonoPara does not require any training and can\nmonotonically lower the perplexity of the paraphrased prompt or instruction,\nleading to improved performance of zero-shot LM prompting as evaluated on a\nwide selection of tasks. In addition, MonoPara is also shown to effectively\nimprove LMs' generalization on perturbed and unseen task instructions.\n","authors":["Qin Liu","Fei Wang","Nan Xu","Tianyi Yan","Tao Meng","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2403.16038v2.pdf","comment":"Under review at ARR 2024 April"},{"id":"http://arxiv.org/abs/2404.11184v2","updated":"2024-04-18T03:57:17Z","published":"2024-04-17T09:01:02Z","title":"FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out\n  Document","summary":"  Through the advent of pre-trained language models, there have been notable\nadvancements in abstractive summarization systems. Simultaneously, a\nconsiderable number of novel methods for evaluating factual consistency in\nabstractive summarization systems has been developed. But these evaluation\napproaches incorporate substantial limitations, especially on refinement and\ninterpretability. In this work, we propose highly effective and interpretable\nfactual inconsistency detection method metric Factual Inconsistency Detection\nby Zoom-in Summary and Zoom-out Document for abstractive summarization systems\nthat is based on fine-grained atomic facts decomposition. Moreover, we align\natomic facts decomposed from the summary with the source document through\nadaptive granularity expansion. These atomic facts represent a more\nfine-grained unit of information, facilitating detailed understanding and\ninterpretability of the summary's factual inconsistency. Experimental results\ndemonstrate that our proposed factual consistency checking system significantly\noutperforms existing systems.\n","authors":["Joonho Yang","Seunghyun Yoon","Byeongjeong Kim","Hwanhee Lee"],"pdf_url":"https://arxiv.org/pdf/2404.11184v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01858v3","updated":"2024-04-18T03:54:39Z","published":"2024-02-02T19:28:33Z","title":"Explaining latent representations of generative models with large\n  multimodal models","summary":"  Learning interpretable representations of data generative latent factors is\nan important topic for the development of artificial intelligence. With the\nrise of the large multimodal model, it can align images with text to generate\nanswers. In this work, we propose a framework to comprehensively explain each\nlatent variable in the generative models using a large multimodal model. We\nfurther measure the uncertainty of our generated explanations, quantitatively\nevaluate the performance of explanation generation among multiple large\nmultimodal models, and qualitatively visualize the variations of each latent\nvariable to learn the disentanglement effects of different generative models on\nexplanations. Finally, we discuss the explanatory capabilities and limitations\nof state-of-the-art large multimodal models.\n","authors":["Mengdan Zhu","Zhenke Liu","Bo Pan","Abhinav Angirekula","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.01858v3.pdf","comment":"ICLR 2024 Workshop on Reliable and Responsible Foundation Models"},{"id":"http://arxiv.org/abs/2404.06714v3","updated":"2024-04-18T03:54:38Z","published":"2024-04-10T03:46:03Z","title":"Llama-VITS: Enhancing TTS Synthesis with Semantic Awareness","summary":"  Recent advancements in Natural Language Processing (NLP) have seen\nLarge-scale Language Models (LLMs) excel at producing high-quality text for\nvarious purposes. Notably, in Text-To-Speech (TTS) systems, the integration of\nBERT for semantic token generation has underscored the importance of semantic\ncontent in producing coherent speech outputs. Despite this, the specific\nutility of LLMs in enhancing TTS synthesis remains considerably limited. This\nresearch introduces an innovative approach, Llama-VITS, which enhances TTS\nsynthesis by enriching the semantic content of text using LLM. Llama-VITS\nintegrates semantic embeddings from Llama2 with the VITS model, a leading\nend-to-end TTS framework. By leveraging Llama2 for the primary speech synthesis\nprocess, our experiments demonstrate that Llama-VITS matches the naturalness of\nthe original VITS (ORI-VITS) and those incorporate BERT (BERT-VITS), on the\nLJSpeech dataset, a substantial collection of neutral, clear speech. Moreover,\nour method significantly enhances emotive expressiveness on the EmoV_DB_bea_sem\ndataset, a curated selection of emotionally consistent speech from the EmoV_DB\ndataset, highlighting its potential to generate emotive speech.\n","authors":["Xincan Feng","Akifumi Yoshimoto"],"pdf_url":"https://arxiv.org/pdf/2404.06714v3.pdf","comment":"9 pages, 2 figures, 4 tables; accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2208.08690v3","updated":"2024-04-18T03:47:27Z","published":"2022-08-18T08:03:45Z","title":"A Survey on Open Information Extraction from Rule-based Model to Large\n  Language Model","summary":"  Open Information Extraction (OpenIE) represents a crucial NLP task aimed at\nderiving structured information from unstructured text, unrestricted by\nrelation type or domain. This survey paper provides an overview of OpenIE\ntechnologies spanning from 2007 to 2024, emphasizing a chronological\nperspective absent in prior surveys. It examines the evolution of task settings\nin OpenIE to align with the advances in recent technologies. The paper\ncategorizes OpenIE approaches into rule-based, neural, and pre-trained large\nlanguage models, discussing each within a chronological framework.\nAdditionally, it highlights prevalent datasets and evaluation metrics currently\nin use. Building on this extensive review, the paper outlines potential future\ndirections in terms of datasets, information sources, output formats,\nmethodologies, and evaluation metrics.\n","authors":["Pai Liu","Wenyang Gao","Wenjie Dong","Lin Ai","Ziwei Gong","Songfang Huang","Zongsheng Li","Ehsan Hoque","Julia Hirschberg","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2208.08690v3.pdf","comment":"The first five authors contributed to this work equally. Names are\n  ordered randomly"},{"id":"http://arxiv.org/abs/2404.11870v1","updated":"2024-04-18T03:03:46Z","published":"2024-04-18T03:03:46Z","title":"Enhancing Length Extrapolation in Sequential Models with\n  Pointer-Augmented Neural Memory","summary":"  We propose Pointer-Augmented Neural Memory (PANM) to help neural networks\nunderstand and apply symbol processing to new, longer sequences of data. PANM\nintegrates an external neural memory that uses novel physical addresses and\npointer manipulation techniques to mimic human and computer symbol processing\nabilities. PANM facilitates pointer assignment, dereference, and arithmetic by\nexplicitly using physical pointers to access memory content. Remarkably, it can\nlearn to perform these operations through end-to-end training on sequence data,\npowering various sequential models. Our experiments demonstrate PANM's\nexceptional length extrapolating capabilities and improved performance in tasks\nthat require symbol processing, such as algorithmic reasoning and Dyck language\nrecognition. PANM helps Transformer achieve up to 100% generalization accuracy\nin compositional learning tasks and significantly better results in\nmathematical reasoning, question answering and machine translation tasks.\n","authors":["Hung Le","Dung Nguyen","Kien Do","Svetha Venkatesh","Truyen Tran"],"pdf_url":"https://arxiv.org/pdf/2404.11870v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2404.11845v1","updated":"2024-04-18T01:48:28Z","published":"2024-04-18T01:48:28Z","title":"Challenging Negative Gender Stereotypes: A Study on the Effectiveness of\n  Automated Counter-Stereotypes","summary":"  Gender stereotypes are pervasive beliefs about individuals based on their\ngender that play a significant role in shaping societal attitudes, behaviours,\nand even opportunities. Recognizing the negative implications of gender\nstereotypes, particularly in online communications, this study investigates\neleven strategies to automatically counter-act and challenge these views. We\npresent AI-generated gender-based counter-stereotypes to (self-identified) male\nand female study participants and ask them to assess their offensiveness,\nplausibility, and potential effectiveness. The strategies of counter-facts and\nbroadening universals (i.e., stating that anyone can have a trait regardless of\ngroup membership) emerged as the most robust approaches, while humour,\nperspective-taking, counter-examples, and empathy for the speaker were\nperceived as less effective. Also, the differences in ratings were more\npronounced for stereotypes about the different targets than between the genders\nof the raters. Alarmingly, many AI-generated counter-stereotypes were perceived\nas offensive and/or implausible. Our analysis and the collected dataset offer\nfoundational insight into counter-stereotype generation, guiding future efforts\nto develop strategies that effectively challenge gender stereotypes in online\ninteractions.\n","authors":["Isar Nejadgholi","Kathleen C. Fraser","Anna Kerkhof","Svetlana Kiritchenko"],"pdf_url":"https://arxiv.org/pdf/2404.11845v1.pdf","comment":"LREC-COLING2024"},{"id":"http://arxiv.org/abs/2401.16553v5","updated":"2024-04-18T01:35:12Z","published":"2024-01-29T20:44:10Z","title":"SelectLLM: Can LLMs Select Important Instructions to Annotate?","summary":"  Instruction tuning benefits from large and diverse datasets, however creating\nsuch datasets involves a high cost of human labeling. While synthetic datasets\ngenerated by large language models (LLMs) have partly solved this issue, they\noften contain low-quality data. One effective solution is selectively\nannotating unlabelled instructions, especially given the relative ease of\nacquiring unlabeled instructions or texts from various sources. However, how to\nselect unlabelled instructions is not well-explored, especially in the context\nof LLMs. Further, traditional data selection methods, relying on input\nembedding space density, tend to underestimate instruction sample complexity,\nwhereas those based on model prediction uncertainty often struggle with\nsynthetic label quality. Therefore, we introduce SelectLLM, an alternative\nframework that leverages the capabilities of LLMs to more effectively select\nunlabeled instructions. SelectLLM consists of two key steps: Coreset-based\nclustering of unlabelled instructions for diversity and then prompting a LLM to\nidentify the most beneficial instructions within each cluster. Our experiments\ndemonstrate that SelectLLM matches or outperforms other state-of-the-art\nmethods in instruction tuning benchmarks. It exhibits remarkable consistency\nacross human and synthetic datasets, along with better cross-dataset\ngeneralization, as evidenced by a 10% performance improvement on the Cleaned\nAlpaca test set when trained on Dolly data. All code and data are publicly\navailable (https://github.com/minnesotanlp/select-llm).\n","authors":["Ritik Sachin Parkar","Jaehyung Kim","Jong Inn Park","Dongyeop Kang"],"pdf_url":"https://arxiv.org/pdf/2401.16553v5.pdf","comment":"First Authors: Ritik Sachin Parkar and Jaehyung Kim | Second Author:\n  Jong Inn Park | PI: Dongyeop Kang"},{"id":"http://arxiv.org/abs/2404.11826v1","updated":"2024-04-18T01:15:41Z","published":"2024-04-18T01:15:41Z","title":"AdvisorQA: Towards Helpful and Harmless Advice-seeking Question\n  Answering with Collective Intelligence","summary":"  As the integration of large language models into daily life is on the rise,\nthere is a clear gap in benchmarks for advising on subjective and personal\ndilemmas. To address this, we introduce AdvisorQA, the first benchmark\ndeveloped to assess LLMs' capability in offering advice for deeply personalized\nconcerns, utilizing the LifeProTips subreddit forum. This forum features a\ndynamic interaction where users post advice-seeking questions, receiving an\naverage of 8.9 advice per query, with 164.2 upvotes from hundreds of users,\nembodying a collective intelligence framework. Therefore, we've completed a\nbenchmark encompassing daily life questions, diverse corresponding responses,\nand majority vote ranking to train our helpfulness metric. Baseline experiments\nvalidate the efficacy of AdvisorQA through our helpfulness metric, GPT-4, and\nhuman evaluation, analyzing phenomena beyond the trade-off between helpfulness\nand harmlessness. AdvisorQA marks a significant leap in enhancing QA systems\nfor providing personalized, empathetic advice, showcasing LLMs' improved\nunderstanding of human subjectivity.\n","authors":["Minbeom Kim","Hwanhee Lee","Joonsuk Park","Hwaran Lee","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2404.11826v1.pdf","comment":"19 pages, 11 figures"},{"id":"http://arxiv.org/abs/2404.11809v1","updated":"2024-04-18T00:05:02Z","published":"2024-04-18T00:05:02Z","title":"Sharing Parameter by Conjugation for Knowledge Graph Embeddings in\n  Complex Space","summary":"  A Knowledge Graph (KG) is the directed graphical representation of entities\nand relations in the real world. KG can be applied in diverse Natural Language\nProcessing (NLP) tasks where knowledge is required. The need to scale up and\ncomplete KG automatically yields Knowledge Graph Embedding (KGE), a shallow\nmachine learning model that is suffering from memory and training time\nconsumption issues. To mitigate the computational load, we propose a\nparameter-sharing method, i.e., using conjugate parameters for complex numbers\nemployed in KGE models. Our method improves memory efficiency by 2x in relation\nembedding while achieving comparable performance to the state-of-the-art\nnon-conjugate models, with faster, or at least comparable, training time. We\ndemonstrated the generalizability of our method on two best-performing KGE\nmodels $5^{\\bigstar}\\mathrm{E}$ and $\\mathrm{ComplEx}$ on five benchmark\ndatasets.\n","authors":["Xincan Feng","Zhi Qu","Yuchang Cheng","Taro Watanabe","Nobuhiro Yugami"],"pdf_url":"https://arxiv.org/pdf/2404.11809v1.pdf","comment":"8 pages, 1 figure, 6 tables, accepted at TextGraphs-16 workshop held\n  in conjunction with COLING 2022"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2404.12391v1","updated":"2024-04-18T17:59:58Z","published":"2024-04-18T17:59:58Z","title":"On the Content Bias in Fr√©chet Video Distance","summary":"  Fr\\'echet Video Distance (FVD), a prominent metric for evaluating video\ngeneration models, is known to conflict with human perception occasionally. In\nthis paper, we aim to explore the extent of FVD's bias toward per-frame quality\nover temporal realism and identify its sources. We first quantify the FVD's\nsensitivity to the temporal axis by decoupling the frame and motion quality and\nfind that the FVD increases only slightly with large temporal corruption. We\nthen analyze the generated videos and show that via careful sampling from a\nlarge set of generated videos that do not contain motions, one can drastically\ndecrease FVD without improving the temporal quality. Both studies suggest FVD's\nbias towards the quality of individual frames. We further observe that the bias\ncan be attributed to the features extracted from a supervised video classifier\ntrained on the content-biased dataset. We show that FVD with features extracted\nfrom the recent large-scale self-supervised video models is less biased toward\nimage quality. Finally, we revisit a few real-world examples to validate our\nhypothesis.\n","authors":["Songwei Ge","Aniruddha Mahapatra","Gaurav Parmar","Jun-Yan Zhu","Jia-Bin Huang"],"pdf_url":"https://arxiv.org/pdf/2404.12391v1.pdf","comment":"CVPR 2024. Project webpage: https://content-debiased-fvd.github.io/"},{"id":"http://arxiv.org/abs/2404.01300v2","updated":"2024-04-18T17:59:57Z","published":"2024-04-01T17:59:55Z","title":"NeRF-MAE: Masked AutoEncoders for Self-Supervised 3D Representation\n  Learning for Neural Radiance Fields","summary":"  Neural fields excel in computer vision and robotics due to their ability to\nunderstand the 3D visual world such as inferring semantics, geometry, and\ndynamics. Given the capabilities of neural fields in densely representing a 3D\nscene from 2D images, we ask the question: Can we scale their self-supervised\npretraining, specifically using masked autoencoders, to generate effective 3D\nrepresentations from posed RGB images. Owing to the astounding success of\nextending transformers to novel data modalities, we employ standard 3D Vision\nTransformers to suit the unique formulation of NeRFs. We leverage NeRF's\nvolumetric grid as a dense input to the transformer, contrasting it with other\n3D representations such as pointclouds where the information density can be\nuneven, and the representation is irregular. Due to the difficulty of applying\nmasked autoencoders to an implicit representation, such as NeRF, we opt for\nextracting an explicit representation that canonicalizes scenes across domains\nby employing the camera trajectory for sampling. Our goal is made possible by\nmasking random patches from NeRF's radiance and density grid and employing a\nstandard 3D Swin Transformer to reconstruct the masked patches. In doing so,\nthe model can learn the semantic and spatial structure of complete scenes. We\npretrain this representation at scale on our proposed curated posed-RGB data,\ntotaling over 1.6 million images. Once pretrained, the encoder is used for\neffective 3D transfer learning. Our novel self-supervised pretraining for\nNeRFs, NeRF-MAE, scales remarkably well and improves performance on various\nchallenging 3D tasks. Utilizing unlabeled posed 2D data for pretraining,\nNeRF-MAE significantly outperforms self-supervised 3D pretraining and NeRF\nscene understanding baselines on Front3D and ScanNet datasets with an absolute\nperformance improvement of over 20% AP50 and 8% AP25 for 3D object detection.\n","authors":["Muhammad Zubair Irshad","Sergey Zakahrov","Vitor Guizilini","Adrien Gaidon","Zsolt Kira","Rares Ambrus"],"pdf_url":"https://arxiv.org/pdf/2404.01300v2.pdf","comment":"29 pages, 13 figures. Project Page: https://nerf-mae.github.io/"},{"id":"http://arxiv.org/abs/2404.12390v1","updated":"2024-04-18T17:59:54Z","published":"2024-04-18T17:59:54Z","title":"BLINK: Multimodal Large Language Models Can See but Not Perceive","summary":"  We introduce Blink, a new benchmark for multimodal language models (LLMs)\nthat focuses on core visual perception abilities not found in other\nevaluations. Most of the Blink tasks can be solved by humans \"within a blink\"\n(e.g., relative depth estimation, visual correspondence, forensics detection,\nand multi-view reasoning). However, we find these perception-demanding tasks\ncast significant challenges for current multimodal LLMs because they resist\nmediation through natural language. Blink reformats 14 classic computer vision\ntasks into 3,807 multiple-choice questions, paired with single or multiple\nimages and visual prompting. While humans get 95.70% accuracy on average, Blink\nis surprisingly challenging for existing multimodal LLMs: even the\nbest-performing GPT-4V and Gemini achieve accuracies of 51.26% and 45.72%, only\n13.17% and 7.63% higher than random guessing, indicating that such perception\nabilities have not \"emerged\" yet in recent multimodal LLMs. Our analysis also\nhighlights that specialist CV models could solve these problems much better,\nsuggesting potential pathways for future improvements. We believe Blink will\nstimulate the community to help multimodal LLMs catch up with human-level\nvisual perception.\n","authors":["Xingyu Fu","Yushi Hu","Bangzheng Li","Yu Feng","Haoyu Wang","Xudong Lin","Dan Roth","Noah A. Smith","Wei-Chiu Ma","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2404.12390v1.pdf","comment":"Multimodal Benchmark, Project Url: https://zeyofu.github.io/blink/"},{"id":"http://arxiv.org/abs/2404.12388v1","updated":"2024-04-18T17:59:53Z","published":"2024-04-18T17:59:53Z","title":"VideoGigaGAN: Towards Detail-rich Video Super-Resolution","summary":"  Video super-resolution (VSR) approaches have shown impressive temporal\nconsistency in upsampled videos. However, these approaches tend to generate\nblurrier results than their image counterparts as they are limited in their\ngenerative capability. This raises a fundamental question: can we extend the\nsuccess of a generative image upsampler to the VSR task while preserving the\ntemporal consistency? We introduce VideoGigaGAN, a new generative VSR model\nthat can produce videos with high-frequency details and temporal consistency.\nVideoGigaGAN builds upon a large-scale image upsampler -- GigaGAN. Simply\ninflating GigaGAN to a video model by adding temporal modules produces severe\ntemporal flickering. We identify several key issues and propose techniques that\nsignificantly improve the temporal consistency of upsampled videos. Our\nexperiments show that, unlike previous VSR methods, VideoGigaGAN generates\ntemporally consistent videos with more fine-grained appearance details. We\nvalidate the effectiveness of VideoGigaGAN by comparing it with\nstate-of-the-art VSR models on public datasets and showcasing video results\nwith $8\\times$ super-resolution.\n","authors":["Yiran Xu","Taesung Park","Richard Zhang","Yang Zhou","Eli Shechtman","Feng Liu","Jia-Bin Huang","Difan Liu"],"pdf_url":"https://arxiv.org/pdf/2404.12388v1.pdf","comment":"project page: https://videogigagan.github.io/"},{"id":"http://arxiv.org/abs/2404.12389v1","updated":"2024-04-18T17:59:53Z","published":"2024-04-18T17:59:53Z","title":"Moving Object Segmentation: All You Need Is SAM (and Flow)","summary":"  The objective of this paper is motion segmentation -- discovering and\nsegmenting the moving objects in a video. This is a much studied area with\nnumerous careful,and sometimes complex, approaches and training schemes\nincluding: self-supervised learning, learning from synthetic datasets,\nobject-centric representations, amodal representations, and many more. Our\ninterest in this paper is to determine if the Segment Anything model (SAM) can\ncontribute to this task. We investigate two models for combining SAM with\noptical flow that harness the segmentation power of SAM with the ability of\nflow to discover and group moving objects. In the first model, we adapt SAM to\ntake optical flow, rather than RGB, as an input. In the second, SAM takes RGB\nas an input, and flow is used as a segmentation prompt. These surprisingly\nsimple methods, without any further modifications, outperform all previous\napproaches by a considerable margin in both single and multi-object benchmarks.\nWe also extend these frame-level segmentations to sequence-level segmentations\nthat maintain object identity. Again, this simple model outperforms previous\nmethods on multiple video object segmentation benchmarks.\n","authors":["Junyu Xie","Charig Yang","Weidi Xie","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2404.12389v1.pdf","comment":"Project Page: https://www.robots.ox.ac.uk/~vgg/research/flowsam/"},{"id":"http://arxiv.org/abs/2404.12387v1","updated":"2024-04-18T17:59:48Z","published":"2024-04-18T17:59:48Z","title":"Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language\n  Models","summary":"  We introduce Reka Core, Flash, and Edge, a series of powerful multimodal\nlanguage models trained from scratch by Reka. Reka models are able to process\nand reason with text, images, video, and audio inputs. This technical report\ndiscusses details of training some of these models and provides comprehensive\nevaluation results. We show that Reka Edge and Reka Flash are not only\nstate-of-the-art but also outperform many much larger models, delivering\noutsized values for their respective compute class. Meanwhile, our most capable\nand largest model, Reka Core, approaches the best frontier models on both\nautomatic evaluations and blind human evaluations. On image question answering\nbenchmarks (e.g. MMMU, VQAv2), Core performs competitively to GPT4-V.\nMeanwhile, on multimodal chat, Core ranks as the second most preferred model\nunder a blind third-party human evaluation setup, outperforming other models\nsuch as Claude 3 Opus. On text benchmarks, Core not only performs competitively\nto other frontier models on a set of well-established benchmarks (e.g. MMLU,\nGSM8K) but also outperforms GPT4-0613 on human evaluation. On video question\nanswering (Perception-Test), Core outperforms Gemini Ultra. Models are shipped\nin production at http://chat.reka.ai . A showcase of non cherry picked\nqualitative examples can also be found at http://showcase.reka.ai .\n","authors":["Aitor Ormazabal","Che Zheng","Cyprien de Masson d'Autume","Dani Yogatama","Deyu Fu","Donovan Ong","Eric Chen","Eugenie Lamprecht","Hai Pham","Isaac Ong","Kaloyan Aleksiev","Lei Li","Matthew Henderson","Max Bain","Mikel Artetxe","Nishant Relan","Piotr Padlewski","Qi Liu","Ren Chen","Samuel Phua","Yazheng Yang","Yi Tay","Yuqi Wang","Zhongkai Zhu","Zhihui Xie"],"pdf_url":"https://arxiv.org/pdf/2404.12387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12386v1","updated":"2024-04-18T17:59:46Z","published":"2024-04-18T17:59:46Z","title":"SOHES: Self-supervised Open-world Hierarchical Entity Segmentation","summary":"  Open-world entity segmentation, as an emerging computer vision task, aims at\nsegmenting entities in images without being restricted by pre-defined classes,\noffering impressive generalization capabilities on unseen images and concepts.\nDespite its promise, existing entity segmentation methods like Segment Anything\nModel (SAM) rely heavily on costly expert annotators. This work presents\nSelf-supervised Open-world Hierarchical Entity Segmentation (SOHES), a novel\napproach that eliminates the need for human annotations. SOHES operates in\nthree phases: self-exploration, self-instruction, and self-correction. Given a\npre-trained self-supervised representation, we produce abundant high-quality\npseudo-labels through visual feature clustering. Then, we train a segmentation\nmodel on the pseudo-labels, and rectify the noises in pseudo-labels via a\nteacher-student mutual-learning procedure. Beyond segmenting entities, SOHES\nalso captures their constituent parts, providing a hierarchical understanding\nof visual entities. Using raw images as the sole training data, our method\nachieves unprecedented performance in self-supervised open-world segmentation,\nmarking a significant milestone towards high-quality open-world entity\nsegmentation in the absence of human-annotated masks. Project page:\nhttps://SOHES.github.io.\n","authors":["Shengcao Cao","Jiuxiang Gu","Jason Kuen","Hao Tan","Ruiyi Zhang","Handong Zhao","Ani Nenkova","Liang-Yan Gui","Tong Sun","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2404.12386v1.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2404.12385v1","updated":"2024-04-18T17:59:41Z","published":"2024-04-18T17:59:41Z","title":"MeshLRM: Large Reconstruction Model for High-Quality Mesh","summary":"  We propose MeshLRM, a novel LRM-based approach that can reconstruct a\nhigh-quality mesh from merely four input images in less than one second.\nDifferent from previous large reconstruction models (LRMs) that focus on\nNeRF-based reconstruction, MeshLRM incorporates differentiable mesh extraction\nand rendering within the LRM framework. This allows for end-to-end mesh\nreconstruction by fine-tuning a pre-trained NeRF LRM with mesh rendering.\nMoreover, we improve the LRM architecture by simplifying several complex\ndesigns in previous LRMs. MeshLRM's NeRF initialization is sequentially trained\nwith low- and high-resolution images; this new LRM training strategy enables\nsignificantly faster convergence and thereby leads to better quality with less\ncompute. Our approach achieves state-of-the-art mesh reconstruction from\nsparse-view inputs and also allows for many downstream applications, including\ntext-to-3D and single-image-to-3D generation. Project page:\nhttps://sarahweiii.github.io/meshlrm/\n","authors":["Xinyue Wei","Kai Zhang","Sai Bi","Hao Tan","Fujun Luan","Valentin Deschaintre","Kalyan Sunkavalli","Hao Su","Zexiang Xu"],"pdf_url":"https://arxiv.org/pdf/2404.12385v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12383v1","updated":"2024-04-18T17:59:28Z","published":"2024-04-18T17:59:28Z","title":"G-HOP: Generative Hand-Object Prior for Interaction Reconstruction and\n  Grasp Synthesis","summary":"  We propose G-HOP, a denoising diffusion based generative prior for\nhand-object interactions that allows modeling both the 3D object and a human\nhand, conditioned on the object category. To learn a 3D spatial diffusion model\nthat can capture this joint distribution, we represent the human hand via a\nskeletal distance field to obtain a representation aligned with the (latent)\nsigned distance field for the object. We show that this hand-object prior can\nthen serve as generic guidance to facilitate other tasks like reconstruction\nfrom interaction clip and human grasp synthesis. We believe that our model,\ntrained by aggregating seven diverse real-world interaction datasets spanning\nacross 155 categories, represents a first approach that allows jointly\ngenerating both hand and object. Our empirical evaluations demonstrate the\nbenefit of this joint prior in video-based reconstruction and human grasp\nsynthesis, outperforming current task-specific baselines.\n  Project website: https://judyye.github.io/ghop-www\n","authors":["Yufei Ye","Abhinav Gupta","Kris Kitani","Shubham Tulsiani"],"pdf_url":"https://arxiv.org/pdf/2404.12383v1.pdf","comment":"accepted to CVPR2024; project page at\n  https://judyye.github.io/ghop-www"},{"id":"http://arxiv.org/abs/2404.12382v1","updated":"2024-04-18T17:59:27Z","published":"2024-04-18T17:59:27Z","title":"Lazy Diffusion Transformer for Interactive Image Editing","summary":"  We introduce a novel diffusion transformer, LazyDiffusion, that generates\npartial image updates efficiently. Our approach targets interactive image\nediting applications in which, starting from a blank canvas or an image, a user\nspecifies a sequence of localized image modifications using binary masks and\ntext prompts. Our generator operates in two phases. First, a context encoder\nprocesses the current canvas and user mask to produce a compact global context\ntailored to the region to generate. Second, conditioned on this context, a\ndiffusion-based transformer decoder synthesizes the masked pixels in a \"lazy\"\nfashion, i.e., it only generates the masked region. This contrasts with\nprevious works that either regenerate the full canvas, wasting time and\ncomputation, or confine processing to a tight rectangular crop around the mask,\nignoring the global image context altogether. Our decoder's runtime scales with\nthe mask size, which is typically small, while our encoder introduces\nnegligible overhead. We demonstrate that our approach is competitive with\nstate-of-the-art inpainting methods in terms of quality and fidelity while\nproviding a 10x speedup for typical user interactions, where the editing mask\nrepresents 10% of the image.\n","authors":["Yotam Nitzan","Zongze Wu","Richard Zhang","Eli Shechtman","Daniel Cohen-Or","Taesung Park","Micha√´l Gharbi"],"pdf_url":"https://arxiv.org/pdf/2404.12382v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12378v1","updated":"2024-04-18T17:58:16Z","published":"2024-04-18T17:58:16Z","title":"6Img-to-3D: Few-Image Large-Scale Outdoor Driving Scene Reconstruction","summary":"  Current 3D reconstruction techniques struggle to infer unbounded scenes from\na few images faithfully. Specifically, existing methods have high computational\ndemands, require detailed pose information, and cannot reconstruct occluded\nregions reliably. We introduce 6Img-to-3D, an efficient, scalable\ntransformer-based encoder-renderer method for single-shot image to 3D\nreconstruction. Our method outputs a 3D-consistent parameterized triplane from\nonly six outward-facing input images for large-scale, unbounded outdoor driving\nscenarios. We take a step towards resolving existing shortcomings by combining\ncontracted custom cross- and self-attention mechanisms for triplane\nparameterization, differentiable volume rendering, scene contraction, and image\nfeature projection. We showcase that six surround-view vehicle images from a\nsingle timestamp without global pose information are enough to reconstruct\n360$^{\\circ}$ scenes during inference time, taking 395 ms. Our method allows,\nfor example, rendering third-person images and birds-eye views. Our code is\navailable at https://github.com/continental/6Img-to-3D, and more examples can\nbe found at our website here https://6Img-to-3D.GitHub.io/.\n","authors":["Th√©o Gieruc","Marius K√§stingsch√§fer","Sebastian Bernhard","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2404.12378v1.pdf","comment":"Joint first authorship. Project page: https://6Img-to-3D.GitHub.io/\n  Code https://github.com/continental/6Img-to-3D"},{"id":"http://arxiv.org/abs/2404.12379v1","updated":"2024-04-18T17:58:16Z","published":"2024-04-18T17:58:16Z","title":"Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos","summary":"  Modern 3D engines and graphics pipelines require mesh as a memory-efficient\nrepresentation, which allows efficient rendering, geometry processing, texture\nediting, and many other downstream operations. However, it is still highly\ndifficult to obtain high-quality mesh in terms of structure and detail from\nmonocular visual observations. The problem becomes even more challenging for\ndynamic scenes and objects. To this end, we introduce Dynamic Gaussians Mesh\n(DG-Mesh), a framework to reconstruct a high-fidelity and time-consistent mesh\ngiven a single monocular video. Our work leverages the recent advancement in 3D\nGaussian Splatting to construct the mesh sequence with temporal consistency\nfrom a video. Building on top of this representation, DG-Mesh recovers\nhigh-quality meshes from the Gaussian points and can track the mesh vertices\nover time, which enables applications such as texture editing on dynamic\nobjects. We introduce the Gaussian-Mesh Anchoring, which encourages evenly\ndistributed Gaussians, resulting better mesh reconstruction through mesh-guided\ndensification and pruning on the deformed Gaussians. By applying\ncycle-consistent deformation between the canonical and the deformed space, we\ncan project the anchored Gaussian back to the canonical space and optimize\nGaussians across all time frames. During the evaluation on different datasets,\nDG-Mesh provides significantly better mesh reconstruction and rendering than\nbaselines.\n","authors":["Isabella Liu","Hao Su","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2404.12379v1.pdf","comment":"Project page: https://www.liuisabella.com/DG-Mesh/"},{"id":"http://arxiv.org/abs/2404.12372v1","updated":"2024-04-18T17:53:19Z","published":"2024-04-18T17:53:19Z","title":"MedThink: Explaining Medical Visual Question Answering via Multimodal\n  Decision-Making Rationale","summary":"  Medical Visual Question Answering (MedVQA), which offers language responses\nto image-based medical inquiries, represents a challenging task and significant\nadvancement in healthcare. It assists medical experts to swiftly interpret\nmedical images, thereby enabling faster and more accurate diagnoses. However,\nthe model interpretability and transparency of existing MedVQA solutions are\noften limited, posing challenges in understanding their decision-making\nprocesses. To address this issue, we devise a semi-automated annotation process\nto streamlining data preparation and build new benchmark MedVQA datasets R-RAD\nand R-SLAKE. The R-RAD and R-SLAKE datasets provide intermediate medical\ndecision-making rationales generated by multimodal large language models and\nhuman annotations for question-answering pairs in existing MedVQA datasets,\ni.e., VQA-RAD and SLAKE. Moreover, we design a novel framework which finetunes\nlightweight pretrained generative models by incorporating medical\ndecision-making rationales into the training process. The framework includes\nthree distinct strategies to generate decision outcomes and corresponding\nrationales, thereby clearly showcasing the medical decision-making process\nduring reasoning. Extensive experiments demonstrate that our method can achieve\nan accuracy of 83.5% on R-RAD and 86.3% on R-SLAKE, significantly outperforming\nexisting state-of-the-art baselines. Dataset and code will be released.\n","authors":["Xiaotang Gai","Chenyi Zhou","Jiaxiang Liu","Yang Feng","Jian Wu","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2404.12372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12368v1","updated":"2024-04-18T17:50:23Z","published":"2024-04-18T17:50:23Z","title":"Gradient-Regularized Out-of-Distribution Detection","summary":"  One of the challenges for neural networks in real-life applications is the\noverconfident errors these models make when the data is not from the original\ntraining distribution.\n  Addressing this issue is known as Out-of-Distribution (OOD) detection.\n  Many state-of-the-art OOD methods employ an auxiliary dataset as a surrogate\nfor OOD data during training to achieve improved performance.\n  However, these methods fail to fully exploit the local information embedded\nin the auxiliary dataset.\n  In this work, we propose the idea of leveraging the information embedded in\nthe gradient of the loss function during training to enable the network to not\nonly learn a desired OOD score for each sample but also to exhibit similar\nbehavior in a local neighborhood around each sample.\n  We also develop a novel energy-based sampling method to allow the network to\nbe exposed to more informative OOD samples during the training phase. This is\nespecially important when the auxiliary dataset is large. We demonstrate the\neffectiveness of our method through extensive experiments on several OOD\nbenchmarks, improving the existing state-of-the-art FPR95 by 4% on our ImageNet\nexperiment.\n  We further provide a theoretical analysis through the lens of certified\nrobustness and Lipschitz analysis to showcase the theoretical foundation of our\nwork. We will publicly release our code after the review process.\n","authors":["Sina Sharifi","Taha Entesari","Bardia Safaei","Vishal M. Patel","Mahyar Fazlyab"],"pdf_url":"https://arxiv.org/pdf/2404.12368v1.pdf","comment":"Under review for the 18th European Conference on Computer Vision\n  (ECCV) 2024"},{"id":"http://arxiv.org/abs/2404.12359v1","updated":"2024-04-18T17:37:53Z","published":"2024-04-18T17:37:53Z","title":"Inverse Neural Rendering for Explainable Multi-Object Tracking","summary":"  Today, most methods for image understanding tasks rely on feed-forward neural\nnetworks. While this approach has allowed for empirical accuracy, efficiency,\nand task adaptation via fine-tuning, it also comes with fundamental\ndisadvantages. Existing networks often struggle to generalize across different\ndatasets, even on the same task. By design, these networks ultimately reason\nabout high-dimensional scene features, which are challenging to analyze. This\nis true especially when attempting to predict 3D information based on 2D\nimages. We propose to recast 3D multi-object tracking from RGB cameras as an\n\\emph{Inverse Rendering (IR)} problem, by optimizing via a differentiable\nrendering pipeline over the latent space of pre-trained 3D object\nrepresentations and retrieve the latents that best represent object instances\nin a given input image. To this end, we optimize an image loss over generative\nlatent spaces that inherently disentangle shape and appearance properties. We\ninvestigate not only an alternate take on tracking but our method also enables\nexamining the generated objects, reasoning about failure situations, and\nresolving ambiguous cases. We validate the generalization and scaling\ncapabilities of our method by learning the generative prior exclusively from\nsynthetic data and assessing camera-based 3D tracking on the nuScenes and Waymo\ndatasets. Both these datasets are completely unseen to our method and do not\nrequire fine-tuning. Videos and code are available at\nhttps://light.princeton.edu/inverse-rendering-tracking/.\n","authors":["Julian Ost","Tanushree Banerjee","Mario Bijelic","Felix Heide"],"pdf_url":"https://arxiv.org/pdf/2404.12359v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12353v1","updated":"2024-04-18T17:32:46Z","published":"2024-04-18T17:32:46Z","title":"V2Xum-LLM: Cross-Modal Video Summarization with Temporal Prompt\n  Instruction Tuning","summary":"  Video summarization aims to create short, accurate, and cohesive summaries of\nlonger videos. Despite the existence of various video summarization datasets, a\nnotable limitation is their limited amount of source videos, which hampers the\neffective fine-tuning of advanced large vision-language models (VLMs).\nAdditionally, most existing datasets are created for video-to-video\nsummarization, overlooking the contemporary need for multimodal video content\nsummarization. Recent efforts have been made to expand from unimodal to\nmultimodal video summarization, categorizing the task into three sub-tasks\nbased on the summary's modality: video-to-video (V2V), video-to-text (V2T), and\na combination of video and text summarization (V2VT). However, the textual\nsummaries in previous multimodal datasets are inadequate. To address these\nissues, we introduce Instruct-V2Xum, a cross-modal video summarization dataset\nfeaturing 30,000 diverse videos sourced from YouTube, with lengths ranging from\n40 to 940 seconds and an average summarization ratio of 16.39\\%. Each video\nsummary in Instruct-V2Xum is paired with a textual summary that references\nspecific frame indexes, facilitating the generation of aligned video and\ntextual summaries. In addition, we propose a new video summarization framework\nnamed V2Xum-LLM. V2Xum-LLM, specifically V2Xum-LLaMA in this study, is the\nfirst framework that unifies different video summarization tasks into one large\nlanguage model's (LLM) text decoder and achieves task-controllable video\nsummarization with temporal prompts and task instructions. Experiments show\nthat V2Xum-LLaMA outperforms strong baseline models on multiple video\nsummarization tasks. Furthermore, we propose an enhanced evaluation metric for\nV2V and V2VT summarization tasks.\n","authors":["Hang Hua","Yunlong Tang","Chenliang Xu","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2404.12353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12352v1","updated":"2024-04-18T17:32:32Z","published":"2024-04-18T17:32:32Z","title":"Point-In-Context: Understanding Point Cloud via In-Context Learning","summary":"  With the emergence of large-scale models trained on diverse datasets,\nin-context learning has emerged as a promising paradigm for multitasking,\nnotably in natural language processing and image processing. However, its\napplication in 3D point cloud tasks remains largely unexplored. In this work,\nwe introduce Point-In-Context (PIC), a novel framework for 3D point cloud\nunderstanding via in-context learning. We address the technical challenge of\neffectively extending masked point modeling to 3D point clouds by introducing a\nJoint Sampling module and proposing a vanilla version of PIC called\nPoint-In-Context-Generalist (PIC-G). PIC-G is designed as a generalist model\nfor various 3D point cloud tasks, with inputs and outputs modeled as\ncoordinates. In this paradigm, the challenging segmentation task is achieved by\nassigning label points with XYZ coordinates for each category; the final\nprediction is then chosen based on the label point closest to the predictions.\nTo break the limitation by the fixed label-coordinate assignment, which has\npoor generalization upon novel classes, we propose two novel training\nstrategies, In-Context Labeling and In-Context Enhancing, forming an extended\nversion of PIC named Point-In-Context-Segmenter (PIC-S), targeting improving\ndynamic context labeling and model training. By utilizing dynamic in-context\nlabels and extra in-context pairs, PIC-S achieves enhanced performance and\ngeneralization capability in and across part segmentation datasets. PIC is a\ngeneral framework so that other tasks or datasets can be seamlessly introduced\ninto our PIC through a unified data format. We conduct extensive experiments to\nvalidate the versatility and adaptability of our proposed methods in handling a\nwide range of tasks and segmenting multi-datasets. Our PIC-S is capable of\ngeneralizing unseen datasets and performing novel part segmentation by\ncustomizing prompts.\n","authors":["Mengyuan Liu","Zhongbin Fang","Xia Li","Joachim M. Buhmann","Xiangtai Li","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2404.12352v1.pdf","comment":"Project page: https://fanglaosi.github.io/Point-In-Context_Pages.\n  arXiv admin note: text overlap with arXiv:2306.08659"},{"id":"http://arxiv.org/abs/2404.08995v2","updated":"2024-04-18T17:26:30Z","published":"2024-04-13T12:41:40Z","title":"Beyond Known Clusters: Probe New Prototypes for Efficient Generalized\n  Class Discovery","summary":"  Generalized Class Discovery (GCD) aims to dynamically assign labels to\nunlabelled data partially based on knowledge learned from labelled data, where\nthe unlabelled data may come from known or novel classes. The prevailing\napproach generally involves clustering across all data and learning conceptions\nby prototypical contrastive learning. However, existing methods largely hinge\non the performance of clustering algorithms and are thus subject to their\ninherent limitations. Firstly, the estimated cluster number is often smaller\nthan the ground truth, making the existing methods suffer from the lack of\nprototypes for comprehensive conception learning. To address this issue, we\npropose an adaptive probing mechanism that introduces learnable potential\nprototypes to expand cluster prototypes (centers). As there is no ground truth\nfor the potential prototype, we develop a self-supervised prototype learning\nframework to optimize the potential prototype in an end-to-end fashion.\nSecondly, clustering is computationally intensive, and the conventional\nstrategy of clustering both labelled and unlabelled instances exacerbates this\nissue. To counteract this inefficiency, we opt to cluster only the unlabelled\ninstances and subsequently expand the cluster prototypes with our introduced\npotential prototypes to fast explore novel classes. Despite the simplicity of\nour proposed method, extensive empirical analysis on a wide range of datasets\nconfirms that our method consistently delivers state-of-the-art results.\nSpecifically, our method surpasses the nearest competitor by a significant\nmargin of \\textbf{9.7}$\\%$ within the Stanford Cars dataset and\n\\textbf{12$\\times$} clustering efficiency within the Herbarium 19 dataset. We\nwill make the code and checkpoints publicly available at\n\\url{https://github.com/xjtuYW/PNP.git}.\n","authors":["Ye Wang","Yaxiong Wang","Yujiao Wu","Bingchen Zhao","Xueming Qian"],"pdf_url":"https://arxiv.org/pdf/2404.08995v2.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2404.12347v1","updated":"2024-04-18T17:24:28Z","published":"2024-04-18T17:24:28Z","title":"AniClipart: Clipart Animation with Text-to-Video Priors","summary":"  Clipart, a pre-made graphic art form, offers a convenient and efficient way\nof illustrating visual content. Traditional workflows to convert static clipart\nimages into motion sequences are laborious and time-consuming, involving\nnumerous intricate steps like rigging, key animation and in-betweening. Recent\nadvancements in text-to-video generation hold great potential in resolving this\nproblem. Nevertheless, direct application of text-to-video generation models\noften struggles to retain the visual identity of clipart images or generate\ncartoon-style motions, resulting in unsatisfactory animation outcomes. In this\npaper, we introduce AniClipart, a system that transforms static clipart images\ninto high-quality motion sequences guided by text-to-video priors. To generate\ncartoon-style and smooth motion, we first define B\\'{e}zier curves over\nkeypoints of the clipart image as a form of motion regularization. We then\nalign the motion trajectories of the keypoints with the provided text prompt by\noptimizing the Video Score Distillation Sampling (VSDS) loss, which encodes\nadequate knowledge of natural motion within a pretrained text-to-video\ndiffusion model. With a differentiable As-Rigid-As-Possible shape deformation\nalgorithm, our method can be end-to-end optimized while maintaining deformation\nrigidity. Experimental results show that the proposed AniClipart consistently\noutperforms existing image-to-video generation models, in terms of text-video\nalignment, visual identity preservation, and motion consistency. Furthermore,\nwe showcase the versatility of AniClipart by adapting it to generate a broader\narray of animation formats, such as layered animation, which allows topological\nchanges.\n","authors":["Ronghuan Wu","Wanchao Su","Kede Ma","Jing Liao"],"pdf_url":"https://arxiv.org/pdf/2404.12347v1.pdf","comment":"Project Page: https://aniclipart.github.io/"},{"id":"http://arxiv.org/abs/2404.12341v1","updated":"2024-04-18T17:10:18Z","published":"2024-04-18T17:10:18Z","title":"Measuring Feature Dependency of Neural Networks by Collapsing Feature\n  Dimensions in the Data Manifold","summary":"  This paper introduces a new technique to measure the feature dependency of\nneural network models. The motivation is to better understand a model by\nquerying whether it is using information from human-understandable features,\ne.g., anatomical shape, volume, or image texture. Our method is based on the\nprinciple that if a model is dependent on a feature, then removal of that\nfeature should significantly harm its performance. A targeted feature is\n\"removed\" by collapsing the dimension in the data distribution that corresponds\nto that feature. We perform this by moving data points along the feature\ndimension to a baseline feature value while staying on the data manifold, as\nestimated by a deep generative model. Then we observe how the model's\nperformance changes on the modified test data set, with the target feature\ndimension removed. We test our method on deep neural network models trained on\nsynthetic image data with known ground truth, an Alzheimer's disease prediction\ntask using MRI and hippocampus segmentations from the OASIS-3 dataset, and a\ncell nuclei classification task using the Lizard dataset.\n","authors":["Yinzhu Jin","Matthew B. Dwyer","P. Thomas Fletcher"],"pdf_url":"https://arxiv.org/pdf/2404.12341v1.pdf","comment":"Accepted and will be pulished in International Symposium on\n  Biomedical Imaging (ISBI) 2024"},{"id":"http://arxiv.org/abs/2404.12339v1","updated":"2024-04-18T17:09:10Z","published":"2024-04-18T17:09:10Z","title":"SPOT: Point Cloud Based Stereo Visual Place Recognition for Similar and\n  Opposing Viewpoints","summary":"  Recognizing places from an opposing viewpoint during a return trip is a\ncommon experience for human drivers. However, the analogous robotics\ncapability, visual place recognition (VPR) with limited field of view cameras\nunder 180 degree rotations, has proven to be challenging to achieve. To address\nthis problem, this paper presents Same Place Opposing Trajectory (SPOT), a\ntechnique for opposing viewpoint VPR that relies exclusively on structure\nestimated through stereo visual odometry (VO). The method extends recent\nadvances in lidar descriptors and utilizes a novel double (similar and\nopposing) distance matrix sequence matching method. We evaluate SPOT on a\npublicly available dataset with 6.7-7.6 km routes driven in similar and\nopposing directions under various lighting conditions. The proposed algorithm\ndemonstrates remarkable improvement over the state-of-the-art, achieving up to\n91.7% recall at 100% precision in opposing viewpoint cases, while requiring\nless storage than all baselines tested and running faster than all but one.\nMoreover, the proposed method assumes no a priori knowledge of whether the\nviewpoint is similar or opposing, and also demonstrates competitive performance\nin similar viewpoint cases.\n","authors":["Spencer Carmichael","Rahul Agrawal","Ram Vasudevan","Katherine A. Skinner"],"pdf_url":"https://arxiv.org/pdf/2404.12339v1.pdf","comment":"Accepted to ICRA 2024, project website:\n  https://umautobots.github.io/spot"},{"id":"http://arxiv.org/abs/2309.16208v2","updated":"2024-04-18T17:08:53Z","published":"2023-09-28T07:17:44Z","title":"Low-rank tensor completion via tensor joint rank with logarithmic\n  composite norm","summary":"  Low-rank tensor completion (LRTC) aims to recover a complete low-rank tensor\nfrom incomplete observed tensor, attracting extensive attention in various\npractical applications such as image processing and computer vision. However,\ncurrent methods often perform well only when there is a sufficient of observed\ninformation, and they perform poorly or may fail when the observed information\nis less than 5\\%. In order to improve the utilization of observed information,\na new method called the tensor joint rank with logarithmic composite norm\n(TJLC) method is proposed. This method simultaneously exploits two types of\ntensor low-rank structures, namely tensor Tucker rank and tubal rank, thereby\nenhancing the inherent correlations between known and missing elements. To\naddress the challenge of applying two tensor ranks with significantly different\ndirectly to LRTC, a new tensor Logarithmic composite norm is further proposed.\nSubsequently, the TJLC model and algorithm for the LRTC problem are proposed.\nAdditionally, theoretical convergence guarantees for the TJLC method are\nprovided. Experiments on various real datasets demonstrate that the proposed\nmethod outperforms state-of-the-art methods significantly. Particularly, the\nproposed method achieves satisfactory recovery even when the observed\ninformation is as low as 1\\%, and the recovery performance improves\nsignificantly as the observed information increases.\n","authors":["Hongbing Zhang"],"pdf_url":"https://arxiv.org/pdf/2309.16208v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12333v1","updated":"2024-04-18T16:59:51Z","published":"2024-04-18T16:59:51Z","title":"Customizing Text-to-Image Diffusion with Camera Viewpoint Control","summary":"  Model customization introduces new concepts to existing text-to-image models,\nenabling the generation of the new concept in novel contexts. However, such\nmethods lack accurate camera view control w.r.t the object, and users must\nresort to prompt engineering (e.g., adding \"top-view\") to achieve coarse view\ncontrol. In this work, we introduce a new task -- enabling explicit control of\ncamera viewpoint for model customization. This allows us to modify object\nproperties amongst various background scenes via text prompts, all while\nincorporating the target camera pose as additional control. This new task\npresents significant challenges in merging a 3D representation from the\nmulti-view images of the new concept with a general, 2D text-to-image model. To\nbridge this gap, we propose to condition the 2D diffusion process on rendered,\nview-dependent features of the new object. During training, we jointly adapt\nthe 2D diffusion modules and 3D feature predictions to reconstruct the object's\nappearance and geometry while reducing overfitting to the input multi-view\nimages. Our method outperforms existing image editing and model personalization\nbaselines in preserving the custom object's identity while following the input\ntext prompt and the object's camera pose.\n","authors":["Nupur Kumari","Grace Su","Richard Zhang","Taesung Park","Eli Shechtman","Jun-Yan Zhu"],"pdf_url":"https://arxiv.org/pdf/2404.12333v1.pdf","comment":"project page: https://customdiffusion360.github.io"},{"id":"http://arxiv.org/abs/2404.12330v1","updated":"2024-04-18T16:58:05Z","published":"2024-04-18T16:58:05Z","title":"A Perspective on Deep Vision Performance with Standard Image and Video\n  Codecs","summary":"  Resource-constrained hardware, such as edge devices or cell phones, often\nrely on cloud servers to provide the required computational resources for\ninference in deep vision models. However, transferring image and video data\nfrom an edge or mobile device to a cloud server requires coding to deal with\nnetwork constraints. The use of standardized codecs, such as JPEG or H.264, is\nprevalent and required to ensure interoperability. This paper aims to examine\nthe implications of employing standardized codecs within deep vision pipelines.\nWe find that using JPEG and H.264 coding significantly deteriorates the\naccuracy across a broad range of vision tasks and models. For instance, strong\ncompression rates reduce semantic segmentation accuracy by more than 80% in\nmIoU. In contrast to previous findings, our analysis extends beyond image and\naction classification to localization and dense prediction tasks, thus\nproviding a more comprehensive perspective.\n","authors":["Christoph Reich","Oliver Hahn","Daniel Cremers","Stefan Roth","Biplob Debnath"],"pdf_url":"https://arxiv.org/pdf/2404.12330v1.pdf","comment":"Accepted at CVPR 2024 Workshop on AI for Streaming (AIS)"},{"id":"http://arxiv.org/abs/2404.12322v1","updated":"2024-04-18T16:53:08Z","published":"2024-04-18T16:53:08Z","title":"Generalizable Face Landmarking Guided by Conditional Face Warping","summary":"  As a significant step for human face modeling, editing, and generation, face\nlandmarking aims at extracting facial keypoints from images. A generalizable\nface landmarker is required in practice because real-world facial images, e.g.,\nthe avatars in animations and games, are often stylized in various ways.\nHowever, achieving generalizable face landmarking is challenging due to the\ndiversity of facial styles and the scarcity of labeled stylized faces. In this\nstudy, we propose a simple but effective paradigm to learn a generalizable face\nlandmarker based on labeled real human faces and unlabeled stylized faces. Our\nmethod learns the face landmarker as the key module of a conditional face\nwarper. Given a pair of real and stylized facial images, the conditional face\nwarper predicts a warping field from the real face to the stylized one, in\nwhich the face landmarker predicts the ending points of the warping field and\nprovides us with high-quality pseudo landmarks for the corresponding stylized\nfacial images. Applying an alternating optimization strategy, we learn the face\nlandmarker to minimize $i)$ the discrepancy between the stylized faces and the\nwarped real ones and $ii)$ the prediction errors of both real and pseudo\nlandmarks. Experiments on various datasets show that our method outperforms\nexisting state-of-the-art domain adaptation methods in face landmarking tasks,\nleading to a face landmarker with better generalizability. Code is available at\nhttps://plustwo0.github.io/project-face-landmarker}{https://plustwo0.github.io/project-face-landmarker.\n","authors":["Jiayi Liang","Haotian Liu","Hongteng Xu","Dixin Luo"],"pdf_url":"https://arxiv.org/pdf/2404.12322v1.pdf","comment":"Accepted in CVPR 2024"},{"id":"http://arxiv.org/abs/2404.12309v1","updated":"2024-04-18T16:38:02Z","published":"2024-04-18T16:38:02Z","title":"iRAG: An Incremental Retrieval Augmented Generation System for Videos","summary":"  Retrieval augmented generation (RAG) systems combine the strengths of\nlanguage generation and information retrieval to power many real-world\napplications like chatbots. Use of RAG for combined understanding of multimodal\ndata such as text, images and videos is appealing but two critical limitations\nexist: one-time, upfront capture of all content in large multimodal data as\ntext descriptions entails high processing times, and not all information in the\nrich multimodal data is typically in the text descriptions. Since the user\nqueries are not known apriori, developing a system for multimodal to text\nconversion and interactive querying of multimodal data is challenging.\n  To address these limitations, we propose iRAG, which augments RAG with a\nnovel incremental workflow to enable interactive querying of large corpus of\nmultimodal data. Unlike traditional RAG, iRAG quickly indexes large\nrepositories of multimodal data, and in the incremental workflow, it uses the\nindex to opportunistically extract more details from select portions of the\nmultimodal data to retrieve context relevant to an interactive user query. Such\nan incremental workflow avoids long multimodal to text conversion times,\novercomes information loss issues by doing on-demand query-specific extraction\nof details in multimodal data, and ensures high quality of responses to\ninteractive user queries that are often not known apriori. To the best of our\nknowledge, iRAG is the first system to augment RAG with an incremental workflow\nto support efficient interactive querying of large, real-world multimodal data.\nExperimental results on real-world long videos demonstrate 23x to 25x faster\nvideo to text ingestion, while ensuring that quality of responses to\ninteractive user queries is comparable to responses from a traditional RAG\nwhere all video data is converted to text upfront before any querying.\n","authors":["Md Adnan Arefeen","Biplob Debnath","Md Yusuf Sarwar Uddin","Srimat Chakradhar"],"pdf_url":"https://arxiv.org/pdf/2404.12309v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12295v1","updated":"2024-04-18T16:18:41Z","published":"2024-04-18T16:18:41Z","title":"When Medical Imaging Met Self-Attention: A Love Story That Didn't Quite\n  Work Out","summary":"  A substantial body of research has focused on developing systems that assist\nmedical professionals during labor-intensive early screening processes, many\nbased on convolutional deep-learning architectures. Recently, multiple studies\nexplored the application of so-called self-attention mechanisms in the vision\ndomain. These studies often report empirical improvements over fully\nconvolutional approaches on various datasets and tasks. To evaluate this trend\nfor medical imaging, we extend two widely adopted convolutional architectures\nwith different self-attention variants on two different medical datasets. With\nthis, we aim to specifically evaluate the possible advantages of additional\nself-attention. We compare our models with similarly sized convolutional and\nattention-based baselines and evaluate performance gains statistically.\nAdditionally, we investigate how including such layers changes the features\nlearned by these models during the training. Following a hyperparameter search,\nand contrary to our expectations, we observe no significant improvement in\nbalanced accuracy over fully convolutional models. We also find that important\nfeatures, such as dermoscopic structures in skin lesion images, are still not\nlearned by employing self-attention. Finally, analyzing local explanations, we\nconfirm biased feature usage. We conclude that merely incorporating attention\nis insufficient to surpass the performance of existing fully convolutional\nmethods.\n","authors":["Tristan Piater","Niklas Penzel","Gideon Stein","Joachim Denzler"],"pdf_url":"https://arxiv.org/pdf/2404.12295v1.pdf","comment":"10 pages, 2 figures, 5 tables, presented at VISAPP 2024"},{"id":"http://arxiv.org/abs/2404.12292v1","updated":"2024-04-18T16:12:38Z","published":"2024-04-18T16:12:38Z","title":"Reducing Bias in Pre-trained Models by Tuning while Penalizing Change","summary":"  Deep models trained on large amounts of data often incorporate implicit\nbiases present during training time. If later such a bias is discovered during\ninference or deployment, it is often necessary to acquire new data and retrain\nthe model. This behavior is especially problematic in critical areas such as\nautonomous driving or medical decision-making. In these scenarios, new data is\noften expensive and hard to come by. In this work, we present a method based on\nchange penalization that takes a pre-trained model and adapts the weights to\nmitigate a previously detected bias. We achieve this by tuning a\nzero-initialized copy of a frozen pre-trained network. Our method needs very\nfew, in extreme cases only a single, examples that contradict the bias to\nincrease performance. Additionally, we propose an early stopping criterion to\nmodify baselines and reduce overfitting. We evaluate our approach on a\nwell-known bias in skin lesion classification and three other datasets from the\ndomain shift literature. We find that our approach works especially well with\nvery few images. Simple fine-tuning combined with our early stopping also leads\nto performance benefits for a larger number of tuning samples.\n","authors":["Niklas Penzel","Gideon Stein","Joachim Denzler"],"pdf_url":"https://arxiv.org/pdf/2404.12292v1.pdf","comment":"12 pages, 12 figures, presented at VISAPP 2024"},{"id":"http://arxiv.org/abs/2404.12285v1","updated":"2024-04-18T16:04:14Z","published":"2024-04-18T16:04:14Z","title":"Performance Evaluation of Segment Anything Model with Variational\n  Prompting for Application to Non-Visible Spectrum Imagery","summary":"  The Segment Anything Model (SAM) is a deep neural network foundational model\ndesigned to perform instance segmentation which has gained significant\npopularity given its zero-shot segmentation ability. SAM operates by generating\nmasks based on various input prompts such as text, bounding boxes, points, or\nmasks, introducing a novel methodology to overcome the constraints posed by\ndataset-specific scarcity. While SAM is trained on an extensive dataset,\ncomprising ~11M images, it mostly consists of natural photographic images with\nonly very limited images from other modalities. Whilst the rapid progress in\nvisual infrared surveillance and X-ray security screening imaging technologies,\ndriven forward by advances in deep learning, has significantly enhanced the\nability to detect, classify and segment objects with high accuracy, it is not\nevident if the SAM zero-shot capabilities can be transferred to such\nmodalities. This work assesses SAM capabilities in segmenting objects of\ninterest in the X-ray/infrared modalities. Our approach reuses the pre-trained\nSAM with three different prompts: bounding box, centroid and random points. We\npresent quantitative/qualitative results to showcase the performance on\nselected datasets. Our results show that SAM can segment objects in the X-ray\nmodality when given a box prompt, but its performance varies for point prompts.\nSpecifically, SAM performs poorly in segmenting slender objects and organic\nmaterials, such as plastic bottles. We find that infrared objects are also\nchallenging to segment with point prompts given the low-contrast nature of this\nmodality. This study shows that while SAM demonstrates outstanding zero-shot\ncapabilities with box prompts, its performance ranges from moderate to poor for\npoint prompts, indicating that special consideration on the cross-modal\ngeneralisation of SAM is needed when considering use on X-ray/infrared imagery.\n","authors":["Yona Falinie A. Gaus","Neelanjan Bhowmik","Brian K. S. Isaac-Medina","Toby P. Breckon"],"pdf_url":"https://arxiv.org/pdf/2404.12285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08273v2","updated":"2024-04-18T15:55:56Z","published":"2024-04-12T06:52:40Z","title":"Struggle with Adversarial Defense? Try Diffusion","summary":"  Adversarial attacks induce misclassification by introducing subtle\nperturbations. Recently, diffusion models are applied to the image classifiers\nto improve adversarial robustness through adversarial training or by purifying\nadversarial noise. However, diffusion-based adversarial training often\nencounters convergence challenges and high computational expenses.\nAdditionally, diffusion-based purification inevitably causes data shift and is\ndeemed susceptible to stronger adaptive attacks. To tackle these issues, we\npropose the Truth Maximization Diffusion Classifier (TMDC), a generative\nBayesian classifier that builds upon pre-trained diffusion models and the\nBayesian theorem. Unlike data-driven classifiers, TMDC, guided by Bayesian\nprinciples, utilizes the conditional likelihood from diffusion models to\ndetermine the class probabilities of input images, thereby insulating against\nthe influences of data shift and the limitations of adversarial training.\nMoreover, to enhance TMDC's resilience against more potent adversarial attacks,\nwe propose an optimization strategy for diffusion classifiers. This strategy\ninvolves post-training the diffusion model on perturbed datasets with\nground-truth labels as conditions, guiding the diffusion model to learn the\ndata distribution and maximizing the likelihood under the ground-truth labels.\nThe proposed method achieves state-of-the-art performance on the CIFAR10\ndataset against heavy white-box attacks and strong adaptive attacks.\nSpecifically, TMDC achieves robust accuracies of 82.81% against $l_{\\infty}$\nnorm-bounded perturbations and 86.05% against $l_{2}$ norm-bounded\nperturbations, respectively, with $\\epsilon=0.05$.\n","authors":["Yujie Li","Yanbin Wang","Haitao Xu","Bin Liu","Jianguo Sun","Zhenhao Guo","Wenrui Ma"],"pdf_url":"https://arxiv.org/pdf/2404.08273v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.04517v4","updated":"2024-04-18T15:50:37Z","published":"2023-01-11T15:31:15Z","title":"A new dataset for measuring the performance of blood vessel segmentation\n  methods under distribution shifts","summary":"  Creating a dataset for training supervised machine learning algorithms can be\na demanding task. This is especially true for medical image segmentation since\none or more specialists are usually required for image annotation, and creating\nground truth labels for just a single image can take up to several hours. In\naddition, it is paramount that the annotated samples represent well the\ndifferent conditions that might affect the imaged tissues as well as possible\nchanges in the image acquisition process. This can only be achieved by\nconsidering samples that are typical in the dataset as well as atypical, or\neven outlier, samples. We introduce VessMAP, a heterogeneous blood vessel\nsegmentation dataset acquired by carefully sampling relevant images from a\nlarger non-annotated dataset. A methodology was developed to select both\nprototypical and atypical samples from the base dataset, thus defining an\nassorted set of images that can be used for measuring the performance of\nsegmentation algorithms on samples that are highly distinct from each other. To\ndemonstrate the potential of the new dataset, we show that the validation\nperformance of a neural network changes significantly depending on the splits\nused for training the network.\n","authors":["Matheus Viana da Silva","Nat√°lia de Carvalho Santos","Julie Ouellette","Baptiste Lacoste","Cesar Henrique Comin"],"pdf_url":"https://arxiv.org/pdf/2301.04517v4.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2310.08475v5","updated":"2024-04-18T15:46:22Z","published":"2023-10-12T16:32:44Z","title":"Can We Edit Multimodal Large Language Models?","summary":"  In this paper, we focus on editing Multimodal Large Language Models (MLLMs).\nCompared to editing single-modal LLMs, multimodal model editing is more\nchallenging, which demands a higher level of scrutiny and careful consideration\nin the editing process. To facilitate research in this area, we construct a new\nbenchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite\nof innovative metrics for evaluation. We conduct comprehensive experiments\ninvolving various model editing baselines and analyze the impact of editing\ndifferent components for multimodal LLMs. Empirically, we notice that previous\nbaselines can implement editing multimodal LLMs to some extent, but the effect\nis still barely satisfactory, indicating the potential difficulty of this task.\nWe hope that our work can provide the NLP community with insights. Code and\ndataset are available in https://github.com/zjunlp/EasyEdit.\n","authors":["Siyuan Cheng","Bozhong Tian","Qingbin Liu","Xi Chen","Yongheng Wang","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.08475v5.pdf","comment":"EMNLP 2023. Add the Exact Match/Accuracy results of Reliability and\n  T-Generality"},{"id":"http://arxiv.org/abs/2309.16388v2","updated":"2024-04-18T15:32:30Z","published":"2023-09-28T12:36:12Z","title":"Exposing Image Splicing Traces in Scientific Publications via\n  Uncertainty-guided Refinement","summary":"  Recently, a surge in scientific publications suspected of image manipulation\nhas led to numerous retractions, bringing the issue of image integrity into\nsharp focus. Although research on forensic detectors for image plagiarism and\nimage synthesis exists, the detection of image splicing traces in scientific\npublications remains unexplored. Compared to image duplication and synthesis,\nimage splicing detection is more challenging due to the lack of reference\nimages and the typically small tampered areas. Furthermore, disruptive factors\nin scientific images, such as artifacts from digital compression, abnormal\npatterns, and noise from physical operations, present misleading features like\nsplicing traces, significantly increasing the difficulty of this task.\nMoreover, the scarcity of high-quality datasets of spliced scientific images\nlimits potential advancements. In this work, we propose an Uncertainty-guided\nRefinement Network (URN) to mitigate the impact of these disruptive factors.\nOur URN can explicitly suppress the propagation of unreliable information flow\ncaused by disruptive factors between regions, thus obtaining robust splicing\nfeatures. Additionally, the URN is designed to concentrate improvements in\nuncertain prediction areas during the decoding phase. We also construct a\ndataset for image splicing detection (SciSp) containing 1,290 spliced images.\nCompared to existing datasets, SciSp includes the largest number of spliced\nimages and the most diverse sources. Comprehensive experiments conducted on\nthree benchmark datasets demonstrate the superiority of our approach. We also\nvalidate the URN's generalisability in resisting cross-dataset domain shifts\nand its robustness against various post-processing techniques, including\nadvanced deep-learning-based inpainting.\n","authors":["Xun Lin","Wenzhong Tang","Haoran Wang","Yizhong Liu","Yakun Ju","Shuai Wang","Zitong Yu"],"pdf_url":"https://arxiv.org/pdf/2309.16388v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15584v3","updated":"2024-04-18T15:29:14Z","published":"2024-02-23T19:51:55Z","title":"State Space Models for Event Cameras","summary":"  Today, state-of-the-art deep neural networks that process event-camera data\nfirst convert a temporal window of events into dense, grid-like input\nrepresentations. As such, they exhibit poor generalizability when deployed at\nhigher inference frequencies (i.e., smaller temporal windows) than the ones\nthey were trained on. We address this challenge by introducing state-space\nmodels (SSMs) with learnable timescale parameters to event-based vision. This\ndesign adapts to varying frequencies without the need to retrain the network at\ndifferent frequencies. Additionally, we investigate two strategies to\ncounteract aliasing effects when deploying the model at higher frequencies. We\ncomprehensively evaluate our approach against existing methods based on RNN and\nTransformer architectures across various benchmarks, including Gen1 and 1 Mpx\nevent camera datasets. Our results demonstrate that SSM-based models train 33%\nfaster and also exhibit minimal performance degradation when tested at higher\nfrequencies than the training input. Traditional RNN and Transformer models\nexhibit performance drops of more than 20 mAP, with SSMs having a drop of 3.76\nmAP, highlighting the effectiveness of SSMs in event-based vision tasks.\n","authors":["Nikola Zubiƒá","Mathias Gehrig","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2402.15584v3.pdf","comment":"18 pages, 5 figures, 6 tables, CVPR 2024 Camera Ready paper"},{"id":"http://arxiv.org/abs/2404.12260v1","updated":"2024-04-18T15:28:34Z","published":"2024-04-18T15:28:34Z","title":"Alleviating Catastrophic Forgetting in Facial Expression Recognition\n  with Emotion-Centered Models","summary":"  Facial expression recognition is a pivotal component in machine learning,\nfacilitating various applications. However, convolutional neural networks\n(CNNs) are often plagued by catastrophic forgetting, impeding their\nadaptability. The proposed method, emotion-centered generative replay (ECgr),\ntackles this challenge by integrating synthetic images from generative\nadversarial networks. Moreover, ECgr incorporates a quality assurance algorithm\nto ensure the fidelity of generated images. This dual approach enables CNNs to\nretain past knowledge while learning new tasks, enhancing their performance in\nemotion recognition. The experimental results on four diverse facial expression\ndatasets demonstrate that incorporating images generated by our\npseudo-rehearsal method enhances training on the targeted dataset and the\nsource dataset while making the CNN retain previously learned knowledge.\n","authors":["Israel A. Laurensi","Alceu de Souza Britto Jr.","Jean Paul Barddal","Alessandro Lameiras Koerich"],"pdf_url":"https://arxiv.org/pdf/2404.12260v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2404.12258v1","updated":"2024-04-18T15:25:59Z","published":"2024-04-18T15:25:59Z","title":"DeepLocalization: Using change point detection for Temporal Action\n  Localization","summary":"  In this study, we introduce DeepLocalization, an innovative framework devised\nfor the real-time localization of actions tailored explicitly for monitoring\ndriver behavior. Utilizing the power of advanced deep learning methodologies,\nour objective is to tackle the critical issue of distracted driving-a\nsignificant factor contributing to road accidents. Our strategy employs a dual\napproach: leveraging Graph-Based Change-Point Detection for pinpointing actions\nin time alongside a Video Large Language Model (Video-LLM) for precisely\ncategorizing activities. Through careful prompt engineering, we customize the\nVideo-LLM to adeptly handle driving activities' nuances, ensuring its\nclassification efficacy even with sparse data. Engineered to be lightweight,\nour framework is optimized for consumer-grade GPUs, making it vastly applicable\nin practical scenarios. We subjected our method to rigorous testing on the\nSynDD2 dataset, a complex benchmark for distracted driving behaviors, where it\ndemonstrated commendable performance-achieving 57.5% accuracy in event\nclassification and 51% in event detection. These outcomes underscore the\nsubstantial promise of DeepLocalization in accurately identifying diverse\ndriver behaviors and their temporal occurrences, all within the bounds of\nlimited computational resources.\n","authors":["Mohammed Shaiqur Rahman","Ibne Farabi Shihab","Lynna Chu","Anuj Sharma"],"pdf_url":"https://arxiv.org/pdf/2404.12258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12257v1","updated":"2024-04-18T15:23:37Z","published":"2024-04-18T15:23:37Z","title":"Food Portion Estimation via 3D Object Scaling","summary":"  Image-based methods to analyze food images have alleviated the user burden\nand biases associated with traditional methods. However, accurate portion\nestimation remains a major challenge due to the loss of 3D information in the\n2D representation of foods captured by smartphone cameras or wearable devices.\nIn this paper, we propose a new framework to estimate both food volume and\nenergy from 2D images by leveraging the power of 3D food models and physical\nreference in the eating scene. Our method estimates the pose of the camera and\nthe food object in the input image and recreates the eating occasion by\nrendering an image of a 3D model of the food with the estimated poses. We also\nintroduce a new dataset, SimpleFood45, which contains 2D images of 45 food\nitems and associated annotations including food volume, weight, and energy. Our\nmethod achieves an average error of 31.10 kCal (17.67%) on this dataset,\noutperforming existing portion estimation methods.\n","authors":["Gautham Vinod","Jiangpeng He","Zeman Shao","Fengqing Zhu"],"pdf_url":"https://arxiv.org/pdf/2404.12257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12252v1","updated":"2024-04-18T15:20:59Z","published":"2024-04-18T15:20:59Z","title":"Deep Gaussian mixture model for unsupervised image segmentation","summary":"  The recent emergence of deep learning has led to a great deal of work on\ndesigning supervised deep semantic segmentation algorithms. As in many tasks\nsufficient pixel-level labels are very difficult to obtain, we propose a method\nwhich combines a Gaussian mixture model (GMM) with unsupervised deep learning\ntechniques. In the standard GMM the pixel values with each sub-region are\nmodelled by a Gaussian distribution. In order to identify the different\nregions, the parameter vector that minimizes the negative log-likelihood (NLL)\nfunction regarding the GMM has to be approximated. For this task, usually\niterative optimization methods such as the expectation-maximization (EM)\nalgorithm are used. In this paper, we propose to estimate these parameters\ndirectly from the image using a convolutional neural network (CNN). We thus\nchange the iterative procedure in the EM algorithm replacing the\nexpectation-step by a gradient-step with regard to the networks parameters.\nThis means that the network is trained to minimize the NLL function of the GMM\nwhich comes with at least two advantages. As once trained, the network is able\nto predict label probabilities very quickly compared with time consuming\niterative optimization methods. Secondly, due to the deep image prior our\nmethod is able to partially overcome one of the main disadvantages of GMM,\nwhich is not taking into account correlation between neighboring pixels, as it\nassumes independence between them. We demonstrate the advantages of our method\nin various experiments on the example of myocardial infarct segmentation on\nmulti-sequence MRI images.\n","authors":["Matthias Schwab","Agnes Mayr","Markus Haltmeier"],"pdf_url":"https://arxiv.org/pdf/2404.12252v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12251v1","updated":"2024-04-18T15:18:14Z","published":"2024-04-18T15:18:14Z","title":"Dynamic Modality and View Selection for Multimodal Emotion Recognition\n  with Missing Modalities","summary":"  The study of human emotions, traditionally a cornerstone in fields like\npsychology and neuroscience, has been profoundly impacted by the advent of\nartificial intelligence (AI). Multiple channels, such as speech (voice) and\nfacial expressions (image), are crucial in understanding human emotions.\nHowever, AI's journey in multimodal emotion recognition (MER) is marked by\nsubstantial technical challenges. One significant hurdle is how AI models\nmanage the absence of a particular modality - a frequent occurrence in\nreal-world situations. This study's central focus is assessing the performance\nand resilience of two strategies when confronted with the lack of one modality:\na novel multimodal dynamic modality and view selection and a cross-attention\nmechanism. Results on the RECOLA dataset show that dynamic selection-based\nmethods are a promising approach for MER. In the missing modalities scenarios,\nall dynamic selection-based methods outperformed the baseline. The study\nconcludes by emphasizing the intricate interplay between audio and video\nmodalities in emotion prediction, showcasing the adaptability of dynamic\nselection methods in handling missing modalities.\n","authors":["Luciana Trinkaus Menon","Luiz Carlos Ribeiro Neduziak","Jean Paul Barddal","Alessandro Lameiras Koerich","Alceu de Souza Britto Jr"],"pdf_url":"https://arxiv.org/pdf/2404.12251v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2404.12246v1","updated":"2024-04-18T15:11:02Z","published":"2024-04-18T15:11:02Z","title":"Blind Localization and Clustering of Anomalies in Textures","summary":"  Anomaly detection and localization in images is a growing field in computer\nvision. In this area, a seemingly understudied problem is anomaly clustering,\ni.e., identifying and grouping different types of anomalies in a fully\nunsupervised manner. In this work, we propose a novel method for clustering\nanomalies in largely stationary images (textures) in a blind setting. That is,\nthe input consists of normal and anomalous images without distinction and\nwithout labels. What contributes to the difficulty of the task is that\nanomalous regions are often small and may present only subtle changes in\nappearance, which can be easily overshadowed by the genuine variance in the\ntexture. Moreover, each anomaly type may have a complex appearance\ndistribution. We introduce a novel scheme for solving this task using a\ncombination of blind anomaly localization and contrastive learning. By\nidentifying the anomalous regions with high fidelity, we can restrict our focus\nto those regions of interest; then, contrastive learning is employed to\nincrease the separability of different anomaly types and reduce the intra-class\nvariation. Our experiments show that the proposed solution yields significantly\nbetter results compared to prior work, setting a new state of the art. Project\npage: https://reality.tf.fau.de/pub/ardelean2024blind.html.\n","authors":["Andrei-Timotei Ardelean","Tim Weyrich"],"pdf_url":"https://arxiv.org/pdf/2404.12246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.11474v4","updated":"2024-04-18T15:10:47Z","published":"2023-05-19T06:55:04Z","title":"Reciprocal Attention Mixing Transformer for Lightweight Image\n  Restoration","summary":"  Although many recent works have made advancements in the image restoration\n(IR) field, they often suffer from an excessive number of parameters. Another\nissue is that most Transformer-based IR methods focus only on either local or\nglobal features, leading to limited receptive fields or deficient parameter\nissues. To address these problems, we propose a lightweight IR network,\nReciprocal Attention Mixing Transformer (RAMiT). It employs our proposed\ndimensional reciprocal attention mixing Transformer (D-RAMiT) blocks, which\ncompute bi-dimensional (spatial and channel) self-attentions in parallel with\ndifferent numbers of multi-heads. The bi-dimensional attentions help each other\nto complement their counterpart's drawbacks and are then mixed. Additionally,\nwe introduce a hierarchical reciprocal attention mixing (H-RAMi) layer that\ncompensates for pixel-level information losses and utilizes semantic\ninformation while maintaining an efficient hierarchical structure. Furthermore,\nwe revisit and modify MobileNet V1 and V2 to attach efficient convolutions to\nour proposed components. The experimental results demonstrate that RAMiT\nachieves state-of-the-art performance on multiple lightweight IR tasks,\nincluding super-resolution, color denoising, grayscale denoising, low-light\nenhancement, and deraining. Codes are available at\nhttps://github.com/rami0205/RAMiT.\n","authors":["Haram Choi","Cheolwoong Na","Jihyeon Oh","Seungjae Lee","Jinseop Kim","Subeen Choe","Jeongmin Lee","Taehoon Kim","Jihoon Yang"],"pdf_url":"https://arxiv.org/pdf/2305.11474v4.pdf","comment":"CVPR 2024 Workshop - NTIRE. Codes are available at\n  https://github.com/rami0205/RAMiT"},{"id":"http://arxiv.org/abs/2404.09683v2","updated":"2024-04-18T14:51:55Z","published":"2024-04-15T11:36:31Z","title":"Post-Training Network Compression for 3D Medical Image Segmentation:\n  Reducing Computational Efforts via Tucker Decomposition","summary":"  We address the computational barrier of deploying advanced deep learning\nsegmentation models in clinical settings by studying the efficacy of network\ncompression through tensor decomposition. We propose a post-training Tucker\nfactorization that enables the decomposition of pre-existing models to reduce\ncomputational requirements without impeding segmentation accuracy. We applied\nTucker decomposition to the convolutional kernels of the TotalSegmentator (TS)\nmodel, an nnU-Net model trained on a comprehensive dataset for automatic\nsegmentation of 117 anatomical structures. Our approach reduced the\nfloating-point operations (FLOPs) and memory required during inference,\noffering an adjustable trade-off between computational efficiency and\nsegmentation quality. This study utilized the publicly available TS dataset,\nemploying various downsampling factors to explore the relationship between\nmodel size, inference speed, and segmentation performance. The application of\nTucker decomposition to the TS model substantially reduced the model parameters\nand FLOPs across various compression rates, with limited loss in segmentation\naccuracy. We removed up to 88% of the model's parameters with no significant\nperformance changes in the majority of classes after fine-tuning. Practical\nbenefits varied across different graphics processing unit (GPU) architectures,\nwith more distinct speed-ups on less powerful hardware. Post-hoc network\ncompression via Tucker decomposition presents a viable strategy for reducing\nthe computational demand of medical image segmentation models without\nsubstantially sacrificing accuracy. This approach enables the broader adoption\nof advanced deep learning technologies in clinical practice, offering a way to\nnavigate the constraints of hardware capabilities.\n","authors":["Tobias Weber","Jakob Dexl","David R√ºgamer","Michael Ingrisch"],"pdf_url":"https://arxiv.org/pdf/2404.09683v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12235v1","updated":"2024-04-18T14:51:42Z","published":"2024-04-18T14:51:42Z","title":"Beyond Average: Individualized Visual Scanpath Prediction","summary":"  Understanding how attention varies across individuals has significant\nscientific and societal impacts. However, existing visual scanpath models treat\nattention uniformly, neglecting individual differences. To bridge this gap,\nthis paper focuses on individualized scanpath prediction (ISP), a new attention\nmodeling task that aims to accurately predict how different individuals shift\ntheir attention in diverse visual tasks. It proposes an ISP method featuring\nthree novel technical components: (1) an observer encoder to characterize and\nintegrate an observer's unique attention traits, (2) an observer-centric\nfeature integration approach that holistically combines visual features, task\nguidance, and observer-specific characteristics, and (3) an adaptive fixation\nprioritization mechanism that refines scanpath predictions by dynamically\nprioritizing semantic feature maps based on individual observers' attention\ntraits. These novel components allow scanpath models to effectively address the\nattention variations across different observers. Our method is generally\napplicable to different datasets, model architectures, and visual tasks,\noffering a comprehensive tool for transforming general scanpath models into\nindividualized ones. Comprehensive evaluations using value-based and\nranking-based metrics verify the method's effectiveness and generalizability.\n","authors":["Xianyu Chen","Ming Jiang","Qi Zhao"],"pdf_url":"https://arxiv.org/pdf/2404.12235v1.pdf","comment":"To appear in CVPR2024"},{"id":"http://arxiv.org/abs/2404.12216v1","updated":"2024-04-18T14:20:30Z","published":"2024-04-18T14:20:30Z","title":"ProTA: Probabilistic Token Aggregation for Text-Video Retrieval","summary":"  Text-video retrieval aims to find the most relevant cross-modal samples for a\ngiven query. Recent methods focus on modeling the whole spatial-temporal\nrelations. However, since video clips contain more diverse content than\ncaptions, the model aligning these asymmetric video-text pairs has a high risk\nof retrieving many false positive results. In this paper, we propose\nProbabilistic Token Aggregation (\\textit{ProTA}) to handle cross-modal\ninteraction with content asymmetry. Specifically, we propose dual\npartial-related aggregation to disentangle and re-aggregate token\nrepresentations in both low-dimension and high-dimension spaces. We propose\ntoken-based probabilistic alignment to generate token-level probabilistic\nrepresentation and maintain the feature representation diversity. In addition,\nan adaptive contrastive loss is proposed to learn compact cross-modal\ndistribution space. Based on extensive experiments, \\textit{ProTA} achieves\nsignificant improvements on MSR-VTT (50.9%), LSMDC (25.8%), and DiDeMo (47.2%).\n","authors":["Han Fang","Xianghao Zang","Chao Ban","Zerun Feng","Lanxiang Zhou","Zhongjiang He","Yongxiang Li","Hao Sun"],"pdf_url":"https://arxiv.org/pdf/2404.12216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12210v1","updated":"2024-04-18T14:14:44Z","published":"2024-04-18T14:14:44Z","title":"Observation, Analysis, and Solution: Exploring Strong Lightweight Vision\n  Transformers via Masked Image Modeling Pre-Training","summary":"  Masked image modeling (MIM) pre-training for large-scale vision transformers\n(ViTs) in computer vision has enabled promising downstream performance on top\nof the learned self-supervised ViT features. In this paper, we question if the\nextremely simple ViTs' fine-tuning performance with a small-scale architecture\ncan also benefit from this pre-training paradigm, which is considerably less\nstudied yet in contrast to the well-established lightweight architecture design\nmethodology with sophisticated components introduced. By carefully adapting\nvarious typical MIM pre-training methods to this lightweight regime and\ncomparing them with the contrastive learning (CL) pre-training on various\ndownstream image classification and dense prediction tasks, we systematically\nobserve different behaviors between MIM and CL with respect to the downstream\nfine-tuning data scales. Furthermore, we analyze the frozen features under\nlinear probing evaluation and also the layer representation similarities and\nattention maps across the obtained models, which clearly show the inferior\nlearning of MIM pre-training on higher layers, leading to unsatisfactory\nfine-tuning performance on data-insufficient downstream tasks. This finding is\nnaturally a guide to choosing appropriate distillation strategies during\npre-training to solve the above deterioration problem. Extensive experiments on\nvarious vision tasks demonstrate the effectiveness of our\nobservation-analysis-solution flow. In particular, our pre-training with\ndistillation on pure lightweight ViTs with vanilla/hierarchical design\n(5.7M/6.5M) can achieve 79.4%/78.9% top-1 accuracy on ImageNet-1K. It also\nenables SOTA performance on the ADE20K semantic segmentation task (42.8% mIoU)\nand LaSOT visual tracking task (66.1% AUC) in the lightweight regime. The\nlatter even surpasses all the current SOTA lightweight CPU-realtime trackers.\n","authors":["Jin Gao","Shubo Lin","Shaoru Wang","Yutong Kou","Zeming Li","Liang Li","Congxuan Zhang","Xiaoqin Zhang","Yizheng Wang","Weiming Hu"],"pdf_url":"https://arxiv.org/pdf/2404.12210v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12209v1","updated":"2024-04-18T14:14:07Z","published":"2024-04-18T14:14:07Z","title":"Partial-to-Partial Shape Matching with Geometric Consistency","summary":"  Finding correspondences between 3D shapes is an important and long-standing\nproblem in computer vision, graphics and beyond. A prominent challenge are\npartial-to-partial shape matching settings, which occur when the shapes to\nmatch are only observed incompletely (e.g. from 3D scanning). Although\npartial-to-partial matching is a highly relevant setting in practice, it is\nrarely explored. Our work bridges the gap between existing (rather artificial)\n3D full shape matching and partial-to-partial real-world settings by exploiting\ngeometric consistency as a strong constraint. We demonstrate that it is indeed\npossible to solve this challenging problem in a variety of settings. For the\nfirst time, we achieve geometric consistency for partial-to-partial matching,\nwhich is realized by a novel integer non-linear program formalism building on\ntriangle product spaces, along with a new pruning algorithm based on linear\ninteger programming. Further, we generate a new inter-class dataset for\npartial-to-partial shape-matching. We show that our method outperforms current\nSOTA methods on both an established intra-class dataset and our novel\ninter-class dataset.\n","authors":["Viktoria Ehm","Maolin Gao","Paul Roetzer","Marvin Eisenberger","Daniel Cremers","Florian Bernard"],"pdf_url":"https://arxiv.org/pdf/2404.12209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12203v1","updated":"2024-04-18T14:07:08Z","published":"2024-04-18T14:07:08Z","title":"GraFIQs: Face Image Quality Assessment Using Gradient Magnitudes","summary":"  Face Image Quality Assessment (FIQA) estimates the utility of face images for\nautomated face recognition (FR) systems. We propose in this work a novel\napproach to assess the quality of face images based on inspecting the required\nchanges in the pre-trained FR model weights to minimize differences between\ntesting samples and the distribution of the FR training dataset. To achieve\nthat, we propose quantifying the discrepancy in Batch Normalization statistics\n(BNS), including mean and variance, between those recorded during FR training\nand those obtained by processing testing samples through the pretrained FR\nmodel. We then generate gradient magnitudes of pretrained FR weights by\nbackpropagating the BNS through the pretrained model. The cumulative absolute\nsum of these gradient magnitudes serves as the FIQ for our approach. Through\ncomprehensive experimentation, we demonstrate the effectiveness of our\ntraining-free and quality labeling-free approach, achieving competitive\nperformance to recent state-of-theart FIQA approaches without relying on\nquality labeling, the need to train regression networks, specialized\narchitectures, or designing and optimizing specific loss functions.\n","authors":["Jan Niklas Kolf","Naser Damer","Fadi Boutros"],"pdf_url":"https://arxiv.org/pdf/2404.12203v1.pdf","comment":"Accepted at CVPR Workshop 2024"},{"id":"http://arxiv.org/abs/2404.12192v1","updated":"2024-04-18T13:56:03Z","published":"2024-04-18T13:56:03Z","title":"Aligning Actions and Walking to LLM-Generated Textual Descriptions","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious domains, including data augmentation and synthetic data generation.\nThis work explores the use of LLMs to generate rich textual descriptions for\nmotion sequences, encompassing both actions and walking patterns. We leverage\nthe expressive power of LLMs to align motion representations with high-level\nlinguistic cues, addressing two distinct tasks: action recognition and\nretrieval of walking sequences based on appearance attributes. For action\nrecognition, we employ LLMs to generate textual descriptions of actions in the\nBABEL-60 dataset, facilitating the alignment of motion sequences with\nlinguistic representations. In the domain of gait analysis, we investigate the\nimpact of appearance attributes on walking patterns by generating textual\ndescriptions of motion sequences from the DenseGait dataset using LLMs. These\ndescriptions capture subtle variations in walking styles influenced by factors\nsuch as clothing choices and footwear. Our approach demonstrates the potential\nof LLMs in augmenting structured motion attributes and aligning multi-modal\nrepresentations. The findings contribute to the advancement of comprehensive\nmotion understanding and open up new avenues for leveraging LLMs in multi-modal\nalignment and data augmentation for motion analysis. We make the code publicly\navailable at https://github.com/Radu1999/WalkAndText\n","authors":["Radu Chivereanu","Adrian Cosma","Andy Catruna","Razvan Rughinis","Emilian Radoi"],"pdf_url":"https://arxiv.org/pdf/2404.12192v1.pdf","comment":"Accepted at 2nd Workshop on Learning with Few or without Annotated\n  Face, Body and Gesture Data"},{"id":"http://arxiv.org/abs/2404.12183v1","updated":"2024-04-18T13:46:16Z","published":"2024-04-18T13:46:16Z","title":"Gait Recognition from Highly Compressed Videos","summary":"  Surveillance footage represents a valuable resource and opportunities for\nconducting gait analysis. However, the typical low quality and high noise\nlevels in such footage can severely impact the accuracy of pose estimation\nalgorithms, which are foundational for reliable gait analysis. Existing\nliterature suggests a direct correlation between the efficacy of pose\nestimation and the subsequent gait analysis results. A common mitigation\nstrategy involves fine-tuning pose estimation models on noisy data to improve\nrobustness. However, this approach may degrade the downstream model's\nperformance on the original high-quality data, leading to a trade-off that is\nundesirable in practice. We propose a processing pipeline that incorporates a\ntask-targeted artifact correction model specifically designed to pre-process\nand enhance surveillance footage before pose estimation. Our artifact\ncorrection model is optimized to work alongside a state-of-the-art pose\nestimation network, HRNet, without requiring repeated fine-tuning of the pose\nestimation model. Furthermore, we propose a simple and robust method for\nobtaining low quality videos that are annotated with poses in an automatic\nmanner with the purpose of training the artifact correction model. We\nsystematically evaluate the performance of our artifact correction model\nagainst a range of noisy surveillance data and demonstrate that our approach\nnot only achieves improved pose estimation on low-quality surveillance footage,\nbut also preserves the integrity of the pose estimation on high resolution\nfootage. Our experiments show a clear enhancement in gait analysis performance,\nsupporting the viability of the proposed method as a superior alternative to\ndirect fine-tuning strategies. Our contributions pave the way for more reliable\ngait analysis using surveillance data in real-world applications, regardless of\ndata quality.\n","authors":["Andrei Niculae","Andy Catruna","Adrian Cosma","Daniel Rosner","Emilian Radoi"],"pdf_url":"https://arxiv.org/pdf/2404.12183v1.pdf","comment":"Accepted at 2nd Workshop on Learning with Few or without Annotated\n  Face, Body and Gesture Data"},{"id":"http://arxiv.org/abs/2404.10335v2","updated":"2024-04-18T13:34:08Z","published":"2024-04-16T07:19:52Z","title":"Efficiently Adversarial Examples Generation for Visual-Language Models\n  under Targeted Transfer Scenarios using Diffusion Models","summary":"  Targeted transfer-based attacks involving adversarial examples pose a\nsignificant threat to large visual-language models (VLMs). However, the\nstate-of-the-art (SOTA) transfer-based attacks incur high costs due to\nexcessive iteration counts. Furthermore, the generated adversarial examples\nexhibit pronounced adversarial noise and demonstrate limited efficacy in\nevading defense methods such as DiffPure. To address these issues, inspired by\nscore matching, we introduce AdvDiffVLM, which utilizes diffusion models to\ngenerate natural, unrestricted adversarial examples. Specifically, AdvDiffVLM\nemploys Adaptive Ensemble Gradient Estimation to modify the score during the\ndiffusion model's reverse generation process, ensuring the adversarial examples\nproduced contain natural adversarial semantics and thus possess enhanced\ntransferability. Simultaneously, to enhance the quality of adversarial examples\nfurther, we employ the GradCAM-guided Mask method to disperse adversarial\nsemantics throughout the image, rather than concentrating them in a specific\narea. Experimental results demonstrate that our method achieves a speedup\nranging from 10X to 30X compared to existing transfer-based attack methods,\nwhile maintaining superior quality of adversarial examples. Additionally, the\ngenerated adversarial examples possess strong transferability and exhibit\nincreased robustness against adversarial defense methods. Notably, AdvDiffVLM\ncan successfully attack commercial VLMs, including GPT-4V, in a black-box\nmanner.\n","authors":["Qi Guo","Shanmin Pang","Xiaojun Jia","Qing Guo"],"pdf_url":"https://arxiv.org/pdf/2404.10335v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02286v3","updated":"2024-04-18T13:33:32Z","published":"2024-02-03T22:51:17Z","title":"Multi-Level Aggregation and Recursive Alignment Architecture for\n  Efficient Parallel Inference Segmentation Network","summary":"  Real-time semantic segmentation is a crucial research for real-world\napplications. However, many methods lay particular emphasis on reducing the\ncomputational complexity and model size, while largely sacrificing the\naccuracy. To tackle this problem, we propose a parallel inference network\ncustomized for semantic segmentation tasks to achieve a good trade-off between\nspeed and accuracy. We employ a shallow backbone to ensure real-time speed, and\npropose three core components to compensate for the reduced model capacity to\nimprove accuracy. Specifically, we first design a dual-pyramidal path\narchitecture (Multi-level Feature Aggregation Module, MFAM) to aggregate\nmulti-level features from the encoder to each scale, providing hierarchical\nclues for subsequent spatial alignment and corresponding in-network inference.\nThen, we build Recursive Alignment Module (RAM) by combining the flow-based\nalignment module with recursive upsampling architecture for accurate spatial\nalignment between multi-scale feature maps with half the computational\ncomplexity of the straightforward alignment method. Finally, we perform\nindependent parallel inference on the aligned features to obtain multi-scale\nscores, and adaptively fuse them through an attention-based Adaptive Scores\nFusion Module (ASFM) so that the final prediction can favor objects of multiple\nscales. Our framework shows a better balance between speed and accuracy than\nstate-of-the-art real-time methods on Cityscapes and CamVid datasets. We also\nconducted systematic ablation studies to gain insight into our motivation and\narchitectural design. Code is available at:\nhttps://github.com/Yanhua-Zhang/MFARANet.\n","authors":["Yanhua Zhang","Ke Zhang","Jingyu Wang","Yulin Wu","Wuwei Wang"],"pdf_url":"https://arxiv.org/pdf/2402.02286v3.pdf","comment":"15 pages, 9 figures and 12 Tables. Manuscript completed on April 30,\n  2022"},{"id":"http://arxiv.org/abs/2404.12172v1","updated":"2024-04-18T13:27:29Z","published":"2024-04-18T13:27:29Z","title":"How to Benchmark Vision Foundation Models for Semantic Segmentation?","summary":"  Recent vision foundation models (VFMs) have demonstrated proficiency in\nvarious tasks but require supervised fine-tuning to perform the task of\nsemantic segmentation effectively. Benchmarking their performance is essential\nfor selecting current models and guiding future model developments for this\ntask. The lack of a standardized benchmark complicates comparisons. Therefore,\nthe primary objective of this paper is to study how VFMs should be benchmarked\nfor semantic segmentation. To do so, various VFMs are fine-tuned under various\nsettings, and the impact of individual settings on the performance ranking and\ntraining time is assessed. Based on the results, the recommendation is to\nfine-tune the ViT-B variants of VFMs with a 16x16 patch size and a linear\ndecoder, as these settings are representative of using a larger model, more\nadvanced decoder and smaller patch size, while reducing training time by more\nthan 13 times. Using multiple datasets for training and evaluation is also\nrecommended, as the performance ranking across datasets and domain shifts\nvaries. Linear probing, a common practice for some VFMs, is not recommended, as\nit is not representative of end-to-end fine-tuning. The benchmarking setup\nrecommended in this paper enables a performance analysis of VFMs for semantic\nsegmentation. The findings of such an analysis reveal that pretraining with\npromptable segmentation is not beneficial, whereas masked image modeling (MIM)\nwith abstract representations is crucial, even more important than the type of\nsupervision used. The code for efficiently fine-tuning VFMs for semantic\nsegmentation can be accessed through the project page at:\nhttps://tue-mps.github.io/benchmark-vfm-ss/.\n","authors":["Tommie Kerssies","Daan de Geus","Gijs Dubbelman"],"pdf_url":"https://arxiv.org/pdf/2404.12172v1.pdf","comment":"CVPR 2024 Workshop Proceedings for the Second Workshop on Foundation\n  Models"},{"id":"http://arxiv.org/abs/2404.12168v1","updated":"2024-04-18T13:22:56Z","published":"2024-04-18T13:22:56Z","title":"Real-World Efficient Blind Motion Deblurring via Blur Pixel\n  Discretization","summary":"  As recent advances in mobile camera technology have enabled the capability to\ncapture high-resolution images, such as 4K images, the demand for an efficient\ndeblurring model handling large motion has increased. In this paper, we\ndiscover that the image residual errors, i.e., blur-sharp pixel differences,\ncan be grouped into some categories according to their motion blur type and how\ncomplex their neighboring pixels are. Inspired by this, we decompose the\ndeblurring (regression) task into blur pixel discretization (pixel-level blur\nclassification) and discrete-to-continuous conversion (regression with blur\nclass map) tasks. Specifically, we generate the discretized image residual\nerrors by identifying the blur pixels and then transform them to a continuous\nform, which is computationally more efficient than naively solving the original\nregression problem with continuous values. Here, we found that the\ndiscretization result, i.e., blur segmentation map, remarkably exhibits visual\nsimilarity with the image residual errors. As a result, our efficient model\nshows comparable performance to state-of-the-art methods in realistic\nbenchmarks, while our method is up to 10 times computationally more efficient.\n","authors":["Insoo Kim","Jae Seok Choi","Geonseok Seo","Kinam Kwon","Jinwoo Shin","Hyong-Euk Lee"],"pdf_url":"https://arxiv.org/pdf/2404.12168v1.pdf","comment":"CVPR2024 Camera-Ready"},{"id":"http://arxiv.org/abs/2311.17116v4","updated":"2024-04-18T13:03:44Z","published":"2023-11-28T12:14:22Z","title":"REF$^2$-NeRF: Reflection and Refraction aware Neural Radiance Field","summary":"  Recently, significant progress has been made in the study of methods for 3D\nreconstruction from multiple images using implicit neural representations,\nexemplified by the neural radiance field (NeRF) method. Such methods, which are\nbased on volume rendering, can model various light phenomena, and various\nextended methods have been proposed to accommodate different scenes and\nsituations. However, when handling scenes with multiple glass objects, e.g.,\nobjects in a glass showcase, modeling the target scene accurately has been\nchallenging due to the presence of multiple reflection and refraction effects.\nThus, this paper proposes a NeRF-based modeling method for scenes containing a\nglass case. In the proposed method, refraction and reflection are modeled using\nelements that are dependent and independent of the viewer's perspective. This\napproach allows us to estimate the surfaces where refraction occurs, i.e.,\nglass surfaces, and enables the separation and modeling of both direct and\nreflected light components. The proposed method requires predetermined camera\nposes, but accurately estimating these poses in scenes with glass objects is\ndifficult. Therefore, we used a robotic arm with an attached camera to acquire\nimages with known poses. Compared to existing methods, the proposed method\nenables more accurate modeling of both glass refraction and the overall scene.\n","authors":["Wooseok Kim","Taiki Fukiage","Takeshi Oishi"],"pdf_url":"https://arxiv.org/pdf/2311.17116v4.pdf","comment":"10 pages, 8 figures, 2 tables"},{"id":"http://arxiv.org/abs/2404.12154v1","updated":"2024-04-18T12:58:55Z","published":"2024-04-18T12:58:55Z","title":"StyleBooth: Image Style Editing with Multimodal Instruction","summary":"  Given an original image, image editing aims to generate an image that align\nwith the provided instruction. The challenges are to accept multimodal inputs\nas instructions and a scarcity of high-quality training data, including crucial\ntriplets of source/target image pairs and multimodal (text and image)\ninstructions. In this paper, we focus on image style editing and present\nStyleBooth, a method that proposes a comprehensive framework for image editing\nand a feasible strategy for building a high-quality style editing dataset. We\nintegrate encoded textual instruction and image exemplar as a unified condition\nfor diffusion model, enabling the editing of original image following\nmultimodal instructions. Furthermore, by iterative style-destyle tuning and\nediting and usability filtering, the StyleBooth dataset provides\ncontent-consistent stylized/plain image pairs in various categories of styles.\nTo show the flexibility of StyleBooth, we conduct experiments on diverse tasks,\nsuch as text-based style editing, exemplar-based style editing and\ncompositional style editing. The results demonstrate that the quality and\nvariety of training data significantly enhance the ability to preserve content\nand improve the overall quality of generated images in editing tasks. Project\npage can be found at https://ali-vilab.github.io/stylebooth-page/.\n","authors":["Zhen Han","Chaojie Mao","Zeyinzi Jiang","Yulin Pan","Jingfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.12154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15260v3","updated":"2024-04-18T12:44:56Z","published":"2023-11-26T10:27:22Z","title":"NeuRAD: Neural Rendering for Autonomous Driving","summary":"  Neural radiance fields (NeRFs) have gained popularity in the autonomous\ndriving (AD) community. Recent methods show NeRFs' potential for closed-loop\nsimulation, enabling testing of AD systems, and as an advanced training data\naugmentation technique. However, existing methods often require long training\ntimes, dense semantic supervision, or lack generalizability. This, in turn,\nhinders the application of NeRFs for AD at scale. In this paper, we propose\nNeuRAD, a robust novel view synthesis method tailored to dynamic AD data. Our\nmethod features simple network design, extensive sensor modeling for both\ncamera and lidar -- including rolling shutter, beam divergence and ray dropping\n-- and is applicable to multiple datasets out of the box. We verify its\nperformance on five popular AD datasets, achieving state-of-the-art performance\nacross the board. To encourage further development, we will openly release the\nNeuRAD source code. See https://github.com/georghess/NeuRAD .\n","authors":["Adam Tonderski","Carl Lindstr√∂m","Georg Hess","William Ljungbergh","Lennart Svensson","Christoffer Petersson"],"pdf_url":"https://arxiv.org/pdf/2311.15260v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12139v1","updated":"2024-04-18T12:41:33Z","published":"2024-04-18T12:41:33Z","title":"Omniview-Tuning: Boosting Viewpoint Invariance of Vision-Language\n  Pre-training Models","summary":"  Vision-Language Pre-training (VLP) models like CLIP have achieved remarkable\nsuccess in computer vision and particularly demonstrated superior robustness to\ndistribution shifts of 2D images. However, their robustness under 3D viewpoint\nvariations is still limited, which can hinder the development for real-world\napplications. This paper successfully addresses this concern while keeping\nVLPs' original performance by breaking through two primary obstacles: 1) the\nscarcity of training data and 2) the suboptimal fine-tuning paradigms. To\ncombat data scarcity, we build the Multi-View Caption (MVCap) dataset -- a\ncomprehensive collection of over four million multi-view image-text pairs\nacross more than 100K objects, providing more potential for VLP models to\ndevelop generalizable viewpoint-invariant representations. To address the\nlimitations of existing paradigms in performance trade-offs and training\nefficiency, we design a novel fine-tuning framework named Omniview-Tuning\n(OVT). Specifically, OVT introduces a Cross-Viewpoint Alignment objective\nthrough a minimax-like optimization strategy, which effectively aligns\nrepresentations of identical objects from diverse viewpoints without causing\noverfitting. Additionally, OVT fine-tunes VLP models in a parameter-efficient\nmanner, leading to minimal computational cost. Extensive experiments on various\nVLP models with different architectures validate that OVT significantly\nimproves the models' resilience to viewpoint shifts and keeps the original\nperformance, establishing a pioneering standard for boosting the viewpoint\ninvariance of VLP models.\n","authors":["Shouwei Ruan","Yinpeng Dong","Hanqing Liu","Yao Huang","Hang Su","Xingxing Wei"],"pdf_url":"https://arxiv.org/pdf/2404.12139v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2311.06634v2","updated":"2024-04-18T12:39:15Z","published":"2023-11-11T18:32:06Z","title":"Back to Basics: Fast Denoising Iterative Algorithm","summary":"  We introduce Back to Basics (BTB), a fast iterative algorithm for noise\nreduction. Our method is computationally efficient, does not require training\nor ground truth data, and can be applied in the presence of independent noise,\nas well as correlated (coherent) noise, where the noise level is unknown. We\nexamine three study cases: natural image denoising in the presence of additive\nwhite Gaussian noise, Poisson-distributed image denoising, and speckle\nsuppression in optical coherence tomography (OCT). Experimental results\ndemonstrate that the proposed approach can effectively improve image quality,\nin challenging noise settings. Theoretical guarantees are provided for\nconvergence stability.\n","authors":["Deborah Pereg"],"pdf_url":"https://arxiv.org/pdf/2311.06634v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12130v1","updated":"2024-04-18T12:31:48Z","published":"2024-04-18T12:31:48Z","title":"One-Shot Sequential Federated Learning for Non-IID Data by Enhancing\n  Local Model Diversity","summary":"  Traditional federated learning mainly focuses on parallel settings (PFL),\nwhich can suffer significant communication and computation costs. In contrast,\none-shot and sequential federated learning (SFL) have emerged as innovative\nparadigms to alleviate these costs. However, the issue of non-IID (Independent\nand Identically Distributed) data persists as a significant challenge in\none-shot and SFL settings, exacerbated by the restricted communication between\nclients. In this paper, we improve the one-shot sequential federated learning\nfor non-IID data by proposing a local model diversity-enhancing strategy.\nSpecifically, to leverage the potential of local model diversity for improving\nmodel performance, we introduce a local model pool for each client that\ncomprises diverse models generated during local training, and propose two\ndistance measurements to further enhance the model diversity and mitigate the\neffect of non-IID data. Consequently, our proposed framework can improve the\nglobal model performance while maintaining low communication costs. Extensive\nexperiments demonstrate that our method exhibits superior performance to\nexisting one-shot PFL methods and achieves better accuracy compared with\nstate-of-the-art one-shot SFL methods on both label-skew and domain-shift tasks\n(e.g., 6%+ accuracy improvement on the CIFAR-10 dataset).\n","authors":["Naibo Wang","Yuchen Deng","Wenjie Feng","Shichen Fan","Jianwei Yin","See-Kiong Ng"],"pdf_url":"https://arxiv.org/pdf/2404.12130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12120v1","updated":"2024-04-18T12:13:09Z","published":"2024-04-18T12:13:09Z","title":"Fortify the Guardian, Not the Treasure: Resilient Adversarial Detectors","summary":"  This paper presents RADAR-Robust Adversarial Detection via Adversarial\nRetraining-an approach designed to enhance the robustness of adversarial\ndetectors against adaptive attacks, while maintaining classifier performance.\nAn adaptive attack is one where the attacker is aware of the defenses and\nadapts their strategy accordingly. Our proposed method leverages adversarial\ntraining to reinforce the ability to detect attacks, without compromising clean\naccuracy. During the training phase, we integrate into the dataset adversarial\nexamples, which were optimized to fool both the classifier and the adversarial\ndetector, enabling the adversarial detector to learn and adapt to potential\nattack scenarios. Experimental evaluations on the CIFAR-10 and SVHN datasets\ndemonstrate that our proposed algorithm significantly improves a detector's\nability to accurately identify adaptive adversarial attacks -- without\nsacrificing clean accuracy.\n","authors":["Raz Lapid","Almog Dubin","Moshe Sipper"],"pdf_url":"https://arxiv.org/pdf/2404.12120v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08182v2","updated":"2024-04-18T11:57:49Z","published":"2023-10-12T10:17:40Z","title":"XIMAGENET-12: An Explainable AI Benchmark Dataset for Model Robustness\n  Evaluation","summary":"  Despite the promising performance of existing visual models on public\nbenchmarks, the critical assessment of their robustness for real-world\napplications remains an ongoing challenge. To bridge this gap, we propose an\nexplainable visual dataset, XIMAGENET-12, to evaluate the robustness of visual\nmodels. XIMAGENET-12 consists of over 200K images with 15,410 manual semantic\nannotations. Specifically, we deliberately selected 12 categories from\nImageNet, representing objects commonly encountered in practical life. To\nsimulate real-world situations, we incorporated six diverse scenarios, such as\noverexposure, blurring, and color changes, etc. We further develop a\nquantitative criterion for robustness assessment, allowing for a nuanced\nunderstanding of how visual models perform under varying conditions, notably in\nrelation to the background. We make the XIMAGENET-12 dataset and its\ncorresponding code openly accessible at\n\\url{https://sites.google.com/view/ximagenet-12/home}. We expect the\nintroduction of the XIMAGENET-12 dataset will empower researchers to thoroughly\nevaluate the robustness of their visual models under challenging conditions.\n","authors":["Qiang Li","Dan Zhang","Shengzhao Lei","Xun Zhao","Porawit Kamnoedboon","WeiWei Li","Junhao Dong","Shuyan Li"],"pdf_url":"https://arxiv.org/pdf/2310.08182v2.pdf","comment":"Paper accepted by Synthetic Data for Computer Vision Workshop @ IEEE\n  CVPR 2024"},{"id":"http://arxiv.org/abs/2404.06211v2","updated":"2024-04-18T11:52:11Z","published":"2024-04-09T11:00:11Z","title":"Unified Physical-Digital Attack Detection Challenge","summary":"  Face Anti-Spoofing (FAS) is crucial to safeguard Face Recognition (FR)\nSystems. In real-world scenarios, FRs are confronted with both physical and\ndigital attacks. However, existing algorithms often address only one type of\nattack at a time, which poses significant limitations in real-world scenarios\nwhere FR systems face hybrid physical-digital threats. To facilitate the\nresearch of Unified Attack Detection (UAD) algorithms, a large-scale\nUniAttackData dataset has been collected. UniAttackData is the largest public\ndataset for Unified Attack Detection, with a total of 28,706 videos, where each\nunique identity encompasses all advanced attack types. Based on this dataset,\nwe organized a Unified Physical-Digital Face Attack Detection Challenge to\nboost the research in Unified Attack Detections. It attracted 136 teams for the\ndevelopment phase, with 13 qualifying for the final round. The results\nre-verified by the organizing team were used for the final ranking. This paper\ncomprehensively reviews the challenge, detailing the dataset introduction,\nprotocol definition, evaluation criteria, and a summary of published results.\nFinally, we focus on the detailed analysis of the highest-performing algorithms\nand offer potential directions for unified physical-digital attack detection\ninspired by this competition. Challenge Website:\nhttps://sites.google.com/view/face-anti-spoofing-challenge/welcome/challengecvpr2024.\n","authors":["Haocheng Yuan","Ajian Liu","Junze Zheng","Jun Wan","Jiankang Deng","Sergio Escalera","Hugo Jair Escalante","Isabelle Guyon","Zhen Lei"],"pdf_url":"https://arxiv.org/pdf/2404.06211v2.pdf","comment":"11 pages, 10 figures"},{"id":"http://arxiv.org/abs/2404.12104v1","updated":"2024-04-18T11:38:25Z","published":"2024-04-18T11:38:25Z","title":"Ethical-Lens: Curbing Malicious Usages of Open-Source Text-to-Image\n  Models","summary":"  The burgeoning landscape of text-to-image models, exemplified by innovations\nsuch as Midjourney and DALLE 3, has revolutionized content creation across\ndiverse sectors. However, these advancements bring forth critical ethical\nconcerns, particularly with the misuse of open-source models to generate\ncontent that violates societal norms. Addressing this, we introduce\nEthical-Lens, a framework designed to facilitate the value-aligned usage of\ntext-to-image tools without necessitating internal model revision. Ethical-Lens\nensures value alignment in text-to-image models across toxicity and bias\ndimensions by refining user commands and rectifying model outputs. Systematic\nevaluation metrics, combining GPT4-V, HEIM, and FairFace scores, assess\nalignment capability. Our experiments reveal that Ethical-Lens enhances\nalignment capabilities to levels comparable with or superior to commercial\nmodels like DALLE 3, ensuring user-generated content adheres to ethical\nstandards while maintaining image quality. This study indicates the potential\nof Ethical-Lens to ensure the sustainable development of open-source\ntext-to-image tools and their beneficial integration into society. Our code is\navailable at https://github.com/yuzhu-cai/Ethical-Lens.\n","authors":["Yuzhu Cai","Sheng Yin","Yuxi Wei","Chenxin Xu","Weibo Mao","Felix Juefei-Xu","Siheng Chen","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2404.12104v1.pdf","comment":"42 pages, 17 figures, 29 tables"},{"id":"http://arxiv.org/abs/2404.12103v1","updated":"2024-04-18T11:36:37Z","published":"2024-04-18T11:36:37Z","title":"S3R-Net: A Single-Stage Approach to Self-Supervised Shadow Removal","summary":"  In this paper we present S3R-Net, the Self-Supervised Shadow Removal Network.\nThe two-branch WGAN model achieves self-supervision relying on the\nunify-and-adaptphenomenon - it unifies the style of the output data and infers\nits characteristics from a database of unaligned shadow-free reference images.\nThis approach stands in contrast to the large body of supervised frameworks.\nS3R-Net also differentiates itself from the few existing self-supervised models\noperating in a cycle-consistent manner, as it is a non-cyclic, unidirectional\nsolution. The proposed framework achieves comparable numerical scores to recent\nselfsupervised shadow removal models while exhibiting superior qualitative\nperformance and keeping the computational cost low.\n","authors":["Nikolina Kubiak","Armin Mustafa","Graeme Phillipson","Stephen Jolly","Simon Hadfield"],"pdf_url":"https://arxiv.org/pdf/2404.12103v1.pdf","comment":"NTIRE workshop @ CVPR 2024. Code & models available at\n  https://github.com/n-kubiak/S3R-Net"},{"id":"http://arxiv.org/abs/2303.13959v4","updated":"2024-04-18T11:31:00Z","published":"2023-03-24T12:33:44Z","title":"Bridging Stereo Geometry and BEV Representation with Reliable Mutual\n  Interaction for Semantic Scene Completion","summary":"  3D semantic scene completion (SSC) is an ill-posed perception task that\nrequires inferring a dense 3D scene from limited observations. Previous\ncamera-based methods struggle to predict accurate semantic scenes due to\ninherent geometric ambiguity and incomplete observations. In this paper, we\nresort to stereo matching technique and bird's-eye-view (BEV) representation\nlearning to address such issues in SSC. Complementary to each other, stereo\nmatching mitigates geometric ambiguity with epipolar constraint while BEV\nrepresentation enhances the hallucination ability for invisible regions with\nglobal semantic context. However, due to the inherent representation gap\nbetween stereo geometry and BEV features, it is non-trivial to bridge them for\ndense prediction task of SSC. Therefore, we further develop a unified\noccupancy-based framework dubbed BRGScene, which effectively bridges these two\nrepresentations with dense 3D volumes for reliable semantic scene completion.\nSpecifically, we design a novel Mutual Interactive Ensemble (MIE) block for\npixel-level reliable aggregation of stereo geometry and BEV features. Within\nthe MIE block, a Bi-directional Reliable Interaction (BRI) module, enhanced\nwith confidence re-weighting, is employed to encourage fine-grained interaction\nthrough mutual guidance. Besides, a Dual Volume Ensemble (DVE) module is\nintroduced to facilitate complementary aggregation through channel-wise\nrecalibration and multi-group voting. Our method outperforms all published\ncamera-based methods on SemanticKITTI for semantic scene completion. Our code\nis available on \\url{https://github.com/Arlo0o/StereoScene}.\n","authors":["Bohan Li","Yasheng Sun","Zhujin Liang","Dalong Du","Zhuanghui Zhang","Xiaofeng Wang","Yunnan Wang","Xin Jin","Wenjun Zeng"],"pdf_url":"https://arxiv.org/pdf/2303.13959v4.pdf","comment":"IJCAI2024"},{"id":"http://arxiv.org/abs/2404.12091v1","updated":"2024-04-18T11:20:53Z","published":"2024-04-18T11:20:53Z","title":"Harnessing Joint Rain-/Detail-aware Representations to Eliminate\n  Intricate Rains","summary":"  Recent advances in image deraining have focused on training powerful models\non mixed multiple datasets comprising diverse rain types and backgrounds.\nHowever, this approach tends to overlook the inherent differences among rainy\nimages, leading to suboptimal results. To overcome this limitation, we focus on\naddressing various rainy images by delving into meaningful representations that\nencapsulate both the rain and background components. Leveraging these\nrepresentations as instructive guidance, we put forth a Context-based\nInstance-level Modulation (CoI-M) mechanism adept at efficiently modulating\nCNN- or Transformer-based models. Furthermore, we devise a rain-/detail-aware\ncontrastive learning strategy to help extract joint rain-/detail-aware\nrepresentations. By integrating CoI-M with the rain-/detail-aware Contrastive\nlearning, we develop CoIC, an innovative and potent algorithm tailored for\ntraining models on mixed datasets. Moreover, CoIC offers insight into modeling\nrelationships of datasets, quantitatively assessing the impact of rain and\ndetails on restoration, and unveiling distinct behaviors of models given\ndiverse inputs. Extensive experiments validate the efficacy of CoIC in boosting\nthe deraining ability of CNN and Transformer models. CoIC also enhances the\nderaining prowess remarkably when real-world dataset is included.\n","authors":["Wu Ran","Peirong Ma","Zhiquan He","Hao Ren","Hong Lu"],"pdf_url":"https://arxiv.org/pdf/2404.12091v1.pdf","comment":"21 pages, 14 figures"},{"id":"http://arxiv.org/abs/2404.12083v1","updated":"2024-04-18T11:09:25Z","published":"2024-04-18T11:09:25Z","title":"MambaPupil: Bidirectional Selective Recurrent model for Event-based Eye\n  tracking","summary":"  Event-based eye tracking has shown great promise with the high temporal\nresolution and low redundancy provided by the event camera. However, the\ndiversity and abruptness of eye movement patterns, including blinking,\nfixating, saccades, and smooth pursuit, pose significant challenges for eye\nlocalization. To achieve a stable event-based eye-tracking system, this paper\nproposes a bidirectional long-term sequence modeling and time-varying state\nselection mechanism to fully utilize contextual temporal information in\nresponse to the variability of eye movements. Specifically, the MambaPupil\nnetwork is proposed, which consists of the multi-layer convolutional encoder to\nextract features from the event representations, a bidirectional Gated\nRecurrent Unit (GRU), and a Linear Time-Varying State Space Module (LTV-SSM),\nto selectively capture contextual correlation from the forward and backward\ntemporal relationship. Furthermore, the Bina-rep is utilized as a compact event\nrepresentation, and the tailor-made data augmentation, called as Event-Cutout,\nis proposed to enhance the model's robustness by applying spatial random\nmasking to the event image. The evaluation on the ThreeET-plus benchmark shows\nthe superior performance of the MambaPupil, which secured the 1st place in\nCVPR'2024 AIS Event-based Eye Tracking challenge.\n","authors":["Zhong Wang","Zengyu Wan","Han Han","Bohao Liao","Yuliang Wu","Wei Zhai","Yang Cao","Zheng-jun Zha"],"pdf_url":"https://arxiv.org/pdf/2404.12083v1.pdf","comment":"Accepted by CVPR 2024 Workshop (AIS: Vision, Graphics and AI for\n  Streaming), top solution of challenge Event-based Eye Tracking, see\n  https://www.kaggle.com/competitions/event-based-eye-tracking-ais2024"},{"id":"http://arxiv.org/abs/2404.12081v1","updated":"2024-04-18T11:05:15Z","published":"2024-04-18T11:05:15Z","title":"MaskCD: A Remote Sensing Change Detection Network Based on Mask\n  Classification","summary":"  Change detection (CD) from remote sensing (RS) images using deep learning has\nbeen widely investigated in the literature. It is typically regarded as a\npixel-wise labeling task that aims to classify each pixel as changed or\nunchanged. Although per-pixel classification networks in encoder-decoder\nstructures have shown dominance, they still suffer from imprecise boundaries\nand incomplete object delineation at various scenes. For high-resolution RS\nimages, partly or totally changed objects are more worthy of attention rather\nthan a single pixel. Therefore, we revisit the CD task from the mask prediction\nand classification perspective and propose MaskCD to detect changed areas by\nadaptively generating categorized masks from input image pairs. Specifically,\nit utilizes a cross-level change representation perceiver (CLCRP) to learn\nmultiscale change-aware representations and capture spatiotemporal relations\nfrom encoded features by exploiting deformable multihead self-attention\n(DeformMHSA). Subsequently, a masked-attention-based detection transformers\n(MA-DETR) decoder is developed to accurately locate and identify changed\nobjects based on masked attention and self-attention mechanisms. It\nreconstructs the desired changed objects by decoding the pixel-wise\nrepresentations into learnable mask proposals and making final predictions from\nthese candidates. Experimental results on five benchmark datasets demonstrate\nthe proposed approach outperforms other state-of-the-art models. Codes and\npretrained models are available online (https://github.com/EricYu97/MaskCD).\n","authors":["Weikang Yu","Xiaokang Zhang","Samiran Das","Xiao Xiang Zhu","Pedram Ghamisi"],"pdf_url":"https://arxiv.org/pdf/2404.12081v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.15663v2","updated":"2024-04-18T10:48:15Z","published":"2024-01-28T13:59:58Z","title":"Low-resolution Prior Equilibrium Network for CT Reconstruction","summary":"  The unrolling method has been investigated for learning variational models in\nX-ray computed tomography. However, it has been observed that directly\nunrolling the regularization model through gradient descent does not produce\nsatisfactory results. In this paper, we present a novel deep learning-based CT\nreconstruction model, where the low-resolution image is introduced to obtain an\neffective regularization term for improving the network`s robustness. Our\napproach involves constructing the backbone network architecture by algorithm\nunrolling that is realized using the deep equilibrium architecture. We\ntheoretically discuss the convergence of the proposed low-resolution prior\nequilibrium model and provide the conditions to guarantee convergence.\nExperimental results on both sparse-view and limited-angle reconstruction\nproblems are provided, demonstrating that our end-to-end low-resolution prior\nequilibrium model outperforms other state-of-the-art methods in terms of noise\nreduction, contrast-to-noise ratio, and preservation of edge details.\n","authors":["Yijie Yang","Qifeng Gao","Yuping Duan"],"pdf_url":"https://arxiv.org/pdf/2401.15663v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04519v3","updated":"2024-04-18T10:40:35Z","published":"2023-12-07T18:38:39Z","title":"Bootstrapping Autonomous Driving Radars with Self-Supervised Learning","summary":"  The perception of autonomous vehicles using radars has attracted increased\nresearch interest due its ability to operate in fog and bad weather. However,\ntraining radar models is hindered by the cost and difficulty of annotating\nlarge-scale radar data. To overcome this bottleneck, we propose a\nself-supervised learning framework to leverage the large amount of unlabeled\nradar data to pre-train radar-only embeddings for self-driving perception\ntasks. The proposed method combines radar-to-radar and radar-to-vision\ncontrastive losses to learn a general representation from unlabeled radar\nheatmaps paired with their corresponding camera images. When used for\ndownstream object detection, we demonstrate that the proposed self-supervision\nframework can improve the accuracy of state-of-the-art supervised baselines by\n$5.8\\%$ in mAP. Code is available at \\url{https://github.com/yiduohao/Radical}.\n","authors":["Yiduo Hao","Sohrab Madani","Junfeng Guan","Mohammed Alloulah","Saurabh Gupta","Haitham Hassanieh"],"pdf_url":"https://arxiv.org/pdf/2312.04519v3.pdf","comment":"12 pages, 5 figures, to be published in Proceedings of the IEEE/CVF\n  Conference on Computer Vision and Pattern Recognition 2024"},{"id":"http://arxiv.org/abs/2404.12064v1","updated":"2024-04-18T10:23:10Z","published":"2024-04-18T10:23:10Z","title":"PureForest: A Large-scale Aerial Lidar and Aerial Imagery Dataset for\n  Tree Species Classification in Monospecific Forests","summary":"  Knowledge of tree species distribution is fundamental to managing forests.\nNew deep learning approaches promise significant accuracy gains for forest\nmapping, and are becoming a critical tool for mapping multiple tree species at\nscale. To advance the field, deep learning researchers need large benchmark\ndatasets with high-quality annotations. To this end, we present the PureForest\ndataset: a large-scale, open, multimodal dataset designed for tree species\nclassification from both Aerial Lidar Scanning (ALS) point clouds and Very High\nResolution (VHR) aerial images. Most current public Lidar datasets for tree\nspecies classification have low diversity as they only span a small area of a\nfew dozen annotated hectares at most. In contrast, PureForest has 18 tree\nspecies grouped into 13 semantic classes, and spans 339 km$^2$ across 449\ndistinct monospecific forests, and is to date the largest and most\ncomprehensive Lidar dataset for the identification of tree species. By making\nPureForest publicly available, we hope to provide a challenging benchmark\ndataset to support the development of deep learning approaches for tree species\nidentification from Lidar and/or aerial imagery. In this data paper, we\ndescribe the annotation workflow, the dataset, the recommended evaluation\nmethodology, and establish a baseline performance from both 3D and 2D\nmodalities.\n","authors":["Charles Gaydon","Floryne Roche"],"pdf_url":"https://arxiv.org/pdf/2404.12064v1.pdf","comment":"14 pages | 5 figures | Dataset is available at\n  http://huggingface.co/datasets/IGNF/PureForest"},{"id":"http://arxiv.org/abs/2404.12062v1","updated":"2024-04-18T10:20:37Z","published":"2024-04-18T10:20:37Z","title":"MIDGET: Music Conditioned 3D Dance Generation","summary":"  In this paper, we introduce a MusIc conditioned 3D Dance GEneraTion model,\nnamed MIDGET based on Dance motion Vector Quantised Variational AutoEncoder\n(VQ-VAE) model and Motion Generative Pre-Training (GPT) model to generate\nvibrant and highquality dances that match the music rhythm. To tackle\nchallenges in the field, we introduce three new components: 1) a pre-trained\nmemory codebook based on the Motion VQ-VAE model to store different human pose\ncodes, 2) employing Motion GPT model to generate pose codes with music and\nmotion Encoders, 3) a simple framework for music feature extraction. We compare\nwith existing state-of-the-art models and perform ablation experiments on\nAIST++, the largest publicly available music-dance dataset. Experiments\ndemonstrate that our proposed framework achieves state-of-the-art performance\non motion quality and its alignment with the music.\n","authors":["Jinwu Wang","Wei Mao","Miaomiao Liu"],"pdf_url":"https://arxiv.org/pdf/2404.12062v1.pdf","comment":"12 pages, 6 figures Published in AI 2023: Advances in Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2312.16867v2","updated":"2024-04-18T10:14:31Z","published":"2023-12-28T07:37:11Z","title":"DualFluidNet: an Attention-based Dual-pipeline Network for FLuid\n  Simulation","summary":"  Fluid motion can be considered as a point cloud transformation when using the\nSPH method. Compared to traditional numerical analysis methods, using machine\nlearning techniques to learn physics simulations can achieve near-accurate\nresults, while significantly increasing efficiency. In this paper, we propose\nan innovative approach for 3D fluid simulations utilizing an Attention-based\nDual-pipeline Network, which employs a dual-pipeline architecture, seamlessly\nintegrated with an Attention-based Feature Fusion Module. Unlike previous\nmethods, which often make difficult trade-offs between global fluid control and\nphysical law constraints, we find a way to achieve a better balance between\nthese two crucial aspects with a well-designed dual-pipeline approach.\nAdditionally, we design a Type-aware Input Module to adaptively recognize\nparticles of different types and perform feature fusion afterward, such that\nfluid-solid coupling issues can be better dealt with. Furthermore, we propose a\nnew dataset, Tank3D, to further explore the network's ability to handle more\ncomplicated scenes. The experiments demonstrate that our approach not only\nattains a quantitative enhancement in various metrics, surpassing the\nstate-of-the-art methods but also signifies a qualitative leap in neural\nnetwork-based simulation by faithfully adhering to the physical laws. Code and\nvideo demonstrations are available at\nhttps://github.com/chenyu-xjtu/DualFluidNet.\n","authors":["Yu Chen","Shuai Zheng","Menglong Jin","Yan Chang","Nianyi Wang"],"pdf_url":"https://arxiv.org/pdf/2312.16867v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2404.12055v1","updated":"2024-04-18T10:10:56Z","published":"2024-04-18T10:10:56Z","title":"Improving the perception of visual fiducial markers in the field using\n  Adaptive Active Exposure Control","summary":"  Accurate localization is fundamental for autonomous underwater vehicles\n(AUVs) to carry out precise tasks, such as manipulation and construction.\nVision-based solutions using fiducial marker are promising, but extremely\nchallenging underwater because of harsh lighting condition underwater. This\npaper introduces a gradient-based active camera exposure control method to\ntackle sharp lighting variations during image acquisition, which can establish\nbetter foundation for subsequent image enhancement procedures. Considering a\ntypical scenario for underwater operations where visual tags are used, we\nproposed several experiments comparing our method with other state-of-the-art\nexposure control method including Active Exposure Control (AEC) and\nGradient-based Exposure Control (GEC). Results show a significant improvement\nin the accuracy of robot localization. This method is an important component\nthat can be used in visual-based state estimation pipeline to improve the\noverall localization accuracy.\n","authors":["Ziang Ren","Samuel Lensgraf","Alberto Quattrini Li"],"pdf_url":"https://arxiv.org/pdf/2404.12055v1.pdf","comment":"Paper accepted by ISER 2023"},{"id":"http://arxiv.org/abs/2404.09624v2","updated":"2024-04-18T10:10:00Z","published":"2024-04-15T09:56:20Z","title":"AesExpert: Towards Multi-modality Foundation Model for Image Aesthetics\n  Perception","summary":"  The highly abstract nature of image aesthetics perception (IAP) poses\nsignificant challenge for current multimodal large language models (MLLMs). The\nlack of human-annotated multi-modality aesthetic data further exacerbates this\ndilemma, resulting in MLLMs falling short of aesthetics perception\ncapabilities. To address the above challenge, we first introduce a\ncomprehensively annotated Aesthetic Multi-Modality Instruction Tuning (AesMMIT)\ndataset, which serves as the footstone for building multi-modality aesthetics\nfoundation models. Specifically, to align MLLMs with human aesthetics\nperception, we construct a corpus-rich aesthetic critique database with 21,904\ndiverse-sourced images and 88K human natural language feedbacks, which are\ncollected via progressive questions, ranging from coarse-grained aesthetic\ngrades to fine-grained aesthetic descriptions. To ensure that MLLMs can handle\ndiverse queries, we further prompt GPT to refine the aesthetic critiques and\nassemble the large-scale aesthetic instruction tuning dataset, i.e. AesMMIT,\nwhich consists of 409K multi-typed instructions to activate stronger aesthetic\ncapabilities. Based on the AesMMIT database, we fine-tune the open-sourced\ngeneral foundation models, achieving multi-modality Aesthetic Expert models,\ndubbed AesExpert. Extensive experiments demonstrate that the proposed AesExpert\nmodels deliver significantly better aesthetic perception performances than the\nstate-of-the-art MLLMs, including the most advanced GPT-4V and\nGemini-Pro-Vision. Source data will be available at\nhttps://github.com/yipoh/AesExpert.\n","authors":["Yipo Huang","Xiangfei Sheng","Zhichao Yang","Quan Yuan","Zhichao Duan","Pengfei Chen","Leida Li","Weisi Lin","Guangming Shi"],"pdf_url":"https://arxiv.org/pdf/2404.09624v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12037v1","updated":"2024-04-18T09:44:56Z","published":"2024-04-18T09:44:56Z","title":"Data-free Knowledge Distillation for Fine-grained Visual Categorization","summary":"  Data-free knowledge distillation (DFKD) is a promising approach for\naddressing issues related to model compression, security privacy, and\ntransmission restrictions. Although the existing methods exploiting DFKD have\nachieved inspiring achievements in coarse-grained classification, in practical\napplications involving fine-grained classification tasks that require more\ndetailed distinctions between similar categories, sub-optimal results are\nobtained. To address this issue, we propose an approach called DFKD-FGVC that\nextends DFKD to fine-grained visual categorization~(FGVC) tasks. Our approach\nutilizes an adversarial distillation framework with attention generator, mixed\nhigh-order attention distillation, and semantic feature contrast learning.\nSpecifically, we introduce a spatial-wise attention mechanism to the generator\nto synthesize fine-grained images with more details of discriminative parts. We\nalso utilize the mixed high-order attention mechanism to capture complex\ninteractions among parts and the subtle differences among discriminative\nfeatures of the fine-grained categories, paying attention to both local\nfeatures and semantic context relationships. Moreover, we leverage the teacher\nand student models of the distillation framework to contrast high-level\nsemantic feature maps in the hyperspace, comparing variances of different\ncategories. We evaluate our approach on three widely-used FGVC benchmarks\n(Aircraft, Cars196, and CUB200) and demonstrate its superior performance.\n","authors":["Renrong Shao","Wei Zhang","Jianhua Yin","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2404.12037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08277v2","updated":"2024-04-18T09:43:26Z","published":"2024-04-12T07:04:56Z","title":"FaceFilterSense: A Filter-Resistant Face Recognition and Facial\n  Attribute Analysis Framework","summary":"  With the advent of social media, fun selfie filters have come into tremendous\nmainstream use affecting the functioning of facial biometric systems as well as\nimage recognition systems. These filters vary from beautification filters and\nAugmented Reality (AR)-based filters to filters that modify facial landmarks.\nHence, there is a need to assess the impact of such filters on the performance\nof existing face recognition systems. The limitation associated with existing\nsolutions is that these solutions focus more on the beautification filters.\nHowever, the current AR-based filters and filters which distort facial key\npoints are in vogue recently and make the faces highly unrecognizable even to\nthe naked eye. Also, the filters considered are mostly obsolete with limited\nvariations. To mitigate these limitations, we aim to perform a holistic impact\nanalysis of the latest filters and propose an user recognition model with the\nfiltered images. We have utilized a benchmark dataset for baseline images, and\napplied the latest filters over them to generate a beautified/filtered dataset.\nNext, we have introduced a model FaceFilterNet for beautified user recognition.\nIn this framework, we also utilize our model to comment on various attributes\nof the person including age, gender, and ethnicity. In addition, we have also\npresented a filter-wise impact analysis on face recognition, age estimation,\ngender, and ethnicity prediction. The proposed method affirms the efficacy of\nour dataset with an accuracy of 87.25% and an optimal accuracy for facial\nattribute analysis.\n","authors":["Shubham Tiwari","Yash Sethia","Ritesh Kumar","Ashwani Tanwar","Rudresh Dwivedi"],"pdf_url":"https://arxiv.org/pdf/2404.08277v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12031v1","updated":"2024-04-18T09:31:03Z","published":"2024-04-18T09:31:03Z","title":"MLS-Track: Multilevel Semantic Interaction in RMOT","summary":"  The new trend in multi-object tracking task is to track objects of interest\nusing natural language. However, the scarcity of paired prompt-instance data\nhinders its progress. To address this challenge, we propose a high-quality yet\nlow-cost data generation method base on Unreal Engine 5 and construct a\nbrand-new benchmark dataset, named Refer-UE-City, which primarily includes\nscenes from intersection surveillance videos, detailing the appearance and\nactions of people and vehicles. Specifically, it provides 14 videos with a\ntotal of 714 expressions, and is comparable in scale to the Refer-KITTI\ndataset. Additionally, we propose a multi-level semantic-guided multi-object\nframework called MLS-Track, where the interaction between the model and text is\nenhanced layer by layer through the introduction of Semantic Guidance Module\n(SGM) and Semantic Correlation Branch (SCB). Extensive experiments on\nRefer-UE-City and Refer-KITTI datasets demonstrate the effectiveness of our\nproposed framework and it achieves state-of-the-art performance. Code and\ndatatsets will be available.\n","authors":["Zeliang Ma","Song Yang","Zhe Cui","Zhicheng Zhao","Fei Su","Delong Liu","Jingyu Wang"],"pdf_url":"https://arxiv.org/pdf/2404.12031v1.pdf","comment":"17 pages 8 figures"},{"id":"http://arxiv.org/abs/2404.12024v1","updated":"2024-04-18T09:21:16Z","published":"2024-04-18T09:21:16Z","title":"Meta-Auxiliary Learning for Micro-Expression Recognition","summary":"  Micro-expressions (MEs) are involuntary movements revealing people's hidden\nfeelings, which has attracted numerous interests for its objectivity in emotion\ndetection. However, despite its wide applications in various scenarios,\nmicro-expression recognition (MER) remains a challenging problem in real life\ndue to three reasons, including (i) data-level: lack of data and imbalanced\nclasses, (ii) feature-level: subtle, rapid changing, and complex features of\nMEs, and (iii) decision-making-level: impact of individual differences. To\naddress these issues, we propose a dual-branch meta-auxiliary learning method,\ncalled LightmanNet, for fast and robust micro-expression recognition.\nSpecifically, LightmanNet learns general MER knowledge from limited data\nthrough a dual-branch bi-level optimization process: (i) In the first level, it\nobtains task-specific MER knowledge by learning in two branches, where the\nfirst branch is for learning MER features via primary MER tasks, while the\nother branch is for guiding the model obtain discriminative features via\nauxiliary tasks, i.e., image alignment between micro-expressions and\nmacro-expressions since their resemblance in both spatial and temporal\nbehavioral patterns. The two branches of learning jointly constrain the model\nof learning meaningful task-specific MER knowledge while avoiding learning\nnoise or superficial connections between MEs and emotions that may damage its\ngeneralization ability. (ii) In the second level, LightmanNet further refines\nthe learned task-specific knowledge, improving model generalization and\nefficiency. Extensive experiments on various benchmark datasets demonstrate the\nsuperior robustness and efficiency of LightmanNet.\n","authors":["Jingyao Wang","Yunhan Tian","Yuxuan Yang","Xiaoxin Chen","Changwen Zheng","Wenwen Qiang"],"pdf_url":"https://arxiv.org/pdf/2404.12024v1.pdf","comment":"10 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2404.12020v1","updated":"2024-04-18T09:16:02Z","published":"2024-04-18T09:16:02Z","title":"Look, Listen, and Answer: Overcoming Biases for Audio-Visual Question\n  Answering","summary":"  Audio-Visual Question Answering (AVQA) is a complex multi-modal reasoning\ntask, demanding intelligent systems to accurately respond to natural language\nqueries based on audio-video input pairs. Nevertheless, prevalent AVQA\napproaches are prone to overlearning dataset biases, resulting in poor\nrobustness. Furthermore, current datasets may not provide a precise diagnostic\nfor these methods. To tackle these challenges, firstly, we propose a novel\ndataset, \\textit{MUSIC-AVQA-R}, crafted in two steps: rephrasing questions\nwithin the test split of a public dataset (\\textit{MUSIC-AVQA}) and\nsubsequently introducing distribution shifts to split questions. The former\nleads to a large, diverse test space, while the latter results in a\ncomprehensive robustness evaluation on rare, frequent, and overall questions.\nSecondly, we propose a robust architecture that utilizes a multifaceted cycle\ncollaborative debiasing strategy to overcome bias learning. Experimental\nresults show that this architecture achieves state-of-the-art performance on\nboth datasets, especially obtaining a significant improvement of 9.68\\% on the\nproposed dataset. Extensive ablation experiments are conducted on these two\ndatasets to validate the effectiveness of the debiasing strategy. Additionally,\nwe highlight the limited robustness of existing multi-modal QA methods through\nthe evaluation on our dataset.\n","authors":["Jie Ma","Min Hu","Pinghui Wang","Wangchun Sun","Lingyun Song","Hongbin Pei","Jun Liu","Youtian Du"],"pdf_url":"https://arxiv.org/pdf/2404.12020v1.pdf","comment":"16 pages, 9 figures,5 Tables"},{"id":"http://arxiv.org/abs/2404.12015v1","updated":"2024-04-18T09:06:05Z","published":"2024-04-18T09:06:05Z","title":"What does CLIP know about peeling a banana?","summary":"  Humans show an innate capability to identify tools to support specific\nactions. The association between objects parts and the actions they facilitate\nis usually named affordance. Being able to segment objects parts depending on\nthe tasks they afford is crucial to enable intelligent robots to use objects of\ndaily living. Traditional supervised learning methods for affordance\nsegmentation require costly pixel-level annotations, while weakly supervised\napproaches, though less demanding, still rely on object-interaction examples\nand support a closed set of actions. These limitations hinder scalability, may\nintroduce biases, and usually restrict models to a limited set of predefined\nactions. This paper proposes AffordanceCLIP, to overcome these limitations by\nleveraging the implicit affordance knowledge embedded within large pre-trained\nVision-Language models like CLIP. We experimentally demonstrate that CLIP,\nalthough not explicitly trained for affordances detection, retains valuable\ninformation for the task. Our AffordanceCLIP achieves competitive zero-shot\nperformance compared to methods with specialized training, while offering\nseveral advantages: i) it works with any action prompt, not just a predefined\nset; ii) it requires training only a small number of additional parameters\ncompared to existing solutions and iii) eliminates the need for direct\nsupervision on action-object pairs, opening new perspectives for\nfunctionality-based reasoning of models.\n","authors":["Claudia Cuttano","Gabriele Rosi","Gabriele Trivigno","Giuseppe Averta"],"pdf_url":"https://arxiv.org/pdf/2404.12015v1.pdf","comment":"Accepted to MAR Workshop at CVPR2024"},{"id":"http://arxiv.org/abs/2311.09104v2","updated":"2024-04-18T09:03:04Z","published":"2023-11-15T16:51:18Z","title":"Cross-view and Cross-pose Completion for 3D Human Understanding","summary":"  Human perception and understanding is a major domain of computer vision\nwhich, like many other vision subdomains recently, stands to gain from the use\nof large models pre-trained on large datasets. We hypothesize that the most\ncommon pre-training strategy of relying on general purpose, object-centric\nimage datasets such as ImageNet, is limited by an important domain shift. On\nthe other hand, collecting domain-specific ground truth such as 2D or 3D labels\ndoes not scale well. Therefore, we propose a pre-training approach based on\nself-supervised learning that works on human-centric data using only images.\nOur method uses pairs of images of humans: the first is partially masked and\nthe model is trained to reconstruct the masked parts given the visible ones and\na second image. It relies on both stereoscopic (cross-view) pairs, and temporal\n(cross-pose) pairs taken from videos, in order to learn priors about 3D as well\nas human motion. We pre-train a model for body-centric tasks and one for\nhand-centric tasks. With a generic transformer architecture, these models\noutperform existing self-supervised pre-training methods on a wide set of\nhuman-centric downstream tasks, and obtain state-of-the-art performance for\ninstance when fine-tuning for model-based and model-free human mesh recovery.\n","authors":["Matthieu Armando","Salma Galaaoui","Fabien Baradel","Thomas Lucas","Vincent Leroy","Romain Br√©gier","Philippe Weinzaepfel","Gr√©gory Rogez"],"pdf_url":"https://arxiv.org/pdf/2311.09104v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2303.12307v4","updated":"2024-04-18T08:54:01Z","published":"2023-03-22T04:49:23Z","title":"Predicting and Enhancing the Fairness of DNNs with the Curvature of\n  Perceptual Manifolds","summary":"  To address the challenges of long-tailed classification, researchers have\nproposed several approaches to reduce model bias, most of which assume that\nclasses with few samples are weak classes. However, recent studies have shown\nthat tail classes are not always hard to learn, and model bias has been\nobserved on sample-balanced datasets, suggesting the existence of other factors\nthat affect model bias. In this work, we first establish a geometric\nperspective for analyzing model fairness and then systematically propose a\nseries of geometric measurements for perceptual manifolds in deep neural\nnetworks. Subsequently, we comprehensively explore the effect of the geometric\ncharacteristics of perceptual manifolds on classification difficulty and how\nlearning shapes the geometric characteristics of perceptual manifolds. An\nunanticipated finding is that the correlation between the class accuracy and\nthe separation degree of perceptual manifolds gradually decreases during\ntraining, while the negative correlation with the curvature gradually\nincreases, implying that curvature imbalance leads to model bias.Building upon\nthese observations, we propose curvature regularization to facilitate the model\nto learn curvature-balanced and flatter perceptual manifolds. Evaluations on\nmultiple long-tailed and non-long-tailed datasets show the excellent\nperformance and exciting generality of our approach, especially in achieving\nsignificant performance improvements based on current state-of-the-art\ntechniques. Our work opens up a geometric analysis perspective on model bias\nand reminds researchers to pay attention to model bias on non-long-tailed and\neven sample-balanced datasets.\n","authors":["Yanbiao Ma","Licheng Jiao","Fang Liu","Maoji Wen","Lingling Li","Wenping Ma","Shuyuan Yang","Xu Liu","Puhua Chen"],"pdf_url":"https://arxiv.org/pdf/2303.12307v4.pdf","comment":"17pages, Accepted by CVPR 2023, Submitted to TPAMI"},{"id":"http://arxiv.org/abs/2311.09590v2","updated":"2024-04-18T08:49:03Z","published":"2023-11-16T06:02:03Z","title":"MARformer: An Efficient Metal Artifact Reduction Transformer for Dental\n  CBCT Images","summary":"  Cone Beam Computed Tomography (CBCT) plays a key role in dental diagnosis and\nsurgery. However, the metal teeth implants could bring annoying metal artifacts\nduring the CBCT imaging process, interfering diagnosis and downstream\nprocessing such as tooth segmentation. In this paper, we develop an efficient\nTransformer to perform metal artifacts reduction (MAR) from dental CBCT images.\nThe proposed MAR Transformer (MARformer) reduces computation complexity in the\nmultihead self-attention by a new Dimension-Reduced Self-Attention (DRSA)\nmodule, based on that the CBCT images have globally similar structure. A\nPatch-wise Perceptive Feed Forward Network (P2FFN) is also proposed to perceive\nlocal image information for fine-grained restoration. Experimental results on\nCBCT images with synthetic and real-world metal artifacts show that our\nMARformer is efficient and outperforms previous MAR methods and two restoration\nTransformers.\n","authors":["Yuxuan Shi","Jun Xu","Dinggang Shen"],"pdf_url":"https://arxiv.org/pdf/2311.09590v2.pdf","comment":"under consideration of Computer Vision and Image Understanding\n  journal"},{"id":"http://arxiv.org/abs/2404.11998v1","updated":"2024-04-18T08:46:12Z","published":"2024-04-18T08:46:12Z","title":"Curriculum Point Prompting for Weakly-Supervised Referring Image\n  Segmentation","summary":"  Referring image segmentation (RIS) aims to precisely segment referents in\nimages through corresponding natural language expressions, yet relying on\ncost-intensive mask annotations. Weakly supervised RIS thus learns from\nimage-text pairs to pixel-level semantics, which is challenging for segmenting\nfine-grained masks. A natural approach to enhancing segmentation precision is\nto empower weakly supervised RIS with the image segmentation foundation model\nSAM. Nevertheless, we observe that simply integrating SAM yields limited\nbenefits and can even lead to performance regression due to the inevitable\nnoise issues and challenges in excessive focus on object parts. In this paper,\nwe present an innovative framework, Point PrompTing (PPT), incorporated with\nthe proposed multi-source curriculum learning strategy to address these\nchallenges. Specifically, the core of PPT is a point generator that not only\nharnesses CLIP's text-image alignment capability and SAM's powerful mask\ngeneration ability but also generates negative point prompts to address the\nnoisy and excessive focus issues inherently and effectively. In addition, we\nintroduce a curriculum learning strategy with object-centric images to help PPT\ngradually learn from simpler yet precise semantic alignment to more complex\nRIS. Experiments demonstrate that our PPT significantly and consistently\noutperforms prior weakly supervised techniques on mIoU by 11.34%, 14.14%, and\n6.97% across RefCOCO, RefCOCO+, and G-Ref, respectively.\n","authors":["Qiyuan Dai","Sibei Yang"],"pdf_url":"https://arxiv.org/pdf/2404.11998v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.15182v2","updated":"2024-04-18T08:40:58Z","published":"2024-03-22T13:11:26Z","title":"PDE-CNNs: Axiomatic Derivations and Applications","summary":"  PDE-based Group Convolutional Neural Networks (PDE-G-CNNs) utilize solvers of\ngeometrically meaningful evolution PDEs as substitutes for the conventional\ncomponents in G-CNNs. PDE-G-CNNs offer several key benefits all at once: fewer\nparameters, inherent equivariance, better performance, data efficiency, and\ngeometric interpretability.\n  In this article we focus on Euclidean equivariant PDE-G-CNNs where the\nfeature maps are two dimensional throughout. We call this variant of the\nframework a PDE-CNN.\n  From a machine learning perspective, we list several practically desirable\naxioms and derive from these which PDEs should be used in a PDE-CNN. Here our\napproach to geometric learning via PDEs is inspired by the axioms of classical\nlinear and morphological scale-space theory, which we generalize by introducing\nsemifield-valued signals.\n  Furthermore, we experimentally confirm for small networks that PDE-CNNs offer\nfewer parameters, increased performance, and better data efficiency when\ncompared to CNNs. We also investigate what effect the use of different\nsemifields has on the performance of the models.\n","authors":["Gijs Bellaard","Sei Sakata","Bart M. N. Smets","Remco Duits"],"pdf_url":"https://arxiv.org/pdf/2403.15182v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04265v5","updated":"2024-04-18T08:33:37Z","published":"2023-12-07T12:43:00Z","title":"Stronger, Fewer, & Superior: Harnessing Vision Foundation Models for\n  Domain Generalized Semantic Segmentation","summary":"  In this paper, we first assess and harness various Vision Foundation Models\n(VFMs) in the context of Domain Generalized Semantic Segmentation (DGSS).\nDriven by the motivation that Leveraging Stronger pre-trained models and Fewer\ntrainable parameters for Superior generalizability, we introduce a robust\nfine-tuning approach, namely Rein, to parameter-efficiently harness VFMs for\nDGSS. Built upon a set of trainable tokens, each linked to distinct instances,\nRein precisely refines and forwards the feature maps from each layer to the\nnext layer within the backbone. This process produces diverse refinements for\ndifferent categories within a single image. With fewer trainable parameters,\nRein efficiently fine-tunes VFMs for DGSS tasks, surprisingly surpassing full\nparameter fine-tuning. Extensive experiments across various settings\ndemonstrate that Rein significantly outperforms state-of-the-art methods.\nRemarkably, with just an extra 1% of trainable parameters within the frozen\nbackbone, Rein achieves a mIoU of 78.4% on the Cityscapes, without accessing\nany real urban-scene datasets.Code is available at\nhttps://github.com/w1oves/Rein.git.\n","authors":["Zhixiang Wei","Lin Chen","Yi Jin","Xiaoxiao Ma","Tianle Liu","Pengyang Ling","Ben Wang","Huaian Chen","Jinjin Zheng"],"pdf_url":"https://arxiv.org/pdf/2312.04265v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05317v4","updated":"2024-04-18T08:29:48Z","published":"2024-04-08T09:08:43Z","title":"WebXR, A-Frame and Networked-Aframe as a Basis for an Open Metaverse: A\n  Conceptual Architecture","summary":"  This work proposes a WebXR-based cross-platform conceptual architecture,\nleveraging the A-Frame and Networked-Aframe frameworks, in order to facilitate\nthe development of an open, accessible, and interoperable metaverse. By\nintroducing the concept of spatial web app, this research contributes to the\ndiscourse on the metaverse, offering an architecture that democratizes access\nto virtual environments and extended reality through the web, and aligns with\nTim Berners-Lee's original vision of the World Wide Web as an open platform in\nthe digital realm.\n","authors":["Giuseppe Macario"],"pdf_url":"https://arxiv.org/pdf/2404.05317v4.pdf","comment":"minor fixes/rephrasing"},{"id":"http://arxiv.org/abs/2404.11987v1","updated":"2024-04-18T08:29:29Z","published":"2024-04-18T08:29:29Z","title":"MultiPhys: Multi-Person Physics-aware 3D Motion Estimation","summary":"  We introduce MultiPhys, a method designed for recovering multi-person motion\nfrom monocular videos. Our focus lies in capturing coherent spatial placement\nbetween pairs of individuals across varying degrees of engagement. MultiPhys,\nbeing physically aware, exhibits robustness to jittering and occlusions, and\neffectively eliminates penetration issues between the two individuals. We\ndevise a pipeline in which the motion estimated by a kinematic-based method is\nfed into a physics simulator in an autoregressive manner. We introduce distinct\ncomponents that enable our model to harness the simulator's properties without\ncompromising the accuracy of the kinematic estimates. This results in final\nmotion estimates that are both kinematically coherent and physically compliant.\nExtensive evaluations on three challenging datasets characterized by\nsubstantial inter-person interaction show that our method significantly reduces\nerrors associated with penetration and foot skating, while performing\ncompetitively with the state-of-the-art on motion accuracy and smoothness.\nResults and code can be found on our project page\n(http://www.iri.upc.edu/people/nugrinovic/multiphys/).\n","authors":["Nicolas Ugrinovic","Boxiao Pan","Georgios Pavlakos","Despoina Paschalidou","Bokui Shen","Jordi Sanchez-Riera","Francesc Moreno-Noguer","Leonidas Guibas"],"pdf_url":"https://arxiv.org/pdf/2404.11987v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11981v1","updated":"2024-04-18T08:23:24Z","published":"2024-04-18T08:23:24Z","title":"Tendency-driven Mutual Exclusivity for Weakly Supervised Incremental\n  Semantic Segmentation","summary":"  Weakly Incremental Learning for Semantic Segmentation (WILSS) leverages a\npre-trained segmentation model to segment new classes using cost-effective and\nreadily available image-level labels. A prevailing way to solve WILSS is the\ngeneration of seed areas for each new class, serving as a form of pixel-level\nsupervision. However, a scenario usually arises where a pixel is concurrently\npredicted as an old class by the pre-trained segmentation model and a new class\nby the seed areas. Such a scenario becomes particularly problematic in WILSS,\nas the lack of pixel-level annotations on new classes makes it intractable to\nascertain whether the pixel pertains to the new class or not. To surmount this\nissue, we propose an innovative, tendency-driven relationship of mutual\nexclusivity, meticulously tailored to govern the behavior of the seed areas and\nthe predictions generated by the pre-trained segmentation model. This\nrelationship stipulates that predictions for the new and old classes must not\nconflict whilst prioritizing the preservation of predictions for the old\nclasses, which not only addresses the conflicting prediction issue but also\neffectively mitigates the inherent challenge of incremental learning -\ncatastrophic forgetting. Furthermore, under the auspices of this\ntendency-driven mutual exclusivity relationship, we generate pseudo masks for\nthe new classes, allowing for concurrent execution with model parameter\nupdating via the resolution of a bi-level optimization problem. Extensive\nexperiments substantiate the effectiveness of our framework, resulting in the\nestablishment of new benchmarks and paving the way for further research in this\nfield.\n","authors":["Chongjie Si","Xuehui Wang","Xiaokang Yang","Wei Shen"],"pdf_url":"https://arxiv.org/pdf/2404.11981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11525v2","updated":"2024-04-18T08:23:05Z","published":"2024-04-17T16:16:12Z","title":"JointViT: Modeling Oxygen Saturation Levels with Joint Supervision on\n  Long-Tailed OCTA","summary":"  The oxygen saturation level in the blood (SaO2) is crucial for health,\nparticularly in relation to sleep-related breathing disorders. However,\ncontinuous monitoring of SaO2 is time-consuming and highly variable depending\non patients' conditions. Recently, optical coherence tomography angiography\n(OCTA) has shown promising development in rapidly and effectively screening\neye-related lesions, offering the potential for diagnosing sleep-related\ndisorders. To bridge this gap, our paper presents three key contributions.\nFirstly, we propose JointViT, a novel model based on the Vision Transformer\narchitecture, incorporating a joint loss function for supervision. Secondly, we\nintroduce a balancing augmentation technique during data preprocessing to\nimprove the model's performance, particularly on the long-tail distribution\nwithin the OCTA dataset. Lastly, through comprehensive experiments on the OCTA\ndataset, our proposed method significantly outperforms other state-of-the-art\nmethods, achieving improvements of up to 12.28% in overall accuracy. This\nadvancement lays the groundwork for the future utilization of OCTA in\ndiagnosing sleep-related disorders. See project website\nhttps://steve-zeyu-zhang.github.io/JointViT\n","authors":["Zeyu Zhang","Xuyin Qi","Mingxi Chen","Guangxi Li","Ryan Pham","Ayub Qassim","Ella Berry","Zhibin Liao","Owen Siggs","Robert Mclaughlin","Jamie Craig","Minh-Son To"],"pdf_url":"https://arxiv.org/pdf/2404.11525v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11979v1","updated":"2024-04-18T08:16:56Z","published":"2024-04-18T08:16:56Z","title":"MTGA: Multi-view Temporal Granularity aligned Aggregation for\n  Event-based Lip-reading","summary":"  Lip-reading is to utilize the visual information of the speaker's lip\nmovements to recognize words and sentences. Existing event-based lip-reading\nsolutions integrate different frame rate branches to learn spatio-temporal\nfeatures of varying granularities. However, aggregating events into event\nframes inevitably leads to the loss of fine-grained temporal information within\nframes. To remedy this drawback, we propose a novel framework termed Multi-view\nTemporal Granularity aligned Aggregation (MTGA). Specifically, we first present\na novel event representation method, namely time-segmented voxel graph list,\nwhere the most significant local voxels are temporally connected into a graph\nlist. Then we design a spatio-temporal fusion module based on temporal\ngranularity alignment, where the global spatial features extracted from event\nframes, together with the local relative spatial and temporal features\ncontained in voxel graph list are effectively aligned and integrated. Finally,\nwe design a temporal aggregation module that incorporates positional encoding,\nwhich enables the capture of local absolute spatial and global temporal\ninformation. Experiments demonstrate that our method outperforms both the\nevent-based and video-based lip-reading counterparts. Our code will be publicly\navailable.\n","authors":["Wenhao Zhang","Jun Wang","Yong Luo","Lei Yu","Wei Yu","Zheng He"],"pdf_url":"https://arxiv.org/pdf/2404.11979v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06244v2","updated":"2024-04-18T08:08:45Z","published":"2024-02-09T08:33:48Z","title":"Quantifying and Enhancing Multi-modal Robustness with Modality\n  Preference","summary":"  Multi-modal models have shown a promising capability to effectively integrate\ninformation from various sources, yet meanwhile, they are found vulnerable to\npervasive perturbations, such as uni-modal attacks and missing conditions. To\ncounter these perturbations, robust multi-modal representations are highly\nexpected, which are positioned well away from the discriminative multi-modal\ndecision boundary. In this paper, different from conventional empirical\nstudies, we focus on a commonly used joint multi-modal framework and\ntheoretically discover that larger uni-modal representation margins and more\nreliable integration for modalities are essential components for achieving\nhigher robustness. This discovery can further explain the limitation of\nmulti-modal robustness and the phenomenon that multi-modal models are often\nvulnerable to attacks on the specific modality. Moreover, our analysis reveals\nhow the widespread issue, that the model has different preferences for\nmodalities, limits the multi-modal robustness by influencing the essential\ncomponents and could lead to attacks on the specific modality highly effective.\nInspired by our theoretical finding, we introduce a training procedure called\nCertifiable Robust Multi-modal Training (CRMT), which can alleviate this\ninfluence from modality preference and explicitly regulate essential components\nto significantly improve robustness in a certifiable manner. Our method\ndemonstrates substantial improvements in performance and robustness compared\nwith existing methods. Furthermore, our training procedure can be easily\nextended to enhance other robust training strategies, highlighting its\ncredibility and flexibility.\n","authors":["Zequn Yang","Yake Wei","Ce Liang","Di Hu"],"pdf_url":"https://arxiv.org/pdf/2402.06244v2.pdf","comment":"Accepted to ICLR 2024"},{"id":"http://arxiv.org/abs/2404.11974v1","updated":"2024-04-18T08:05:23Z","published":"2024-04-18T08:05:23Z","title":"Device (In)Dependence of Deep Learning-based Image Age Approximation","summary":"  The goal of temporal image forensic is to approximate the age of a digital\nimage relative to images from the same device. Usually, this is based on traces\nleft during the image acquisition pipeline. For example, several methods exist\nthat exploit the presence of in-field sensor defects for this purpose. In\naddition to these 'classical' methods, there is also an approach in which a\nConvolutional Neural Network (CNN) is trained to approximate the image age. One\nadvantage of a CNN is that it independently learns the age features used. This\nwould make it possible to exploit other (different) age traces in addition to\nthe known ones (i.e., in-field sensor defects). In a previous work, we have\nshown that the presence of strong in-field sensor defects is irrelevant for a\nCNN to predict the age class. Based on this observation, the question arises\nhow device (in)dependent the learned features are. In this work, we empirically\nasses this by training a network on images from a single device and then apply\nthe trained model to images from different devices. This evaluation is\nperformed on 14 different devices, including 10 devices from the publicly\navailable 'Northumbria Temporal Image Forensics' database. These 10 different\ndevices are based on five different device pairs (i.e., with the identical\ncamera model).\n","authors":["Robert J√∂chl","Andreas Uhl"],"pdf_url":"https://arxiv.org/pdf/2404.11974v1.pdf","comment":"This work was accepted and presented in: 2022 ICPR-Workshop on\n  Artificial Intelligence for Multimedia Forensics and Disinformation\n  Detection. Montreal, Quebec, Canada. However, due to a technical issue on the\n  publishing companies' side, the work does not appear in the workshop\n  proceedings"},{"id":"http://arxiv.org/abs/2305.00220v2","updated":"2024-04-18T08:01:26Z","published":"2023-04-29T10:10:25Z","title":"Relaxed forced choice improves performance of visual quality assessment\n  methods","summary":"  In image quality assessment, a collective visual quality score for an image\nor video is obtained from the individual ratings of many subjects. One commonly\nused format for these experiments is the two-alternative forced choice method.\nTwo stimuli with the same content but differing visual quality are presented\nsequentially or side-by-side. Subjects are asked to select the one of better\nquality, and when uncertain, they are required to guess. The relaxed\nalternative forced choice format aims to reduce the cognitive load and the\nnoise in the responses due to the guessing by providing a third response\noption, namely, ``not sure''. This work presents a large and comprehensive\ncrowdsourcing experiment to compare these two response formats: the one with\nthe ``not sure'' option and the one without it. To provide unambiguous ground\ntruth for quality evaluation, subjects were shown pairs of images with\ndiffering numbers of dots and asked each time to choose the one with more dots.\nOur crowdsourcing study involved 254 participants and was conducted using a\nwithin-subject design. Each participant was asked to respond to 40 pair\ncomparisons with and without the ``not sure'' response option and completed a\nquestionnaire to evaluate their cognitive load for each testing condition. The\nexperimental results show that the inclusion of the ``not sure'' response\noption in the forced choice method reduced mental load and led to models with\nbetter data fit and correspondence to ground truth. We also tested for the\nequivalence of the models and found that they were different. The dataset is\navailable at http://database.mmsp-kn.de/cogvqa-database.html.\n","authors":["Mohsen Jenadeleh","Johannes Zagermann","Harald Reiterer","Ulf-Dietrich Reips","Raouf Hamzaoui","Dietmar Saupe"],"pdf_url":"https://arxiv.org/pdf/2305.00220v2.pdf","comment":"6 pages, 3 figures, accepted at the 2023 15th International\n  Conference on Quality of Multimedia Experience (QoMEX). Database is publicly\n  accessible at http://database.mmsp-kn.de/cogvqa-database.html"},{"id":"http://arxiv.org/abs/2404.11962v1","updated":"2024-04-18T07:48:00Z","published":"2024-04-18T07:48:00Z","title":"¬©Plug-in Authorization for Human Content Copyright Protection\n  in Text-to-Image Model","summary":"  This paper addresses the contentious issue of copyright infringement in\nimages generated by text-to-image models, sparking debates among AI developers,\ncontent creators, and legal entities. State-of-the-art models create\nhigh-quality content without crediting original creators, causing concern in\nthe artistic community. To mitigate this, we propose the \\copyright Plug-in\nAuthorization framework, introducing three operations: addition, extraction,\nand combination. Addition involves training a \\copyright plug-in for specific\ncopyright, facilitating proper credit attribution. Extraction allows creators\nto reclaim copyright from infringing models, and combination enables users to\nmerge different \\copyright plug-ins. These operations act as permits,\nincentivizing fair use and providing flexibility in authorization. We present\ninnovative approaches,\"Reverse LoRA\" for extraction and \"EasyMerge\" for\nseamless combination. Experiments in artist-style replication and cartoon IP\nrecreation demonstrate \\copyright plug-ins' effectiveness, offering a valuable\nsolution for human copyright protection in the age of generative AIs.\n","authors":["Chao Zhou","Huishuai Zhang","Jiang Bian","Weiming Zhang","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2404.11962v1.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2311.01188v2","updated":"2024-04-18T07:42:19Z","published":"2023-11-02T12:34:23Z","title":"Terrain-Informed Self-Supervised Learning: Enhancing Building Footprint\n  Extraction from LiDAR Data with Limited Annotations","summary":"  Estimating building footprint maps from geospatial data is of paramount\nimportance in urban planning, development, disaster management, and various\nother applications. Deep learning methodologies have gained prominence in\nbuilding segmentation maps, offering the promise of precise footprint\nextraction without extensive post-processing. However, these methods face\nchallenges in generalization and label efficiency, particularly in remote\nsensing, where obtaining accurate labels can be both expensive and\ntime-consuming. To address these challenges, we propose terrain-aware\nself-supervised learning, tailored to remote sensing, using digital elevation\nmodels from LiDAR data. We propose to learn a model to differentiate between\nbare Earth and superimposed structures enabling the network to implicitly learn\ndomain-relevant features without the need for extensive pixel-level\nannotations. We test the effectiveness of our approach by evaluating building\nsegmentation performance on test datasets with varying label fractions.\nRemarkably, with only 1% of the labels (equivalent to 25 labeled examples), our\nmethod improves over ImageNet pre-training, showing the advantage of leveraging\nunlabeled data for feature extraction in the domain of remote sensing. The\nperformance improvement is more pronounced in few-shot scenarios and gradually\ncloses the gap with ImageNet pre-training as the label fraction increases. We\ntest on a dataset characterized by substantial distribution shifts and labeling\nerrors to demonstrate the generalizability of our approach. When compared to\nother baselines, including ImageNet pretraining and more complex architectures,\nour approach consistently performs better, demonstrating the efficiency and\neffectiveness of self-supervised terrain-aware feature learning.\n","authors":["Anuja Vats","David V√∂lgyes","Martijn Vermeer","Marius Pedersen","Kiran Raja","Daniele S. M. Fantin","Jacob Alexander Hay"],"pdf_url":"https://arxiv.org/pdf/2311.01188v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11459v2","updated":"2024-04-18T07:32:52Z","published":"2024-04-17T15:07:06Z","title":"Octopus v3: Technical Report for On-device Sub-billion Multimodal AI\n  Agent","summary":"  A multimodal AI agent is characterized by its ability to process and learn\nfrom various types of data, including natural language, visual, and audio\ninputs, to inform its actions. Despite advancements in large language models\nthat incorporate visual data, such as GPT-4V, effectively translating\nimage-based data into actionable outcomes for AI agents continues to be\nchallenging. In this paper, we introduce a multimodal model that incorporates\nthe concept of functional token specifically designed for AI agent\napplications. To ensure compatibility with edge devices, our model is optimized\nto a compact size of less than 1B parameters. Like GPT-4, our model can process\nboth English and Chinese. We demonstrate that this model is capable of\noperating efficiently on a wide range of edge devices, including as constrained\nas a Raspberry Pi.\n","authors":["Wei Chen","Zhiyuan Li"],"pdf_url":"https://arxiv.org/pdf/2404.11459v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11958v1","updated":"2024-04-18T07:25:59Z","published":"2024-04-18T07:25:59Z","title":"Not All Voxels Are Equal: Hardness-Aware Semantic Scene Completion with\n  Self-Distillation","summary":"  Semantic scene completion, also known as semantic occupancy prediction, can\nprovide dense geometric and semantic information for autonomous vehicles, which\nattracts the increasing attention of both academia and industry. Unfortunately,\nexisting methods usually formulate this task as a voxel-wise classification\nproblem and treat each voxel equally in 3D space during training. As the hard\nvoxels have not been paid enough attention, the performance in some challenging\nregions is limited. The 3D dense space typically contains a large number of\nempty voxels, which are easy to learn but require amounts of computation due to\nhandling all the voxels uniformly for the existing models. Furthermore, the\nvoxels in the boundary region are more challenging to differentiate than those\nin the interior. In this paper, we propose HASSC approach to train the semantic\nscene completion model with hardness-aware design. The global hardness from the\nnetwork optimization process is defined for dynamical hard voxel selection.\nThen, the local hardness with geometric anisotropy is adopted for voxel-wise\nrefinement. Besides, self-distillation strategy is introduced to make training\nprocess stable and consistent. Extensive experiments show that our HASSC scheme\ncan effectively promote the accuracy of the baseline model without incurring\nthe extra inference cost. Source code is available at:\nhttps://github.com/songw-zju/HASSC.\n","authors":["Song Wang","Jiawei Yu","Wentong Li","Wenyu Liu","Xiaolu Liu","Junbo Chen","Jianke Zhu"],"pdf_url":"https://arxiv.org/pdf/2404.11958v1.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2404.11957v1","updated":"2024-04-18T07:22:38Z","published":"2024-04-18T07:22:38Z","title":"The devil is in the object boundary: towards annotation-free instance\n  segmentation using Foundation Models","summary":"  Foundation models, pre-trained on a large amount of data have demonstrated\nimpressive zero-shot capabilities in various downstream tasks. However, in\nobject detection and instance segmentation, two fundamental computer vision\ntasks heavily reliant on extensive human annotations, foundation models such as\nSAM and DINO struggle to achieve satisfactory performance. In this study, we\nreveal that the devil is in the object boundary, \\textit{i.e.}, these\nfoundation models fail to discern boundaries between individual objects. For\nthe first time, we probe that CLIP, which has never accessed any instance-level\nannotations, can provide a highly beneficial and strong instance-level boundary\nprior in the clustering results of its particular intermediate layer. Following\nthis surprising observation, we propose $\\textbf{Zip}$ which $\\textbf{Z}$ips up\nCL$\\textbf{ip}$ and SAM in a novel classification-first-then-discovery\npipeline, enabling annotation-free, complex-scene-capable, open-vocabulary\nobject detection and instance segmentation. Our Zip significantly boosts SAM's\nmask AP on COCO dataset by 12.5% and establishes state-of-the-art performance\nin various settings, including training-free, self-training, and\nlabel-efficient finetuning. Furthermore, annotation-free Zip even achieves\ncomparable performance to the best-performing open-vocabulary object detecters\nusing base annotations. Code is released at\nhttps://github.com/ChengShiest/Zip-Your-CLIP\n","authors":["Cheng Shi","Sibei Yang"],"pdf_url":"https://arxiv.org/pdf/2404.11957v1.pdf","comment":"ICLR2024, Code is released at\n  https://github.com/ChengShiest/Zip-Your-CLIP"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2404.12366v1","updated":"2024-04-18T17:49:02Z","published":"2024-04-18T17:49:02Z","title":"Accounting for AI and Users Shaping One Another: The Role of\n  Mathematical Models","summary":"  As AI systems enter into a growing number of societal domains, these systems\nincreasingly shape and are shaped by user preferences, opinions, and behaviors.\nHowever, the design of AI systems rarely accounts for how AI and users shape\none another. In this position paper, we argue for the development of formal\ninteraction models which mathematically specify how AI and users shape one\nanother. Formal interaction models can be leveraged to (1) specify interactions\nfor implementation, (2) monitor interactions through empirical analysis, (3)\nanticipate societal impacts via counterfactual analysis, and (4) control\nsocietal impacts via interventions. The design space of formal interaction\nmodels is vast, and model design requires careful consideration of factors such\nas style, granularity, mathematical complexity, and measurability. Using\ncontent recommender systems as a case study, we critically examine the nascent\nliterature of formal interaction models with respect to these use-cases and\ndesign axes. More broadly, we call for the community to leverage formal\ninteraction models when designing, evaluating, or auditing any AI system which\ninteracts with users.\n","authors":["Sarah Dean","Evan Dong","Meena Jagadeesan","Liu Leqi"],"pdf_url":"https://arxiv.org/pdf/2404.12366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12365v1","updated":"2024-04-18T17:48:05Z","published":"2024-04-18T17:48:05Z","title":"When LLMs are Unfit Use FastFit: Fast and Effective Text Classification\n  with Many Classes","summary":"  We present FastFit, a method, and a Python package design to provide fast and\naccurate few-shot classification, especially for scenarios with many\nsemantically similar classes. FastFit utilizes a novel approach integrating\nbatch contrastive learning and token-level similarity score. Compared to\nexisting few-shot learning packages, such as SetFit, Transformers, or few-shot\nprompting of large language models via API calls, FastFit significantly\nimproves multiclass classification performance in speed and accuracy across\nFewMany, our newly curated English benchmark, and Multilingual datasets.\nFastFit demonstrates a 3-20x improvement in training speed, completing training\nin just a few seconds. The FastFit package is now available on GitHub and PyPi,\npresenting a user-friendly solution for NLP practitioners.\n","authors":["Asaf Yehudai","Elron Bendel"],"pdf_url":"https://arxiv.org/pdf/2404.12365v1.pdf","comment":"Accepted to NAACL"},{"id":"http://arxiv.org/abs/2404.12309v1","updated":"2024-04-18T16:38:02Z","published":"2024-04-18T16:38:02Z","title":"iRAG: An Incremental Retrieval Augmented Generation System for Videos","summary":"  Retrieval augmented generation (RAG) systems combine the strengths of\nlanguage generation and information retrieval to power many real-world\napplications like chatbots. Use of RAG for combined understanding of multimodal\ndata such as text, images and videos is appealing but two critical limitations\nexist: one-time, upfront capture of all content in large multimodal data as\ntext descriptions entails high processing times, and not all information in the\nrich multimodal data is typically in the text descriptions. Since the user\nqueries are not known apriori, developing a system for multimodal to text\nconversion and interactive querying of multimodal data is challenging.\n  To address these limitations, we propose iRAG, which augments RAG with a\nnovel incremental workflow to enable interactive querying of large corpus of\nmultimodal data. Unlike traditional RAG, iRAG quickly indexes large\nrepositories of multimodal data, and in the incremental workflow, it uses the\nindex to opportunistically extract more details from select portions of the\nmultimodal data to retrieve context relevant to an interactive user query. Such\nan incremental workflow avoids long multimodal to text conversion times,\novercomes information loss issues by doing on-demand query-specific extraction\nof details in multimodal data, and ensures high quality of responses to\ninteractive user queries that are often not known apriori. To the best of our\nknowledge, iRAG is the first system to augment RAG with an incremental workflow\nto support efficient interactive querying of large, real-world multimodal data.\nExperimental results on real-world long videos demonstrate 23x to 25x faster\nvideo to text ingestion, while ensuring that quality of responses to\ninteractive user queries is comparable to responses from a traditional RAG\nwhere all video data is converted to text upfront before any querying.\n","authors":["Md Adnan Arefeen","Biplob Debnath","Md Yusuf Sarwar Uddin","Srimat Chakradhar"],"pdf_url":"https://arxiv.org/pdf/2404.12309v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.11527v3","updated":"2024-04-18T16:20:19Z","published":"2023-05-19T08:51:11Z","title":"InstructIE: A Bilingual Instruction-based Information Extraction Dataset","summary":"  Large language models can perform well on general natural language tasks, but\ntheir effectiveness is still not optimal for information extraction. Recent\nworks indicate that the main reason lies in the lack of extensive data on\ninformation extraction instructions. Note that the existing datasets on\ninformation extraction instructions not only have limited coverage but also\ninvolve high construction costs. To address this issue, we introduce\nInstructIE, a bilingual instruction-based information extraction dataset, which\ncovers 12 diverse domains. Specifically, we propose KG2Instruction, a framework\nspecifically for the automatic generation of such datasets. Experimental\nresults demonstrate that large language models trained with InstructIE can not\nonly obtain better information extraction capabilities but also enhance\nzero-shot performance compared with baselines.\n","authors":["Honghao Gui","Shuofei Qiao","Jintian Zhang","Hongbin Ye","Mengshu Sun","Lei Liang","Jeff Z. Pan","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.11527v3.pdf","comment":"Work in progress; project homepage:\n  https://www.zjukg.org/project/InstructIE/ dataset:\n  https://huggingface.co/datasets/zjunlp/InstructIE"},{"id":"http://arxiv.org/abs/2404.12237v1","updated":"2024-04-18T14:51:55Z","published":"2024-04-18T14:51:55Z","title":"De-DSI: Decentralised Differentiable Search Index","summary":"  This study introduces De-DSI, a novel framework that fuses large language\nmodels (LLMs) with genuine decentralization for information retrieval,\nparticularly employing the differentiable search index (DSI) concept in a\ndecentralized setting. Focused on efficiently connecting novel user queries\nwith document identifiers without direct document access, De-DSI operates\nsolely on query-docid pairs. To enhance scalability, an ensemble of DSI models\nis introduced, where the dataset is partitioned into smaller shards for\nindividual model training. This approach not only maintains accuracy by\nreducing the number of data each model needs to handle but also facilitates\nscalability by aggregating outcomes from multiple models. This aggregation uses\na beam search to identify top docids and applies a softmax function for score\nnormalization, selecting documents with the highest scores for retrieval. The\ndecentralized implementation demonstrates that retrieval success is comparable\nto centralized methods, with the added benefit of the possibility of\ndistributing computational complexity across the network. This setup also\nallows for the retrieval of multimedia items through magnet links, eliminating\nthe need for platforms or intermediaries.\n","authors":["Petru Neague","Marcel Gregoriadis","Johan Pouwelse"],"pdf_url":"https://arxiv.org/pdf/2404.12237v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2404.12391v1","updated":"2024-04-18T17:59:58Z","published":"2024-04-18T17:59:58Z","title":"On the Content Bias in Fr√©chet Video Distance","summary":"  Fr\\'echet Video Distance (FVD), a prominent metric for evaluating video\ngeneration models, is known to conflict with human perception occasionally. In\nthis paper, we aim to explore the extent of FVD's bias toward per-frame quality\nover temporal realism and identify its sources. We first quantify the FVD's\nsensitivity to the temporal axis by decoupling the frame and motion quality and\nfind that the FVD increases only slightly with large temporal corruption. We\nthen analyze the generated videos and show that via careful sampling from a\nlarge set of generated videos that do not contain motions, one can drastically\ndecrease FVD without improving the temporal quality. Both studies suggest FVD's\nbias towards the quality of individual frames. We further observe that the bias\ncan be attributed to the features extracted from a supervised video classifier\ntrained on the content-biased dataset. We show that FVD with features extracted\nfrom the recent large-scale self-supervised video models is less biased toward\nimage quality. Finally, we revisit a few real-world examples to validate our\nhypothesis.\n","authors":["Songwei Ge","Aniruddha Mahapatra","Gaurav Parmar","Jun-Yan Zhu","Jia-Bin Huang"],"pdf_url":"https://arxiv.org/pdf/2404.12391v1.pdf","comment":"CVPR 2024. Project webpage: https://content-debiased-fvd.github.io/"},{"id":"http://arxiv.org/abs/2404.01300v2","updated":"2024-04-18T17:59:57Z","published":"2024-04-01T17:59:55Z","title":"NeRF-MAE: Masked AutoEncoders for Self-Supervised 3D Representation\n  Learning for Neural Radiance Fields","summary":"  Neural fields excel in computer vision and robotics due to their ability to\nunderstand the 3D visual world such as inferring semantics, geometry, and\ndynamics. Given the capabilities of neural fields in densely representing a 3D\nscene from 2D images, we ask the question: Can we scale their self-supervised\npretraining, specifically using masked autoencoders, to generate effective 3D\nrepresentations from posed RGB images. Owing to the astounding success of\nextending transformers to novel data modalities, we employ standard 3D Vision\nTransformers to suit the unique formulation of NeRFs. We leverage NeRF's\nvolumetric grid as a dense input to the transformer, contrasting it with other\n3D representations such as pointclouds where the information density can be\nuneven, and the representation is irregular. Due to the difficulty of applying\nmasked autoencoders to an implicit representation, such as NeRF, we opt for\nextracting an explicit representation that canonicalizes scenes across domains\nby employing the camera trajectory for sampling. Our goal is made possible by\nmasking random patches from NeRF's radiance and density grid and employing a\nstandard 3D Swin Transformer to reconstruct the masked patches. In doing so,\nthe model can learn the semantic and spatial structure of complete scenes. We\npretrain this representation at scale on our proposed curated posed-RGB data,\ntotaling over 1.6 million images. Once pretrained, the encoder is used for\neffective 3D transfer learning. Our novel self-supervised pretraining for\nNeRFs, NeRF-MAE, scales remarkably well and improves performance on various\nchallenging 3D tasks. Utilizing unlabeled posed 2D data for pretraining,\nNeRF-MAE significantly outperforms self-supervised 3D pretraining and NeRF\nscene understanding baselines on Front3D and ScanNet datasets with an absolute\nperformance improvement of over 20% AP50 and 8% AP25 for 3D object detection.\n","authors":["Muhammad Zubair Irshad","Sergey Zakahrov","Vitor Guizilini","Adrien Gaidon","Zsolt Kira","Rares Ambrus"],"pdf_url":"https://arxiv.org/pdf/2404.01300v2.pdf","comment":"29 pages, 13 figures. Project Page: https://nerf-mae.github.io/"},{"id":"http://arxiv.org/abs/2404.12386v1","updated":"2024-04-18T17:59:46Z","published":"2024-04-18T17:59:46Z","title":"SOHES: Self-supervised Open-world Hierarchical Entity Segmentation","summary":"  Open-world entity segmentation, as an emerging computer vision task, aims at\nsegmenting entities in images without being restricted by pre-defined classes,\noffering impressive generalization capabilities on unseen images and concepts.\nDespite its promise, existing entity segmentation methods like Segment Anything\nModel (SAM) rely heavily on costly expert annotators. This work presents\nSelf-supervised Open-world Hierarchical Entity Segmentation (SOHES), a novel\napproach that eliminates the need for human annotations. SOHES operates in\nthree phases: self-exploration, self-instruction, and self-correction. Given a\npre-trained self-supervised representation, we produce abundant high-quality\npseudo-labels through visual feature clustering. Then, we train a segmentation\nmodel on the pseudo-labels, and rectify the noises in pseudo-labels via a\nteacher-student mutual-learning procedure. Beyond segmenting entities, SOHES\nalso captures their constituent parts, providing a hierarchical understanding\nof visual entities. Using raw images as the sole training data, our method\nachieves unprecedented performance in self-supervised open-world segmentation,\nmarking a significant milestone towards high-quality open-world entity\nsegmentation in the absence of human-annotated masks. Project page:\nhttps://SOHES.github.io.\n","authors":["Shengcao Cao","Jiuxiang Gu","Jason Kuen","Hao Tan","Ruiyi Zhang","Handong Zhao","Ani Nenkova","Liang-Yan Gui","Tong Sun","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2404.12386v1.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2404.12378v1","updated":"2024-04-18T17:58:16Z","published":"2024-04-18T17:58:16Z","title":"6Img-to-3D: Few-Image Large-Scale Outdoor Driving Scene Reconstruction","summary":"  Current 3D reconstruction techniques struggle to infer unbounded scenes from\na few images faithfully. Specifically, existing methods have high computational\ndemands, require detailed pose information, and cannot reconstruct occluded\nregions reliably. We introduce 6Img-to-3D, an efficient, scalable\ntransformer-based encoder-renderer method for single-shot image to 3D\nreconstruction. Our method outputs a 3D-consistent parameterized triplane from\nonly six outward-facing input images for large-scale, unbounded outdoor driving\nscenarios. We take a step towards resolving existing shortcomings by combining\ncontracted custom cross- and self-attention mechanisms for triplane\nparameterization, differentiable volume rendering, scene contraction, and image\nfeature projection. We showcase that six surround-view vehicle images from a\nsingle timestamp without global pose information are enough to reconstruct\n360$^{\\circ}$ scenes during inference time, taking 395 ms. Our method allows,\nfor example, rendering third-person images and birds-eye views. Our code is\navailable at https://github.com/continental/6Img-to-3D, and more examples can\nbe found at our website here https://6Img-to-3D.GitHub.io/.\n","authors":["Th√©o Gieruc","Marius K√§stingsch√§fer","Sebastian Bernhard","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2404.12378v1.pdf","comment":"Joint first authorship. Project page: https://6Img-to-3D.GitHub.io/\n  Code https://github.com/continental/6Img-to-3D"},{"id":"http://arxiv.org/abs/2404.12376v1","updated":"2024-04-18T17:57:53Z","published":"2024-04-18T17:57:53Z","title":"Matching the Statistical Query Lower Bound for k-sparse Parity Problems\n  with Stochastic Gradient Descent","summary":"  The $k$-parity problem is a classical problem in computational complexity and\nalgorithmic theory, serving as a key benchmark for understanding computational\nclasses. In this paper, we solve the $k$-parity problem with stochastic\ngradient descent (SGD) on two-layer fully-connected neural networks. We\ndemonstrate that SGD can efficiently solve the $k$-sparse parity problem on a\n$d$-dimensional hypercube ($k\\le O(\\sqrt{d})$) with a sample complexity of\n$\\tilde{O}(d^{k-1})$ using $2^{\\Theta(k)}$ neurons, thus matching the\nestablished $\\Omega(d^{k})$ lower bounds of Statistical Query (SQ) models. Our\ntheoretical analysis begins by constructing a good neural network capable of\ncorrectly solving the $k$-parity problem. We then demonstrate how a trained\nneural network with SGD can effectively approximate this good network, solving\nthe $k$-parity problem with small statistical errors. Our theoretical results\nand findings are supported by empirical evidence, showcasing the efficiency and\nefficacy of our approach.\n","authors":["Yiwen Kou","Zixiang Chen","Quanquan Gu","Sham M. Kakade"],"pdf_url":"https://arxiv.org/pdf/2404.12376v1.pdf","comment":"36 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2404.12369v1","updated":"2024-04-18T17:51:02Z","published":"2024-04-18T17:51:02Z","title":"KDk: A Defense Mechanism Against Label Inference Attacks in Vertical\n  Federated Learning","summary":"  Vertical Federated Learning (VFL) is a category of Federated Learning in\nwhich models are trained collaboratively among parties with vertically\npartitioned data. Typically, in a VFL scenario, the labels of the samples are\nkept private from all the parties except for the aggregating server, that is\nthe label owner. Nevertheless, recent works discovered that by exploiting\ngradient information returned by the server to bottom models, with the\nknowledge of only a small set of auxiliary labels on a very limited subset of\ntraining data points, an adversary can infer the private labels. These attacks\nare known as label inference attacks in VFL. In our work, we propose a novel\nframework called KDk, that combines Knowledge Distillation and k-anonymity to\nprovide a defense mechanism against potential label inference attacks in a VFL\nscenario. Through an exhaustive experimental campaign we demonstrate that by\napplying our approach, the performance of the analyzed label inference attacks\ndecreases consistently, even by more than 60%, maintaining the accuracy of the\nwhole VFL almost unaltered.\n","authors":["Marco Arazzi","Serena Nicolazzo","Antonino Nocera"],"pdf_url":"https://arxiv.org/pdf/2404.12369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12368v1","updated":"2024-04-18T17:50:23Z","published":"2024-04-18T17:50:23Z","title":"Gradient-Regularized Out-of-Distribution Detection","summary":"  One of the challenges for neural networks in real-life applications is the\noverconfident errors these models make when the data is not from the original\ntraining distribution.\n  Addressing this issue is known as Out-of-Distribution (OOD) detection.\n  Many state-of-the-art OOD methods employ an auxiliary dataset as a surrogate\nfor OOD data during training to achieve improved performance.\n  However, these methods fail to fully exploit the local information embedded\nin the auxiliary dataset.\n  In this work, we propose the idea of leveraging the information embedded in\nthe gradient of the loss function during training to enable the network to not\nonly learn a desired OOD score for each sample but also to exhibit similar\nbehavior in a local neighborhood around each sample.\n  We also develop a novel energy-based sampling method to allow the network to\nbe exposed to more informative OOD samples during the training phase. This is\nespecially important when the auxiliary dataset is large. We demonstrate the\neffectiveness of our method through extensive experiments on several OOD\nbenchmarks, improving the existing state-of-the-art FPR95 by 4% on our ImageNet\nexperiment.\n  We further provide a theoretical analysis through the lens of certified\nrobustness and Lipschitz analysis to showcase the theoretical foundation of our\nwork. We will publicly release our code after the review process.\n","authors":["Sina Sharifi","Taha Entesari","Bardia Safaei","Vishal M. Patel","Mahyar Fazlyab"],"pdf_url":"https://arxiv.org/pdf/2404.12368v1.pdf","comment":"Under review for the 18th European Conference on Computer Vision\n  (ECCV) 2024"},{"id":"http://arxiv.org/abs/2404.12367v1","updated":"2024-04-18T17:50:15Z","published":"2024-04-18T17:50:15Z","title":"Information theory unifies atomistic machine learning, uncertainty\n  quantification, and materials thermodynamics","summary":"  An accurate description of information is relevant for a range of problems in\natomistic modeling, such as sampling methods, detecting rare events, analyzing\ndatasets, or performing uncertainty quantification (UQ) in machine learning\n(ML)-driven simulations. Although individual methods have been proposed for\neach of these tasks, they lack a common theoretical background integrating\ntheir solutions. Here, we introduce an information theoretical framework that\nunifies predictions of phase transformations, kinetic events, dataset\noptimality, and model-free UQ from atomistic simulations, thus bridging\nmaterials modeling, ML, and statistical mechanics. We first demonstrate that,\nfor a proposed representation, the information entropy of a distribution of\natom-centered environments is a surrogate value for thermodynamic entropy.\nUsing molecular dynamics (MD) simulations, we show that information entropy\ndifferences from trajectories can be used to build phase diagrams, identify\nrare events, and recover classical theories of nucleation. Building on these\nresults, we use this general concept of entropy to quantify information in\ndatasets for ML interatomic potentials (IPs), informing compression, explaining\ntrends in testing errors, and evaluating the efficiency of active learning\nstrategies. Finally, we propose a model-free UQ method for MLIPs using\ninformation entropy, showing it reliably detects extrapolation regimes, scales\nto millions of atoms, and goes beyond model errors. This method is made\navailable as the package QUESTS: Quick Uncertainty and Entropy via STructural\nSimilarity, providing a new unifying theory for data-driven atomistic modeling\nand combining efforts in ML, first-principles thermodynamics, and simulations.\n","authors":["Daniel Schwalbe-Koda","Sebastien Hamel","Babak Sadigh","Fei Zhou","Vincenzo Lordi"],"pdf_url":"https://arxiv.org/pdf/2404.12367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12366v1","updated":"2024-04-18T17:49:02Z","published":"2024-04-18T17:49:02Z","title":"Accounting for AI and Users Shaping One Another: The Role of\n  Mathematical Models","summary":"  As AI systems enter into a growing number of societal domains, these systems\nincreasingly shape and are shaped by user preferences, opinions, and behaviors.\nHowever, the design of AI systems rarely accounts for how AI and users shape\none another. In this position paper, we argue for the development of formal\ninteraction models which mathematically specify how AI and users shape one\nanother. Formal interaction models can be leveraged to (1) specify interactions\nfor implementation, (2) monitor interactions through empirical analysis, (3)\nanticipate societal impacts via counterfactual analysis, and (4) control\nsocietal impacts via interventions. The design space of formal interaction\nmodels is vast, and model design requires careful consideration of factors such\nas style, granularity, mathematical complexity, and measurability. Using\ncontent recommender systems as a case study, we critically examine the nascent\nliterature of formal interaction models with respect to these use-cases and\ndesign axes. More broadly, we call for the community to leverage formal\ninteraction models when designing, evaluating, or auditing any AI system which\ninteracts with users.\n","authors":["Sarah Dean","Evan Dong","Meena Jagadeesan","Liu Leqi"],"pdf_url":"https://arxiv.org/pdf/2404.12366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12365v1","updated":"2024-04-18T17:48:05Z","published":"2024-04-18T17:48:05Z","title":"When LLMs are Unfit Use FastFit: Fast and Effective Text Classification\n  with Many Classes","summary":"  We present FastFit, a method, and a Python package design to provide fast and\naccurate few-shot classification, especially for scenarios with many\nsemantically similar classes. FastFit utilizes a novel approach integrating\nbatch contrastive learning and token-level similarity score. Compared to\nexisting few-shot learning packages, such as SetFit, Transformers, or few-shot\nprompting of large language models via API calls, FastFit significantly\nimproves multiclass classification performance in speed and accuracy across\nFewMany, our newly curated English benchmark, and Multilingual datasets.\nFastFit demonstrates a 3-20x improvement in training speed, completing training\nin just a few seconds. The FastFit package is now available on GitHub and PyPi,\npresenting a user-friendly solution for NLP practitioners.\n","authors":["Asaf Yehudai","Elron Bendel"],"pdf_url":"https://arxiv.org/pdf/2404.12365v1.pdf","comment":"Accepted to NAACL"},{"id":"http://arxiv.org/abs/2404.12362v1","updated":"2024-04-18T17:45:19Z","published":"2024-04-18T17:45:19Z","title":"Transformer tricks: Removing weights for skipless transformers","summary":"  He and Hofmann (arXiv:2311.01906) detailed a skipless transformer without the\nV and P (post-attention projection) linear layers, which reduces the total\nnumber of weights. However, this scheme is only applicable to MHA (multi-head\nattention), but not for MQA (multi-query attention) and GQA (grouped-query\nattention). The latter schemes are used by many popular LLMs such as Llama 2,\nMistral, Mixtral, PaLM, and Gemma. Therefore, this micro-paper proposes\nmathematically equivalent versions that are suitable for MQA and GQA. For\nexample, removing Q and P from a skipless version of Mistral-7B would remove\n15% of its weights (and thus reduce its compute and memory complexity). See\narXiv:2402.13388 and https://github.com/OpenMachine-ai/transformer-tricks for\ncode and more transformer tricks.\n","authors":["Nils Graef"],"pdf_url":"https://arxiv.org/pdf/2404.12362v1.pdf","comment":"6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2404.12358v1","updated":"2024-04-18T17:37:02Z","published":"2024-04-18T17:37:02Z","title":"From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function","summary":"  Reinforcement Learning From Human Feedback (RLHF) has been a critical to the\nsuccess of the latest generation of generative AI models. In response to the\ncomplex nature of the classical RLHF pipeline, direct alignment algorithms such\nas Direct Preference Optimization (DPO) have emerged as an alternative\napproach. Although DPO solves the same objective as the standard RLHF setup,\nthere is a mismatch between the two approaches. Standard RLHF deploys\nreinforcement learning in a specific token-level MDP, while DPO is derived as a\nbandit problem in which the whole response of the model is treated as a single\narm. In this work we rectify this difference, first we theoretically show that\nwe can derive DPO in the token-level MDP as a general inverse Q-learning\nalgorithm, which satisfies the Bellman equation. Using our theoretical results,\nwe provide three concrete empirical insights. First, we show that because of\nits token level interpretation, DPO is able to perform some type of credit\nassignment. Next, we prove that under the token level formulation, classical\nsearch-based algorithms, such as MCTS, which have recently been applied to the\nlanguage generation space, are equivalent to likelihood-based search on a DPO\npolicy. Empirically we show that a simple beam search yields meaningful\nimprovement over the base DPO policy. Finally, we show how the choice of\nreference policy causes implicit rewards to decline during training. We\nconclude by discussing applications of our work, including information\nelicitation in multi-tun dialogue, reasoning, agentic applications and\nend-to-end training of multi-model systems.\n","authors":["Rafael Rafailov","Joey Hejna","Ryan Park","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2404.12358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12213v2","updated":"2024-04-18T17:35:16Z","published":"2024-03-18T19:54:59Z","title":"Private graphon estimation via sum-of-squares","summary":"  We develop the first pure node-differentially-private algorithms for learning\nstochastic block models and for graphon estimation with polynomial running time\nfor any constant number of blocks. The statistical utility guarantees match\nthose of the previous best information-theoretic (exponential-time)\nnode-private mechanisms for these problems. The algorithm is based on an\nexponential mechanism for a score function defined in terms of a sum-of-squares\nrelaxation whose level depends on the number of blocks. The key ingredients of\nour results are (1) a characterization of the distance between the block\ngraphons in terms of a quadratic optimization over the polytope of doubly\nstochastic matrices, (2) a general sum-of-squares convergence result for\npolynomial optimization over arbitrary polytopes, and (3) a general approach to\nperform Lipschitz extensions of score functions as part of the sum-of-squares\nalgorithmic paradigm.\n","authors":["Hongjie Chen","Jingqiu Ding","Tommaso d'Orsi","Yiding Hua","Chih-Hung Liu","David Steurer"],"pdf_url":"https://arxiv.org/pdf/2403.12213v2.pdf","comment":"71 pages, accepted to STOC 2024"},{"id":"http://arxiv.org/abs/2404.12356v1","updated":"2024-04-18T17:34:47Z","published":"2024-04-18T17:34:47Z","title":"Improving the interpretability of GNN predictions through\n  conformal-based graph sparsification","summary":"  Graph Neural Networks (GNNs) have achieved state-of-the-art performance in\nsolving graph classification tasks. However, most GNN architectures aggregate\ninformation from all nodes and edges in a graph, regardless of their relevance\nto the task at hand, thus hindering the interpretability of their predictions.\nIn contrast to prior work, in this paper we propose a GNN \\emph{training}\napproach that jointly i) finds the most predictive subgraph by removing edges\nand/or nodes -- -\\emph{without making assumptions about the subgraph structure}\n-- while ii) optimizing the performance of the graph classification task. To\nthat end, we rely on reinforcement learning to solve the resulting bi-level\noptimization with a reward function based on conformal predictions to account\nfor the current in-training uncertainty of the classifier. Our empirical\nresults on nine different graph classification datasets show that our method\ncompetes in performance with baselines while relying on significantly sparser\nsubgraphs, leading to more interpretable GNN-based predictions.\n","authors":["Pablo Sanchez-Martin","Kinaan Aamir Khan","Isabel Valera"],"pdf_url":"https://arxiv.org/pdf/2404.12356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12355v1","updated":"2024-04-18T17:34:20Z","published":"2024-04-18T17:34:20Z","title":"Towards a Foundation Model for Partial Differential Equation:\n  Multi-Operator Learning and Extrapolation","summary":"  Foundation models, such as large language models, have demonstrated success\nin addressing various language and image processing tasks. In this work, we\nintroduce a multi-modal foundation model for scientific problems, named\nPROSE-PDE. Our model, designed for bi-modality to bi-modality learning, is a\nmulti-operator learning approach which can predict future states of\nspatiotemporal systems while concurrently learning the underlying governing\nequations of the physical system. Specifically, we focus on multi-operator\nlearning by training distinct one-dimensional time-dependent nonlinear constant\ncoefficient partial differential equations, with potential applications to many\nphysical applications including physics, geology, and biology. More\nimportantly, we provide three extrapolation studies to demonstrate that\nPROSE-PDE can generalize physical features through the robust training of\nmultiple operators and that the proposed model can extrapolate to predict PDE\nsolutions whose models or data were unseen during the training. Furthermore, we\nshow through systematic numerical experiments that the utilization of the\nsymbolic modality in our model effectively resolves the well-posedness problems\nwith training multiple operators and thus enhances our model's predictive\ncapabilities.\n","authors":["Jingmin Sun","Yuxuan Liu","Zecheng Zhang","Hayden Schaeffer"],"pdf_url":"https://arxiv.org/pdf/2404.12355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08995v2","updated":"2024-04-18T17:26:30Z","published":"2024-04-13T12:41:40Z","title":"Beyond Known Clusters: Probe New Prototypes for Efficient Generalized\n  Class Discovery","summary":"  Generalized Class Discovery (GCD) aims to dynamically assign labels to\nunlabelled data partially based on knowledge learned from labelled data, where\nthe unlabelled data may come from known or novel classes. The prevailing\napproach generally involves clustering across all data and learning conceptions\nby prototypical contrastive learning. However, existing methods largely hinge\non the performance of clustering algorithms and are thus subject to their\ninherent limitations. Firstly, the estimated cluster number is often smaller\nthan the ground truth, making the existing methods suffer from the lack of\nprototypes for comprehensive conception learning. To address this issue, we\npropose an adaptive probing mechanism that introduces learnable potential\nprototypes to expand cluster prototypes (centers). As there is no ground truth\nfor the potential prototype, we develop a self-supervised prototype learning\nframework to optimize the potential prototype in an end-to-end fashion.\nSecondly, clustering is computationally intensive, and the conventional\nstrategy of clustering both labelled and unlabelled instances exacerbates this\nissue. To counteract this inefficiency, we opt to cluster only the unlabelled\ninstances and subsequently expand the cluster prototypes with our introduced\npotential prototypes to fast explore novel classes. Despite the simplicity of\nour proposed method, extensive empirical analysis on a wide range of datasets\nconfirms that our method consistently delivers state-of-the-art results.\nSpecifically, our method surpasses the nearest competitor by a significant\nmargin of \\textbf{9.7}$\\%$ within the Stanford Cars dataset and\n\\textbf{12$\\times$} clustering efficiency within the Herbarium 19 dataset. We\nwill make the code and checkpoints publicly available at\n\\url{https://github.com/xjtuYW/PNP.git}.\n","authors":["Ye Wang","Yaxiong Wang","Yujiao Wu","Bingchen Zhao","Xueming Qian"],"pdf_url":"https://arxiv.org/pdf/2404.08995v2.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2302.03098v5","updated":"2024-04-18T17:14:37Z","published":"2023-02-06T19:58:28Z","title":"One-shot Empirical Privacy Estimation for Federated Learning","summary":"  Privacy estimation techniques for differentially private (DP) algorithms are\nuseful for comparing against analytical bounds, or to empirically measure\nprivacy loss in settings where known analytical bounds are not tight. However,\nexisting privacy auditing techniques usually make strong assumptions on the\nadversary (e.g., knowledge of intermediate model iterates or the training data\ndistribution), are tailored to specific tasks, model architectures, or DP\nalgorithm, and/or require retraining the model many times (typically on the\norder of thousands). These shortcomings make deploying such techniques at scale\ndifficult in practice, especially in federated settings where model training\ncan take days or weeks. In this work, we present a novel \"one-shot\" approach\nthat can systematically address these challenges, allowing efficient auditing\nor estimation of the privacy loss of a model during the same, single training\nrun used to fit model parameters, and without requiring any a priori knowledge\nabout the model architecture, task, or DP training algorithm. We show that our\nmethod provides provably correct estimates for the privacy loss under the\nGaussian mechanism, and we demonstrate its performance on well-established FL\nbenchmark datasets under several adversarial threat models.\n","authors":["Galen Andrew","Peter Kairouz","Sewoong Oh","Alina Oprea","H. Brendan McMahan","Vinith M. Suriyakumar"],"pdf_url":"https://arxiv.org/pdf/2302.03098v5.pdf","comment":"Final revision, oral presentation at ICLR 2024"},{"id":"http://arxiv.org/abs/2404.12341v1","updated":"2024-04-18T17:10:18Z","published":"2024-04-18T17:10:18Z","title":"Measuring Feature Dependency of Neural Networks by Collapsing Feature\n  Dimensions in the Data Manifold","summary":"  This paper introduces a new technique to measure the feature dependency of\nneural network models. The motivation is to better understand a model by\nquerying whether it is using information from human-understandable features,\ne.g., anatomical shape, volume, or image texture. Our method is based on the\nprinciple that if a model is dependent on a feature, then removal of that\nfeature should significantly harm its performance. A targeted feature is\n\"removed\" by collapsing the dimension in the data distribution that corresponds\nto that feature. We perform this by moving data points along the feature\ndimension to a baseline feature value while staying on the data manifold, as\nestimated by a deep generative model. Then we observe how the model's\nperformance changes on the modified test data set, with the target feature\ndimension removed. We test our method on deep neural network models trained on\nsynthetic image data with known ground truth, an Alzheimer's disease prediction\ntask using MRI and hippocampus segmentations from the OASIS-3 dataset, and a\ncell nuclei classification task using the Lizard dataset.\n","authors":["Yinzhu Jin","Matthew B. Dwyer","P. Thomas Fletcher"],"pdf_url":"https://arxiv.org/pdf/2404.12341v1.pdf","comment":"Accepted and will be pulished in International Symposium on\n  Biomedical Imaging (ISBI) 2024"},{"id":"http://arxiv.org/abs/2212.11737v2","updated":"2024-04-18T17:00:21Z","published":"2022-12-22T14:29:43Z","title":"The State of the Art in Enhancing Trust in Machine Learning Models with\n  the Use of Visualizations","summary":"  Machine learning (ML) models are nowadays used in complex applications in\nvarious domains, such as medicine, bioinformatics, and other sciences. Due to\ntheir black box nature, however, it may sometimes be hard to understand and\ntrust the results they provide. This has increased the demand for reliable\nvisualization tools related to enhancing trust in ML models, which has become a\nprominent topic of research in the visualization community over the past\ndecades. To provide an overview and present the frontiers of current research\non the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in\nML models with the use of interactive visualization. We define and describe the\nbackground of the topic, introduce a categorization for visualization\ntechniques that aim to accomplish this goal, and discuss insights and\nopportunities for future research directions. Among our contributions is a\ncategorization of trust against different facets of interactive ML, expanded\nand improved from previous research. Our results are investigated from\ndifferent analytical perspectives: (a) providing a statistical overview, (b)\nsummarizing key findings, (c) performing topic analyses, and (d) exploring the\ndata sets used in the individual papers, all with the support of an interactive\nweb-based survey browser. We intend this survey to be beneficial for\nvisualization researchers whose interests involve making ML models more\ntrustworthy, as well as researchers and practitioners from other disciplines in\ntheir search for effective visualization techniques suitable for solving their\ntasks with confidence and conveying meaning to their data.\n","authors":["A. Chatzimparmpas","R. Martins","I. Jusufi","K. Kucher","Fabrice Rossi","A. Kerren"],"pdf_url":"https://arxiv.org/pdf/2212.11737v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.00334v5","updated":"2024-04-18T16:59:06Z","published":"2021-12-01T08:01:02Z","title":"VisRuler: Visual Analytics for Extracting Decision Rules from Bagged and\n  Boosted Decision Trees","summary":"  Bagging and boosting are two popular ensemble methods in machine learning\n(ML) that produce many individual decision trees. Due to the inherent ensemble\ncharacteristic of these methods, they typically outperform single decision\ntrees or other ML models in predictive performance. However, numerous decision\npaths are generated for each decision tree, increasing the overall complexity\nof the model and hindering its use in domains that require trustworthy and\nexplainable decisions, such as finance, social care, and health care. Thus, the\ninterpretability of bagging and boosting algorithms, such as random forest and\nadaptive boosting, reduces as the number of decisions rises. In this paper, we\npropose a visual analytics tool that aims to assist users in extracting\ndecisions from such ML models via a thorough visual inspection workflow that\nincludes selecting a set of robust and diverse models (originating from\ndifferent ensemble learning algorithms), choosing important features according\nto their global contribution, and deciding which decisions are essential for\nglobal explanation (or locally, for specific cases). The outcome is a final\ndecision based on the class agreement of several models and the explored manual\ndecisions exported by users. We evaluated the applicability and effectiveness\nof VisRuler via a use case, a usage scenario, and a user study. The evaluation\nrevealed that most users managed to successfully use our system to explore\ndecision rules visually, performing the proposed tasks and answering the given\nquestions in a satisfying way.\n","authors":["Angelos Chatzimparmpas","Rafael M. Martins","Andreas Kerren"],"pdf_url":"https://arxiv.org/pdf/2112.00334v5.pdf","comment":"This manuscript is accepted for publication in the Information\n  Visualization (IV) - SAGE Journals"},{"id":"http://arxiv.org/abs/2404.12315v1","updated":"2024-04-18T16:51:12Z","published":"2024-04-18T16:51:12Z","title":"Adjoint Sensitivities of Chaotic Flows without Adjoint Solvers: A\n  Data-Driven Approach","summary":"  In one calculation, adjoint sensitivity analysis provides the gradient of a\nquantity of interest with respect to all system's parameters. Conventionally,\nadjoint solvers need to be implemented by differentiating computational models,\nwhich can be a cumbersome task and is code-specific. To propose an adjoint\nsolver that is not code-specific, we develop a data-driven strategy. We\ndemonstrate its application on the computation of gradients of long-time\naverages of chaotic flows. First, we deploy a parameter-aware echo state\nnetwork (ESN) to accurately forecast and simulate the dynamics of a dynamical\nsystem for a range of system's parameters. Second, we derive the adjoint of the\nparameter-aware ESN. Finally, we combine the parameter-aware ESN with its\nadjoint version to compute the sensitivities to the system parameters. We\nshowcase the method on a prototypical chaotic system. Because adjoint\nsensitivities in chaotic regimes diverge for long integration times, we analyse\nthe application of ensemble adjoint method to the ESN. We find that the adjoint\nsensitivities obtained from the ESN match closely with the original system.\nThis work opens possibilities for sensitivity analysis without code-specific\nadjoint solvers.\n","authors":["Defne E. Ozan","Luca Magri"],"pdf_url":"https://arxiv.org/pdf/2404.12315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12314v1","updated":"2024-04-18T16:50:46Z","published":"2024-04-18T16:50:46Z","title":"Guided Discrete Diffusion for Electronic Health Record Generation","summary":"  Electronic health records (EHRs) are a pivotal data source that enables\nnumerous applications in computational medicine, e.g., disease progression\nprediction, clinical trial design, and health economics and outcomes research.\nDespite wide usability, their sensitive nature raises privacy and\nconfidentially concerns, which limit potential use cases. To tackle these\nchallenges, we explore the use of generative models to synthesize artificial,\nyet realistic EHRs. While diffusion-based methods have recently demonstrated\nstate-of-the-art performance in generating other data modalities and overcome\nthe training instability and mode collapse issues that plague previous\nGAN-based approaches, their applications in EHR generation remain\nunderexplored. The discrete nature of tabular medical code data in EHRs poses\nchallenges for high-quality data generation, especially for continuous\ndiffusion models. To this end, we introduce a novel tabular EHR generation\nmethod, EHR-D3PM, which enables both unconditional and conditional generation\nusing the discrete diffusion model. Our experiments demonstrate that EHR-D3PM\nsignificantly outperforms existing generative baselines on comprehensive\nfidelity and utility metrics while maintaining less membership vulnerability\nrisks. Furthermore, we show EHR-D3PM is effective as a data augmentation method\nand enhances performance on downstream tasks when combined with real data.\n","authors":["Zixiang Chen","Jun Han","Yongqian Li","Yiwen Kou","Eran Halperin","Robert E. Tillman","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2404.12314v1.pdf","comment":"24 pages, 9 figures, 12 tables"},{"id":"http://arxiv.org/abs/2304.00133v5","updated":"2024-04-18T16:46:45Z","published":"2023-03-31T21:17:15Z","title":"DeforestVis: Behavior Analysis of Machine Learning Models with Surrogate\n  Decision Stumps","summary":"  As the complexity of machine learning (ML) models increases and their\napplication in different (and critical) domains grows, there is a strong demand\nfor more interpretable and trustworthy ML. A direct, model-agnostic, way to\ninterpret such models is to train surrogate models-such as rule sets and\ndecision trees-that sufficiently approximate the original ones while being\nsimpler and easier-to-explain. Yet, rule sets can become very lengthy, with\nmany if-else statements, and decision tree depth grows rapidly when accurately\nemulating complex ML models. In such cases, both approaches can fail to meet\ntheir core goal-providing users with model interpretability. To tackle this, we\npropose DeforestVis, a visual analytics tool that offers summarization of the\nbehaviour of complex ML models by providing surrogate decision stumps\n(one-level decision trees) generated with the Adaptive Boosting (AdaBoost)\ntechnique. DeforestVis helps users to explore the complexity versus fidelity\ntrade-off by incrementally generating more stumps, creating attribute-based\nexplanations with weighted stumps to justify decision making, and analysing the\nimpact of rule overriding on training instance allocation between one or more\nstumps. An independent test set allows users to monitor the effectiveness of\nmanual rule changes and form hypotheses based on case-by-case analyses. We show\nthe applicability and usefulness of DeforestVis with two use cases and expert\ninterviews with data analysts and model developers.\n","authors":["Angelos Chatzimparmpas","Rafael M. Martins","Alexandru C. Telea","Andreas Kerren"],"pdf_url":"https://arxiv.org/pdf/2304.00133v5.pdf","comment":"This manuscript is accepted for publication in Computer Graphics\n  Forum (CGF)"},{"id":"http://arxiv.org/abs/2404.12312v1","updated":"2024-04-18T16:46:08Z","published":"2024-04-18T16:46:08Z","title":"A Mean-Field Analysis of Neural Gradient Descent-Ascent: Applications to\n  Functional Conditional Moment Equations","summary":"  We study minimax optimization problems defined over infinite-dimensional\nfunction classes. In particular, we restrict the functions to the class of\noverparameterized two-layer neural networks and study (i) the convergence of\nthe gradient descent-ascent algorithm and (ii) the representation learning of\nthe neural network. As an initial step, we consider the minimax optimization\nproblem stemming from estimating a functional equation defined by conditional\nexpectations via adversarial estimation, where the objective function is\nquadratic in the functional space. For this problem, we establish convergence\nunder the mean-field regime by considering the continuous-time and\ninfinite-width limit of the optimization dynamics. Under this regime, gradient\ndescent-ascent corresponds to a Wasserstein gradient flow over the space of\nprobability measures defined over the space of neural network parameters. We\nprove that the Wasserstein gradient flow converges globally to a stationary\npoint of the minimax objective at a $\\mathcal{O}(T^{-1} + \\alpha^{-1} ) $\nsublinear rate, and additionally finds the solution to the functional equation\nwhen the regularizer of the minimax objective is strongly convex. Here $T$\ndenotes the time and $\\alpha$ is a scaling parameter of the neural network. In\nterms of representation learning, our results show that the feature\nrepresentation induced by the neural networks is allowed to deviate from the\ninitial one by the magnitude of $\\mathcal{O}(\\alpha^{-1})$, measured in terms\nof the Wasserstein distance. Finally, we apply our general results to concrete\nexamples including policy evaluation, nonparametric instrumental variable\nregression, and asset pricing.\n","authors":["Yuchen Zhu","Yufeng Zhang","Zhaoran Wang","Zhuoran Yang","Xiaohong Chen"],"pdf_url":"https://arxiv.org/pdf/2404.12312v1.pdf","comment":"72 pages, submitted"},{"id":"http://arxiv.org/abs/2208.10878v2","updated":"2024-04-18T16:41:58Z","published":"2022-08-23T11:25:16Z","title":"Transferability Ranking of Adversarial Examples","summary":"  Adversarial transferability in black-box scenarios presents a unique\nchallenge: while attackers can employ surrogate models to craft adversarial\nexamples, they lack assurance on whether these examples will successfully\ncompromise the target model. Until now, the prevalent method to ascertain\nsuccess has been trial and error-testing crafted samples directly on the victim\nmodel. This approach, however, risks detection with every attempt, forcing\nattackers to either perfect their first try or face exposure. Our paper\nintroduces a ranking strategy that refines the transfer attack process,\nenabling the attacker to estimate the likelihood of success without repeated\ntrials on the victim's system. By leveraging a set of diverse surrogate models,\nour method can predict transferability of adversarial examples. This strategy\ncan be used to either select the best sample to use in an attack or the best\nperturbation to apply to a specific sample. Using our strategy, we were able to\nraise the transferability of adversarial examples from a mere 20% - akin to\nrandom selection-up to near upper-bound levels, with some scenarios even\nwitnessing a 100% success rate. This substantial improvement not only sheds\nlight on the shared susceptibilities across diverse architectures but also\ndemonstrates that attackers can forego the detectable trial-and-error tactics\nraising increasing the threat of surrogate-based attacks.\n","authors":["Mosh Levy","Guy Amit","Yuval Elovici","Yisroel Mirsky"],"pdf_url":"https://arxiv.org/pdf/2208.10878v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12309v1","updated":"2024-04-18T16:38:02Z","published":"2024-04-18T16:38:02Z","title":"iRAG: An Incremental Retrieval Augmented Generation System for Videos","summary":"  Retrieval augmented generation (RAG) systems combine the strengths of\nlanguage generation and information retrieval to power many real-world\napplications like chatbots. Use of RAG for combined understanding of multimodal\ndata such as text, images and videos is appealing but two critical limitations\nexist: one-time, upfront capture of all content in large multimodal data as\ntext descriptions entails high processing times, and not all information in the\nrich multimodal data is typically in the text descriptions. Since the user\nqueries are not known apriori, developing a system for multimodal to text\nconversion and interactive querying of multimodal data is challenging.\n  To address these limitations, we propose iRAG, which augments RAG with a\nnovel incremental workflow to enable interactive querying of large corpus of\nmultimodal data. Unlike traditional RAG, iRAG quickly indexes large\nrepositories of multimodal data, and in the incremental workflow, it uses the\nindex to opportunistically extract more details from select portions of the\nmultimodal data to retrieve context relevant to an interactive user query. Such\nan incremental workflow avoids long multimodal to text conversion times,\novercomes information loss issues by doing on-demand query-specific extraction\nof details in multimodal data, and ensures high quality of responses to\ninteractive user queries that are often not known apriori. To the best of our\nknowledge, iRAG is the first system to augment RAG with an incremental workflow\nto support efficient interactive querying of large, real-world multimodal data.\nExperimental results on real-world long videos demonstrate 23x to 25x faster\nvideo to text ingestion, while ensuring that quality of responses to\ninteractive user queries is comparable to responses from a traditional RAG\nwhere all video data is converted to text upfront before any querying.\n","authors":["Md Adnan Arefeen","Biplob Debnath","Md Yusuf Sarwar Uddin","Srimat Chakradhar"],"pdf_url":"https://arxiv.org/pdf/2404.12309v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12308v1","updated":"2024-04-18T16:35:38Z","published":"2024-04-18T16:35:38Z","title":"ASID: Active Exploration for System Identification in Robotic\n  Manipulation","summary":"  Model-free control strategies such as reinforcement learning have shown the\nability to learn control strategies without requiring an accurate model or\nsimulator of the world. While this is appealing due to the lack of modeling\nrequirements, such methods can be sample inefficient, making them impractical\nin many real-world domains. On the other hand, model-based control techniques\nleveraging accurate simulators can circumvent these challenges and use a large\namount of cheap simulation data to learn controllers that can effectively\ntransfer to the real world. The challenge with such model-based techniques is\nthe requirement for an extremely accurate simulation, requiring both the\nspecification of appropriate simulation assets and physical parameters. This\nrequires considerable human effort to design for every environment being\nconsidered. In this work, we propose a learning system that can leverage a\nsmall amount of real-world data to autonomously refine a simulation model and\nthen plan an accurate control strategy that can be deployed in the real world.\nOur approach critically relies on utilizing an initial (possibly inaccurate)\nsimulator to design effective exploration policies that, when deployed in the\nreal world, collect high-quality data. We demonstrate the efficacy of this\nparadigm in identifying articulation, mass, and other physical parameters in\nseveral challenging robotic manipulation tasks, and illustrate that only a\nsmall amount of real-world data can allow for effective sim-to-real transfer.\nProject website at https://weirdlabuw.github.io/asid\n","authors":["Marius Memmel","Andrew Wagenmaker","Chuning Zhu","Patrick Yin","Dieter Fox","Abhishek Gupta"],"pdf_url":"https://arxiv.org/pdf/2404.12308v1.pdf","comment":"Project website at https://weirdlabuw.github.io/asid"},{"id":"http://arxiv.org/abs/2404.12299v1","updated":"2024-04-18T16:24:12Z","published":"2024-04-18T16:24:12Z","title":"Simultaneous Interpretation Corpus Construction by Large Language Models\n  in Distant Language Pair","summary":"  In Simultaneous Machine Translation (SiMT) systems, training with a\nsimultaneous interpretation (SI) corpus is an effective method for achieving\nhigh-quality yet low-latency systems. However, it is very challenging to curate\nsuch a corpus due to limitations in the abilities of annotators, and hence,\nexisting SI corpora are limited. Therefore, we propose a method to convert\nexisting speech translation corpora into interpretation-style data, maintaining\nthe original word order and preserving the entire source content using Large\nLanguage Models (LLM-SI-Corpus). We demonstrate that fine-tuning SiMT models in\ntext-to-text and speech-to-text settings with the LLM-SI-Corpus reduces\nlatencies while maintaining the same level of quality as the models trained\nwith offline datasets. The LLM-SI-Corpus is available at\n\\url{https://github.com/yusuke1997/LLM-SI-Corpus}.\n","authors":["Yusuke Sakai","Mana Makinae","Hidetaka Kamigaito","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2404.12299v1.pdf","comment":"23 pages, 9 figures"},{"id":"http://arxiv.org/abs/2012.01205v4","updated":"2024-04-18T16:23:23Z","published":"2020-12-02T13:43:37Z","title":"VisEvol: Visual Analytics to Support Hyperparameter Search through\n  Evolutionary Optimization","summary":"  During the training phase of machine learning (ML) models, it is usually\nnecessary to configure several hyperparameters. This process is computationally\nintensive and requires an extensive search to infer the best hyperparameter set\nfor the given problem. The challenge is exacerbated by the fact that most ML\nmodels are complex internally, and training involves trial-and-error processes\nthat could remarkably affect the predictive result. Moreover, each\nhyperparameter of an ML algorithm is potentially intertwined with the others,\nand changing it might result in unforeseeable impacts on the remaining\nhyperparameters. Evolutionary optimization is a promising method to try and\naddress those issues. According to this method, performant models are stored,\nwhile the remainder are improved through crossover and mutation processes\ninspired by genetic algorithms. We present VisEvol, a visual analytics tool\nthat supports interactive exploration of hyperparameters and intervention in\nthis evolutionary procedure. In summary, our proposed tool helps the user to\ngenerate new models through evolution and eventually explore powerful\nhyperparameter combinations in diverse regions of the extensive hyperparameter\nspace. The outcome is a voting ensemble (with equal rights) that boosts the\nfinal predictive performance. The utility and applicability of VisEvol are\ndemonstrated with two use cases and interviews with ML experts who evaluated\nthe effectiveness of the tool.\n","authors":["Angelos Chatzimparmpas","Rafael M. Martins","Kostiantyn Kucher","Andreas Kerren"],"pdf_url":"https://arxiv.org/pdf/2012.01205v4.pdf","comment":"This manuscript is accepted for publication in a special issue of\n  Computer Graphics Forum (CGF)"},{"id":"http://arxiv.org/abs/2305.11527v3","updated":"2024-04-18T16:20:19Z","published":"2023-05-19T08:51:11Z","title":"InstructIE: A Bilingual Instruction-based Information Extraction Dataset","summary":"  Large language models can perform well on general natural language tasks, but\ntheir effectiveness is still not optimal for information extraction. Recent\nworks indicate that the main reason lies in the lack of extensive data on\ninformation extraction instructions. Note that the existing datasets on\ninformation extraction instructions not only have limited coverage but also\ninvolve high construction costs. To address this issue, we introduce\nInstructIE, a bilingual instruction-based information extraction dataset, which\ncovers 12 diverse domains. Specifically, we propose KG2Instruction, a framework\nspecifically for the automatic generation of such datasets. Experimental\nresults demonstrate that large language models trained with InstructIE can not\nonly obtain better information extraction capabilities but also enhance\nzero-shot performance compared with baselines.\n","authors":["Honghao Gui","Shuofei Qiao","Jintian Zhang","Hongbin Ye","Mengshu Sun","Lei Liang","Jeff Z. Pan","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.11527v3.pdf","comment":"Work in progress; project homepage:\n  https://www.zjukg.org/project/InstructIE/ dataset:\n  https://huggingface.co/datasets/zjunlp/InstructIE"},{"id":"http://arxiv.org/abs/2404.12294v1","updated":"2024-04-18T16:16:02Z","published":"2024-04-18T16:16:02Z","title":"floZ: Evidence estimation from posterior samples with normalizing flows","summary":"  We propose a novel method (floZ), based on normalizing flows, for estimating\nthe Bayesian evidence (and its numerical uncertainty) from a set of samples\ndrawn from the unnormalized posterior distribution. We validate it on\ndistributions whose evidence is known analytically, up to 15 parameter space\ndimensions, and compare with two state-of-the-art techniques for estimating the\nevidence: nested sampling (which computes the evidence as its main target) and\na k-nearest-neighbors technique that produces evidence estimates from posterior\nsamples. Provided representative samples from the target posterior are\navailable, our method is more robust to posterior distributions with sharp\nfeatures, especially in higher dimensions. It has wide applicability, e.g., to\nestimate the evidence from variational inference, Markov-chain Monte Carlo\nsamples, or any other method that delivers samples from the unnormalized\nposterior density.\n","authors":["Rahul Srinivasan","Marco Crisostomi","Roberto Trotta","Enrico Barausse","Matteo Breschi"],"pdf_url":"https://arxiv.org/pdf/2404.12294v1.pdf","comment":"10 pages, 4 figures, 1 table"},{"id":"http://arxiv.org/abs/2212.03539v3","updated":"2024-04-18T16:14:17Z","published":"2022-12-07T09:38:02Z","title":"MetaStackVis: Visually-Assisted Performance Evaluation of Metamodels","summary":"  Stacking (or stacked generalization) is an ensemble learning method with one\nmain distinctiveness from the rest: even though several base models are trained\non the original data set, their predictions are further used as input data for\none or more metamodels arranged in at least one extra layer. Composing a stack\nof models can produce high-performance outcomes, but it usually involves a\ntrial-and-error process. Therefore, our previously developed visual analytics\nsystem, StackGenVis, was mainly designed to assist users in choosing a set of\ntop-performing and diverse models by measuring their predictive performance.\nHowever, it only employs a single logistic regression metamodel. In this paper,\nwe investigate the impact of alternative metamodels on the performance of\nstacking ensembles using a novel visualization tool, called MetaStackVis. Our\ninteractive tool helps users to visually explore different singular and pairs\nof metamodels according to their predictive probabilities and multiple\nvalidation metrics, as well as their ability to predict specific problematic\ndata instances. MetaStackVis was evaluated with a usage scenario based on a\nmedical data set and via expert interviews.\n","authors":["Ilya Ploshchik","Angelos Chatzimparmpas","Andreas Kerren"],"pdf_url":"https://arxiv.org/pdf/2212.03539v3.pdf","comment":"This manuscript is accepted for publication in Proceedings of the\n  16th IEEE Pacific Visualization Symposium (PacificVis '23)"},{"id":"http://arxiv.org/abs/2404.12293v1","updated":"2024-04-18T16:13:58Z","published":"2024-04-18T16:13:58Z","title":"Singular-limit analysis of gradient descent with noise injection","summary":"  We study the limiting dynamics of a large class of noisy gradient descent\nsystems in the overparameterized regime. In this regime the set of global\nminimizers of the loss is large, and when initialized in a neighbourhood of\nthis zero-loss set a noisy gradient descent algorithm slowly evolves along this\nset. In some cases this slow evolution has been related to better\ngeneralisation properties. We characterize this evolution for the broad class\nof noisy gradient descent systems in the limit of small step size. Our results\nshow that the structure of the noise affects not just the form of the limiting\nprocess, but also the time scale at which the evolution takes place. We apply\nthe theory to Dropout, label noise and classical SGD (minibatching) noise, and\nshow that these evolve on different two time scales. Classical SGD even yields\na trivial evolution on both time scales, implying that additional noise is\nrequired for regularization. The results are inspired by the training of neural\nnetworks, but the theorems apply to noisy gradient descent of any loss that has\na non-trivial zero-loss set.\n","authors":["Anna Shalova","Andr√© Schlichting","Mark Peletier"],"pdf_url":"https://arxiv.org/pdf/2404.12293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12290v1","updated":"2024-04-18T16:11:16Z","published":"2024-04-18T16:11:16Z","title":"Debiased Distribution Compression","summary":"  Modern compression methods can summarize a target distribution $\\mathbb{P}$\nmore succinctly than i.i.d. sampling but require access to a low-bias input\nsequence like a Markov chain converging quickly to $\\mathbb{P}$. We introduce a\nnew suite of compression methods suitable for compression with biased input\nsequences. Given $n$ points targeting the wrong distribution and quadratic\ntime, Stein Kernel Thinning (SKT) returns $\\sqrt{n}$ equal-weighted points with\n$\\widetilde{O}(n^{-1/2})$ maximum mean discrepancy (MMD) to $\\mathbb {P}$. For\nlarger-scale compression tasks, Low-rank SKT achieves the same feat in\nsub-quadratic time using an adaptive low-rank debiasing procedure that may be\nof independent interest. For downstream tasks that support simplex or\nconstant-preserving weights, Stein Recombination and Stein Cholesky achieve\neven greater parsimony, matching the guarantees of SKT with as few as\n$\\operatorname{poly-log}(n)$ weighted points. Underlying these advances are new\nguarantees for the quality of simplex-weighted coresets, the spectral decay of\nkernel matrices, and the covering numbers of Stein kernel Hilbert spaces. In\nour experiments, our techniques provide succinct and accurate posterior\nsummaries while overcoming biases due to burn-in, approximate Markov chain\nMonte Carlo, and tempering.\n","authors":["Lingxiao Li","Raaz Dwivedi","Lester Mackey"],"pdf_url":"https://arxiv.org/pdf/2404.12290v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2002.06910v5","updated":"2024-04-18T16:03:37Z","published":"2020-02-17T12:22:34Z","title":"t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections","summary":"  t-Distributed Stochastic Neighbor Embedding (t-SNE) for the visualization of\nmultidimensional data has proven to be a popular approach, with successful\napplications in a wide range of domains. Despite their usefulness, t-SNE\nprojections can be hard to interpret or even misleading, which hurts the\ntrustworthiness of the results. Understanding the details of t-SNE itself and\nthe reasons behind specific patterns in its output may be a daunting task,\nespecially for non-experts in dimensionality reduction. In this work, we\npresent t-viSNE, an interactive tool for the visual exploration of t-SNE\nprojections that enables analysts to inspect different aspects of their\naccuracy and meaning, such as the effects of hyper-parameters, distance and\nneighborhood preservation, densities and costs of specific neighborhoods, and\nthe correlations between dimensions and visual patterns. We propose a coherent,\naccessible, and well-integrated collection of different views for the\nvisualization of t-SNE projections. The applicability and usability of t-viSNE\nare demonstrated through hypothetical usage scenarios with real data sets.\nFinally, we present the results of a user study where the tool's effectiveness\nwas evaluated. By bringing to light information that would normally be lost\nafter running t-SNE, we hope to support analysts in using t-SNE and making its\nresults better understandable.\n","authors":["Angelos Chatzimparmpas","Rafael M. Martins","Andreas Kerren"],"pdf_url":"https://arxiv.org/pdf/2002.06910v5.pdf","comment":"This manuscript is published in the IEEE Transactions on\n  Visualization and Computer Graphics Journal (IEEE TVCG)"},{"id":"http://arxiv.org/abs/2404.12282v1","updated":"2024-04-18T15:58:31Z","published":"2024-04-18T15:58:31Z","title":"Investigating Guiding Information for Adaptive Collocation Point\n  Sampling in PINNs","summary":"  Physics-informed neural networks (PINNs) provide a means of obtaining\napproximate solutions of partial differential equations and systems through the\nminimisation of an objective function which includes the evaluation of a\nresidual function at a set of collocation points within the domain. The quality\nof a PINNs solution depends upon numerous parameters, including the number and\ndistribution of these collocation points. In this paper we consider a number of\nstrategies for selecting these points and investigate their impact on the\noverall accuracy of the method. In particular, we suggest that no single\napproach is likely to be ``optimal'' but we show how a number of important\nmetrics can have an impact in improving the quality of the results obtained\nwhen using a fixed number of residual evaluations. We illustrate these\napproaches through the use of two benchmark test problems: Burgers' equation\nand the Allen-Cahn equation.\n","authors":["Jose Florido","He Wang","Amirul Khan","Peter K. Jimack"],"pdf_url":"https://arxiv.org/pdf/2404.12282v1.pdf","comment":"15 pages, 8 figures, 2 tables. Accepted for publication in the\n  conference proceedings of the International Conference on Computational\n  Science (ICCS) 2024"},{"id":"http://arxiv.org/abs/2404.12273v1","updated":"2024-04-18T15:46:26Z","published":"2024-04-18T15:46:26Z","title":"FedEval-LLM: Federated Evaluation of Large Language Models on Downstream\n  Tasks with Collective Wisdom","summary":"  Federated Learning (FL) has emerged as a promising solution for collaborative\ntraining of large language models (LLMs). However, the integration of LLMs into\nFL introduces new challenges, particularly concerning the evaluation of LLMs.\nTraditional evaluation methods that rely on labeled test sets and\nsimilarity-based metrics cover only a subset of the acceptable answers, thereby\nfailing to accurately reflect the performance of LLMs on generative tasks.\nMeanwhile, although automatic evaluation methods that leverage advanced LLMs\npresent potential, they face critical risks of data leakage due to the need to\ntransmit data to external servers and suboptimal performance on downstream\ntasks due to the lack of domain knowledge. To address these issues, we propose\na Federated Evaluation framework of Large Language Models, named FedEval-LLM,\nthat provides reliable performance measurements of LLMs on downstream tasks\nwithout the reliance on labeled test sets and external tools, thus ensuring\nstrong privacy-preserving capability. FedEval-LLM leverages a consortium of\npersonalized LLMs from participants as referees to provide domain knowledge and\ncollective evaluation capability, thus aligning to the respective downstream\ntasks and mitigating uncertainties and biases associated with a single referee.\nExperimental results demonstrate a significant improvement in the evaluation\ncapability of personalized evaluation models on downstream tasks. When applied\nto FL, these evaluation models exhibit strong agreement with human preference\nand RougeL-score on meticulously curated test sets. FedEval-LLM effectively\novercomes the limitations of traditional metrics and the reliance on external\nservices, making it a promising framework for the evaluation of LLMs within\ncollaborative training scenarios.\n","authors":["Yuanqin He","Yan Kang","Lixin Fan","Qiang Yang"],"pdf_url":"https://arxiv.org/pdf/2404.12273v1.pdf","comment":"In Progress"},{"id":"http://arxiv.org/abs/2310.08475v5","updated":"2024-04-18T15:46:22Z","published":"2023-10-12T16:32:44Z","title":"Can We Edit Multimodal Large Language Models?","summary":"  In this paper, we focus on editing Multimodal Large Language Models (MLLMs).\nCompared to editing single-modal LLMs, multimodal model editing is more\nchallenging, which demands a higher level of scrutiny and careful consideration\nin the editing process. To facilitate research in this area, we construct a new\nbenchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite\nof innovative metrics for evaluation. We conduct comprehensive experiments\ninvolving various model editing baselines and analyze the impact of editing\ndifferent components for multimodal LLMs. Empirically, we notice that previous\nbaselines can implement editing multimodal LLMs to some extent, but the effect\nis still barely satisfactory, indicating the potential difficulty of this task.\nWe hope that our work can provide the NLP community with insights. Code and\ndataset are available in https://github.com/zjunlp/EasyEdit.\n","authors":["Siyuan Cheng","Bozhong Tian","Qingbin Liu","Xi Chen","Yongheng Wang","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.08475v5.pdf","comment":"EMNLP 2023. Add the Exact Match/Accuracy results of Reliability and\n  T-Generality"},{"id":"http://arxiv.org/abs/2404.12267v1","updated":"2024-04-18T15:38:14Z","published":"2024-04-18T15:38:14Z","title":"Physics-integrated generative modeling using attentive planar\n  normalizing flow based variational autoencoder","summary":"  Physics-integrated generative modeling is a class of hybrid or grey-box\nmodeling in which we augment the the data-driven model with the physics\nknowledge governing the data distribution. The use of physics knowledge allows\nthe generative model to produce output in a controlled way, so that the output,\nby construction, complies with the physical laws. It imparts improved\ngeneralization ability to extrapolate beyond the training distribution as well\nas improved interpretability because the model is partly grounded in firm\ndomain knowledge. In this work, we aim to improve the fidelity of\nreconstruction and robustness to noise in the physics integrated generative\nmodel. To this end, we use variational-autoencoder as a generative model. To\nimprove the reconstruction results of the decoder, we propose to learn the\nlatent posterior distribution of both the physics as well as the trainable\ndata-driven components using planar normalizng flow. Normalizng flow based\nposterior distribution harnesses the inherent dynamical structure of the data\ndistribution, hence the learned model gets closer to the true underlying data\ndistribution. To improve the robustness of generative model against noise\ninjected in the model, we propose a modification in the encoder part of the\nnormalizing flow based VAE. We designed the encoder to incorporate scaled dot\nproduct attention based contextual information in the noisy latent vector which\nwill mitigate the adverse effect of noise in the latent vector and make the\nmodel more robust. We empirically evaluated our models on human locomotion\ndataset [33] and the results validate the efficacy of our proposed models in\nterms of improvement in reconstruction quality as well as robustness against\nnoise injected in the model.\n","authors":["Sheikh Waqas Akhtar"],"pdf_url":"https://arxiv.org/pdf/2404.12267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15584v3","updated":"2024-04-18T15:29:14Z","published":"2024-02-23T19:51:55Z","title":"State Space Models for Event Cameras","summary":"  Today, state-of-the-art deep neural networks that process event-camera data\nfirst convert a temporal window of events into dense, grid-like input\nrepresentations. As such, they exhibit poor generalizability when deployed at\nhigher inference frequencies (i.e., smaller temporal windows) than the ones\nthey were trained on. We address this challenge by introducing state-space\nmodels (SSMs) with learnable timescale parameters to event-based vision. This\ndesign adapts to varying frequencies without the need to retrain the network at\ndifferent frequencies. Additionally, we investigate two strategies to\ncounteract aliasing effects when deploying the model at higher frequencies. We\ncomprehensively evaluate our approach against existing methods based on RNN and\nTransformer architectures across various benchmarks, including Gen1 and 1 Mpx\nevent camera datasets. Our results demonstrate that SSM-based models train 33%\nfaster and also exhibit minimal performance degradation when tested at higher\nfrequencies than the training input. Traditional RNN and Transformer models\nexhibit performance drops of more than 20 mAP, with SSMs having a drop of 3.76\nmAP, highlighting the effectiveness of SSMs in event-based vision tasks.\n","authors":["Nikola Zubiƒá","Mathias Gehrig","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2402.15584v3.pdf","comment":"18 pages, 5 figures, 6 tables, CVPR 2024 Camera Ready paper"},{"id":"http://arxiv.org/abs/2404.12260v1","updated":"2024-04-18T15:28:34Z","published":"2024-04-18T15:28:34Z","title":"Alleviating Catastrophic Forgetting in Facial Expression Recognition\n  with Emotion-Centered Models","summary":"  Facial expression recognition is a pivotal component in machine learning,\nfacilitating various applications. However, convolutional neural networks\n(CNNs) are often plagued by catastrophic forgetting, impeding their\nadaptability. The proposed method, emotion-centered generative replay (ECgr),\ntackles this challenge by integrating synthetic images from generative\nadversarial networks. Moreover, ECgr incorporates a quality assurance algorithm\nto ensure the fidelity of generated images. This dual approach enables CNNs to\nretain past knowledge while learning new tasks, enhancing their performance in\nemotion recognition. The experimental results on four diverse facial expression\ndatasets demonstrate that incorporating images generated by our\npseudo-rehearsal method enhances training on the targeted dataset and the\nsource dataset while making the CNN retain previously learned knowledge.\n","authors":["Israel A. Laurensi","Alceu de Souza Britto Jr.","Jean Paul Barddal","Alessandro Lameiras Koerich"],"pdf_url":"https://arxiv.org/pdf/2404.12260v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2404.12257v1","updated":"2024-04-18T15:23:37Z","published":"2024-04-18T15:23:37Z","title":"Food Portion Estimation via 3D Object Scaling","summary":"  Image-based methods to analyze food images have alleviated the user burden\nand biases associated with traditional methods. However, accurate portion\nestimation remains a major challenge due to the loss of 3D information in the\n2D representation of foods captured by smartphone cameras or wearable devices.\nIn this paper, we propose a new framework to estimate both food volume and\nenergy from 2D images by leveraging the power of 3D food models and physical\nreference in the eating scene. Our method estimates the pose of the camera and\nthe food object in the input image and recreates the eating occasion by\nrendering an image of a 3D model of the food with the estimated poses. We also\nintroduce a new dataset, SimpleFood45, which contains 2D images of 45 food\nitems and associated annotations including food volume, weight, and energy. Our\nmethod achieves an average error of 31.10 kCal (17.67%) on this dataset,\noutperforming existing portion estimation methods.\n","authors":["Gautham Vinod","Jiangpeng He","Zeman Shao","Fengqing Zhu"],"pdf_url":"https://arxiv.org/pdf/2404.12257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12256v1","updated":"2024-04-18T15:22:29Z","published":"2024-04-18T15:22:29Z","title":"An Online Spatial-Temporal Graph Trajectory Planner for Autonomous\n  Vehicles","summary":"  The autonomous driving industry is expected to grow by over 20 times in the\ncoming decade and, thus, motivate researchers to delve into it. The primary\nfocus of their research is to ensure safety, comfort, and efficiency. An\nautonomous vehicle has several modules responsible for one or more of the\naforementioned items. Among these modules, the trajectory planner plays a\npivotal role in the safety of the vehicle and the comfort of its passengers.\nThe module is also responsible for respecting kinematic constraints and any\napplicable road constraints. In this paper, a novel online spatial-temporal\ngraph trajectory planner is introduced to generate safe and comfortable\ntrajectories. First, a spatial-temporal graph is constructed using the\nautonomous vehicle, its surrounding vehicles, and virtual nodes along the road\nwith respect to the vehicle itself. Next, the graph is forwarded into a\nsequential network to obtain the desired states. To support the planner, a\nsimple behavioral layer is also presented that determines kinematic constraints\nfor the planner. Furthermore, a novel potential function is also proposed to\ntrain the network. Finally, the proposed planner is tested on three different\ncomplex driving tasks, and the performance is compared with two frequently used\nmethods. The results show that the proposed planner generates safe and feasible\ntrajectories while achieving similar or longer distances in the forward\ndirection and comparable comfort ride.\n","authors":["Jilan Samiuddin","Benoit Boulet","Di Wu"],"pdf_url":"https://arxiv.org/pdf/2404.12256v1.pdf","comment":"This is the accepted version and published in the \"Early Access\" area\n  of IEEE Xplore for the IEEE Transactions on Intelligent Vehicles on 16 April\n  2024. Article statistics: 11 pages, 9 figures, 2 tables"},{"id":"http://arxiv.org/abs/2403.12005v2","updated":"2024-04-18T15:20:41Z","published":"2024-03-18T17:42:27Z","title":"Visualization for Trust in Machine Learning Revisited: The State of the\n  Field in 2023","summary":"  Visualization for explainable and trustworthy machine learning remains one of\nthe most important and heavily researched fields within information\nvisualization and visual analytics with various application domains, such as\nmedicine, finance, and bioinformatics. After our 2020 state-of-the-art report\ncomprising 200 techniques, we have persistently collected peer-reviewed\narticles describing visualization techniques, categorized them based on the\npreviously established categorization schema consisting of 119 categories, and\nprovided the resulting collection of 542 techniques in an online survey\nbrowser. In this survey article, we present the updated findings of new\nanalyses of this dataset as of fall 2023 and discuss trends, insights, and\neight open challenges for using visualizations in machine learning. Our results\ncorroborate the rapidly growing trend of visualization techniques for\nincreasing trust in machine learning models in the past three years, with\nvisualization found to help improve popular model explainability methods and\ncheck new deep learning architectures, for instance.\n","authors":["Angelos Chatzimparmpas","Kostiantyn Kucher","Andreas Kerren"],"pdf_url":"https://arxiv.org/pdf/2403.12005v2.pdf","comment":"This manuscript is accepted for publication in the IEEE Computer\n  Graphics and Applications Journal (IEEE CG&A)"},{"id":"http://arxiv.org/abs/2404.12251v1","updated":"2024-04-18T15:18:14Z","published":"2024-04-18T15:18:14Z","title":"Dynamic Modality and View Selection for Multimodal Emotion Recognition\n  with Missing Modalities","summary":"  The study of human emotions, traditionally a cornerstone in fields like\npsychology and neuroscience, has been profoundly impacted by the advent of\nartificial intelligence (AI). Multiple channels, such as speech (voice) and\nfacial expressions (image), are crucial in understanding human emotions.\nHowever, AI's journey in multimodal emotion recognition (MER) is marked by\nsubstantial technical challenges. One significant hurdle is how AI models\nmanage the absence of a particular modality - a frequent occurrence in\nreal-world situations. This study's central focus is assessing the performance\nand resilience of two strategies when confronted with the lack of one modality:\na novel multimodal dynamic modality and view selection and a cross-attention\nmechanism. Results on the RECOLA dataset show that dynamic selection-based\nmethods are a promising approach for MER. In the missing modalities scenarios,\nall dynamic selection-based methods outperformed the baseline. The study\nconcludes by emphasizing the intricate interplay between audio and video\nmodalities in emotion prediction, showcasing the adaptability of dynamic\nselection methods in handling missing modalities.\n","authors":["Luciana Trinkaus Menon","Luiz Carlos Ribeiro Neduziak","Jean Paul Barddal","Alessandro Lameiras Koerich","Alceu de Souza Britto Jr"],"pdf_url":"https://arxiv.org/pdf/2404.12251v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2211.10280v2","updated":"2024-04-18T15:07:35Z","published":"2022-11-18T15:11:44Z","title":"TensAIR: Real-Time Training of Neural Networks from Data-streams","summary":"  Online learning (OL) from data streams is an emerging area of research that\nencompasses numerous challenges from stream processing, machine learning, and\nnetworking. Stream-processing platforms, such as Apache Kafka and Flink, have\nbasic extensions for the training of Artificial Neural Networks (ANNs) in a\nstream-processing pipeline. However, these extensions were not designed to\ntrain ANNs in real-time, and they suffer from performance and scalability\nissues when doing so. This paper presents TensAIR, the first OL system for\ntraining ANNs in real time. TensAIR achieves remarkable performance and\nscalability by using a decentralized and asynchronous architecture to train ANN\nmodels (either freshly initialized or pre-trained) via DASGD (decentralized and\nasynchronous stochastic gradient descent). We empirically demonstrate that\nTensAIR achieves a nearly linear scale-out performance in terms of (1) the\nnumber of worker nodes deployed in the network, and (2) the throughput at which\nthe data batches arrive at the dataflow operators. We depict the versatility of\nTensAIR by investigating both sparse (word embedding) and dense (image\nclassification) use cases, for which TensAIR achieved from 6 to 116 times\nhigher sustainable throughput rates than state-of-the-art systems for training\nANN in a stream-processing pipeline.\n","authors":["Mauro D. L. Tosi","Vinu E. Venugopal","Martin Theobald"],"pdf_url":"https://arxiv.org/pdf/2211.10280v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12238v1","updated":"2024-04-18T14:57:17Z","published":"2024-04-18T14:57:17Z","title":"Neural Networks with Causal Graph Constraints: A New Approach for\n  Treatment Effects Estimation","summary":"  In recent years, there has been a growing interest in using machine learning\ntechniques for the estimation of treatment effects. Most of the best-performing\nmethods rely on representation learning strategies that encourage shared\nbehavior among potential outcomes to increase the precision of treatment effect\nestimates. In this paper we discuss and classify these models in terms of their\nalgorithmic inductive biases and present a new model, NN-CGC, that considers\nadditional information from the causal graph. NN-CGC tackles bias resulting\nfrom spurious variable interactions by implementing novel constraints on\nmodels, and it can be integrated with other representation learning methods. We\ntest the effectiveness of our method using three different base models on\ncommon benchmarks. Our results indicate that our model constraints lead to\nsignificant improvements, achieving new state-of-the-art results in treatment\neffects estimation. We also show that our method is robust to imperfect causal\ngraphs and that using partial causal information is preferable to ignoring it.\n","authors":["Roger Pros","Jordi Vitri√†"],"pdf_url":"https://arxiv.org/pdf/2404.12238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05728v3","updated":"2024-04-18T14:56:50Z","published":"2024-04-08T17:59:44Z","title":"A Large-Scale Exploration of $Œº$-Transfer","summary":"  Large neural network models have become a mainstay of natural language\nprocessing and computer vision, yet their initialization and learning rates are\nset in a largely heuristic fashion, potentially varying from paper to paper and\none model size to the next. The $\\mu$-Parameterization ($\\mu$P) offers a\npotential solution to these challenges, yielding scaling rules for model\ninitialization and learning rates, and reportedly enabling zero-shot\nhyperparameter transfer from small to large models in a variety of cases.\n  Despite the evident promise, the $\\mu$P scaling rules are not yet widely\nadopted, perhaps due to higher implementation complexity, many variations, or\ncomplex theoretical background. This work investigates $\\mu$P empirically,\nfocusing on the ubiquitous transformer architecture, and aims to answer a\nsimple question: does $\\mu$-Transfer yield optimal learning rates in practice?\nStudying models with up to 10B parameters and training budgets of up to 190B\ntokens, we find $\\mu$-Transfer works as intended for the majority of important\ncases, yet also identify a few cases where it may not.\n  Our experiment codebase is available at\nhttps://github.com/lucaslingle/mu_transformer/\n","authors":["Lucas Lingle"],"pdf_url":"https://arxiv.org/pdf/2404.05728v3.pdf","comment":"V3: Formatting, refs, extra data. V2: Formatting, SP experiment"},{"id":"http://arxiv.org/abs/2404.09683v2","updated":"2024-04-18T14:51:55Z","published":"2024-04-15T11:36:31Z","title":"Post-Training Network Compression for 3D Medical Image Segmentation:\n  Reducing Computational Efforts via Tucker Decomposition","summary":"  We address the computational barrier of deploying advanced deep learning\nsegmentation models in clinical settings by studying the efficacy of network\ncompression through tensor decomposition. We propose a post-training Tucker\nfactorization that enables the decomposition of pre-existing models to reduce\ncomputational requirements without impeding segmentation accuracy. We applied\nTucker decomposition to the convolutional kernels of the TotalSegmentator (TS)\nmodel, an nnU-Net model trained on a comprehensive dataset for automatic\nsegmentation of 117 anatomical structures. Our approach reduced the\nfloating-point operations (FLOPs) and memory required during inference,\noffering an adjustable trade-off between computational efficiency and\nsegmentation quality. This study utilized the publicly available TS dataset,\nemploying various downsampling factors to explore the relationship between\nmodel size, inference speed, and segmentation performance. The application of\nTucker decomposition to the TS model substantially reduced the model parameters\nand FLOPs across various compression rates, with limited loss in segmentation\naccuracy. We removed up to 88% of the model's parameters with no significant\nperformance changes in the majority of classes after fine-tuning. Practical\nbenefits varied across different graphics processing unit (GPU) architectures,\nwith more distinct speed-ups on less powerful hardware. Post-hoc network\ncompression via Tucker decomposition presents a viable strategy for reducing\nthe computational demand of medical image segmentation models without\nsubstantially sacrificing accuracy. This approach enables the broader adoption\nof advanced deep learning technologies in clinical practice, offering a way to\nnavigate the constraints of hardware capabilities.\n","authors":["Tobias Weber","Jakob Dexl","David R√ºgamer","Michael Ingrisch"],"pdf_url":"https://arxiv.org/pdf/2404.09683v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12228v1","updated":"2024-04-18T14:44:08Z","published":"2024-04-18T14:44:08Z","title":"Relationship Discovery for Drug Recommendation","summary":"  Medication recommendation systems are designed to deliver personalized drug\nsuggestions that are closely aligned with individual patient needs. Previous\nstudies have primarily concentrated on developing medication embeddings,\nachieving significant progress. Nonetheless, these approaches often fall short\nin accurately reflecting individual patient profiles, mainly due to challenges\nin distinguishing between various patient conditions and the inability to\nestablish precise correlations between specific conditions and appropriate\nmedications. In response to these issues, we introduce DisMed, a model that\nfocuses on patient conditions to enhance personalization. DisMed employs causal\ninference to discern clear, quantifiable causal links. It then examines patient\nconditions in depth, recognizing and adapting to the evolving nuances of these\nconditions, and mapping them directly to corresponding medications.\nAdditionally, DisMed leverages data from multiple patient visits to propose\ncombinations of medications. Comprehensive testing on real-world datasets\ndemonstrates that DisMed not only improves the customization of patient\nprofiles but also surpasses leading models in both precision and safety.\n","authors":["Xiang Li","Shunpan Liang","Yu Lei","Chen Li","Yulei Hou","Tengfei Ma"],"pdf_url":"https://arxiv.org/pdf/2404.12228v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08015v3","updated":"2024-04-18T14:31:11Z","published":"2022-07-16T21:06:26Z","title":"Parallel Best Arm Identification in Heterogeneous Environments","summary":"  In this paper, we study the tradeoffs between the time and the number of\ncommunication rounds of the best arm identification problem in the\nheterogeneous collaborative learning model, where multiple agents interact with\npossibly different environments and they want to learn in parallel an objective\nfunction in the aggregated environment. By proving almost tight upper and lower\nbounds, we show that collaborative learning in the heterogeneous setting is\ninherently more difficult than that in the homogeneous setting in terms of the\ntime-round tradeoff.\n","authors":["Nikolai Karpov","Qin Zhang"],"pdf_url":"https://arxiv.org/pdf/2207.08015v3.pdf","comment":"15 pages (published in SPAA 2024)"},{"id":"http://arxiv.org/abs/2404.12219v1","updated":"2024-04-18T14:30:46Z","published":"2024-04-18T14:30:46Z","title":"A Quadrature Approach for General-Purpose Batch Bayesian Optimization\n  via Probabilistic Lifting","summary":"  Parallelisation in Bayesian optimisation is a common strategy but faces\nseveral challenges: the need for flexibility in acquisition functions and\nkernel choices, flexibility dealing with discrete and continuous variables\nsimultaneously, model misspecification, and lastly fast massive\nparallelisation. To address these challenges, we introduce a versatile and\nmodular framework for batch Bayesian optimisation via probabilistic lifting\nwith kernel quadrature, called SOBER, which we present as a Python library\nbased on GPyTorch/BoTorch. Our framework offers the following unique benefits:\n(1) Versatility in downstream tasks under a unified approach. (2) A\ngradient-free sampler, which does not require the gradient of acquisition\nfunctions, offering domain-agnostic sampling (e.g., discrete and mixed\nvariables, non-Euclidean space). (3) Flexibility in domain prior distribution.\n(4) Adaptive batch size (autonomous determination of the optimal batch size).\n(5) Robustness against a misspecified reproducing kernel Hilbert space. (6)\nNatural stopping criterion.\n","authors":["Masaki Adachi","Satoshi Hayakawa","Martin J√∏rgensen","Saad Hamid","Harald Oberhauser","Michael A. Osborne"],"pdf_url":"https://arxiv.org/pdf/2404.12219v1.pdf","comment":"48 pages, 11 figures"},{"id":"http://arxiv.org/abs/2401.11609v3","updated":"2024-04-18T14:29:29Z","published":"2024-01-21T22:11:29Z","title":"Graph Edits for Counterfactual Explanations: A comparative study","summary":"  Counterfactuals have been established as a popular explainability technique\nwhich leverages a set of minimal edits to alter the prediction of a classifier.\nWhen considering conceptual counterfactuals on images, the edits requested\nshould correspond to salient concepts present in the input data. At the same\ntime, conceptual distances are defined by knowledge graphs, ensuring the\noptimality of conceptual edits. In this work, we extend previous endeavors on\ngraph edits as counterfactual explanations by conducting a comparative study\nwhich encompasses both supervised and unsupervised Graph Neural Network (GNN)\napproaches. To this end, we pose the following significant research question:\nshould we represent input data as graphs, which is the optimal GNN approach in\nterms of performance and time efficiency to generate minimal and meaningful\ncounterfactual explanations for black-box image classifiers?\n","authors":["Angeliki Dimitriou","Nikolaos Chaidos","Maria Lymperaiou","Giorgos Stamou"],"pdf_url":"https://arxiv.org/pdf/2401.11609v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12215v1","updated":"2024-04-18T14:20:19Z","published":"2024-04-18T14:20:19Z","title":"Quantifying Aleatoric and Epistemic Uncertainty with Proper Scoring\n  Rules","summary":"  Uncertainty representation and quantification are paramount in machine\nlearning and constitute an important prerequisite for safety-critical\napplications. In this paper, we propose novel measures for the quantification\nof aleatoric and epistemic uncertainty based on proper scoring rules, which are\nloss functions with the meaningful property that they incentivize the learner\nto predict ground-truth (conditional) probabilities. We assume two common\nrepresentations of (epistemic) uncertainty, namely, in terms of a credal set,\ni.e. a set of probability distributions, or a second-order distribution, i.e.,\na distribution over probability distributions. Our framework establishes a\nnatural bridge between these representations. We provide a formal justification\nof our approach and introduce new measures of epistemic and aleatoric\nuncertainty as concrete instantiations.\n","authors":["Paul Hofman","Yusuf Sale","Eyke H√ºllermeier"],"pdf_url":"https://arxiv.org/pdf/2404.12215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16078v2","updated":"2024-04-18T14:13:19Z","published":"2024-02-25T13:05:25Z","title":"Beyond Spatio-Temporal Representations: Evolving Fourier Transform for\n  Temporal Graphs","summary":"  We present the Evolving Graph Fourier Transform (EFT), the first invertible\nspectral transform that captures evolving representations on temporal graphs.\nWe motivate our work by the inadequacy of existing methods for capturing the\nevolving graph spectra, which are also computationally expensive due to the\ntemporal aspect along with the graph vertex domain. We view the problem as an\noptimization over the Laplacian of the continuous time dynamic graph.\nAdditionally, we propose pseudo-spectrum relaxations that decompose the\ntransformation process, making it highly computationally efficient. The EFT\nmethod adeptly captures the evolving graph's structural and positional\nproperties, making it effective for downstream tasks on evolving graphs. Hence,\nas a reference implementation, we develop a simple neural model induced with\nEFT for capturing evolving graph spectra. We empirically validate our\ntheoretical findings on a number of large-scale and standard temporal graph\nbenchmarks and demonstrate that our model achieves state-of-the-art\nperformance.\n","authors":["Anson Bastos","Kuldeep Singh","Abhishek Nadgeri","Manish Singh","Toyotaro Suzumura"],"pdf_url":"https://arxiv.org/pdf/2402.16078v2.pdf","comment":"Accepted as a full conference paper in the International Conference\n  on Learning Representations 2024"},{"id":"http://arxiv.org/abs/2404.12195v1","updated":"2024-04-18T13:57:18Z","published":"2024-04-18T13:57:18Z","title":"OpenBezoar: Small, Cost-Effective and Open Models Trained on Mixes of\n  Instruction Data","summary":"  Instruction fine-tuning pretrained LLMs for diverse downstream tasks has\ndemonstrated remarkable success and has captured the interest of both academics\nand practitioners. To ensure such fine-tuned LLMs align with human preferences,\ntechniques such as RLHF and DPO have emerged. At the same time, there is\nincreasing interest in smaller parameter counts for models. In this work, using\nOpenLLaMA 3Bv2 as a base model, we describe the recipe used to fine-tune the\nOpenBezoar family of models. In this recipe: We first generate synthetic\ninstruction fine-tuning data using an open and commercially non-restrictive\ninstruction fine-tuned variant of the Falcon-40B model under three schemes\nbased on: LaMini-LM, WizardLM/Evol-Instruct (with databricks-dolly-15k as a\nseed dataset) and Orca (with the Flan Collection as a seed dataset), then\nfilter these generations using GPT-4 as a human proxy. We then perform\ncost-effective QLoRA-based supervised fine-tuning sequentially with each\nscheme. The resulting checkpoint is further fine-tuned with a subset of the\nHH-RLHF dataset to minimize distribution shift prior to using the DPO loss to\nobtain the final checkpoint. Evaluation is done with the LM Eval Harness\ntasks/metrics as well as on MT-Bench using the \"LLM-as-a-judge\" framework with\nClaude 2.1, with the finding that the final checkpoint,\n\"OpenBezoar-HH-RLHF-DPO\", demonstrates superior performance over many models at\nthe 3B parameter scale, even outperforming the top model in one of the\ncategories on the Huggingface Open LLM Leaderboard. We release\n\"OpenBezoar-SFT\", \"OpenBezoar-HH-RLHF-SFT\", \"OpenBezoar-HH-RLHF-DPO\"\ncheckpoints, alongside our generated datasets on HuggingFace at\nhttps://huggingface.co/collections/SurgeGlobal/open-bezoar-6620a24923e12127e9e2b9cc\nand our codebase at\nhttps://bitbucket.org/paladinanalytics/workspace/projects/OP.\n","authors":["Chandeepa Dissanayake","Lahiru Lowe","Sachith Gunasekara","Yasiru Ratnayake"],"pdf_url":"https://arxiv.org/pdf/2404.12195v1.pdf","comment":"25 pages, 27 Figures, 8 Tables"},{"id":"http://arxiv.org/abs/2402.06885v2","updated":"2024-04-18T13:56:10Z","published":"2024-02-10T04:50:36Z","title":"DimVis: Interpreting Visual Clusters in Dimensionality Reduction With\n  Explainable Boosting Machine","summary":"  Dimensionality Reduction (DR) techniques such as t-SNE and UMAP are popular\nfor transforming complex datasets into simpler visual representations. However,\nwhile effective in uncovering general dataset patterns, these methods may\nintroduce artifacts and suffer from interpretability issues. This paper\npresents DimVis, a visualization tool that employs supervised Explainable\nBoosting Machine (EBM) models (trained on user-selected data of interest) as an\ninterpretation assistant for DR projections. Our tool facilitates\nhigh-dimensional data analysis by providing an interpretation of feature\nrelevance in visual clusters through interactive exploration of UMAP\nprojections. Specifically, DimVis uses a contrastive EBM model that is trained\nin real time to differentiate between the data inside and outside a cluster of\ninterest. Taking advantage of the inherent explainable nature of the EBM, we\nthen use this model to interpret the cluster itself via single and pairwise\nfeature comparisons in a ranking based on the EBM model's feature importance.\nThe applicability and effectiveness of DimVis are demonstrated via a use case\nand a usage scenario with real-world data. We also discuss the limitations and\npotential directions for future research.\n","authors":["Parisa Salmanian","Angelos Chatzimparmpas","Ali Can Karaca","Rafael M. Martins"],"pdf_url":"https://arxiv.org/pdf/2402.06885v2.pdf","comment":"This manuscript is accepted for publication in EuroVis 2024 MLVis\n  Workshop"},{"id":"http://arxiv.org/abs/2404.12190v1","updated":"2024-04-18T13:53:32Z","published":"2024-04-18T13:53:32Z","title":"Estimating the Hessian Matrix of Ranking Objectives for Stochastic\n  Learning to Rank with Gradient Boosted Trees","summary":"  Stochastic learning to rank (LTR) is a recent branch in the LTR field that\nconcerns the optimization of probabilistic ranking models. Their probabilistic\nbehavior enables certain ranking qualities that are impossible with\ndeterministic models. For example, they can increase the diversity of displayed\ndocuments, increase fairness of exposure over documents, and better balance\nexploitation and exploration through randomization. A core difficulty in LTR is\ngradient estimation, for this reason, existing stochastic LTR methods have been\nlimited to differentiable ranking models (e.g., neural networks). This is in\nstark contrast with the general field of LTR where Gradient Boosted Decision\nTrees (GBDTs) have long been considered the state-of-the-art.\n  In this work, we address this gap by introducing the first stochastic LTR\nmethod for GBDTs. Our main contribution is a novel estimator for the\nsecond-order derivatives, i.e., the Hessian matrix, which is a requirement for\neffective GBDTs. To efficiently compute both the first and second-order\nderivatives simultaneously, we incorporate our estimator into the existing\nPL-Rank framework, which was originally designed for first-order derivatives\nonly. Our experimental results indicate that stochastic LTR without the Hessian\nhas extremely poor performance, whilst the performance is competitive with the\ncurrent state-of-the-art with our estimated Hessian. Thus, through the\ncontribution of our novel Hessian estimation method, we have successfully\nintroduced GBDTs to stochastic LTR.\n","authors":["Jingwei Kang","Maarten de Rijke","Harrie Oosterhuis"],"pdf_url":"https://arxiv.org/pdf/2404.12190v1.pdf","comment":"SIGIR2024 conference Short paper track"},{"id":"http://arxiv.org/abs/2404.12187v1","updated":"2024-04-18T13:49:09Z","published":"2024-04-18T13:49:09Z","title":"Stability-informed Bayesian Optimization for MPC Cost Function Learning","summary":"  Designing predictive controllers towards optimal closed-loop performance\nwhile maintaining safety and stability is challenging. This work explores\nclosed-loop learning for predictive control parameters under imperfect\ninformation while considering closed-loop stability. We employ constrained\nBayesian optimization to learn a model predictive controller's (MPC) cost\nfunction parametrized as a feedforward neural network, optimizing closed-loop\nbehavior as well as minimizing model-plant mismatch. Doing so offers a high\ndegree of freedom and, thus, the opportunity for efficient and global\noptimization towards the desired and optimal closed-loop behavior. We extend\nthis framework by stability constraints on the learned controller parameters,\nexploiting the optimal value function of the underlying MPC as a Lyapunov\ncandidate. The effectiveness of the proposed approach is underlined in\nsimulations, highlighting its performance and safety capabilities.\n","authors":["Sebastian Hirt","Maik Pfefferkorn","Ali Mesbah","Rolf Findeisen"],"pdf_url":"https://arxiv.org/pdf/2404.12187v1.pdf","comment":"7 pages, 3 figures, accepted for NMPC 2024"},{"id":"http://arxiv.org/abs/2404.12186v1","updated":"2024-04-18T13:49:07Z","published":"2024-04-18T13:49:07Z","title":"Privacy-Preserving UCB Decision Process Verification via zk-SNARKs","summary":"  With the increasingly widespread application of machine learning, how to\nstrike a balance between protecting the privacy of data and algorithm\nparameters and ensuring the verifiability of machine learning has always been a\nchallenge. This study explores the intersection of reinforcement learning and\ndata privacy, specifically addressing the Multi-Armed Bandit (MAB) problem with\nthe Upper Confidence Bound (UCB) algorithm. We introduce zkUCB, an innovative\nalgorithm that employs the Zero-Knowledge Succinct Non-Interactive Argument of\nKnowledge (zk-SNARKs) to enhance UCB. zkUCB is carefully designed to safeguard\nthe confidentiality of training data and algorithmic parameters, ensuring\ntransparent UCB decision-making. Experiments highlight zkUCB's superior\nperformance, attributing its enhanced reward to judicious quantization bit\nusage that reduces information entropy in the decision-making process. zkUCB's\nproof size and verification time scale linearly with the execution steps of\nzkUCB. This showcases zkUCB's adept balance between data security and\noperational efficiency. This approach contributes significantly to the ongoing\ndiscourse on reinforcing data privacy in complex decision-making processes,\noffering a promising solution for privacy-sensitive applications.\n","authors":["Xikun Jiang","He Lyu","Chenhao Ying","Yibin Xu","Boris D√ºdder","Yuan Luo"],"pdf_url":"https://arxiv.org/pdf/2404.12186v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02286v3","updated":"2024-04-18T13:33:32Z","published":"2024-02-03T22:51:17Z","title":"Multi-Level Aggregation and Recursive Alignment Architecture for\n  Efficient Parallel Inference Segmentation Network","summary":"  Real-time semantic segmentation is a crucial research for real-world\napplications. However, many methods lay particular emphasis on reducing the\ncomputational complexity and model size, while largely sacrificing the\naccuracy. To tackle this problem, we propose a parallel inference network\ncustomized for semantic segmentation tasks to achieve a good trade-off between\nspeed and accuracy. We employ a shallow backbone to ensure real-time speed, and\npropose three core components to compensate for the reduced model capacity to\nimprove accuracy. Specifically, we first design a dual-pyramidal path\narchitecture (Multi-level Feature Aggregation Module, MFAM) to aggregate\nmulti-level features from the encoder to each scale, providing hierarchical\nclues for subsequent spatial alignment and corresponding in-network inference.\nThen, we build Recursive Alignment Module (RAM) by combining the flow-based\nalignment module with recursive upsampling architecture for accurate spatial\nalignment between multi-scale feature maps with half the computational\ncomplexity of the straightforward alignment method. Finally, we perform\nindependent parallel inference on the aligned features to obtain multi-scale\nscores, and adaptively fuse them through an attention-based Adaptive Scores\nFusion Module (ASFM) so that the final prediction can favor objects of multiple\nscales. Our framework shows a better balance between speed and accuracy than\nstate-of-the-art real-time methods on Cityscapes and CamVid datasets. We also\nconducted systematic ablation studies to gain insight into our motivation and\narchitectural design. Code is available at:\nhttps://github.com/Yanhua-Zhang/MFARANet.\n","authors":["Yanhua Zhang","Ke Zhang","Jingyu Wang","Yulin Wu","Wuwei Wang"],"pdf_url":"https://arxiv.org/pdf/2402.02286v3.pdf","comment":"15 pages, 9 figures and 12 Tables. Manuscript completed on April 30,\n  2022"},{"id":"http://arxiv.org/abs/2312.05771v3","updated":"2024-04-18T13:31:28Z","published":"2023-12-10T05:33:40Z","title":"Hacking Task Confounder in Meta-Learning","summary":"  Meta-learning enables rapid generalization to new tasks by learning knowledge\nfrom various tasks. It is intuitively assumed that as the training progresses,\na model will acquire richer knowledge, leading to better generalization\nperformance. However, our experiments reveal an unexpected result: there is\nnegative knowledge transfer between tasks, affecting generalization\nperformance. To explain this phenomenon, we conduct Structural Causal Models\n(SCMs) for causal analysis. Our investigation uncovers the presence of spurious\ncorrelations between task-specific causal factors and labels in meta-learning.\nFurthermore, the confounding factors differ across different batches. We refer\nto these confounding factors as \"Task Confounders\". Based on these findings, we\npropose a plug-and-play Meta-learning Causal Representation Learner (MetaCRL)\nto eliminate task confounders. It encodes decoupled generating factors from\nmultiple tasks and utilizes an invariant-based bi-level optimization mechanism\nto ensure their causality for meta-learning. Extensive experiments on various\nbenchmark datasets demonstrate that our work achieves state-of-the-art (SOTA)\nperformance.\n","authors":["Jingyao Wang","Yi Ren","Zeen Song","Jianqi Zhang","Changwen Zheng","Wenwen Qiang"],"pdf_url":"https://arxiv.org/pdf/2312.05771v3.pdf","comment":"Accepted by IJCAI 2024, 9 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2404.12172v1","updated":"2024-04-18T13:27:29Z","published":"2024-04-18T13:27:29Z","title":"How to Benchmark Vision Foundation Models for Semantic Segmentation?","summary":"  Recent vision foundation models (VFMs) have demonstrated proficiency in\nvarious tasks but require supervised fine-tuning to perform the task of\nsemantic segmentation effectively. Benchmarking their performance is essential\nfor selecting current models and guiding future model developments for this\ntask. The lack of a standardized benchmark complicates comparisons. Therefore,\nthe primary objective of this paper is to study how VFMs should be benchmarked\nfor semantic segmentation. To do so, various VFMs are fine-tuned under various\nsettings, and the impact of individual settings on the performance ranking and\ntraining time is assessed. Based on the results, the recommendation is to\nfine-tune the ViT-B variants of VFMs with a 16x16 patch size and a linear\ndecoder, as these settings are representative of using a larger model, more\nadvanced decoder and smaller patch size, while reducing training time by more\nthan 13 times. Using multiple datasets for training and evaluation is also\nrecommended, as the performance ranking across datasets and domain shifts\nvaries. Linear probing, a common practice for some VFMs, is not recommended, as\nit is not representative of end-to-end fine-tuning. The benchmarking setup\nrecommended in this paper enables a performance analysis of VFMs for semantic\nsegmentation. The findings of such an analysis reveal that pretraining with\npromptable segmentation is not beneficial, whereas masked image modeling (MIM)\nwith abstract representations is crucial, even more important than the type of\nsupervision used. The code for efficiently fine-tuning VFMs for semantic\nsegmentation can be accessed through the project page at:\nhttps://tue-mps.github.io/benchmark-vfm-ss/.\n","authors":["Tommie Kerssies","Daan de Geus","Gijs Dubbelman"],"pdf_url":"https://arxiv.org/pdf/2404.12172v1.pdf","comment":"CVPR 2024 Workshop Proceedings for the Second Workshop on Foundation\n  Models"},{"id":"http://arxiv.org/abs/2401.06091v3","updated":"2024-04-18T13:25:26Z","published":"2024-01-11T18:11:42Z","title":"A Closer Look at AUROC and AUPRC under Class Imbalance","summary":"  In machine learning (ML), a widespread adage is that the area under the\nprecision-recall curve (AUPRC) is a superior metric for model comparison to the\narea under the receiver operating characteristic (AUROC) for binary\nclassification tasks with class imbalance. This paper challenges this notion\nthrough novel mathematical analysis, illustrating that AUROC and AUPRC can be\nconcisely related in probabilistic terms. We demonstrate that AUPRC, contrary\nto popular belief, is not superior in cases of class imbalance and might even\nbe a harmful metric, given its inclination to unduly favor model improvements\nin subpopulations with more frequent positive labels. This bias can\ninadvertently heighten algorithmic disparities. Prompted by these insights, a\nthorough review of existing ML literature was conducted, utilizing large\nlanguage models to analyze over 1.5 million papers from arXiv. Our\ninvestigation focused on the prevalence and substantiation of the purported\nAUPRC superiority. The results expose a significant deficit in empirical\nbacking and a trend of misattributions that have fuelled the widespread\nacceptance of AUPRC's supposed advantages. Our findings represent a dual\ncontribution: a significant technical advancement in understanding metric\nbehaviors and a stark warning about unchecked assumptions in the ML community.\nAll experiments are accessible at\nhttps://github.com/mmcdermott/AUC_is_all_you_need.\n","authors":["Matthew B. A. McDermott","Lasse Hyldig Hansen","Haoran Zhang","Giovanni Angelotti","Jack Gallifant"],"pdf_url":"https://arxiv.org/pdf/2401.06091v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07982v2","updated":"2024-04-18T13:21:34Z","published":"2024-04-11T17:58:05Z","title":"Language Imbalance Can Boost Cross-lingual Generalisation","summary":"  Multilinguality is crucial for extending recent advancements in language\nmodelling to diverse linguistic communities. To maintain high performance while\nrepresenting multiple languages, multilingual models ideally align\nrepresentations, allowing what is learned in one language to generalise to\nothers. Prior research has emphasised the importance of parallel data and\nshared vocabulary elements as key factors for such alignment. In this study, we\ninvestigate an unintuitive novel driver of cross-lingual generalisation:\nlanguage imbalance. In controlled experiments on perfectly equivalent cloned\nlanguages, we observe that the existence of a predominant language during\ntraining boosts the performance of less frequent languages and leads to\nstronger alignment of model representations across languages. Furthermore, we\nfind that this trend is amplified with scale: with large enough models or long\nenough training, we observe that bilingual training data with a 90/10 language\nsplit yields better performance on both languages than a balanced 50/50 split.\nBuilding on these insights, we design training schemes that can improve\nperformance in all cloned languages, even without altering the training data.\nAs we extend our analysis to real languages, we find that infrequent languages\nstill benefit from frequent ones, yet whether language imbalance causes\ncross-lingual generalisation there is not conclusive.\n","authors":["Anton Sch√§fer","Shauli Ravfogel","Thomas Hofmann","Tiago Pimentel","Imanol Schlag"],"pdf_url":"https://arxiv.org/pdf/2404.07982v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11515v2","updated":"2024-04-18T12:56:02Z","published":"2024-02-18T09:07:30Z","title":"Optimal Parallelization Strategies for Active Flow Control in Deep\n  Reinforcement Learning-Based Computational Fluid Dynamics","summary":"  Deep Reinforcement Learning (DRL) has emerged as a promising approach for\nhandling highly dynamic and nonlinear Active Flow Control (AFC) problems.\nHowever, the computational cost associated with training DRL models presents a\nsignificant performance bottleneck. To address this challenge and enable\nefficient scaling on high-performance computing architectures, this study\nfocuses on optimizing DRL-based algorithms in parallel settings. We validate an\nexisting state-of-the-art DRL framework used for AFC problems and discuss its\nefficiency bottlenecks. Subsequently, by deconstructing the overall framework\nand conducting extensive scalability benchmarks for individual components, we\ninvestigate various hybrid parallelization configurations and propose efficient\nparallelization strategies. Moreover, we refine input/output (I/O) operations\nin multi-environment DRL training to tackle critical overhead associated with\ndata movement. Finally, we demonstrate the optimized framework for a typical\nAFC problem where near-linear scaling can be obtained for the overall\nframework. We achieve a significant boost in parallel efficiency from around\n49% to approximately 78%, and the training process is accelerated by\napproximately 47 times using 60 CPU cores. These findings are expected to\nprovide valuable insights for further advancements in DRL-based AFC studies.\n","authors":["Wang Jia","Hang Xu"],"pdf_url":"https://arxiv.org/pdf/2402.11515v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12150v1","updated":"2024-04-18T12:55:18Z","published":"2024-04-18T12:55:18Z","title":"Aligning language models with human preferences","summary":"  Language models (LMs) trained on vast quantities of text data can acquire\nsophisticated skills such as generating summaries, answering questions or\ngenerating code. However, they also manifest behaviors that violate human\npreferences, e.g., they can generate offensive content, falsehoods or\nperpetuate social biases. In this thesis, I explore several approaches to\naligning LMs with human preferences. First, I argue that aligning LMs can be\nseen as Bayesian inference: conditioning a prior (base, pretrained LM) on\nevidence about human preferences (Chapter 2). Conditioning on human preferences\ncan be implemented in numerous ways. In Chapter 3, I investigate the relation\nbetween two approaches to finetuning pretrained LMs using feedback given by a\nscoring function: reinforcement learning from human feedback (RLHF) and\ndistribution matching. I show that RLHF can be seen as a special case of\ndistribution matching but distributional matching is strictly more general. In\nchapter 4, I show how to extend the distribution matching to conditional\nlanguage models. Finally, in chapter 5 I explore a different root: conditioning\nan LM on human preferences already during pretraining. I show that involving\nhuman feedback from the very start tends to be more effective than using it\nonly during supervised finetuning. Overall, these results highlight the room\nfor alignment techniques different from and complementary to RLHF.\n","authors":["Tomasz Korbak"],"pdf_url":"https://arxiv.org/pdf/2404.12150v1.pdf","comment":"PhD thesis"},{"id":"http://arxiv.org/abs/2301.04872v2","updated":"2024-04-18T12:51:25Z","published":"2023-01-12T08:38:23Z","title":"Explainable Ponzi Schemes Detection on Ethereum","summary":"  Blockchain technology has been successfully exploited for deploying new\neconomic applications. However, it has started arousing the interest of\nmalicious actors who deliver scams to deceive honest users and to gain economic\nadvantages. Ponzi schemes are one of the most common scams. Here, we present a\nclassifier for detecting smart Ponzi contracts on Ethereum, which can be used\nas the backbone for developing detection tools. First, we release a labelled\ndata set with 4422 unique real-world smart contracts to address the problem of\nthe unavailability of labelled data. Then, we show that our classifier\noutperforms the ones proposed in the literature when considering the AUC as a\nmetric. Finally, we identify a small and effective set of features that ensures\na good classification quality and investigate their impacts on the\nclassification using eXplainable AI techniques.\n","authors":["Letterio Galletta","Fabio Pinelli"],"pdf_url":"https://arxiv.org/pdf/2301.04872v2.pdf","comment":"Accepted to ACM SAC'24"},{"id":"http://arxiv.org/abs/2404.12141v1","updated":"2024-04-18T12:43:39Z","published":"2024-04-18T12:43:39Z","title":"MolCRAFT: Structure-Based Drug Design in Continuous Parameter Space","summary":"  Generative models for structure-based drug design (SBDD) have shown promising\nresults in recent years. Existing works mainly focus on how to generate\nmolecules with higher binding affinity, ignoring the feasibility prerequisites\nfor generated 3D poses and resulting in false positives. We conduct thorough\nstudies on key factors of ill-conformational problems when applying\nautoregressive methods and diffusion to SBDD, including mode collapse and\nhybrid continuous-discrete space. In this paper, we introduce \\ours, the first\nSBDD model that operates in the continuous parameter space, together with a\nnovel noise reduced sampling strategy. Empirical results show that our model\nconsistently achieves superior performance in binding affinity with more stable\n3D structure, demonstrating our ability to accurately model interatomic\ninteractions. To our best knowledge, MolCRAFT is the first to achieve\nreference-level Vina Scores (-6.59 kcal/mol), outperforming other strong\nbaselines by a wide margin (-0.84 kcal/mol). Code is available at\nhttps://github.com/AlgoMole/MolCRAFT.\n","authors":["Yanru Qu","Keyue Qiu","Yuxuan Song","Jingjing Gong","Jiawei Han","Mingyue Zheng","Hao Zhou","Wei-Ying Ma"],"pdf_url":"https://arxiv.org/pdf/2404.12141v1.pdf","comment":"19 pages, 11 figures"},{"id":"http://arxiv.org/abs/2404.12130v1","updated":"2024-04-18T12:31:48Z","published":"2024-04-18T12:31:48Z","title":"One-Shot Sequential Federated Learning for Non-IID Data by Enhancing\n  Local Model Diversity","summary":"  Traditional federated learning mainly focuses on parallel settings (PFL),\nwhich can suffer significant communication and computation costs. In contrast,\none-shot and sequential federated learning (SFL) have emerged as innovative\nparadigms to alleviate these costs. However, the issue of non-IID (Independent\nand Identically Distributed) data persists as a significant challenge in\none-shot and SFL settings, exacerbated by the restricted communication between\nclients. In this paper, we improve the one-shot sequential federated learning\nfor non-IID data by proposing a local model diversity-enhancing strategy.\nSpecifically, to leverage the potential of local model diversity for improving\nmodel performance, we introduce a local model pool for each client that\ncomprises diverse models generated during local training, and propose two\ndistance measurements to further enhance the model diversity and mitigate the\neffect of non-IID data. Consequently, our proposed framework can improve the\nglobal model performance while maintaining low communication costs. Extensive\nexperiments demonstrate that our method exhibits superior performance to\nexisting one-shot PFL methods and achieves better accuracy compared with\nstate-of-the-art one-shot SFL methods on both label-skew and domain-shift tasks\n(e.g., 6%+ accuracy improvement on the CIFAR-10 dataset).\n","authors":["Naibo Wang","Yuchen Deng","Wenjie Feng","Shichen Fan","Jianwei Yin","See-Kiong Ng"],"pdf_url":"https://arxiv.org/pdf/2404.12130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13846v2","updated":"2024-04-18T12:22:12Z","published":"2024-03-18T05:18:19Z","title":"A Clustering Method with Graph Maximum Decoding Information","summary":"  The clustering method based on graph models has garnered increased attention\nfor its widespread applicability across various knowledge domains. Its\nadaptability to integrate seamlessly with other relevant applications endows\nthe graph model-based clustering analysis with the ability to robustly extract\n\"natural associations\" or \"graph structures\" within datasets, facilitating the\nmodelling of relationships between data points. Despite its efficacy, the\ncurrent clustering method utilizing the graph-based model overlooks the\nuncertainty associated with random walk access between nodes and the embedded\nstructural information in the data. To address this gap, we present a novel\nClustering method for Maximizing Decoding Information within graph-based\nmodels, named CMDI. CMDI innovatively incorporates two-dimensional structural\ninformation theory into the clustering process, consisting of two phases: graph\nstructure extraction and graph vertex partitioning. Within CMDI, graph\npartitioning is reformulated as an abstract clustering problem, leveraging\nmaximum decoding information to minimize uncertainty associated with random\nvisits to vertices. Empirical evaluations on three real-world datasets\ndemonstrate that CMDI outperforms classical baseline methods, exhibiting a\nsuperior decoding information ratio (DI-R). Furthermore, CMDI showcases\nheightened efficiency, particularly when considering prior knowledge (PK).\nThese findings underscore the effectiveness of CMDI in enhancing decoding\ninformation quality and computational efficiency, positioning it as a valuable\ntool in graph-based clustering analyses.\n","authors":["Xinrun Xu","Manying Lv","Zhanbiao Lian","Yurong Wu","Jin Yan","Shan Jiang","Zhiming Ding"],"pdf_url":"https://arxiv.org/pdf/2403.13846v2.pdf","comment":"9 pages, 9 figures, IJCNN 2024"},{"id":"http://arxiv.org/abs/2310.08182v2","updated":"2024-04-18T11:57:49Z","published":"2023-10-12T10:17:40Z","title":"XIMAGENET-12: An Explainable AI Benchmark Dataset for Model Robustness\n  Evaluation","summary":"  Despite the promising performance of existing visual models on public\nbenchmarks, the critical assessment of their robustness for real-world\napplications remains an ongoing challenge. To bridge this gap, we propose an\nexplainable visual dataset, XIMAGENET-12, to evaluate the robustness of visual\nmodels. XIMAGENET-12 consists of over 200K images with 15,410 manual semantic\nannotations. Specifically, we deliberately selected 12 categories from\nImageNet, representing objects commonly encountered in practical life. To\nsimulate real-world situations, we incorporated six diverse scenarios, such as\noverexposure, blurring, and color changes, etc. We further develop a\nquantitative criterion for robustness assessment, allowing for a nuanced\nunderstanding of how visual models perform under varying conditions, notably in\nrelation to the background. We make the XIMAGENET-12 dataset and its\ncorresponding code openly accessible at\n\\url{https://sites.google.com/view/ximagenet-12/home}. We expect the\nintroduction of the XIMAGENET-12 dataset will empower researchers to thoroughly\nevaluate the robustness of their visual models under challenging conditions.\n","authors":["Qiang Li","Dan Zhang","Shengzhao Lei","Xun Zhao","Porawit Kamnoedboon","WeiWei Li","Junhao Dong","Shuyan Li"],"pdf_url":"https://arxiv.org/pdf/2310.08182v2.pdf","comment":"Paper accepted by Synthetic Data for Computer Vision Workshop @ IEEE\n  CVPR 2024"},{"id":"http://arxiv.org/abs/2307.15361v2","updated":"2024-04-18T11:44:26Z","published":"2023-07-28T07:23:01Z","title":"Confident Feature Ranking","summary":"  Machine learning models are widely applied in various fields. Stakeholders\noften use post-hoc feature importance methods to better understand the input\nfeatures' contribution to the models' predictions. The interpretation of the\nimportance values provided by these methods is frequently based on the relative\norder of the features (their ranking) rather than the importance values\nthemselves. Since the order may be unstable, we present a framework for\nquantifying the uncertainty in global importance values. We propose a novel\nmethod for the post-hoc interpretation of feature importance values that is\nbased on the framework and pairwise comparisons of the feature importance\nvalues. This method produces simultaneous confidence intervals for the\nfeatures' ranks, which include the ``true'' (infinite sample) ranks with high\nprobability, and enables the selection of the set of the top-k important\nfeatures.\n","authors":["Bitya Neuhof","Yuval Benjamini"],"pdf_url":"https://arxiv.org/pdf/2307.15361v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12104v1","updated":"2024-04-18T11:38:25Z","published":"2024-04-18T11:38:25Z","title":"Ethical-Lens: Curbing Malicious Usages of Open-Source Text-to-Image\n  Models","summary":"  The burgeoning landscape of text-to-image models, exemplified by innovations\nsuch as Midjourney and DALLE 3, has revolutionized content creation across\ndiverse sectors. However, these advancements bring forth critical ethical\nconcerns, particularly with the misuse of open-source models to generate\ncontent that violates societal norms. Addressing this, we introduce\nEthical-Lens, a framework designed to facilitate the value-aligned usage of\ntext-to-image tools without necessitating internal model revision. Ethical-Lens\nensures value alignment in text-to-image models across toxicity and bias\ndimensions by refining user commands and rectifying model outputs. Systematic\nevaluation metrics, combining GPT4-V, HEIM, and FairFace scores, assess\nalignment capability. Our experiments reveal that Ethical-Lens enhances\nalignment capabilities to levels comparable with or superior to commercial\nmodels like DALLE 3, ensuring user-generated content adheres to ethical\nstandards while maintaining image quality. This study indicates the potential\nof Ethical-Lens to ensure the sustainable development of open-source\ntext-to-image tools and their beneficial integration into society. Our code is\navailable at https://github.com/yuzhu-cai/Ethical-Lens.\n","authors":["Yuzhu Cai","Sheng Yin","Yuxi Wei","Chenxin Xu","Weibo Mao","Felix Juefei-Xu","Siheng Chen","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2404.12104v1.pdf","comment":"42 pages, 17 figures, 29 tables"},{"id":"http://arxiv.org/abs/2404.12097v1","updated":"2024-04-18T11:29:43Z","published":"2024-04-18T11:29:43Z","title":"MPC of Uncertain Nonlinear Systems with Meta-Learning for Fast\n  Adaptation of Neural Predictive Models","summary":"  In this paper, we consider the problem of reference tracking in uncertain\nnonlinear systems. A neural State-Space Model (NSSM) is used to approximate the\nnonlinear system, where a deep encoder network learns the nonlinearity from\ndata, and a state-space component captures the temporal relationship. This\ntransforms the nonlinear system into a linear system in a latent space,\nenabling the application of model predictive control (MPC) to determine\neffective control actions. Our objective is to design the optimal controller\nusing limited data from the \\textit{target system} (the system of interest). To\nthis end, we employ an implicit model-agnostic meta-learning (iMAML) framework\nthat leverages information from \\textit{source systems} (systems that share\nsimilarities with the target system) to expedite training in the target system\nand enhance its control performance. The framework consists of two phases: the\n(offine) meta-training phase learns a aggregated NSSM using data from source\nsystems, and the (online) meta-inference phase quickly adapts this aggregated\nmodel to the target system using only a few data points and few online training\niterations, based on local loss function gradients. The iMAML algorithm\nexploits the implicit function theorem to exactly compute the gradient during\ntraining, without relying on the entire optimization path. By focusing solely\non the optimal solution, rather than the path, we can meta-train with less\nstorage complexity and fewer approximations than other contemporary\nmeta-learning algorithms. We demonstrate through numerical examples that our\nproposed method can yield accurate predictive models by adaptation, resulting\nin a downstream MPC that outperforms several baselines.\n","authors":["Jiaqi Yan","Ankush Chakrabarty","Alisa Rupenyan","John Lygeros"],"pdf_url":"https://arxiv.org/pdf/2404.12097v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12096v1","updated":"2024-04-18T11:29:23Z","published":"2024-04-18T11:29:23Z","title":"LongEmbed: Extending Embedding Models for Long Context Retrieval","summary":"  Embedding models play a pivot role in modern NLP applications such as IR and\nRAG. While the context limit of LLMs has been pushed beyond 1 million tokens,\nembedding models are still confined to a narrow context window not exceeding 8k\ntokens, refrained from application scenarios requiring long inputs such as\nlegal contracts. This paper explores context window extension of existing\nembedding models, pushing the limit to 32k without requiring additional\ntraining. First, we examine the performance of current embedding models for\nlong context retrieval on our newly constructed LongEmbed benchmark. LongEmbed\ncomprises two synthetic tasks and four carefully chosen real-world tasks,\nfeaturing documents of varying length and dispersed target information.\nBenchmarking results underscore huge room for improvement in these models.\nBased on this, comprehensive experiments show that training-free context window\nextension strategies like position interpolation can effectively extend the\ncontext window of existing embedding models by several folds, regardless of\ntheir original context being 512 or beyond 4k. Furthermore, for models\nemploying absolute position encoding (APE), we show the possibility of further\nfine-tuning to harvest notable performance gains while strictly preserving\noriginal behavior for short inputs. For models using rotary position embedding\n(RoPE), significant enhancements are observed when employing RoPE-specific\nmethods, such as NTK and SelfExtend, indicating RoPE's superiority over APE for\ncontext window extension. To facilitate future research, we release E5-Base-4k\nand E5-RoPE-Base, along with the LongEmbed benchmark.\n","authors":["Dawei Zhu","Liang Wang","Nan Yang","Yifan Song","Wenhao Wu","Furu Wei","Sujian Li"],"pdf_url":"https://arxiv.org/pdf/2404.12096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.11368v4","updated":"2024-04-18T11:18:20Z","published":"2022-11-21T11:35:25Z","title":"Precise Asymptotics for Spectral Methods in Mixed Generalized Linear\n  Models","summary":"  In a mixed generalized linear model, the objective is to learn multiple\nsignals from unlabeled observations: each sample comes from exactly one signal,\nbut it is not known which one. We consider the prototypical problem of\nestimating two statistically independent signals in a mixed generalized linear\nmodel with Gaussian covariates. Spectral methods are a popular class of\nestimators which output the top two eigenvectors of a suitable data-dependent\nmatrix. However, despite the wide applicability, their design is still obtained\nvia heuristic considerations, and the number of samples $n$ needed to guarantee\nrecovery is super-linear in the signal dimension $d$. In this paper, we develop\nexact asymptotics on spectral methods in the challenging proportional regime in\nwhich $n, d$ grow large and their ratio converges to a finite constant. By\ndoing so, we are able to optimize the design of the spectral method, and\ncombine it with a simple linear estimator, in order to minimize the estimation\nerror. Our characterization exploits a mix of tools from random matrices, free\nprobability and the theory of approximate message passing algorithms. Numerical\nsimulations for mixed linear regression and phase retrieval demonstrate the\nadvantage enabled by our analysis over existing designs of spectral methods.\n","authors":["Yihan Zhang","Marco Mondelli","Ramji Venkataramanan"],"pdf_url":"https://arxiv.org/pdf/2211.11368v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11172v2","updated":"2024-04-18T11:17:43Z","published":"2024-04-17T08:42:42Z","title":"Deep Neural Networks via Complex Network Theory: a Perspective","summary":"  Deep Neural Networks (DNNs) can be represented as graphs whose links and\nvertices iteratively process data and solve tasks sub-optimally. Complex\nNetwork Theory (CNT), merging statistical physics with graph theory, provides a\nmethod for interpreting neural networks by analysing their weights and neuron\nstructures. However, classic works adapt CNT metrics that only permit a\ntopological analysis as they do not account for the effect of the input data.\nIn addition, CNT metrics have been applied to a limited range of architectures,\nmainly including Fully Connected neural networks. In this work, we extend the\nexisting CNT metrics with measures that sample from the DNNs' training\ndistribution, shifting from a purely topological analysis to one that connects\nwith the interpretability of deep learning. For the novel metrics, in addition\nto the existing ones, we provide a mathematical formalisation for Fully\nConnected, AutoEncoder, Convolutional and Recurrent neural networks, of which\nwe vary the activation functions and the number of hidden layers. We show that\nthese metrics differentiate DNNs based on the architecture, the number of\nhidden layers, and the activation function. Our contribution provides a method\nrooted in physics for interpreting DNNs that offers insights beyond the\ntraditional input-output relationship and the CNT topological analysis.\n","authors":["Emanuele La Malfa","Gabriele La Malfa","Giuseppe Nicosia","Vito Latora"],"pdf_url":"https://arxiv.org/pdf/2404.11172v2.pdf","comment":"IJCAI'24 (full paper, main track)"},{"id":"http://arxiv.org/abs/2404.12077v1","updated":"2024-04-18T10:59:54Z","published":"2024-04-18T10:59:54Z","title":"TIMIT Speaker Profiling: A Comparison of Multi-task learning and\n  Single-task learning Approaches","summary":"  This study employs deep learning techniques to explore four speaker profiling\ntasks on the TIMIT dataset, namely gender classification, accent\nclassification, age estimation, and speaker identification, highlighting the\npotential and challenges of multi-task learning versus single-task models. The\nmotivation for this research is twofold: firstly, to empirically assess the\nadvantages and drawbacks of multi-task learning over single-task models in the\ncontext of speaker profiling; secondly, to emphasize the undiminished\nsignificance of skillful feature engineering for speaker recognition tasks. The\nfindings reveal challenges in accent classification, and multi-task learning is\nfound advantageous for tasks of similar complexity. Non-sequential features are\nfavored for speaker recognition, but sequential ones can serve as starting\npoints for complex models. The study underscores the necessity of meticulous\nexperimentation and parameter tuning for deep learning models.\n","authors":["Rong Wang","Kun Sun"],"pdf_url":"https://arxiv.org/pdf/2404.12077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12070v1","updated":"2024-04-18T10:45:47Z","published":"2024-04-18T10:45:47Z","title":"Towards an Approximation Theory of Observable Operator Models","summary":"  Observable operator models (OOMs) offer a powerful framework for modelling\nstochastic processes, surpassing the traditional hidden Markov models (HMMs) in\ngenerality and efficiency. However, using OOMs to model infinite-dimensional\nprocesses poses significant theoretical challenges. This article explores a\nrigorous approach to developing an approximation theory for OOMs of\ninfinite-dimensional processes. Building upon foundational work outlined in an\nunpublished tutorial [Jae98], an inner product structure on the space of future\ndistributions is rigorously established and the continuity of observable\noperators with respect to the associated 2-norm is proven. The original theorem\nproven in this thesis describes a fundamental obstacle in making an\ninfinite-dimensional space of future distributions into a Hilbert space. The\npresented findings lay the groundwork for future research in approximating\nobservable operators of infinite-dimensional processes, while a remedy to the\nencountered obstacle is suggested.\n","authors":["Wojciech Anyszka"],"pdf_url":"https://arxiv.org/pdf/2404.12070v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2106.09435v3","updated":"2024-04-18T10:41:49Z","published":"2021-06-17T12:34:18Z","title":"Multi-Agent Training beyond Zero-Sum with Correlated Equilibrium\n  Meta-Solvers","summary":"  Two-player, constant-sum games are well studied in the literature, but there\nhas been limited progress outside of this setting. We propose Joint\nPolicy-Space Response Oracles (JPSRO), an algorithm for training agents in\nn-player, general-sum extensive form games, which provably converges to an\nequilibrium. We further suggest correlated equilibria (CE) as promising\nmeta-solvers, and propose a novel solution concept Maximum Gini Correlated\nEquilibrium (MGCE), a principled and computationally efficient family of\nsolutions for solving the correlated equilibrium selection problem. We conduct\nseveral experiments using CE meta-solvers for JPSRO and demonstrate convergence\non n-player, general-sum games.\n","authors":["Luke Marris","Paul Muller","Marc Lanctot","Karl Tuyls","Thore Graepel"],"pdf_url":"https://arxiv.org/pdf/2106.09435v3.pdf","comment":"ICML 2021, 9 pages, coded implementation available in\n  https://github.com/deepmind/open_spiel/ (jpsro.py in examples)"},{"id":"http://arxiv.org/abs/2404.12064v1","updated":"2024-04-18T10:23:10Z","published":"2024-04-18T10:23:10Z","title":"PureForest: A Large-scale Aerial Lidar and Aerial Imagery Dataset for\n  Tree Species Classification in Monospecific Forests","summary":"  Knowledge of tree species distribution is fundamental to managing forests.\nNew deep learning approaches promise significant accuracy gains for forest\nmapping, and are becoming a critical tool for mapping multiple tree species at\nscale. To advance the field, deep learning researchers need large benchmark\ndatasets with high-quality annotations. To this end, we present the PureForest\ndataset: a large-scale, open, multimodal dataset designed for tree species\nclassification from both Aerial Lidar Scanning (ALS) point clouds and Very High\nResolution (VHR) aerial images. Most current public Lidar datasets for tree\nspecies classification have low diversity as they only span a small area of a\nfew dozen annotated hectares at most. In contrast, PureForest has 18 tree\nspecies grouped into 13 semantic classes, and spans 339 km$^2$ across 449\ndistinct monospecific forests, and is to date the largest and most\ncomprehensive Lidar dataset for the identification of tree species. By making\nPureForest publicly available, we hope to provide a challenging benchmark\ndataset to support the development of deep learning approaches for tree species\nidentification from Lidar and/or aerial imagery. In this data paper, we\ndescribe the annotation workflow, the dataset, the recommended evaluation\nmethodology, and establish a baseline performance from both 3D and 2D\nmodalities.\n","authors":["Charles Gaydon","Floryne Roche"],"pdf_url":"https://arxiv.org/pdf/2404.12064v1.pdf","comment":"14 pages | 5 figures | Dataset is available at\n  http://huggingface.co/datasets/IGNF/PureForest"},{"id":"http://arxiv.org/abs/2404.12063v1","updated":"2024-04-18T10:21:28Z","published":"2024-04-18T10:21:28Z","title":"FastVPINNs: Tensor-Driven Acceleration of VPINNs for Complex Geometries","summary":"  Variational Physics-Informed Neural Networks (VPINNs) utilize a variational\nloss function to solve partial differential equations, mirroring Finite Element\nAnalysis techniques. Traditional hp-VPINNs, while effective for high-frequency\nproblems, are computationally intensive and scale poorly with increasing\nelement counts, limiting their use in complex geometries. This work introduces\nFastVPINNs, a tensor-based advancement that significantly reduces computational\noverhead and improves scalability. Using optimized tensor operations,\nFastVPINNs achieve a 100-fold reduction in the median training time per epoch\ncompared to traditional hp-VPINNs. With proper choice of hyperparameters,\nFastVPINNs surpass conventional PINNs in both speed and accuracy, especially in\nproblems with high-frequency solutions. Demonstrated effectiveness in solving\ninverse problems on complex domains underscores FastVPINNs' potential for\nwidespread application in scientific and engineering challenges, opening new\navenues for practical implementations in scientific machine learning.\n","authors":["Thivin Anandh","Divij Ghose","Himanshu Jain","Sashikumaar Ganesan"],"pdf_url":"https://arxiv.org/pdf/2404.12063v1.pdf","comment":"31 pages, 19 figures, 4 algorithms"},{"id":"http://arxiv.org/abs/2306.01162v2","updated":"2024-04-18T10:04:16Z","published":"2023-06-01T21:35:20Z","title":"Integrated Sensing-Communication-Computation for Edge Artificial\n  Intelligence","summary":"  Edge artificial intelligence (AI) has been a promising solution towards 6G to\nempower a series of advanced techniques such as digital twins, holographic\nprojection, semantic communications, and auto-driving, for achieving\nintelligence of everything. The performance of edge AI tasks, including edge\nlearning and edge AI inference, depends on the quality of three highly coupled\nprocesses, i.e., sensing for data acquisition, computation for information\nextraction, and communication for information transmission. However, these\nthree modules need to compete for network resources for enhancing their own\nquality-of-services. To this end, integrated sensing-communication-computation\n(ISCC) is of paramount significance for improving resource utilization as well\nas achieving the customized goals of edge AI tasks. By investigating the\ninterplay among the three modules, this article presents various kinds of ISCC\nschemes for federated edge learning tasks and edge AI inference tasks in both\napplication and physical layers.\n","authors":["Dingzhu Wen","Xiaoyang Li","Yong Zhou","Yuanming Shi","Sheng Wu","Chunxiao Jiang"],"pdf_url":"https://arxiv.org/pdf/2306.01162v2.pdf","comment":"This paper was accepted by IEEE Internet of Things Magazine on\n  April-18-2024"},{"id":"http://arxiv.org/abs/2310.01884v2","updated":"2024-04-18T09:25:34Z","published":"2023-10-03T08:37:21Z","title":"Enhanced LFTSformer: A Novel Long-Term Financial Time Series Prediction\n  Model Using Advanced Feature Engineering and the DS Encoder Informer\n  Architecture","summary":"  This study presents a groundbreaking model for forecasting long-term\nfinancial time series, termed the Enhanced LFTSformer. The model distinguishes\nitself through several significant innovations:\n  (1) VMD-MIC+FE Feature Engineering: The incorporation of sophisticated\nfeature engineering techniques, specifically through the integration of\nVariational Mode Decomposition (VMD), Maximal Information Coefficient (MIC),\nand feature engineering (FE) methods, enables comprehensive perception and\nextraction of deep-level features from complex and variable financial datasets.\n(2) DS Encoder Informer: The architecture of the original Informer has been\nmodified by adopting a Stacked Informer structure in the encoder, and an\ninnovative introduction of a multi-head decentralized sparse attention\nmechanism, referred to as the Distributed Informer. This modification has led\nto a reduction in the number of attention blocks, thereby enhancing both the\ntraining accuracy and speed. (3) GC Enhanced Adam \\& Dynamic Loss Function: The\ndeployment of a Gradient Clipping-enhanced Adam optimization algorithm and a\ndynamic loss function represents a pioneering approach within the domain of\nfinancial time series prediction. This novel methodology optimizes model\nperformance and adapts more dynamically to evolving data patterns.\n  Systematic experimentation on a range of benchmark stock market datasets\ndemonstrates that the Enhanced LFTSformer outperforms traditional machine\nlearning models and other Informer-based architectures in terms of prediction\naccuracy, adaptability, and generality. Furthermore, the paper identifies\npotential avenues for future enhancements, with a particular focus on the\nidentification and quantification of pivotal impacting events and news. This is\naimed at further refining the predictive efficacy of the model.\n","authors":["Jianan Zhang","Hongyi Duan"],"pdf_url":"https://arxiv.org/pdf/2310.01884v2.pdf","comment":"The methodology, experiments, and language of the original version\n  have been completely updated. Detailed adjustments will be made in the future"},{"id":"http://arxiv.org/abs/2404.12025v1","updated":"2024-04-18T09:22:08Z","published":"2024-04-18T09:22:08Z","title":"PID Tuning using Cross-Entropy Deep Learning: a Lyapunov Stability\n  Analysis","summary":"  Underwater Unmanned Vehicles (UUVs) have to constantly compensate for the\nexternal disturbing forces acting on their body. Adaptive Control theory is\ncommonly used there to grant the control law some flexibility in its response\nto process variation. Today, learning-based (LB) adaptive methods are leading\nthe field where model-based control structures are combined with deep\nmodel-free learning algorithms. This work proposes experiments and metrics to\nempirically study the stability of such a controller. We perform this stability\nanalysis on a LB adaptive control system whose adaptive parameters are\ndetermined using a Cross-Entropy Deep Learning method.\n","authors":["Hector Kohler","Benoit Clement","Thomas Chaffre","Gilles Le Chenadec"],"pdf_url":"https://arxiv.org/pdf/2404.12025v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12010v1","updated":"2024-04-18T09:02:45Z","published":"2024-04-18T09:02:45Z","title":"ParaFusion: A Large-Scale LLM-Driven English Paraphrase Dataset Infused\n  with High-Quality Lexical and Syntactic Diversity","summary":"  Paraphrase generation is a pivotal task in natural language processing (NLP).\nExisting datasets in the domain lack syntactic and lexical diversity, resulting\nin paraphrases that closely resemble the source sentences. Moreover, these\ndatasets often contain hate speech and noise, and may unintentionally include\nnon-English language sentences. This research introduces ParaFusion, a\nlarge-scale, high-quality English paraphrase dataset developed using Large\nLanguage Models (LLM) to address these challenges. ParaFusion augments existing\ndatasets with high-quality data, significantly enhancing both lexical and\nsyntactic diversity while maintaining close semantic similarity. It also\nmitigates the presence of hate speech and reduces noise, ensuring a cleaner and\nmore focused English dataset. Results show that ParaFusion offers at least a\n25% improvement in both syntactic and lexical diversity, measured across\nseveral metrics for each data source. The paper also aims to set a gold\nstandard for paraphrase evaluation as it contains one of the most comprehensive\nevaluation strategies to date. The results underscore the potential of\nParaFusion as a valuable resource for improving NLP applications.\n","authors":["Lasal Jayawardena","Prasan Yapa"],"pdf_url":"https://arxiv.org/pdf/2404.12010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17402v2","updated":"2024-04-18T08:58:27Z","published":"2024-02-27T10:48:56Z","title":"Beacon, a lightweight deep reinforcement learning benchmark library for\n  flow control","summary":"  Recently, the increasing use of deep reinforcement learning for flow control\nproblems has led to a new area of research, focused on the coupling and the\nadaptation of the existing algorithms to the control of numerical fluid\ndynamics environments. Although still in its infancy, the field has seen\nmultiple successes in a short time span, and its fast development pace can\ncertainly be partly imparted to the open-source effort that drives the\nexpansion of the community. Yet, this emerging domain still misses a common\nground to (i) ensure the reproducibility of the results, and (ii) offer a\nproper ad-hoc benchmarking basis. To this end, we propose Beacon, an\nopen-source benchmark library composed of seven lightweight 1D and 2D flow\ncontrol problems with various characteristics, action and observation space\ncharacteristics, and CPU requirements. In this contribution, the seven\nconsidered problems are described, and reference control solutions are\nprovided. The sources for the following work are available at\nhttps://github.com/jviquerat/beacon.\n","authors":["Jonathan Viquerat","Philippe Meliga","Pablo Jeken","Elie Hachem"],"pdf_url":"https://arxiv.org/pdf/2402.17402v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.02465v2","updated":"2024-04-18T08:49:21Z","published":"2023-08-04T17:04:58Z","title":"Label Inference Attacks against Node-level Vertical Federated GNNs","summary":"  Federated learning enables collaborative training of machine learning models\nby keeping the raw data of the involved workers private. Three of its main\nobjectives are to improve the models' privacy, security, and scalability.\nVertical Federated Learning (VFL) offers an efficient cross-silo setting where\na few parties collaboratively train a model without sharing the same features.\nIn such a scenario, classification labels are commonly considered sensitive\ninformation held exclusively by one (active) party, while other (passive)\nparties use only their local information. Recent works have uncovered important\nflaws of VFL, leading to possible label inference attacks under the assumption\nthat the attacker has some, even limited, background knowledge on the relation\nbetween labels and data. In this work, we are the first (to the best of our\nknowledge) to investigate label inference attacks on VFL using a\nzero-background knowledge strategy. To formulate our proposal, we focus on\nGraph Neural Networks (GNNs) as a target model for the underlying VFL. In\nparticular, we refer to node classification tasks, which are widely studied,\nand GNNs have shown promising results. Our proposed attack, BlindSage, provides\nimpressive results in the experiments, achieving nearly 100% accuracy in most\ncases. Even when the attacker has no information about the used architecture or\nthe number of classes, the accuracy remains above 90% in most instances.\nFinally, we observe that well-known defenses cannot mitigate our attack without\naffecting the model's performance on the main classification task.\n","authors":["Marco Arazzi","Mauro Conti","Stefanos Koffas","Marina Krcek","Antonino Nocera","Stjepan Picek","Jing Xu"],"pdf_url":"https://arxiv.org/pdf/2308.02465v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15182v2","updated":"2024-04-18T08:40:58Z","published":"2024-03-22T13:11:26Z","title":"PDE-CNNs: Axiomatic Derivations and Applications","summary":"  PDE-based Group Convolutional Neural Networks (PDE-G-CNNs) utilize solvers of\ngeometrically meaningful evolution PDEs as substitutes for the conventional\ncomponents in G-CNNs. PDE-G-CNNs offer several key benefits all at once: fewer\nparameters, inherent equivariance, better performance, data efficiency, and\ngeometric interpretability.\n  In this article we focus on Euclidean equivariant PDE-G-CNNs where the\nfeature maps are two dimensional throughout. We call this variant of the\nframework a PDE-CNN.\n  From a machine learning perspective, we list several practically desirable\naxioms and derive from these which PDEs should be used in a PDE-CNN. Here our\napproach to geometric learning via PDEs is inspired by the axioms of classical\nlinear and morphological scale-space theory, which we generalize by introducing\nsemifield-valued signals.\n  Furthermore, we experimentally confirm for small networks that PDE-CNNs offer\nfewer parameters, increased performance, and better data efficiency when\ncompared to CNNs. We also investigate what effect the use of different\nsemifields has on the performance of the models.\n","authors":["Gijs Bellaard","Sei Sakata","Bart M. N. Smets","Remco Duits"],"pdf_url":"https://arxiv.org/pdf/2403.15182v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2404.12330v1","updated":"2024-04-18T16:58:05Z","published":"2024-04-18T16:58:05Z","title":"A Perspective on Deep Vision Performance with Standard Image and Video\n  Codecs","summary":"  Resource-constrained hardware, such as edge devices or cell phones, often\nrely on cloud servers to provide the required computational resources for\ninference in deep vision models. However, transferring image and video data\nfrom an edge or mobile device to a cloud server requires coding to deal with\nnetwork constraints. The use of standardized codecs, such as JPEG or H.264, is\nprevalent and required to ensure interoperability. This paper aims to examine\nthe implications of employing standardized codecs within deep vision pipelines.\nWe find that using JPEG and H.264 coding significantly deteriorates the\naccuracy across a broad range of vision tasks and models. For instance, strong\ncompression rates reduce semantic segmentation accuracy by more than 80% in\nmIoU. In contrast to previous findings, our analysis extends beyond image and\naction classification to localization and dense prediction tasks, thus\nproviding a more comprehensive perspective.\n","authors":["Christoph Reich","Oliver Hahn","Daniel Cremers","Stefan Roth","Biplob Debnath"],"pdf_url":"https://arxiv.org/pdf/2404.12330v1.pdf","comment":"Accepted at CVPR 2024 Workshop on AI for Streaming (AIS)"},{"id":"http://arxiv.org/abs/2310.08475v5","updated":"2024-04-18T15:46:22Z","published":"2023-10-12T16:32:44Z","title":"Can We Edit Multimodal Large Language Models?","summary":"  In this paper, we focus on editing Multimodal Large Language Models (MLLMs).\nCompared to editing single-modal LLMs, multimodal model editing is more\nchallenging, which demands a higher level of scrutiny and careful consideration\nin the editing process. To facilitate research in this area, we construct a new\nbenchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite\nof innovative metrics for evaluation. We conduct comprehensive experiments\ninvolving various model editing baselines and analyze the impact of editing\ndifferent components for multimodal LLMs. Empirically, we notice that previous\nbaselines can implement editing multimodal LLMs to some extent, but the effect\nis still barely satisfactory, indicating the potential difficulty of this task.\nWe hope that our work can provide the NLP community with insights. Code and\ndataset are available in https://github.com/zjunlp/EasyEdit.\n","authors":["Siyuan Cheng","Bozhong Tian","Qingbin Liu","Xi Chen","Yongheng Wang","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.08475v5.pdf","comment":"EMNLP 2023. Add the Exact Match/Accuracy results of Reliability and\n  T-Generality"},{"id":"http://arxiv.org/abs/2404.12257v1","updated":"2024-04-18T15:23:37Z","published":"2024-04-18T15:23:37Z","title":"Food Portion Estimation via 3D Object Scaling","summary":"  Image-based methods to analyze food images have alleviated the user burden\nand biases associated with traditional methods. However, accurate portion\nestimation remains a major challenge due to the loss of 3D information in the\n2D representation of foods captured by smartphone cameras or wearable devices.\nIn this paper, we propose a new framework to estimate both food volume and\nenergy from 2D images by leveraging the power of 3D food models and physical\nreference in the eating scene. Our method estimates the pose of the camera and\nthe food object in the input image and recreates the eating occasion by\nrendering an image of a 3D model of the food with the estimated poses. We also\nintroduce a new dataset, SimpleFood45, which contains 2D images of 45 food\nitems and associated annotations including food volume, weight, and energy. Our\nmethod achieves an average error of 31.10 kCal (17.67%) on this dataset,\noutperforming existing portion estimation methods.\n","authors":["Gautham Vinod","Jiangpeng He","Zeman Shao","Fengqing Zhu"],"pdf_url":"https://arxiv.org/pdf/2404.12257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12169v1","updated":"2024-04-18T13:23:05Z","published":"2024-04-18T13:23:05Z","title":"Shotit: compute-efficient image-to-video search engine for the cloud","summary":"  With the rapid growth of information technology, users are exposed to a\nmassive amount of data online, including image, music, and video. This has led\nto strong needs to provide effective corresponsive search services such as\nimage, music, and video search services. Most of them are operated based on\nkeywords, namely using keywords to find related image, music, and video.\nAdditionally, there are image-to-image search services that enable users to\nfind similar images using one input image. Given that videos are essentially\ncomposed of image frames, then similar videos can be searched by one input\nimage or screenshot. We want to target this scenario and provide an efficient\nmethod and implementation in this paper.\n  We present Shotit, a cloud-native image-to-video search engine that tailors\nthis search scenario in a compute-efficient approach. One main limitation faced\nin this scenario is the scale of its dataset. A typical image-to-image search\nengine only handles one-to-one relationships, colloquially, one image\ncorresponds to another single image. But image-to-video proliferates. Take a\n24-min length video as an example, it will generate roughly 20,000 image\nframes. As the number of videos grows, the scale of the dataset explodes\nexponentially. In this case, a compute-efficient approach ought to be\nconsidered, and the system design should cater to the cloud-native trend.\nChoosing an emerging technology - vector database as its backbone, Shotit fits\nthese two metrics performantly. Experiments for two different datasets, a 50\nthousand-scale Blender Open Movie dataset, and a 50 million-scale proprietary\nTV genre dataset at a 4 Core 32GB RAM Intel Xeon Gold 6271C cloud machine with\nobject storage reveal the effectiveness of Shotit. A demo regarding the Blender\nOpen Movie dataset is illustrated within this paper.\n","authors":["Leslie Wong"],"pdf_url":"https://arxiv.org/pdf/2404.12169v1.pdf","comment":"Submitted to ACM ICMR 2024"},{"id":"http://arxiv.org/abs/2404.05317v4","updated":"2024-04-18T08:29:48Z","published":"2024-04-08T09:08:43Z","title":"WebXR, A-Frame and Networked-Aframe as a Basis for an Open Metaverse: A\n  Conceptual Architecture","summary":"  This work proposes a WebXR-based cross-platform conceptual architecture,\nleveraging the A-Frame and Networked-Aframe frameworks, in order to facilitate\nthe development of an open, accessible, and interoperable metaverse. By\nintroducing the concept of spatial web app, this research contributes to the\ndiscourse on the metaverse, offering an architecture that democratizes access\nto virtual environments and extended reality through the web, and aligns with\nTim Berners-Lee's original vision of the World Wide Web as an open platform in\nthe digital realm.\n","authors":["Giuseppe Macario"],"pdf_url":"https://arxiv.org/pdf/2404.05317v4.pdf","comment":"minor fixes/rephrasing"},{"id":"http://arxiv.org/abs/2402.06244v2","updated":"2024-04-18T08:08:45Z","published":"2024-02-09T08:33:48Z","title":"Quantifying and Enhancing Multi-modal Robustness with Modality\n  Preference","summary":"  Multi-modal models have shown a promising capability to effectively integrate\ninformation from various sources, yet meanwhile, they are found vulnerable to\npervasive perturbations, such as uni-modal attacks and missing conditions. To\ncounter these perturbations, robust multi-modal representations are highly\nexpected, which are positioned well away from the discriminative multi-modal\ndecision boundary. In this paper, different from conventional empirical\nstudies, we focus on a commonly used joint multi-modal framework and\ntheoretically discover that larger uni-modal representation margins and more\nreliable integration for modalities are essential components for achieving\nhigher robustness. This discovery can further explain the limitation of\nmulti-modal robustness and the phenomenon that multi-modal models are often\nvulnerable to attacks on the specific modality. Moreover, our analysis reveals\nhow the widespread issue, that the model has different preferences for\nmodalities, limits the multi-modal robustness by influencing the essential\ncomponents and could lead to attacks on the specific modality highly effective.\nInspired by our theoretical finding, we introduce a training procedure called\nCertifiable Robust Multi-modal Training (CRMT), which can alleviate this\ninfluence from modality preference and explicitly regulate essential components\nto significantly improve robustness in a certifiable manner. Our method\ndemonstrates substantial improvements in performance and robustness compared\nwith existing methods. Furthermore, our training procedure can be easily\nextended to enhance other robust training strategies, highlighting its\ncredibility and flexibility.\n","authors":["Zequn Yang","Yake Wei","Ce Liang","Di Hu"],"pdf_url":"https://arxiv.org/pdf/2402.06244v2.pdf","comment":"Accepted to ICLR 2024"},{"id":"http://arxiv.org/abs/2404.11938v1","updated":"2024-04-18T06:38:02Z","published":"2024-04-18T06:38:02Z","title":"HyDiscGAN: A Hybrid Distributed cGAN for Audio-Visual Privacy\n  Preservation in Multimodal Sentiment Analysis","summary":"  Multimodal Sentiment Analysis (MSA) aims to identify speakers' sentiment\ntendencies in multimodal video content, raising serious concerns about privacy\nrisks associated with multimodal data, such as voiceprints and facial images.\nRecent distributed collaborative learning has been verified as an effective\nparadigm for privacy preservation in multimodal tasks. However, they often\noverlook the privacy distinctions among different modalities, struggling to\nstrike a balance between performance and privacy preservation. Consequently, it\nposes an intriguing question of maximizing multimodal utilization to improve\nperformance while simultaneously protecting necessary modalities. This paper\nforms the first attempt at modality-specified (i.e., audio and visual) privacy\npreservation in MSA tasks. We propose a novel Hybrid Distributed cross-modality\ncGAN framework (HyDiscGAN), which learns multimodality alignment to generate\nfake audio and visual features conditioned on shareable de-identified textual\ndata. The objective is to leverage the fake features to approximate real audio\nand visual content to guarantee privacy preservation while effectively\nenhancing performance. Extensive experiments show that compared with the\nstate-of-the-art MSA model, HyDiscGAN can achieve superior or competitive\nperformance while preserving privacy.\n","authors":["Zhuojia Wu","Qi Zhang","Duoqian Miao","Kun Yi","Wei Fan","Liang Hu"],"pdf_url":"https://arxiv.org/pdf/2404.11938v1.pdf","comment":"13 pages, IJCAI-2024"}]}}